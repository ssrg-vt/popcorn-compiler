diff --git a/clang/include/clang/AST/ASTContext.h b/clang/include/clang/AST/ASTContext.h
index a2bd55a089e..464c67b7afb 100644
--- a/clang/include/clang/AST/ASTContext.h
+++ b/clang/include/clang/AST/ASTContext.h
@@ -32,6 +32,7 @@
 #include "clang/Basic/PartialDiagnostic.h"
 #include "clang/Basic/SanitizerBlacklist.h"
 #include "clang/Basic/VersionTuple.h"
+#include "clang/Sema/PrefetchAnalysis.h"
 #include "llvm/ADT/DenseMap.h"
 #include "llvm/ADT/FoldingSet.h"
 #include "llvm/ADT/IntrusiveRefCntPtr.h"
@@ -295,6 +296,9 @@ class ASTContext : public RefCountedBase<ASTContext> {
   /// definitions of that entity.
   llvm::DenseMap<NamedDecl*, llvm::TinyPtrVector<Module*>> MergedDefModules;
 
+  /// \brief Analysis on statements for which prefetching has been enabled.
+  mutable llvm::DenseMap<const Stmt *, PrefetchAnalysis> PrefetchAnalyses;
+
 public:
   /// \brief A type synonym for the TemplateOrInstantiation mapping.
   typedef llvm::PointerUnion<VarTemplateDecl *, MemberSpecializationInfo *>
@@ -898,6 +902,20 @@ public:
   /// \brief Retrieve the declaration for a 128-bit float stub type.
   TypeDecl *getFloat128StubType() const;
 
+  /// \brief Add a new prefetching analysis.  Note that analysis should be done
+  /// before adding it to the context.
+  void addPrefetchAnalysis(const Stmt *S, PrefetchAnalysis &PA)
+  { PrefetchAnalyses[S] = PA; }
+
+  /// \brief Retrieve prefetching analysis for a statement if it exists, or
+  /// return a nullptr otherwise.
+  const PrefetchAnalysis *getPrefetchAnalysis(const Stmt *S) const {
+    llvm::DenseMap<const Stmt *, PrefetchAnalysis>::iterator it;
+    it = PrefetchAnalyses.find(S);
+    if(it != PrefetchAnalyses.end()) return &it->second;
+    else return nullptr;
+  }
+
   //===--------------------------------------------------------------------===//
   //                           Type Constructors
   //===--------------------------------------------------------------------===//
diff --git a/clang/include/clang/AST/DataRecursiveASTVisitor.h b/clang/include/clang/AST/DataRecursiveASTVisitor.h
index dd167fe27c0..0ccb9dc9d7b 100644
--- a/clang/include/clang/AST/DataRecursiveASTVisitor.h
+++ b/clang/include/clang/AST/DataRecursiveASTVisitor.h
@@ -2663,6 +2663,14 @@ bool RecursiveASTVisitor<Derived>::VisitOMPDependClause(OMPDependClause *C) {
   return true;
 }
 
+template <typename Derived>
+bool RecursiveASTVisitor<Derived>::VisitOMPPrefetchClause(OMPPrefetchClause *C) {
+  TRY_TO(VisitOMPClauseList(C));
+  TRY_TO(TraverseStmt(C->getStartOfRange()));
+  TRY_TO(TraverseStmt(C->getEndOfRange()));
+  return true;
+}
+
 // FIXME: look at the following tricky-seeming exprs to see if we
 // need to recurse on anything.  These are ones that have methods
 // returning decls or qualtypes or nestednamespecifier -- though I'm
diff --git a/clang/include/clang/AST/OpenMPClause.h b/clang/include/clang/AST/OpenMPClause.h
index fcfa1dd4753..3d913bff757 100644
--- a/clang/include/clang/AST/OpenMPClause.h
+++ b/clang/include/clang/AST/OpenMPClause.h
@@ -2300,6 +2300,59 @@ public:
   }
 };
 
+/// \brief This represents a memory prefetch request for Popcorn Linux.  This
+/// should only be used for prefetching contiguous blocks of memory, e.g.,
+/// arrays or pointers to chunks of memory.
+class OMPPrefetchClause : public OMPVarListClause<OMPPrefetchClause> {
+private:
+  /// \brief What type of prefetching to perform.
+  OpenMPPrefetchClauseKind Kind;
+
+  /// \brief Expressions describing the memory range to be prefetched.
+  ///
+  /// 1. If both are nullptr, then the entire array should be prefetched
+  /// 2. If Start is valid and End is nullptr, then use the expression (which
+  ///    should be affine to a for-loop iteration variable) to prefetch the
+  ///    region of memory based on the loop iterations assigned to the thread
+  /// 3. If both Start and End are valid, then prefetch the absolute range
+  ///    denoted by the starting & ending expressions
+  Expr *Start, *End;
+
+  /// \brief Locations of the kind specifier, colons used to separate the
+  /// variable list and range expression(s).  These are valid in conjunction
+  /// with Start & End, respectively.
+  SourceLocation KindLoc, FirstColonLoc, SecondColonLoc;
+
+  OMPPrefetchClause(SourceLocation StartLoc, SourceLocation LParenLoc,
+                    SourceLocation FirstColonLoc,
+                    SourceLocation SecondColonLoc, SourceLocation EndLoc,
+                    unsigned N)
+      : OMPVarListClause<OMPPrefetchClause>(OMPC_prefetch, StartLoc, LParenLoc,
+                                            EndLoc, N),
+        FirstColonLoc(FirstColonLoc), SecondColonLoc(SecondColonLoc) {}
+public:
+  static OMPPrefetchClause *
+  Create(const ASTContext &C, OpenMPPrefetchClauseKind Kind,
+         SourceLocation KindLoc, ArrayRef<Expr *> VL, Expr *Start, Expr *End,
+         SourceLocation StartLoc, SourceLocation LParenLoc,
+         SourceLocation FirstColonLoc, SourceLocation SecondColonLoc,
+         SourceLocation EndLoc);
+
+  OpenMPPrefetchClauseKind getPrefetchKind() const { return Kind; }
+  void setPrefetchKind(OpenMPPrefetchClauseKind Kind) { this->Kind = Kind; }
+
+  Expr *getStartOfRange() const { return Start; }
+  void setStartOfRange(Expr *Start) { this->Start = Start; }
+  Expr *getEndOfRange() const { return End; }
+  void setEndOfRange(Expr *End) { this->End = End; }
+
+  SourceLocation getPrefetchKindLoc() const { return KindLoc; }
+  void setPrefetchKindLoc(SourceLocation Loc) { KindLoc = Loc; }
+  SourceLocation getFirstColonLoc() const { return FirstColonLoc; }
+  SourceLocation getSecondColonLoc() const { return SecondColonLoc; }
+
+};
+
 } // end namespace clang
 
 #endif
diff --git a/clang/include/clang/AST/Prefetch.h b/clang/include/clang/AST/Prefetch.h
new file mode 100644
index 00000000000..1fe4127c15c
--- /dev/null
+++ b/clang/include/clang/AST/Prefetch.h
@@ -0,0 +1,38 @@
+//===--- Prefetch.h - Prefetching classes for Popcorn Linux -----*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file defines classes for enabling prefetching analysis & instrumentation
+// on Popcorn Linux.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_CLANG_AST_PREFETCH_H
+#define LLVM_CLANG_AST_PREFETCH_H
+
+namespace clang {
+
+/// Statements for which prefetching analysis & instrumentation can be performed
+/// should inherit this class.
+class Prefetchable {
+protected:
+  /// \brief Whether prefetching has been enabled for the statement.
+  bool PrefetchEnabled;
+
+public:
+  Prefetchable(bool PrefetchEnabled = false)
+    : PrefetchEnabled(PrefetchEnabled) {}
+
+  void setPrefetchEnabled(bool Enable) { PrefetchEnabled = Enable; }
+  bool prefetchEnabled() const { return PrefetchEnabled; }
+};
+
+} // end namespace clang
+
+#endif
+
diff --git a/clang/include/clang/AST/RecursiveASTVisitor.h b/clang/include/clang/AST/RecursiveASTVisitor.h
index 1017656b662..f296a570fce 100644
--- a/clang/include/clang/AST/RecursiveASTVisitor.h
+++ b/clang/include/clang/AST/RecursiveASTVisitor.h
@@ -2695,6 +2695,14 @@ bool RecursiveASTVisitor<Derived>::VisitOMPDependClause(OMPDependClause *C) {
   return true;
 }
 
+template <typename Derived>
+bool RecursiveASTVisitor<Derived>::VisitOMPPrefetchClause(OMPPrefetchClause *C) {
+  TRY_TO(VisitOMPClauseList(C));
+  TRY_TO(TraverseStmt(C->getStartOfRange()));
+  TRY_TO(TraverseStmt(C->getEndOfRange()));
+  return true;
+}
+
 // FIXME: look at the following tricky-seeming exprs to see if we
 // need to recurse on anything.  These are ones that have methods
 // returning decls or qualtypes or nestednamespecifier -- though I'm
diff --git a/clang/include/clang/AST/Stmt.h b/clang/include/clang/AST/Stmt.h
index ce9449dc46f..14112d930cd 100644
--- a/clang/include/clang/AST/Stmt.h
+++ b/clang/include/clang/AST/Stmt.h
@@ -16,6 +16,7 @@
 
 #include "clang/AST/DeclGroup.h"
 #include "clang/AST/StmtIterator.h"
+#include "clang/AST/Prefetch.h"
 #include "clang/Basic/CapturedStmt.h"
 #include "clang/Basic/IdentifierTable.h"
 #include "clang/Basic/LLVM.h"
@@ -1141,7 +1142,7 @@ public:
 /// the init/cond/inc parts of the ForStmt will be null if they were not
 /// specified in the source.
 ///
-class ForStmt : public Stmt {
+class ForStmt : public Stmt, public Prefetchable {
   SourceLocation ForLoc;
   enum { INIT, CONDVAR, COND, INC, BODY, END_EXPR };
   Stmt* SubExprs[END_EXPR]; // SubExprs[INIT] is an expression or declstmt.
@@ -2069,6 +2070,12 @@ private:
   /// \brief The record for captured variables, a RecordDecl or CXXRecordDecl.
   RecordDecl *TheRecordDecl;
 
+  /// \brief For captured OpenMP parallel regions, variables declared in the
+  /// shared clause may be stored on the main thread's stack.  This causes
+  /// false sharing in Popcorn's distributed execution.  If set, offload shared
+  /// variables to global memory for the capture.
+  bool OffloadShared;
+
   /// \brief Construct a captured statement.
   CapturedStmt(Stmt *S, CapturedRegionKind Kind, ArrayRef<Capture> Captures,
                ArrayRef<Expr *> CaptureInits, CapturedDecl *CD, RecordDecl *RD);
@@ -2134,6 +2141,10 @@ public:
   /// \brief True if this variable has been captured.
   bool capturesVariable(const VarDecl *Var) const;
 
+  /// \brief Getters/setters for the offloading shared variables flag
+  bool offloadShared() const { return OffloadShared; }
+  void setOffloadShared(bool OS) { OffloadShared = OS; }
+
   /// \brief An iterator that walks over the captures.
   typedef Capture *capture_iterator;
   typedef const Capture *const_capture_iterator;
diff --git a/clang/include/clang/AST/StmtOpenMP.h b/clang/include/clang/AST/StmtOpenMP.h
index 708b8667335..1588b1a7303 100644
--- a/clang/include/clang/AST/StmtOpenMP.h
+++ b/clang/include/clang/AST/StmtOpenMP.h
@@ -42,6 +42,8 @@ class OMPExecutableDirective : public Stmt {
   const unsigned NumClauses;
   /// \brief Number of child expressions/stmts.
   const unsigned NumChildren;
+  /// \brief Enable prefetching code generation for Popcorn Linux
+  bool Prefetch;
   /// \brief Offset from this to the start of clauses.
   /// There are NumClauses pointers to clauses, they are followed by
   /// NumChildren pointers to child stmts/exprs (if the directive type
@@ -69,7 +71,7 @@ protected:
                          unsigned NumClauses, unsigned NumChildren)
       : Stmt(SC), Kind(K), StartLoc(std::move(StartLoc)),
         EndLoc(std::move(EndLoc)), NumClauses(NumClauses),
-        NumChildren(NumChildren),
+        NumChildren(NumChildren), Prefetch(false),
         ClausesOffset(llvm::RoundUpToAlignment(sizeof(T),
                                                llvm::alignOf<OMPClause *>())) {}
 
@@ -205,6 +207,9 @@ public:
   ArrayRef<OMPClause *> clauses() const {
     return const_cast<OMPExecutableDirective *>(this)->getClauses();
   }
+
+  bool prefetchingEnabled() const { return Prefetch; }
+  void setPrefetching(bool Prefetch) { this->Prefetch = Prefetch; }
 };
 
 /// \brief This represents '#pragma omp parallel' directive.
diff --git a/clang/include/clang/Basic/DiagnosticParseKinds.td b/clang/include/clang/Basic/DiagnosticParseKinds.td
index e4f85994326..baac9df533d 100644
--- a/clang/include/clang/Basic/DiagnosticParseKinds.td
+++ b/clang/include/clang/Basic/DiagnosticParseKinds.td
@@ -993,6 +993,14 @@ def err_omp_immediate_directive : Error<
   "'#pragma omp %0' cannot be an immediate substatement">;
 def err_omp_expected_identifier_for_critical : Error<
   "expected identifier specifying the name of the 'omp critical' directive">;
+def err_omp_invalid_prefetch_kind : Error<
+  "invalid argument; expected 'read', 'write' or 'smart'">;
+// TODO Technically these are semantics issues, but we're doing checking in the
+// parser for this particular clause
+def err_omp_invalid_prefetch_capture : Error<
+  "can only prefetch variables used in the loop body">;
+def err_omp_invalid_prefetch_loop_var : Error<
+  "can only prefetch loop iteration range using a loop iteration variable">;
 
 // Pragma loop support.
 def err_pragma_loop_missing_argument : Error<
@@ -1008,6 +1016,22 @@ def err_pragma_invalid_keyword : Error<
 def warn_pragma_unroll_cuda_value_in_parens : Warning<
   "argument to '#pragma unroll' should not be in parentheses in CUDA C/C++">,
   InGroup<CudaCompat>;
+
+// Pragma popcorn support.
+def warn_pragma_popcorn_ignored : Warning<
+  "Popcorn: unexpected '#pragma popcorn...' in program">, DefaultIgnore;
+def warn_pragma_popcorn_no_arg : Warning<
+  "Popcorn: missing argument; expected 'prefetch'">;
+def warn_pragma_popcorn_invalid_option : Warning<
+  "Popcorn: invalid pragma argument '%0'; expected 'prefetch'">;
+def err_pragma_popcorn_invalid_clause : Error<
+  "Popcorn: invalid clause '%0'; expected 'ignore'">;
+def err_pragma_popcorn_expected_var_name : Error<
+  "Popcorn: expected variable name">;
+
+// Pragma popcorn prefetch support.
+def warn_pragma_popcorn_prefetch_invalid_stmt : Warning<
+  "Popcorn: cannot prefetch for this statement">;
 } // end of Parse Issue category.
 
 let CategoryName = "Modules Issue" in {
diff --git a/clang/include/clang/Basic/DiagnosticSemaKinds.td b/clang/include/clang/Basic/DiagnosticSemaKinds.td
index 82f51213088..f572b1b26c8 100644
--- a/clang/include/clang/Basic/DiagnosticSemaKinds.td
+++ b/clang/include/clang/Basic/DiagnosticSemaKinds.td
@@ -7649,6 +7649,10 @@ def err_omp_parent_cancel_region_nowait : Error<
   "parent region for 'omp %select{cancellation point/cancel}0' construct cannot be nowait">;
 def err_omp_parent_cancel_region_ordered : Error<
   "parent region for 'omp %select{cancellation point/cancel}0' construct cannot be ordered">;
+def err_omp_invalid_prefetch_var_type : Error<
+  "invalid variable type; must be %0 type">;
+def err_omp_invalid_prefetch_range_type : Error<
+  "invalid range specifier type; must be of integer (signed or unsigned) type">;
 } // end of OpenMP category
 
 let CategoryName = "Related Result Type Issue" in {
diff --git a/clang/include/clang/Basic/LangOptions.def b/clang/include/clang/Basic/LangOptions.def
index c184df77c37..02cc5d798df 100644
--- a/clang/include/clang/Basic/LangOptions.def
+++ b/clang/include/clang/Basic/LangOptions.def
@@ -230,6 +230,9 @@ LANGOPT(SanitizeAddressFieldPadding, 2, 0, "controls how aggressive is ASan "
                                            "field padding (0: none, 1:least "
                                            "aggressive, 2: more aggressive)")
 
+// Optimize OpenMP code generation for distributed execution on Popcorn Linux
+BENIGN_LANGOPT(DistributedOmp, 1, 0, "Optimize OpenMP for distributed execution")
+
 #undef LANGOPT
 #undef COMPATIBLE_LANGOPT
 #undef BENIGN_LANGOPT
diff --git a/clang/include/clang/Basic/OpenMPKinds.def b/clang/include/clang/Basic/OpenMPKinds.def
index 67a5068cc2c..ea558d47408 100644
--- a/clang/include/clang/Basic/OpenMPKinds.def
+++ b/clang/include/clang/Basic/OpenMPKinds.def
@@ -72,6 +72,9 @@
 #ifndef OPENMP_DEPEND_KIND
 #define OPENMP_DEPEND_KIND(Name)
 #endif
+#ifndef OPENMP_PREFETCH_KIND
+#define OPENMP_PREFETCH_KIND(Name)
+#endif
 
 // OpenMP directives.
 OPENMP_DIRECTIVE(threadprivate)
@@ -129,6 +132,7 @@ OPENMP_CLAUSE(update, OMPUpdateClause)
 OPENMP_CLAUSE(capture, OMPCaptureClause)
 OPENMP_CLAUSE(seq_cst, OMPSeqCstClause)
 OPENMP_CLAUSE(depend, OMPDependClause)
+OPENMP_CLAUSE(prefetch, OMPPrefetchClause)
 
 // Clauses allowed for OpenMP directive 'parallel'.
 OPENMP_PARALLEL_CLAUSE(if)
@@ -159,6 +163,7 @@ OPENMP_FOR_CLAUSE(collapse)
 OPENMP_FOR_CLAUSE(schedule)
 OPENMP_FOR_CLAUSE(ordered)
 OPENMP_FOR_CLAUSE(nowait)
+OPENMP_FOR_CLAUSE(prefetch)
 
 // Clauses allowed for directive 'omp for simd'.
 OPENMP_FOR_SIMD_CLAUSE(private)
@@ -171,6 +176,7 @@ OPENMP_FOR_SIMD_CLAUSE(nowait)
 OPENMP_FOR_SIMD_CLAUSE(safelen)
 OPENMP_FOR_SIMD_CLAUSE(linear)
 OPENMP_FOR_SIMD_CLAUSE(aligned)
+OPENMP_FOR_SIMD_CLAUSE(prefetch)
 
 // Clauses allowed for OpenMP directive 'omp sections'.
 OPENMP_SECTIONS_CLAUSE(private)
@@ -200,12 +206,18 @@ OPENMP_SCHEDULE_KIND(dynamic)
 OPENMP_SCHEDULE_KIND(guided)
 OPENMP_SCHEDULE_KIND(auto)
 OPENMP_SCHEDULE_KIND(runtime)
+OPENMP_SCHEDULE_KIND(hetprobe)
 
 // Static attributes for 'depend' clause.
 OPENMP_DEPEND_KIND(in)
 OPENMP_DEPEND_KIND(out)
 OPENMP_DEPEND_KIND(inout)
 
+// Static attributes for 'prefetch' clause.
+OPENMP_PREFETCH_KIND(read)
+OPENMP_PREFETCH_KIND(write)
+OPENMP_PREFETCH_KIND(smart)
+
 // Clauses allowed for OpenMP directive 'parallel for'.
 OPENMP_PARALLEL_FOR_CLAUSE(if)
 OPENMP_PARALLEL_FOR_CLAUSE(num_threads)
@@ -220,6 +232,7 @@ OPENMP_PARALLEL_FOR_CLAUSE(lastprivate)
 OPENMP_PARALLEL_FOR_CLAUSE(collapse)
 OPENMP_PARALLEL_FOR_CLAUSE(schedule)
 OPENMP_PARALLEL_FOR_CLAUSE(ordered)
+OPENMP_PARALLEL_FOR_CLAUSE(prefetch)
 
 // Clauses allowed for OpenMP directive 'parallel for simd'.
 OPENMP_PARALLEL_FOR_SIMD_CLAUSE(if)
@@ -237,6 +250,7 @@ OPENMP_PARALLEL_FOR_SIMD_CLAUSE(schedule)
 OPENMP_PARALLEL_FOR_SIMD_CLAUSE(safelen)
 OPENMP_PARALLEL_FOR_SIMD_CLAUSE(linear)
 OPENMP_PARALLEL_FOR_SIMD_CLAUSE(aligned)
+OPENMP_PARALLEL_FOR_SIMD_CLAUSE(prefetch)
 
 // Clauses allowed for OpenMP directive 'parallel sections'.
 OPENMP_PARALLEL_SECTIONS_CLAUSE(if)
@@ -280,6 +294,7 @@ OPENMP_TEAMS_CLAUSE(firstprivate)
 OPENMP_TEAMS_CLAUSE(shared)
 OPENMP_TEAMS_CLAUSE(reduction)
 
+#undef OPENMP_PREFETCH_KIND
 #undef OPENMP_DEPEND_KIND
 #undef OPENMP_SCHEDULE_KIND
 #undef OPENMP_PROC_BIND_KIND
diff --git a/clang/include/clang/Basic/OpenMPKinds.h b/clang/include/clang/Basic/OpenMPKinds.h
index 83939bb079a..2bda89e2c41 100644
--- a/clang/include/clang/Basic/OpenMPKinds.h
+++ b/clang/include/clang/Basic/OpenMPKinds.h
@@ -70,6 +70,14 @@ enum OpenMPDependClauseKind {
   OMPC_DEPEND_unknown
 };
 
+/// \brief OpenMP attributes for 'prefetch' clause.
+enum OpenMPPrefetchClauseKind {
+#define OPENMP_PREFETCH_KIND(Name) \
+  OMPC_PREFETCH_##Name,
+#include "clang/Basic/OpenMPKinds.def"
+  OMPC_PREFETCH_unknown
+};
+
 OpenMPDirectiveKind getOpenMPDirectiveKind(llvm::StringRef Str);
 const char *getOpenMPDirectiveName(OpenMPDirectiveKind Kind);
 
diff --git a/clang/include/clang/Basic/TokenKinds.def b/clang/include/clang/Basic/TokenKinds.def
index 8333a4ccf8d..36b847a05ea 100644
--- a/clang/include/clang/Basic/TokenKinds.def
+++ b/clang/include/clang/Basic/TokenKinds.def
@@ -752,6 +752,12 @@ ANNOTATION(module_include)
 ANNOTATION(module_begin)
 ANNOTATION(module_end)
 
+// Annotations for Popcorn Linux - #pragma popcorn ...
+// The lexer produces these so that they only take effect when the parser
+// handles #pragma popcorn ... directives.
+ANNOTATION(pragma_popcorn_prefetch)
+ANNOTATION(pragma_popcorn_prefetch_end)
+
 #undef ANNOTATION
 #undef TESTING_KEYWORD
 #undef OBJC2_AT_KEYWORD
diff --git a/clang/include/clang/CodeGen/BackendUtil.h b/clang/include/clang/CodeGen/BackendUtil.h
index 8586e778894..a532ec45542 100644
--- a/clang/include/clang/CodeGen/BackendUtil.h
+++ b/clang/include/clang/CodeGen/BackendUtil.h
@@ -28,13 +28,32 @@ namespace clang {
     Backend_EmitLL,        ///< Emit human-readable LLVM assembly
     Backend_EmitNothing,   ///< Don't emit anything (benchmarking mode)
     Backend_EmitMCNull,    ///< Run CodeGen, but don't emit anything
-    Backend_EmitObj        ///< Emit native object files
+    Backend_EmitObj,       ///< Emit native object files
+    Backend_EmitMultiObj   ///< Emit native object files for multiple ISAs
   };
 
+  /// Run both IR optimization passes and backend passes to generate code
   void EmitBackendOutput(DiagnosticsEngine &Diags, const CodeGenOptions &CGOpts,
                          const TargetOptions &TOpts, const LangOptions &LOpts,
                          StringRef TDesc, llvm::Module *M, BackendAction Action,
                          raw_pwrite_stream *OS);
+
+  /// Run IR optimization passes
+  void ApplyIROptimizations(DiagnosticsEngine &Diags,
+                            const CodeGenOptions &CGOpts,
+                            const TargetOptions &TOpts,
+                            const LangOptions &LOpts,
+                            llvm::Module *M, BackendAction Action,
+                            raw_pwrite_stream *OS);
+
+  /// Run backend code-generation passes
+  void CodegenBackendOutput(DiagnosticsEngine &Diags,
+                            const CodeGenOptions &CGOpts,
+                            const TargetOptions &TOpts,
+                            const LangOptions &LOpts,
+                            StringRef TDesc, llvm::Module *M,
+                            BackendAction Action,
+                            raw_pwrite_stream *OS);
 }
 
 #endif
diff --git a/clang/include/clang/CodeGen/CodeGenAction.h b/clang/include/clang/CodeGen/CodeGenAction.h
index 264780d01ca..5e4b2554255 100644
--- a/clang/include/clang/CodeGen/CodeGenAction.h
+++ b/clang/include/clang/CodeGen/CodeGenAction.h
@@ -11,6 +11,7 @@
 #define LLVM_CLANG_CODEGEN_CODEGENACTION_H
 
 #include "clang/Frontend/FrontendAction.h"
+#include "clang/CodeGen/BackendUtil.h"
 #include <memory>
 
 namespace llvm {
@@ -20,16 +21,16 @@ namespace llvm {
 
 namespace clang {
 class BackendConsumer;
+class CoverageSourceInfo;
 
 class CodeGenAction : public ASTFrontendAction {
-private:
+protected:
   unsigned Act;
   std::unique_ptr<llvm::Module> TheModule;
   llvm::Module *LinkModule;
   llvm::LLVMContext *VMContext;
   bool OwnsVMContext;
 
-protected:
   /// Create a new code generation action.  If the optional \p _VMContext
   /// parameter is supplied, the action uses it without taking ownership,
   /// otherwise it creates a fresh LLVM context and takes ownership.
@@ -37,6 +38,14 @@ protected:
 
   bool hasIRSupport() const override;
 
+  /// Helpers called in CreateASTConsumer
+  llvm::Module *getLinkModuleToUse(CompilerInstance &CI);
+  CoverageSourceInfo *getCoverageInfo(CompilerInstance &CI);
+
+  /// Helper called in ExecuteAction.  Returns true if the compilation is
+  /// invalid and should therefore be aborted.
+  bool ExecuteActionIRCommon(BackendAction &BA, CompilerInstance &CI);
+
   std::unique_ptr<ASTConsumer> CreateASTConsumer(CompilerInstance &CI,
                                                  StringRef InFile) override;
 
@@ -98,6 +107,23 @@ public:
   EmitObjAction(llvm::LLVMContext *_VMContext = nullptr);
 };
 
+/// Emit multiple object files using a single set of IR.  Used by the Popcorn
+/// Linux compiler toolchain.
+class EmitMultiObjAction : public CodeGenAction {
+  virtual void anchor();
+  SmallVector<std::string, 2> Targets;
+  SmallVector<raw_pwrite_stream *, 2> OutFiles;
+  SmallVector<std::shared_ptr<TargetOptions>, 2> TargetOpts;
+  SmallVector<TargetInfo *, 2> TargetInfos;
+protected:
+  bool InitializeTargets(CompilerInstance &CI, StringRef InFile);
+  std::unique_ptr<ASTConsumer> CreateASTConsumer(CompilerInstance &CI,
+                                                 StringRef InFile) override;
+  void ExecuteAction() override;
+public:
+  EmitMultiObjAction(llvm::LLVMContext *_VMContext = nullptr);
+};
+
 }
 
 #endif
diff --git a/clang/include/clang/CodeGen/PopcornUtil.h b/clang/include/clang/CodeGen/PopcornUtil.h
new file mode 100644
index 00000000000..b1a64b3e58b
--- /dev/null
+++ b/clang/include/clang/CodeGen/PopcornUtil.h
@@ -0,0 +1,47 @@
+//===--- PopcornUtil.h - Popcorn Linux Utilities ----------------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_CLANG_CODEGEN_POPCORNUTIL_H
+#define LLVM_CLANG_CODEGEN_POPCORNUTIL_H
+
+#include <llvm/ADT/StringRef.h>
+#include <llvm/IR/Module.h>
+#include <clang/Basic/TargetOptions.h>
+#include <memory>
+
+namespace clang {
+namespace Popcorn {
+
+/// Return whether a given target is supported by the compiler.
+bool SupportedTarget(const llvm::StringRef Target);
+
+/// Populate an array with all targets currently supported by the Popcorn
+/// compiler.
+void GetAllTargets(llvm::SmallVector<std::string, 2> &Targets);
+
+/// Return a TargetOptions with features appropriate for Popcorn Linux
+std::shared_ptr<TargetOptions>
+GetPopcornTargetOpts(const llvm::StringRef TripleStr);
+
+/// Strip target-specific CPUs & features from function attributes in all
+/// functions in the module.  This silences warnings from the compiler about
+/// unsupported target features when compiling the IR for multiple
+/// architectures.
+void StripTargetAttributes(llvm::Module &M);
+
+
+/// Add the target-features attribute specified in TargetOpts to every function
+/// in module M.
+void AddArchSpecificTargetFeatures(llvm::Module &M,
+                                   std::shared_ptr<TargetOptions> TargetOpts);
+
+} /* end Popcorn namespace */
+} /* end clang namespace */
+
+#endif
diff --git a/clang/include/clang/CodeGen/PrefetchBuilder.h b/clang/include/clang/CodeGen/PrefetchBuilder.h
new file mode 100644
index 00000000000..ee347d248f1
--- /dev/null
+++ b/clang/include/clang/CodeGen/PrefetchBuilder.h
@@ -0,0 +1,56 @@
+//===- Prefetch.h - Prefetching Analysis for Statements -----------*- C++ --*-//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file defines the interface for building prefetching calls based on the
+// prefetching analysis.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_CLANG_CODEGEN_PREFETCHBUILDER_H
+#define LLVM_CLANG_CODEGEN_PREFETCHBUILDER_H
+
+#include "CodeGenFunction.h"
+#include "clang/Sema/PrefetchAnalysis.h"
+#include "llvm/Support/raw_ostream.h"
+
+namespace clang {
+
+/// Generate calls to the prefetching API for analyzed regions.
+class PrefetchBuilder {
+public:
+  PrefetchBuilder(clang::CodeGen::CodeGenFunction *CGF)
+    : CGM(CGF->CGM), CGF(*CGF), Ctx(CGF->getContext()) {}
+
+  /// Emit prefetching API declarations.
+  void EmitPrefetchCallDeclarations();
+
+  /// Emit a prefetch call for a particular range of memory.
+  void EmitPrefetchCall(const PrefetchRange &P);
+
+  /// Emit a call to send the prefetch requests to the OS.
+  void EmitPrefetchExecuteCall();
+
+  // TODO print & dump
+
+private:
+  clang::CodeGen::CodeGenModule &CGM;
+  clang::CodeGen::CodeGenFunction &CGF;
+  ASTContext &Ctx;
+
+  // Prefetch API declarations
+  llvm::Constant *Prefetch, *Execute;
+
+  Expr *buildAddrOf(Expr *ArrSub);
+  Expr *buildArrayIndex(VarDecl *Base, Expr *Subscript);
+};
+
+} // end namespace clang
+
+#endif
+
diff --git a/clang/include/clang/Driver/Options.td b/clang/include/clang/Driver/Options.td
index 9d3e2cfccdc..bdb56ecec69 100644
--- a/clang/include/clang/Driver/Options.td
+++ b/clang/include/clang/Driver/Options.td
@@ -83,6 +83,8 @@ def m_ppc_Features_Group  : OptionGroup<"<m ppc features group>">, Group<m_Group
 def m_libc_Group          : OptionGroup<"<m libc group>">, Group<m_Group>;
 def u_Group               : OptionGroup<"<u group>">;
 
+def Popcorn_Target_Group  : OptionGroup<"<Popcorn target group>">, Group<CompileOnly_Group>;
+
 def pedantic_Group        : OptionGroup<"<pedantic group>">,
   Group<CompileOnly_Group>;
 def reserved_lib_Group   : OptionGroup<"<reserved libs group>">;
@@ -1538,6 +1540,12 @@ def pedantic : Flag<["-", "--"], "pedantic">, Group<pedantic_Group>, Flags<[CC1O
 def pg : Flag<["-"], "pg">, HelpText<"Enable mcount instrumentation">, Flags<[CC1Option]>;
 def pipe : Flag<["-", "--"], "pipe">,
   HelpText<"Use pipes between commands, when possible">;
+def popcorn_migratable : Flag<["-"], "popcorn-migratable">, HelpText<"Instrument code to be migratable on Popcorn Linux (implies -popcorn-alignment)">, Flags<[CC1Option]>;
+def popcorn_metadata : Flag<["-"], "popcorn-metadata">, HelpText<"Generate stack transformation metadata without inserting migration points (implies -popcorn-alignment)">, Flags<[CC1Option]>;
+def popcorn_libc : Flag<["-"], "popcorn-libc">, HelpText<"Compile libc code with appropriate instrumentation for migration (implies -popcorn-alignment)">, Flags<[CC1Option]>;
+def popcorn_alignment : Flag<["-"], "popcorn-alignment">, HelpText<"Run Popcorn passes to prepare for link-time symbol alignment">, Flags<[CC1Option]>;
+def popcorn_target : Joined<["-"], "popcorn-target=">, HelpText<"Targets for which to generate object files (requires -popcorn-migratable)">, Group<Popcorn_Target_Group>, Flags<[CC1Option]>, MetaVarName<"<target>">;
+def distributed_omp : Flag<["-"], "distributed-omp">, HelpText<"Optimize OpenMP code generation for distributed execution on Popcorn Linux">, Flags<[CC1Option]>;
 def prebind__all__twolevel__modules : Flag<["-"], "prebind_all_twolevel_modules">;
 def prebind : Flag<["-"], "prebind">;
 def preload : Flag<["-"], "preload">;
diff --git a/clang/include/clang/Frontend/CodeGenOptions.def b/clang/include/clang/Frontend/CodeGenOptions.def
index 803d0233046..4cc9079a7eb 100644
--- a/clang/include/clang/Frontend/CodeGenOptions.def
+++ b/clang/include/clang/Frontend/CodeGenOptions.def
@@ -158,6 +158,12 @@ CODEGENOPT(DebugColumnInfo, 1, 0) ///< Whether or not to use column information
 
 CODEGENOPT(EmitLLVMUseLists, 1, 0) ///< Control whether to serialize use-lists.
 
+/// Select places inside functions to instrument with migration points
+CODEGENOPT(PopcornMigratable, 1, 0)
+
+/// Adjust linkage of global values for symbol alignment
+CODEGENOPT(PopcornAlignment, 1, 0)
+
 /// The user specified number of registers to be used for integral arguments,
 /// or 0 if unspecified.
 VALUE_CODEGENOPT(NumRegisterParameters, 32, 0)
diff --git a/clang/include/clang/Frontend/CodeGenOptions.h b/clang/include/clang/Frontend/CodeGenOptions.h
index 53246bcf22c..8d41bd9e2fc 100644
--- a/clang/include/clang/Frontend/CodeGenOptions.h
+++ b/clang/include/clang/Frontend/CodeGenOptions.h
@@ -201,6 +201,9 @@ public:
   /// Set of sanitizer checks that trap rather than diagnose.
   SanitizerSet SanitizeTrap;
 
+  /// Targets for which to emit object code
+  std::vector<std::string> PopcornTargets;
+
 public:
   // Define accessors/mutators for code generation options of enumeration type.
 #define CODEGENOPT(Name, Bits, Default)
diff --git a/clang/include/clang/Frontend/CompilerInstance.h b/clang/include/clang/Frontend/CompilerInstance.h
index 45e5ed12046..f24f867a0a9 100644
--- a/clang/include/clang/Frontend/CompilerInstance.h
+++ b/clang/include/clang/Frontend/CompilerInstance.h
@@ -260,6 +260,13 @@ public:
     return Invocation->getCodeGenOpts();
   }
 
+  CodeGenOptions &getCodeGenNoOpts() {
+    return Invocation->getCodeGenNoOpts();
+  }
+  const CodeGenOptions &getCodeGenNoOpts() const {
+    return Invocation->getCodeGenNoOpts();
+  }
+
   DependencyOutputOptions &getDependencyOutputOpts() {
     return Invocation->getDependencyOutputOpts();
   }
diff --git a/clang/include/clang/Frontend/CompilerInvocation.h b/clang/include/clang/Frontend/CompilerInvocation.h
index 7d125480439..f454e13c5ef 100644
--- a/clang/include/clang/Frontend/CompilerInvocation.h
+++ b/clang/include/clang/Frontend/CompilerInvocation.h
@@ -108,6 +108,9 @@ class CompilerInvocation : public CompilerInvocationBase {
   /// Options controlling IRgen and the backend.
   CodeGenOptions CodeGenOpts;
 
+  /// Options controlling IRgen and the backend (with optimization disabled).
+  CodeGenOptions CodeGenOptsNoOpt;
+
   /// Options controlling dependency output.
   DependencyOutputOptions DependencyOutputOpts;
 
@@ -179,6 +182,11 @@ public:
     return CodeGenOpts;
   }
 
+  CodeGenOptions &getCodeGenNoOpts() { return CodeGenOptsNoOpt; }
+  const CodeGenOptions &getCodeGenNoOpts() const {
+    return CodeGenOptsNoOpt;
+  }
+
   DependencyOutputOptions &getDependencyOutputOpts() {
     return DependencyOutputOpts;
   }
diff --git a/clang/include/clang/Frontend/FrontendOptions.h b/clang/include/clang/Frontend/FrontendOptions.h
index c3aa226ea90..0fa3efa85d8 100644
--- a/clang/include/clang/Frontend/FrontendOptions.h
+++ b/clang/include/clang/Frontend/FrontendOptions.h
@@ -37,6 +37,7 @@ namespace frontend {
     EmitLLVMOnly,           ///< Generate LLVM IR, but do not emit anything.
     EmitCodeGenOnly,        ///< Generate machine code, but don't emit anything.
     EmitObj,                ///< Emit a .o file.
+    EmitMultiObj,           ///< Emit a .o file for multiple ISAs.
     FixIt,                  ///< Parse and apply any fixits to the source.
     GenerateModule,         ///< Generate pre-compiled module.
     GeneratePCH,            ///< Generate pre-compiled header.
diff --git a/clang/include/clang/Parse/Parser.h b/clang/include/clang/Parse/Parser.h
index 8719555be91..dbca29f58c3 100644
--- a/clang/include/clang/Parse/Parser.h
+++ b/clang/include/clang/Parse/Parser.h
@@ -167,6 +167,7 @@ class Parser : public CodeCompletionHandler {
   std::unique_ptr<PragmaHandler> LoopHintHandler;
   std::unique_ptr<PragmaHandler> UnrollHintHandler;
   std::unique_ptr<PragmaHandler> NoUnrollHintHandler;
+  std::unique_ptr<PragmaHandler> PopcornHandler;
 
   std::unique_ptr<CommentHandler> CommentSemaHandler;
 
@@ -528,6 +529,13 @@ private:
   /// #pragma clang loop and #pragma unroll.
   bool HandlePragmaLoopHint(LoopHint &Hint);
 
+  /// \brief Handle the annotation token produced for
+  /// #pragma popcorn...
+  StmtResult HandlePragmaPopcorn();
+
+  /// \brief Parse a comma-separated variable list
+  void ParseVarList(llvm::SmallPtrSet<VarDecl *, 4> &Vars);
+
   /// GetLookAheadToken - This peeks ahead N tokens and returns that token
   /// without consuming any tokens.  LookAhead(0) returns 'Tok', LookAhead(1)
   /// returns the token after Tok, etc.
@@ -2418,6 +2426,10 @@ private:
 
   //===--------------------------------------------------------------------===//
   // OpenMP: Directives and clauses.
+  /// \brief Checks 'prefetch' clauses for correctness.  Note that we can only
+  /// perform some semantic checks *after* the entire compound statement
+  /// representing the directive's body has been parsed.
+  void CheckOpenMPPrefetchClauses(StmtResult Directive);
   /// \brief Parses declarative OpenMP directives.
   DeclGroupPtrTy ParseOpenMPDeclarativeDirective();
   /// \brief Parses simple list of variables.
diff --git a/clang/include/clang/Sema/PrefetchAnalysis.h b/clang/include/clang/Sema/PrefetchAnalysis.h
new file mode 100644
index 00000000000..ff66e7eca57
--- /dev/null
+++ b/clang/include/clang/Sema/PrefetchAnalysis.h
@@ -0,0 +1,145 @@
+//===- PrefetchAnalysis.h - Prefetching Analysis for Statements ---*- C++ --*-//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file defines the interface for prefetching analysis over structured
+// blocks.  The analysis traverses the AST to determine how arrays are accessed
+// in structured blocks and generates expressions defining ranges of elements
+// accessed inside arrays.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_CLANG_AST_PREFETCHANALYSIS_H
+#define LLVM_CLANG_AST_PREFETCHANALYSIS_H
+
+#include "clang/AST/Decl.h"
+#include "clang/AST/Expr.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/Support/raw_ostream.h"
+#include <memory>
+
+class LoopNestTraversal;
+class ArrayAccessPattern;
+
+namespace clang {
+
+class ASTContext;
+
+/// A range of memory to be prefetched.
+class PrefetchRange {
+public:
+  /// Access type for array.  Sorted in increasing importance.
+  enum Type { Read, Write };
+
+  PrefetchRange(enum Type Ty, VarDecl *Array, Expr *Start, Expr *End)
+    : Ty(Ty), Array(Array), Start(Start), End(End) {}
+
+  enum Type getType() const { return Ty; }
+  VarDecl *getArray() const { return Array; }
+  Expr *getStart() const { return Start; }
+  Expr *getEnd() const { return End; }
+  void setType(enum Type Ty) { this->Ty = Ty; }
+  void setArray(VarDecl *Array) { this->Array = Array; }
+  void setStart(Expr *Start) { this->Start = Start; }
+  void setEnd(Expr *Start) { this->End = End; }
+
+  /// Return true if the other prefetch range is equal to this one (ignoring
+  /// prefetch type differences), or false otherwise.
+  bool equalExceptType(const PrefetchRange &RHS);
+
+  /// Return true if the other prefetch range is equal to this one, or false
+  /// otherwise.
+  bool operator==(const PrefetchRange &RHS);
+
+  // TODO print & dump
+  const char *getTypeName() const {
+    switch(Ty) {
+    case Read: return "read";
+    case Write: return "write";
+    default: return "unknown";
+    }
+  }
+
+private:
+  enum Type Ty;
+  VarDecl *Array;
+  Expr *Start, *End;
+};
+
+class PrefetchAnalysis {
+public:
+  /// Default constructor, really only defined to enable storage in a DenseMap.
+  PrefetchAnalysis() : Ctx(nullptr), S(nullptr) {}
+
+  /// Construct a new prefetch analysis object to analyze a statement.  Doesn't
+  /// run the analysis.
+  PrefetchAnalysis(ASTContext *Ctx, Stmt *S) : Ctx(Ctx), S(S) {}
+
+  /// Ignore a set of variables during access analysis.  In other words, ignore
+  /// memory accesses which use these variables as their base.
+  void ignoreVars(const llvm::SmallPtrSet<VarDecl *, 4> &Ignore)
+  { this->Ignore = Ignore; }
+
+  /// Analyze the statement to capture loop information & array accesses.
+  void analyzeStmt();
+
+  /// Construct prefetch ranges from array accesses & induction variables.
+  void calculatePrefetchRanges();
+
+  /// Get prefetch ranges discovered by analysis.
+  const SmallVector<PrefetchRange, 8> &getArraysToPrefetch() const
+  { return ToPrefetch; }
+
+  /// Return true if the QualType is both scalar and of integer type, or false
+  /// otherwise.
+  static bool isScalarIntType(const QualType &Ty);
+
+  /// Return the size in bits of a builtin integer type, or UINT32_MAX if not a
+  /// builtin integer type.
+  static unsigned getTypeSize(BuiltinType::Kind K);
+
+  /// Cast the value declaration to a variable declaration if it is a varaible
+  /// of scalar integer type.
+  static VarDecl *getVarIfScalarInt(ValueDecl *VD);
+
+  void print(llvm::raw_ostream &O) const;
+  void dump() const { print(llvm::errs()); }
+
+private:
+  ASTContext *Ctx;
+  Stmt *S;
+
+  /// Analysis information.
+  std::shared_ptr<LoopNestTraversal> Loops;
+  std::shared_ptr<ArrayAccessPattern> ArrAccesses;
+
+  /// Variables (i.e., arrays) to ignore during analysis
+  llvm::SmallPtrSet<VarDecl *, 4> Ignore;
+
+  /// The good stuff -- ranges of memory to prefetch
+  llvm::SmallVector<PrefetchRange, 8> ToPrefetch;
+
+  /// Analyze individual types of statements.
+  void analyzeForStmt();
+
+  /// Merge overlapping or contiguous prefetch ranges.
+  void mergePrefetchRanges();
+
+  /// Remove trivial or redundant array accesses.  This is split into two as
+  /// some array accesses may only become redundant after expansion into a
+  /// prefetch range.
+  void pruneArrayAccesses();
+  void prunePrefetchRanges();
+};
+
+} // end namespace clang
+
+#endif
+
diff --git a/clang/include/clang/Sema/PrefetchDataflow.h b/clang/include/clang/Sema/PrefetchDataflow.h
new file mode 100644
index 00000000000..07aa621cd31
--- /dev/null
+++ b/clang/include/clang/Sema/PrefetchDataflow.h
@@ -0,0 +1,84 @@
+//=- PrefetchDataflow.cpp - Dataflow analysis for prefetching ------------*-==//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements the dataflow of expressions as required for prefetching
+// analysis.  This is required to correctly discover how variables are used in
+// memory accesses in order to construct memory access ranges.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef _AST_PREFETCHDATAFLOW_H
+#define _AST_PREFETCHDATAFLOW_H
+
+#include "clang/Analysis/CFG.h"
+#include "clang/Analysis/CFGStmtMap.h"
+#include "clang/AST/ParentMap.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include <memory>
+
+namespace clang {
+
+/// Data structures for containing symbolic execution values.
+typedef llvm::SmallPtrSet<Expr *, 1> ExprList;
+typedef llvm::DenseMap<const VarDecl *, ExprList> SymbolicValueMap;
+typedef std::pair<const VarDecl *, ExprList> SymbolicValuePair;
+typedef llvm::SmallPtrSet<const CFGBlock *, 32> CFGBlockSet;
+
+/// Class which runs dataflow analysis over the specified statement.  Tracks
+/// the value of a given set of variables as they change throughout the
+/// statement.
+class PrefetchDataflow {
+public:
+  typedef llvm::SmallPtrSet<const VarDecl *, 8> VarSet;
+
+  /// A lot of boilerplate so we can embed analysis into data structures like
+  /// llvm::DenseMap.  Required because objects use std::unique_ptrs.
+  PrefetchDataflow();
+  PrefetchDataflow(ASTContext *Ctx);
+
+  /// Copy constructor -- only copies the AST.
+  PrefetchDataflow(const PrefetchDataflow &RHS);
+
+  /// Assignment operator -- only copies the AST.
+  PrefetchDataflow &operator=(const PrefetchDataflow &RHS);
+
+  /// Run dataflow analysis over the statement specified at build time.
+  void runDataflow(Stmt *S, VarSet &VarsToTrack);
+
+  /// Get possible values of a variable at a specific use in a statement, if
+  /// any.  The list argument will be populated with expressions.
+  void getVariableValues(VarDecl *Var, const Stmt *Use, ExprList &Exprs) const;
+
+  /// Reset any previous analysis.
+  void reset();
+
+  void print(llvm::raw_ostream &O) const;
+  void dump() const;
+
+private:
+  ASTContext *Ctx;
+  Stmt *S;
+
+  /// Data used in analysis/retrieving results
+  std::unique_ptr<CFG> TheCFG;
+  std::unique_ptr<ParentMap> PMap;
+  std::unique_ptr<CFGStmtMap> StmtToBlock;
+
+  /// Analysis results -- keep an expression used to calculate a variable's
+  /// value for each control flow block.
+  typedef std::pair<const CFGBlock *, SymbolicValueMap> BlockValuesPair;
+  typedef llvm::DenseMap<const CFGBlock *, SymbolicValueMap> BlockValuesMap;
+  BlockValuesMap VarValues;
+};
+
+}
+
+#endif
+
diff --git a/clang/include/clang/Sema/PrefetchExprBuilder.h b/clang/include/clang/Sema/PrefetchExprBuilder.h
new file mode 100644
index 00000000000..755d9bfff5c
--- /dev/null
+++ b/clang/include/clang/Sema/PrefetchExprBuilder.h
@@ -0,0 +1,102 @@
+//===- PrefetchExprBuilder.h - Prefetching expression builder -----*- C++ --*-//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file defines a set of utilities for building expressions for
+// prefetching.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_CLANG_AST_PREFETCHEXPRBUILDER_H
+#define LLVM_CLANG_AST_PREFETCHEXPRBUILDER_H
+
+#include "clang/AST/Decl.h"
+#include "clang/AST/Expr.h"
+#include "llvm/ADT/DenseMap.h"
+
+namespace clang {
+
+class ASTContext;
+
+typedef std::pair<VarDecl *, Expr *> ReplacePair;
+typedef llvm::DenseMap<VarDecl *, Expr *> ReplaceMap;
+
+/// Utilities for compairing expressions by value.
+namespace PrefetchExprEquality {
+
+/// Compare two expressions by value to see if they're equal.
+bool exprEqual(const Expr *A, const Expr *B);
+
+}
+
+/// Utilities for building expressions.
+namespace PrefetchExprBuilder {
+
+/// Information describing how a statement should be modified.
+struct Modifier {
+  enum Type { Add, Sub, Mul, Div, None, Unknown };
+  void ClassifyModifier(const Expr *E, const ASTContext *Ctx);
+  enum Type getType() const { return Ty; }
+  const llvm::APInt &getVal() const { return Val; }
+private:
+  enum Type Ty;
+  llvm::APInt Val;
+};
+
+/// Information needed for building expressions.
+struct BuildInfo {
+public:
+  BuildInfo(ASTContext *Ctx, ReplaceMap &VarReplace, bool dumpInColor)
+    : Ctx(Ctx), VarReplace(VarReplace), dumpInColor(dumpInColor) {}
+
+  ASTContext *Ctx;
+  ReplaceMap &VarReplace;
+  llvm::SmallPtrSet<VarDecl *, 8> SeenVars;
+  bool dumpInColor;
+
+  void reset() {
+    VarReplace.clear();
+    SeenVars.clear();
+  }
+};
+
+/// Reconstruct expressions with variables replaced by user-supplied
+/// expressions (in Info.VarReplacements).
+Expr *cloneWithReplacement(Expr *E, BuildInfo &Info);
+
+/// Clone an expression, but don't replace any variables.
+Expr *clone(Expr *E, ASTContext *Ctx);
+
+/// Clone a binary operation.
+Expr *cloneBinaryOperator(BinaryOperator *B, BuildInfo &Info);
+
+/// Clone a unary operation.
+Expr *cloneUnaryOperator(UnaryOperator *U, BuildInfo &Info);
+
+/// Clone an array subscript.
+Expr *cloneArraySubscriptExpr(ArraySubscriptExpr *A, BuildInfo &Info);
+
+/// Clone a declaration reference.  If it's an induction variable, replace
+/// with the bound specified by the Upper flag.
+Expr *cloneDeclRefExpr(DeclRefExpr *D, BuildInfo &Info);
+
+/// Clone an implicit cast.
+Expr *cloneImplicitCastExpr(ImplicitCastExpr *E, BuildInfo &Info);
+
+/// Clone an integer literal.
+Expr *cloneIntegerLiteral(IntegerLiteral *L, BuildInfo &Info);
+
+/// Modify an expression according to a configuration.
+Expr *cloneAndModifyExpr(Expr *E, const Modifier &Mod, ASTContext *Ctx);
+
+} // end namespace PrefetchExprBuilder
+
+} // end namespace clang
+
+#endif
+
diff --git a/clang/include/clang/Sema/Sema.h b/clang/include/clang/Sema/Sema.h
index 72044336a83..3dcebb33132 100644
--- a/clang/include/clang/Sema/Sema.h
+++ b/clang/include/clang/Sema/Sema.h
@@ -7950,11 +7950,12 @@ public:
 
   OMPClause *ActOnOpenMPVarListClause(
       OpenMPClauseKind Kind, ArrayRef<Expr *> Vars, Expr *TailExpr,
-      SourceLocation StartLoc, SourceLocation LParenLoc,
-      SourceLocation ColonLoc, SourceLocation EndLoc,
-      CXXScopeSpec &ReductionIdScopeSpec,
+      Expr *EndExpr, SourceLocation StartLoc, SourceLocation LParenLoc,
+      SourceLocation ColonLoc, SourceLocation EndColonLoc,
+      SourceLocation EndLoc, CXXScopeSpec &ReductionIdScopeSpec,
       const DeclarationNameInfo &ReductionId, OpenMPDependClauseKind DepKind,
-      SourceLocation DepLoc);
+      SourceLocation DepLoc, OpenMPPrefetchClauseKind PrefKind,
+      SourceLocation PrefLoc);
   /// \brief Called on well-formed 'private' clause.
   OMPClause *ActOnOpenMPPrivateClause(ArrayRef<Expr *> VarList,
                                       SourceLocation StartLoc,
@@ -8018,6 +8019,18 @@ public:
                           SourceLocation StartLoc, SourceLocation LParenLoc,
                           SourceLocation EndLoc);
 
+  /// \brief Called on well-formed 'prefetch' clause.
+  OMPClause *
+  ActOnOpenMPPrefetchClause(OpenMPPrefetchClauseKind PrefKind,
+                            SourceLocation PrefLoc,
+                            ArrayRef<Expr *> VarList,
+                            Expr *Start, Expr *End,
+                            SourceLocation StartLoc,
+                            SourceLocation LParenLoc,
+                            SourceLocation FirstColonLoc,
+                            SourceLocation SecondColonLoc,
+                            SourceLocation EndLoc);
+
   /// \brief The kind of conversion being performed.
   enum CheckedConversionKind {
     /// \brief An implicit conversion.
diff --git a/clang/lib/AST/Stmt.cpp b/clang/lib/AST/Stmt.cpp
index e6292b49536..1df4b6cd2f0 100644
--- a/clang/lib/AST/Stmt.cpp
+++ b/clang/lib/AST/Stmt.cpp
@@ -1623,6 +1623,28 @@ OMPExecutableDirective::getSingleClause(OpenMPClauseKind K) const {
   return nullptr;
 }
 
+OMPPrefetchClause *
+OMPPrefetchClause::Create(const ASTContext &C, OpenMPPrefetchClauseKind Kind,
+                          SourceLocation KindLoc, ArrayRef<Expr *> VL,
+                          Expr *Start, Expr *End, SourceLocation StartLoc,
+                          SourceLocation LParenLoc,
+                          SourceLocation FirstColonLoc,
+                          SourceLocation SecondColonLoc,
+                          SourceLocation EndLoc) {
+  void *Mem = C.Allocate(llvm::RoundUpToAlignment(sizeof(OMPPrefetchClause),
+                                                  llvm::alignOf<Expr *>()) +
+                         sizeof(Expr *) * VL.size());
+  OMPPrefetchClause *Clause =
+    new (Mem) OMPPrefetchClause(StartLoc, LParenLoc, FirstColonLoc, SecondColonLoc,
+                                EndLoc, VL.size());
+  Clause->setPrefetchKind(Kind);
+  Clause->setPrefetchKindLoc(KindLoc);
+  Clause->setStartOfRange(Start);
+  Clause->setEndOfRange(End);
+  Clause->setVarRefs(VL);
+  return Clause;
+}
+
 OMPParallelDirective *OMPParallelDirective::Create(
                                               const ASTContext &C,
                                               SourceLocation StartLoc,
diff --git a/clang/lib/AST/StmtPrinter.cpp b/clang/lib/AST/StmtPrinter.cpp
index 79600773f56..e7ee3aeba91 100644
--- a/clang/lib/AST/StmtPrinter.cpp
+++ b/clang/lib/AST/StmtPrinter.cpp
@@ -810,6 +810,21 @@ void OMPClausePrinter::VisitOMPDependClause(OMPDependClause *Node) {
     OS << ")";
   }
 }
+
+void OMPClausePrinter::VisitOMPPrefetchClause(OMPPrefetchClause *Node) {
+  if (!Node->varlist_empty()) {
+    OS << "prefetch(";
+    OS << getOpenMPSimpleClauseTypeName(Node->getClauseKind(),
+                                        Node->getPrefetchKind())
+       << " :";
+    VisitOMPClauseList(Node, ' ');
+    OS << ",";
+    Node->getStartOfRange()->printPretty(OS, nullptr, Policy, 0);
+    OS << ",";
+    Node->getEndOfRange()->printPretty(OS, nullptr, Policy, 0);
+    OS << ")";
+  }
+}
 }
 
 //===----------------------------------------------------------------------===//
diff --git a/clang/lib/AST/StmtProfile.cpp b/clang/lib/AST/StmtProfile.cpp
index da996920c42..8b862fa9098 100644
--- a/clang/lib/AST/StmtProfile.cpp
+++ b/clang/lib/AST/StmtProfile.cpp
@@ -428,6 +428,11 @@ void OMPClauseProfiler::VisitOMPFlushClause(const OMPFlushClause *C) {
 void OMPClauseProfiler::VisitOMPDependClause(const OMPDependClause *C) {
   VisitOMPClauseList(C);
 }
+void OMPClauseProfiler::VisitOMPPrefetchClause(const OMPPrefetchClause *C) {
+  VisitOMPClauseList(C);
+  Profiler->VisitStmt(C->getStartOfRange());
+  Profiler->VisitStmt(C->getEndOfRange());
+}
 }
 
 void
diff --git a/clang/lib/Analysis/CFG.cpp b/clang/lib/Analysis/CFG.cpp
index 54d15bd232a..0a1a0a075a7 100644
--- a/clang/lib/Analysis/CFG.cpp
+++ b/clang/lib/Analysis/CFG.cpp
@@ -1,4 +1,4 @@
-  //===--- CFG.cpp - Classes for representing and building CFGs----*- C++ -*-===//
+//===--- CFG.cpp - Classes for representing and building CFGs----*- C++ -*-===//
 //
 //                     The LLVM Compiler Infrastructure
 //
diff --git a/clang/lib/Basic/OpenMPKinds.cpp b/clang/lib/Basic/OpenMPKinds.cpp
index b7407f60e6d..235d4397e40 100644
--- a/clang/lib/Basic/OpenMPKinds.cpp
+++ b/clang/lib/Basic/OpenMPKinds.cpp
@@ -96,6 +96,11 @@ unsigned clang::getOpenMPSimpleClauseType(OpenMPClauseKind Kind,
 #define OPENMP_DEPEND_KIND(Name) .Case(#Name, OMPC_DEPEND_##Name)
 #include "clang/Basic/OpenMPKinds.def"
         .Default(OMPC_DEPEND_unknown);
+  case OMPC_prefetch:
+    return llvm::StringSwitch<OpenMPPrefetchClauseKind>(Str)
+#define OPENMP_PREFETCH_KIND(Name) .Case(#Name, OMPC_PREFETCH_##Name)
+#include "clang/Basic/OpenMPKinds.def"
+        .Default(OMPC_PREFETCH_unknown);
   case OMPC_unknown:
   case OMPC_threadprivate:
   case OMPC_if:
@@ -195,6 +200,7 @@ const char *clang::getOpenMPSimpleClauseTypeName(OpenMPClauseKind Kind,
   case OMPC_update:
   case OMPC_capture:
   case OMPC_seq_cst:
+  case OMPC_prefetch:
     break;
   }
   llvm_unreachable("Invalid OpenMP simple clause kind");
diff --git a/clang/lib/CodeGen/BackendUtil.cpp b/clang/lib/CodeGen/BackendUtil.cpp
index afcb9e5c505..2692f6a4056 100644
--- a/clang/lib/CodeGen/BackendUtil.cpp
+++ b/clang/lib/CodeGen/BackendUtil.cpp
@@ -15,6 +15,7 @@
 #include "clang/Frontend/FrontendDiagnostic.h"
 #include "clang/Frontend/Utils.h"
 #include "llvm/ADT/StringSwitch.h"
+#include "llvm/Analysis/Passes.h"
 #include "llvm/Analysis/TargetLibraryInfo.h"
 #include "llvm/Analysis/TargetTransformInfo.h"
 #include "llvm/Bitcode/BitcodeWriterPass.h"
@@ -39,6 +40,7 @@
 #include "llvm/Transforms/Instrumentation.h"
 #include "llvm/Transforms/ObjCARC.h"
 #include "llvm/Transforms/Scalar.h"
+#include "llvm/Transforms/Utils.h"
 #include "llvm/Transforms/Utils/SymbolRewriter.h"
 #include <memory>
 using namespace clang;
@@ -132,6 +134,16 @@ public:
 
   std::unique_ptr<TargetMachine> TM;
 
+  /// Set up the assembly helper, including gathering passes
+  void SetupAssemblyHelper(BackendAction Action,
+                           raw_pwrite_stream *OS);
+
+  /// Run only IR optimization passes on a module
+  void ApplyIROptPasses(Module* TheModule);
+
+  /// Run backend passes to generate code
+  void ApplyCodegenPasses(Module* TheModule);
+
   void EmitAssembly(BackendAction Action, raw_pwrite_stream *OS);
 };
 
@@ -271,6 +283,18 @@ static void addSymbolRewriterPass(const CodeGenOptions &Opts,
   MPM->add(createRewriteSymbolsPass(DL));
 }
 
+static void addPopcornMigPointPasses(const PassManagerBuilder &Builder,
+                                     legacy::PassManagerBase &PM) {
+  PM.add(createPopcornCompatibilityPass());
+  PM.add(createSelectMigrationPointsPass());
+}
+
+static void addPopcornAlignmentPasses(const PassManagerBuilder &Builder,
+                                      legacy::PassManagerBase &PM) {
+  PM.add(createNameStringLiteralsPass());
+  PM.add(createStaticVarSectionsPass());
+}
+
 void EmitAssemblyHelper::CreatePasses() {
   unsigned OptLevel = CodeGenOpts.OptimizationLevel;
   CodeGenOptions::InliningMethod Inlining = CodeGenOpts.getInlining();
@@ -420,6 +444,29 @@ void EmitAssemblyHelper::CreatePasses() {
     MPM->add(createInstrProfilingPass(Options));
   }
 
+  // Popcorn Compiler Toolchain passes -- add after IR optimization passes
+  // Select migration points.
+  if (CodeGenOpts.PopcornMigratable) {
+    if (OptLevel > 0)
+      PMBuilder.addExtension(PassManagerBuilder::EP_OptimizerLast,
+                             addPopcornMigPointPasses);
+    else {
+      MPM->add(createPopcornCompatibilityPass());
+      MPM->add(createSelectMigrationPointsPass());
+    }
+  }
+
+  // Adjust global symbol linkage for alignment.
+  if (CodeGenOpts.PopcornAlignment) {
+    if (OptLevel > 0)
+      PMBuilder.addExtension(PassManagerBuilder::EP_OptimizerLast,
+                             addPopcornAlignmentPasses);
+    else {
+      MPM->add(createNameStringLiteralsPass());
+      MPM->add(createStaticVarSectionsPass());
+    }
+  }
+
   PMBuilder.populateModulePassManager(*MPM);
 }
 
@@ -570,7 +617,7 @@ bool EmitAssemblyHelper::AddEmitPasses(BackendAction Action,
   // Normal mode, emit a .s or .o file by running the code generator. Note,
   // this also adds codegenerator level optimization passes.
   TargetMachine::CodeGenFileType CGFT = TargetMachine::CGFT_AssemblyFile;
-  if (Action == Backend_EmitObj)
+  if (Action == Backend_EmitObj || Action == Backend_EmitMultiObj)
     CGFT = TargetMachine::CGFT_ObjectFile;
   else if (Action == Backend_EmitMCNull)
     CGFT = TargetMachine::CGFT_Null;
@@ -592,8 +639,8 @@ bool EmitAssemblyHelper::AddEmitPasses(BackendAction Action,
   return true;
 }
 
-void EmitAssemblyHelper::EmitAssembly(BackendAction Action,
-                                      raw_pwrite_stream *OS) {
+void EmitAssemblyHelper::SetupAssemblyHelper(BackendAction Action,
+                                             raw_pwrite_stream *OS) {
   TimeRegion Region(llvm::TimePassesIsEnabled ? &CodeGenerationTime : nullptr);
 
   bool UsesCodeGen = (Action != Backend_EmitNothing &&
@@ -629,7 +676,9 @@ void EmitAssemblyHelper::EmitAssembly(BackendAction Action,
 
   // Before executing passes, print the final values of the LLVM options.
   cl::PrintOptionValues();
+}
 
+void EmitAssemblyHelper::ApplyIROptPasses(Module* M) {
   // Run passes. For now we do all passes at once, but eventually we
   // would like to have the option of streaming code generation.
 
@@ -647,13 +696,22 @@ void EmitAssemblyHelper::EmitAssembly(BackendAction Action,
     PrettyStackTraceString CrashInfo("Per-module optimization passes");
     PerModulePasses->run(*TheModule);
   }
+}
 
+void EmitAssemblyHelper::ApplyCodegenPasses(Module* M) {
   if (CodeGenPasses) {
     PrettyStackTraceString CrashInfo("Code generation");
     CodeGenPasses->run(*TheModule);
   }
 }
 
+void EmitAssemblyHelper::EmitAssembly(BackendAction Action,
+                                      raw_pwrite_stream *OS) {
+  SetupAssemblyHelper(Action, OS);
+  ApplyIROptPasses(TheModule);
+  ApplyCodegenPasses(TheModule);
+}
+
 void clang::EmitBackendOutput(DiagnosticsEngine &Diags,
                               const CodeGenOptions &CGOpts,
                               const clang::TargetOptions &TOpts,
@@ -677,3 +735,38 @@ void clang::EmitBackendOutput(DiagnosticsEngine &Diags,
     }
   }
 }
+
+void clang::ApplyIROptimizations(DiagnosticsEngine &Diags,
+                                 const CodeGenOptions &CGOpts,
+                                 const clang::TargetOptions &TOpts,
+                                 const LangOptions &LOpts, Module *M,
+                                 BackendAction Action, raw_pwrite_stream *OS) {
+  EmitAssemblyHelper AsmHelper(Diags, CGOpts, TOpts, LOpts, M);
+  AsmHelper.SetupAssemblyHelper(Action, OS);
+  AsmHelper.ApplyIROptPasses(M);
+}
+
+void clang::CodegenBackendOutput(DiagnosticsEngine &Diags,
+                                 const CodeGenOptions &CGOpts,
+                                 const clang::TargetOptions &TOpts,
+                                 const LangOptions &LOpts, StringRef TDesc,
+                                 Module *M, BackendAction Action,
+                                 raw_pwrite_stream *OS) {
+  EmitAssemblyHelper AsmHelper(Diags, CGOpts, TOpts, LOpts, M);
+  AsmHelper.SetupAssemblyHelper(Action, OS);
+
+  // If an optional clang TargetInfo description string was passed in, use it to
+  // verify the LLVM TargetMachine's DataLayout.
+  if (AsmHelper.TM && !TDesc.empty()) {
+    std::string DLDesc =
+        AsmHelper.TM->getDataLayout()->getStringRepresentation();
+    if (DLDesc != TDesc) {
+      unsigned DiagID = Diags.getCustomDiagID(
+          DiagnosticsEngine::Error, "backend data layout '%0' does not match "
+                                    "expected target description '%1'");
+      Diags.Report(DiagID) << DLDesc << TDesc;
+    }
+  }
+
+  AsmHelper.ApplyCodegenPasses(M);
+}
diff --git a/clang/lib/CodeGen/CGOpenMPRuntime.cpp b/clang/lib/CodeGen/CGOpenMPRuntime.cpp
index 81488398bb8..1239d0bfdaa 100644
--- a/clang/lib/CodeGen/CGOpenMPRuntime.cpp
+++ b/clang/lib/CodeGen/CGOpenMPRuntime.cpp
@@ -1577,6 +1577,7 @@ enum OpenMPSchedType {
   OMP_sch_guided_chunked = 36,
   OMP_sch_runtime = 37,
   OMP_sch_auto = 38,
+  OMP_sch_hetprobe = 39,
   /// \brief Lower bound for 'ordered' versions.
   OMP_ord_lower = 64,
   OMP_ord_static_chunked = 65,
@@ -1603,6 +1604,8 @@ static OpenMPSchedType getRuntimeSchedule(OpenMPScheduleClauseKind ScheduleKind,
     return Ordered ? OMP_ord_runtime : OMP_sch_runtime;
   case OMPC_SCHEDULE_auto:
     return Ordered ? OMP_ord_auto : OMP_sch_auto;
+  case OMPC_SCHEDULE_hetprobe:
+    return OMP_sch_hetprobe;
   case OMPC_SCHEDULE_unknown:
     assert(!Chunked && "chunk was specified but schedule kind not known");
     return Ordered ? OMP_ord_static : OMP_sch_static;
diff --git a/clang/lib/CodeGen/CGStmt.cpp b/clang/lib/CodeGen/CGStmt.cpp
index 7a0b8a35be0..b931299d5db 100644
--- a/clang/lib/CodeGen/CGStmt.cpp
+++ b/clang/lib/CodeGen/CGStmt.cpp
@@ -18,6 +18,7 @@
 #include "clang/AST/StmtVisitor.h"
 #include "clang/Basic/PrettyStackTrace.h"
 #include "clang/Basic/TargetInfo.h"
+#include "clang/CodeGen/PrefetchBuilder.h"
 #include "clang/Sema/LoopHint.h"
 #include "clang/Sema/SemaDiagnostic.h"
 #include "llvm/ADT/StringExtras.h"
@@ -840,6 +841,19 @@ void CodeGenFunction::EmitForStmt(const ForStmt &S,
 
   LexicalScope ForScope(*this, S.getSourceRange());
 
+  if(S.prefetchEnabled()) {
+    const PrefetchAnalysis *PA = getContext().getPrefetchAnalysis(&S);
+    if(PA) {
+      PrefetchBuilder PB(this);
+      const SmallVector<PrefetchRange, 8> &Pref = PA->getArraysToPrefetch();
+      if(Pref.size()) {
+        PB.EmitPrefetchCallDeclarations();
+        for(auto &Range : Pref) PB.EmitPrefetchCall(Range);
+        PB.EmitPrefetchExecuteCall();
+      }
+    }
+  }
+
   // Evaluate the first part before the loop.
   if (S.getInit())
     EmitStmt(S.getInit());
@@ -2147,14 +2161,103 @@ void CodeGenFunction::EmitAsmStmt(const AsmStmt &S) {
   }
 }
 
+
+void CodeGenFunction::addOffloaded(const CapturedStmt *S, ValueDecl *L,
+                                   ValueDecl *G) {
+  assert(S && isa<CapturedStmt>(S) && "Invalid captured statement");
+  assert(L && G && "Invalid value declarations");
+  OffloadedLocals[S].emplace_back(L, G);
+}
+
+const CodeGenFunction::OffloadList &
+CodeGenFunction::getOffloaded(const CapturedStmt *S) const {
+  assert(S && isa<CapturedStmt>(S) && "Invalid captured statement");
+  OffloadMap::const_iterator it = OffloadedLocals.find(S);
+  assert((it != OffloadedLocals.end()) && "Captured statement not visited?");
+  return it->second;
+}
+
+VarDecl *CodeGenFunction::CreateOffloadedGlobal(const Stmt &S,
+                                                QualType Ty,
+                                                std::string &Name) {
+  ASTContext &AST = getContext();
+  DeclContext *TUC = AST.getTranslationUnitDecl();
+  SourceRange SR = S.getSourceRange();
+  IdentifierInfo *II = &AST.Idents.get("distr_omp_" +
+      std::string(CurFn->getName()) + "_" + Name);
+  if(Ty.isLocalConstQualified()) Ty.removeLocalConst();
+  TypeSourceInfo *TSI = AST.getTrivialTypeSourceInfo(Ty, SR.getBegin());
+  return VarDecl::Create(AST, TUC, SR.getBegin(), SR.getEnd(),
+                         II, Ty, TSI, clang::SC_Static);
+}
+
+VarDecl *CodeGenFunction::CreateOffloadedGlobal(const Stmt &S, const Expr *I) {
+  std::string name(cast<DeclRefExpr>(I)->getDecl()->getNameAsString());
+  return CreateOffloadedGlobal(S, I->getType(), name);
+}
+
+LValue CodeGenFunction::GetVarDeclLValue(QualType SrcType, VarDecl *VD) {
+  if(SrcType.isLocalConstQualified()) SrcType.removeLocalConst();
+  llvm::Type *Ty = CGM.getTypes().ConvertType(SrcType);
+  llvm::Constant *GV = CGM.GetAddrOfGlobalVar(VD, Ty);
+  llvm::GlobalVariable *CastGV = cast<llvm::GlobalVariable>(GV);
+  CastGV->setInitializer(llvm::Constant::getNullValue(Ty));
+  CastGV->setLinkage(llvm::GlobalValue::InternalLinkage);
+  return MakeNaturalAlignAddrLValue(GV, SrcType);
+}
+
+LValue CodeGenFunction::GetVarDeclLValue(const Expr *I, VarDecl *VD) {
+  return GetVarDeclLValue(I->getType(), VD);
+}
+
+DeclRefExpr *CodeGenFunction::GetDeclRefForOffload(ValueDecl *VD) {
+  ASTContext &AST = getContext();
+  QualType Ty = VD->getType();
+  DeclRefExpr *DRE = DeclRefExpr::Create(AST, NestedNameSpecifierLoc(),
+                                         SourceLocation(), VD, false,
+                                         VD->getSourceRange().getBegin(),
+                                         Ty, VK_LValue);
+  DRE->getDecl()->setIsUsed();
+  return DRE;
+}
+
+void CodeGenFunction::RestoreOffloadedLocals(const CapturedStmt *S) {
+  Expr *Global;
+  const OffloadList &Offloaded = getOffloaded(S);
+  for(auto Pair : Offloaded) {
+    if(Pair.first->getType().isLocalConstQualified()) continue;
+    LValue LocalLV(EmitDeclRefLValue(GetDeclRefForOffload(Pair.first)));
+    Global = GetDeclRefForOffload(Pair.second);
+    EmitAnyExprToMem(Global, LocalLV.getAddress(), LocalLV.getQuals(), false);
+  }
+}
+
 LValue CodeGenFunction::InitCapturedStruct(const CapturedStmt &S) {
   const RecordDecl *RD = S.getCapturedRecordDecl();
   QualType RecordTy = getContext().getRecordType(RD);
 
+  // We have to manually track the context number to avoid naming collisions
+  // for files with multiple captured regions
+  static int CtxNum = 0;
+
   // Initialize the captured struct.
-  LValue SlotLV = MakeNaturalAlignAddrLValue(
+  LValue SlotLV;
+  if(S.offloadShared()) {
+    // Name the captured context corresponding to the generated anonymous
+    // structure type for the context
+    std::string Name("agg.captured");
+    if(CtxNum) Name += "." + std::to_string(CtxNum - 1);
+    CtxNum++;
+
+    VarDecl *GlobalCtx = CreateOffloadedGlobal(S, RecordTy, Name);
+    SlotLV = GetVarDeclLValue(RecordTy, GlobalCtx);
+  }
+  else
+    SlotLV = MakeNaturalAlignAddrLValue(
       CreateMemTemp(RecordTy, "agg.captured"), RecordTy);
 
+  if(S.offloadShared()) OffloadedLocals[&S] = OffloadList();
+
   RecordDecl::field_iterator CurField = RD->field_begin();
   for (CapturedStmt::capture_init_iterator I = S.capture_init_begin(),
                                            E = S.capture_init_end();
@@ -2164,7 +2267,17 @@ LValue CodeGenFunction::InitCapturedStruct(const CapturedStmt &S) {
       auto VAT = CurField->getCapturedVLAType();
       EmitStoreThroughLValue(RValue::get(VLASizeMap[VAT->getSizeExpr()]), LV);
     } else {
-      EmitInitializerForField(*CurField, LV, *I, None);
+      if(S.offloadShared() && isa<DeclRefExpr>(*I)) {
+        // If distributed, create global variable, emit initializer & place
+        // address of new global into capture struct
+        VarDecl *GLD = CreateOffloadedGlobal(S, *I);
+        LValue GLV = GetVarDeclLValue(*I, GLD);
+        EmitAnyExprToMem(*I, GLV.getAddress(), GLV.getQuals(), false);
+        Expr *GlobalRef = GetDeclRefForOffload(GLD);
+        EmitInitializerForField(*CurField, LV, GlobalRef, None);
+        addOffloaded(&S, cast<DeclRefExpr>(*I)->getDecl(), GLD);
+      }
+      else EmitInitializerForField(*CurField, LV, *I, None);
     }
   }
 
diff --git a/clang/lib/CodeGen/CGStmtOpenMP.cpp b/clang/lib/CodeGen/CGStmtOpenMP.cpp
index e5f507aa41b..3ae8fe5bc0e 100644
--- a/clang/lib/CodeGen/CGStmtOpenMP.cpp
+++ b/clang/lib/CodeGen/CGStmtOpenMP.cpp
@@ -17,6 +17,7 @@
 #include "TargetInfo.h"
 #include "clang/AST/Stmt.h"
 #include "clang/AST/StmtOpenMP.h"
+#include "llvm/IR/CallSite.h"
 using namespace clang;
 using namespace CodeGen;
 
@@ -489,6 +490,8 @@ static void emitCommonOMPParallelDirective(CodeGenFunction &CGF,
   }
   CGF.CGM.getOpenMPRuntime().emitParallelCall(CGF, S.getLocStart(), OutlinedFn,
                                               CapturedStruct, IfCond);
+
+  if(CS->offloadShared()) CGF.RestoreOffloadedLocals(CS);
 }
 
 void CodeGenFunction::EmitOMPParallelDirective(const OMPParallelDirective &S) {
@@ -743,6 +746,162 @@ static void emitSafelenClause(CodeGenFunction &CGF,
   }
 }
 
+/// Ease the variable lookup burden for captures.
+typedef llvm::DenseMap<const VarDecl *, DeclRefExpr *> CaptureMap;
+
+/// Map variable declarations captured in the outer function to their field in
+/// the captured struct.
+static void
+buildCapturedMap(ASTContext &C, CapturedStmt *CS, CaptureMap &Map) {
+  // Captures are in a 1-to-1 correspondence with captured record fields
+  Map.clear();
+  for(auto Child : CS->children()) {
+    if(isa<DeclRefExpr>(Child)) {
+      DeclRefExpr *DRE = cast<DeclRefExpr>(Child);
+      ValueDecl *Decl = DRE->getDecl();
+      if(isa<VarDecl>(Decl)) Map[cast<VarDecl>(Decl)] = DRE;
+    }
+  }
+}
+
+/// Create an expression representing the address of some array index.
+static Expr *getPrefetchAddr(ASTContext &C, Expr *Ptr, Expr *Subscript) {
+  QualType BaseTy = Ptr->getType().getDesugaredType(C), IdxTy;
+
+  // Get an array subscript
+  if(isa<ArrayType>(BaseTy)) IdxTy = cast<ArrayType>(BaseTy)->getElementType();
+  else IdxTy = cast<PointerType>(BaseTy)->getPointeeType();
+  Expr *Index = new (C) ArraySubscriptExpr(Ptr, Subscript, IdxTy,
+                                           VK_RValue, OK_Ordinary,
+                                           SourceLocation());
+
+  // Take the address of the array subscript
+  QualType RePtrTy = C.getPointerType(IdxTy);
+  UnaryOperator *Addr = new (C) UnaryOperator(Index, UO_AddrOf, RePtrTy, VK_LValue,
+                               OK_Ordinary, SourceLocation());
+
+  // Finally, cast to a const void * type
+  QualType VoidPtrTy = C.getPointerType(C.VoidTy.withConst());
+  return ImplicitCastExpr::Create(C, VoidPtrTy, CK_BitCast, Addr, nullptr,
+                                  VK_RValue);
+}
+
+static Expr *getArrayIndexAddr(ASTContext &C, Expr *Arr, const llvm::APInt &Idx) {
+  QualType Ty = Arr->getType();
+  if(Ty->isArrayType()) {
+    Ty = C.getPointerType(cast<ArrayType>(Ty)->getElementType());
+    Arr = ImplicitCastExpr::Create(C, Ty, CK_ArrayToPointerDecay, Arr, nullptr,
+                                   VK_RValue);
+  }
+  Expr *IdxLiteral =
+    IntegerLiteral::Create(C, Idx, C.LongTy, SourceLocation());
+  return getPrefetchAddr(C, Arr, IdxLiteral);
+}
+
+static Expr *getArrayIndexAddr(ASTContext &C, Expr *Arr, int64_t Idx) {
+  return getArrayIndexAddr(C, Arr, llvm::APInt(64, Idx, true));
+}
+
+static llvm::Constant *getPrefetchKind(CodeGenFunction &CGF,
+                                       OpenMPPrefetchClauseKind Kind) {
+  llvm::Type *Ty = llvm::Type::getInt32Ty(CGF.CurFn->getContext());
+  switch(Kind) {
+  case OMPC_PREFETCH_read: return llvm::ConstantInt::get(Ty, 0);
+  case OMPC_PREFETCH_write: return llvm::ConstantInt::get(Ty, 1);
+  //case OMPC_PREFETCH_release: return llvm::ConstantInt::get(Ty, 3);
+  default:
+    llvm_unreachable("Invalid prefetch type\n");
+    return nullptr;
+  }
+}
+
+void CodeGenFunction::EmitOMPPrefetchClauses(const OMPLoopDirective &D) {
+  ASTContext &AST = getContext();
+  CaptureMap AllCaptures;
+  CaptureMap::iterator Captured;
+  const ConstantArrayType *ArrTy;
+  Expr *Base, *Start, *End, *StartAddr, *EndAddr;
+  RValue LoweredStart, LoweredEnd;
+  std::vector<llvm::Value *> Params;
+  std::vector<llvm::Type *> ParamTypes;
+  llvm::FunctionType *FnType;
+  llvm::Constant *Prefetch, *Execute;
+
+  bool HasPrefetch = !D.getClausesOfKind(OMPC_prefetch).empty();
+  if(HasPrefetch) {
+    // declare void @popcorn_prefetch(i32, i8*, i8*)
+    ParamTypes = { Int32Ty, Int8PtrTy, Int8PtrTy };
+    FnType = llvm::FunctionType::get(VoidTy, ParamTypes, false);
+    Prefetch = CGM.CreateRuntimeFunction(FnType, "popcorn_prefetch");
+
+    // declare i64 @popcorn_prefetch_execute()
+    ParamTypes.clear();
+    FnType = llvm::FunctionType::get(Int64Ty, ParamTypes, false);
+    Execute = CGM.CreateRuntimeFunction(FnType, "popcorn_prefetch_execute");
+
+    // For each prefetched variable, construct start & end range expressions
+    // and call @popcorn_prefetch
+    CapturedStmt *CS = cast<CapturedStmt>(D.getAssociatedStmt());
+    buildCapturedMap(AST, CS, AllCaptures);
+
+    for(auto &&I = D.getClausesOfKind(OMPC_prefetch); I; ++I) {
+      auto *C = cast<OMPPrefetchClause>(*I);
+      Start = C->getStartOfRange();
+      End = C->getEndOfRange();
+
+      for(auto &V : C->varlists()) {
+        const DeclRefExpr *DR = cast<DeclRefExpr>(V);
+        const VarDecl *VD = cast<VarDecl>(DR->getDecl());
+        Captured = AllCaptures.find(VD);
+
+        // TODO the current mechanism for calculating addresses applies an
+        // "inbounds" tag to the array index addressing expression, but we
+        // don't necessarily know this is true.
+        if(Captured != AllCaptures.end()) {
+          Base = Captured->second;
+          if(Start && End) {
+            // User specified entire range
+            StartAddr = getPrefetchAddr(AST, Base, Start);
+            EndAddr = getPrefetchAddr(AST, Base, End);
+          }
+          else if(Start) {
+            // User specified an expression affine to loop iteration variable
+            // TODO if expression is affine transformation of loop induction
+            // variable, need to re-generate for lower/upper bound variables
+            assert(isa<DeclRefExpr>(Start) &&
+                   "Can't handle transformations on loop variables yet");
+            StartAddr = getPrefetchAddr(AST, Base, D.getLowerBoundVariable());
+            EndAddr = getPrefetchAddr(AST, Base, D.getUpperBoundVariable());
+          }
+          else {
+            // User didn't specify a range, prefetch the entire array (note:
+            // should have type checked it's an array by now).
+            QualType QTy = Base->getType();
+            while(isa<DecayedType>(QTy))
+              QTy = cast<DecayedType>(QTy)->getOriginalType();
+            ArrTy = cast<ConstantArrayType>(QTy);
+            const llvm::APInt &Size = ArrTy->getSize();
+            StartAddr = getArrayIndexAddr(AST, Base, 0);
+            EndAddr = getArrayIndexAddr(AST, Base, Size);
+          }
+
+          LoweredStart = EmitAnyExpr(StartAddr);
+          LoweredEnd = EmitAnyExpr(EndAddr);
+          Params = { getPrefetchKind(*this, C->getPrefetchKind()),
+                     LoweredStart.getScalarVal(),
+                     LoweredEnd.getScalarVal() };
+          EmitCallOrInvoke(Prefetch, Params);
+        }
+        else llvm_unreachable("Invalid prefetch variable");
+      }
+    }
+
+    // Finally, call @popcorn_prefetch_execute to issue requests
+    Params.clear();
+    EmitCallOrInvoke(Execute, Params);
+  }
+}
+
 void CodeGenFunction::EmitOMPSimdInit(const OMPLoopDirective &D) {
   // Walk clauses and process safelen/lastprivate.
   LoopStack.setParallel();
@@ -1137,6 +1296,8 @@ bool CodeGenFunction::EmitOMPWorksharingLoop(const OMPLoopDirective &S) {
         auto LoopExit = getJumpDestInCurrentScope(createBasicBlock("omp.loop.exit"));
         // UB = min(UB, GlobalUB);
         EmitIgnoredExpr(S.getEnsureUpperBound());
+        // Popcorn: emit prefetch function declarations & requests
+        if(S.prefetchingEnabled()) EmitOMPPrefetchClauses(S);
         // IV = LB;
         EmitIgnoredExpr(S.getInit());
         // while (idx <= UB) { BODY; ++idx; }
@@ -2049,6 +2210,7 @@ static void EmitOMPAtomicExpr(CodeGenFunction &CGF, OpenMPClauseKind Kind,
   case OMPC_threadprivate:
   case OMPC_depend:
   case OMPC_mergeable:
+  case OMPC_prefetch:
     llvm_unreachable("Clause is not allowed in 'omp atomic'.");
   }
 }
diff --git a/clang/lib/CodeGen/CMakeLists.txt b/clang/lib/CodeGen/CMakeLists.txt
index 10bda76b6b6..992f33ae263 100644
--- a/clang/lib/CodeGen/CMakeLists.txt
+++ b/clang/lib/CodeGen/CMakeLists.txt
@@ -73,6 +73,8 @@ add_clang_library(clangCodeGen
   MicrosoftCXXABI.cpp
   ModuleBuilder.cpp
   ObjectFilePCHContainerOperations.cpp
+  PopcornUtil.cpp
+  PrefetchBuilder.cpp
   SanitizerMetadata.cpp
   TargetInfo.cpp
 
diff --git a/clang/lib/CodeGen/CodeGenAction.cpp b/clang/lib/CodeGen/CodeGenAction.cpp
index 0e7b6d8a71d..e05c73d9e77 100644
--- a/clang/lib/CodeGen/CodeGenAction.cpp
+++ b/clang/lib/CodeGen/CodeGenAction.cpp
@@ -18,6 +18,7 @@
 #include "clang/CodeGen/BackendUtil.h"
 #include "clang/CodeGen/CodeGenAction.h"
 #include "clang/CodeGen/ModuleBuilder.h"
+#include "clang/CodeGen/PopcornUtil.h"
 #include "clang/Frontend/CompilerInstance.h"
 #include "clang/Frontend/FrontendDiagnostic.h"
 #include "clang/Lex/Preprocessor.h"
@@ -34,6 +35,7 @@
 #include "llvm/Support/MemoryBuffer.h"
 #include "llvm/Support/SourceMgr.h"
 #include "llvm/Support/Timer.h"
+#include "llvm/Transforms/Utils/Cloning.h"
 #include <memory>
 using namespace clang;
 using namespace llvm;
@@ -41,6 +43,7 @@ using namespace llvm;
 namespace clang {
   class BackendConsumer : public ASTConsumer {
     virtual void anchor();
+  protected:
     DiagnosticsEngine &Diags;
     BackendAction Action;
     const CodeGenOptions &CodeGenOpts;
@@ -129,7 +132,7 @@ namespace clang {
         LLVMIRGeneration.stopTimer();
     }
 
-    void HandleTranslationUnit(ASTContext &C) override {
+    void HandleTranslationUnitCommon(ASTContext &C) {
       {
         PrettyStackTraceString CrashInfo("Per-file LLVM IR generation");
         if (llvm::TimePassesIsEnabled)
@@ -165,6 +168,10 @@ namespace clang {
                 [=](const DiagnosticInfo &DI) { linkerDiagnosticHandler(DI); }))
           return;
       }
+    }
+
+    void HandleTranslationUnit(ASTContext &C) override {
+      HandleTranslationUnitCommon(C);
 
       // Install an inline asm handler so that diagnostics get printed through
       // our diagnostics hooks.
@@ -259,8 +266,78 @@ namespace clang {
     void OptimizationFailureHandler(
         const llvm::DiagnosticInfoOptimizationFailure &D);
   };
-  
+
   void BackendConsumer::anchor() {}
+
+  class MultiBackendConsumer : public BackendConsumer {
+  private:
+    virtual void anchor() override;
+    SmallVector<raw_pwrite_stream *, 2> &AsmOutStreams;
+    const SmallVector<std::shared_ptr<TargetOptions>, 2> &AsmTargetOpts;
+    const SmallVector<TargetInfo *, 2> &AsmTargetInfos;
+    const CodeGenOptions &NoOptCodegen;
+  public:
+    MultiBackendConsumer(DiagnosticsEngine &Diags,
+                    const HeaderSearchOptions &HeaderSearchOpts,
+                    const PreprocessorOptions &PPOpts,
+                    const CodeGenOptions &CodeGenOpts,
+                    const CodeGenOptions &NoOptCodegen,
+                    const SmallVector<std::shared_ptr<TargetOptions>, 2> &TargetOpts,
+                    const LangOptions &LangOpts, bool TimePasses,
+                    const std::string &InFile, llvm::Module *LinkModule,
+                    SmallVector<raw_pwrite_stream *, 2> &OSs, LLVMContext &C,
+                    const SmallVector<TargetInfo *, 2> &TargetInfos,
+                    CoverageSourceInfo *CoverageInfo = nullptr)
+        : BackendConsumer(Backend_EmitMultiObj, Diags, HeaderSearchOpts,
+                          PPOpts, CodeGenOpts, *TargetOpts[0], LangOpts,
+                          TimePasses, InFile, LinkModule, OSs[0], C,
+                          CoverageInfo),
+          AsmOutStreams(OSs), AsmTargetOpts(TargetOpts),
+          AsmTargetInfos(TargetInfos), NoOptCodegen(NoOptCodegen) {}
+
+    void HandleTranslationUnit(ASTContext &C) override {
+      HandleTranslationUnitCommon(C);
+
+      // Install an inline asm handler so that diagnostics get printed through
+      // our diagnostics hooks.
+      LLVMContext &Ctx = TheModule->getContext();
+      LLVMContext::InlineAsmDiagHandlerTy OldHandler =
+        Ctx.getInlineAsmDiagnosticHandler();
+      void *OldContext = Ctx.getInlineAsmDiagnosticContext();
+      Ctx.setInlineAsmDiagnosticHandler(InlineAsmDiagHandler, this);
+
+      LLVMContext::DiagnosticHandlerTy OldDiagnosticHandler =
+          Ctx.getDiagnosticHandler();
+      void *OldDiagnosticContext = Ctx.getDiagnosticContext();
+      Ctx.setDiagnosticHandler(DiagnosticHandler, this);
+
+      // Apply IR optimizations, but strip target-specific attributes from
+      // all functions added by analyses
+      std::shared_ptr<TargetOptions> IRTargetOpts =
+        Popcorn::GetPopcornTargetOpts(TheModule->getTargetTriple());
+      ApplyIROptimizations(Diags, CodeGenOpts, *IRTargetOpts, LangOpts,
+                           TheModule.get(), Action, nullptr);
+      Popcorn::StripTargetAttributes(*TheModule);
+
+      // Generate machine code for each target
+      for(size_t i = 0; i < AsmTargetOpts.size(); i++) {
+        llvm::Module* ArchModule = CloneModule(TheModule.get());
+        ArchModule->setTargetTriple(AsmTargetInfos[i]->getTriple().getTriple());
+        ArchModule->setDataLayout(AsmTargetInfos[i]->getTargetDescription());
+        Popcorn::AddArchSpecificTargetFeatures(*ArchModule, AsmTargetOpts[i]);
+        CodegenBackendOutput(Diags, NoOptCodegen, *AsmTargetOpts[i], LangOpts,
+                             AsmTargetInfos[i]->getTargetDescription(),
+                             ArchModule, Action, AsmOutStreams[i]);
+        delete ArchModule;
+      }
+
+      Ctx.setInlineAsmDiagnosticHandler(OldHandler, OldContext);
+
+      Ctx.setDiagnosticHandler(OldDiagnosticHandler, OldDiagnosticContext);
+    }
+  };
+
+  void MultiBackendConsumer::anchor() {}
 }
 
 /// ConvertBackendLocation - Convert a location in a temporary llvm::SourceMgr
@@ -626,19 +703,14 @@ GetOutputStream(CompilerInstance &CI, StringRef InFile, BackendAction Action) {
   case Backend_EmitMCNull:
     return CI.createNullOutputFile();
   case Backend_EmitObj:
+  case Backend_EmitMultiObj:
     return CI.createDefaultOutputFile(true, InFile, "o");
   }
 
   llvm_unreachable("Invalid action!");
 }
 
-std::unique_ptr<ASTConsumer>
-CodeGenAction::CreateASTConsumer(CompilerInstance &CI, StringRef InFile) {
-  BackendAction BA = static_cast<BackendAction>(Act);
-  raw_pwrite_stream *OS = GetOutputStream(CI, InFile, BA);
-  if (BA != Backend_EmitNothing && !OS)
-    return nullptr;
-
+llvm::Module *CodeGenAction::getLinkModuleToUse(CompilerInstance &CI) {
   llvm::Module *LinkModuleToUse = LinkModule;
 
   // If we were not given a link module, and the user requested that one be
@@ -662,6 +734,10 @@ CodeGenAction::CreateASTConsumer(CompilerInstance &CI, StringRef InFile) {
     LinkModuleToUse = ModuleOrErr.get().release();
   }
 
+  return LinkModuleToUse;
+}
+
+CoverageSourceInfo *CodeGenAction::getCoverageInfo(CompilerInstance &CI) {
   CoverageSourceInfo *CoverageInfo = nullptr;
   // Add the preprocessor callback only when the coverage mapping is generated.
   if (CI.getCodeGenOpts().CoverageMapping) {
@@ -669,6 +745,19 @@ CodeGenAction::CreateASTConsumer(CompilerInstance &CI, StringRef InFile) {
     CI.getPreprocessor().addPPCallbacks(
                                     std::unique_ptr<PPCallbacks>(CoverageInfo));
   }
+  return CoverageInfo;
+}
+
+std::unique_ptr<ASTConsumer>
+CodeGenAction::CreateASTConsumer(CompilerInstance &CI, StringRef InFile) {
+  BackendAction BA = static_cast<BackendAction>(Act);
+  raw_pwrite_stream *OS = GetOutputStream(CI, InFile, BA);
+  if (BA != Backend_EmitNothing && !OS)
+    return nullptr;
+
+  llvm::Module *LinkModuleToUse = getLinkModuleToUse(CI);
+  CoverageSourceInfo *CoverageInfo = getCoverageInfo(CI);
+
   std::unique_ptr<BackendConsumer> Result(new BackendConsumer(
       BA, CI.getDiagnostics(), CI.getHeaderSearchOpts(),
       CI.getPreprocessorOpts(), CI.getCodeGenOpts(), CI.getTargetOpts(),
@@ -684,6 +773,42 @@ static void BitcodeInlineAsmDiagHandler(const llvm::SMDiagnostic &SM,
   SM.print(nullptr, llvm::errs());
 }
 
+bool CodeGenAction::ExecuteActionIRCommon(BackendAction &BA,
+                                          CompilerInstance &CI) {
+  bool Invalid;
+  SourceManager &SM = CI.getSourceManager();
+  FileID FID = SM.getMainFileID();
+  llvm::MemoryBuffer *MainFile = SM.getBuffer(FID, &Invalid);
+  if (Invalid)
+    return true;
+
+  llvm::SMDiagnostic Err;
+  TheModule = parseIR(MainFile->getMemBufferRef(), Err, *VMContext);
+  if (!TheModule) {
+    // Translate from the diagnostic info to the SourceManager location if
+    // available.
+    // TODO: Unify this with ConvertBackendLocation()
+    SourceLocation Loc;
+    if (Err.getLineNo() > 0) {
+      assert(Err.getColumnNo() >= 0);
+      Loc = SM.translateFileLineCol(SM.getFileEntryForID(FID),
+                                    Err.getLineNo(), Err.getColumnNo() + 1);
+    }
+
+    // Strip off a leading diagnostic code if there is one.
+    StringRef Msg = Err.getMessage();
+    if (Msg.startswith("error: "))
+      Msg = Msg.substr(7);
+
+    unsigned DiagID =
+        CI.getDiagnostics().getCustomDiagID(DiagnosticsEngine::Error, "%0");
+
+    CI.getDiagnostics().Report(Loc, DiagID) << Msg;
+    return true;
+  }
+  return false;
+}
+
 void CodeGenAction::ExecuteAction() {
   // If this is an IR file, we have to treat it specially.
   if (getCurrentFileKind() == IK_LLVM_IR) {
@@ -693,37 +818,9 @@ void CodeGenAction::ExecuteAction() {
     if (BA != Backend_EmitNothing && !OS)
       return;
 
-    bool Invalid;
-    SourceManager &SM = CI.getSourceManager();
-    FileID FID = SM.getMainFileID();
-    llvm::MemoryBuffer *MainFile = SM.getBuffer(FID, &Invalid);
-    if (Invalid)
+    if(ExecuteActionIRCommon(BA, CI))
       return;
 
-    llvm::SMDiagnostic Err;
-    TheModule = parseIR(MainFile->getMemBufferRef(), Err, *VMContext);
-    if (!TheModule) {
-      // Translate from the diagnostic info to the SourceManager location if
-      // available.
-      // TODO: Unify this with ConvertBackendLocation()
-      SourceLocation Loc;
-      if (Err.getLineNo() > 0) {
-        assert(Err.getColumnNo() >= 0);
-        Loc = SM.translateFileLineCol(SM.getFileEntryForID(FID),
-                                      Err.getLineNo(), Err.getColumnNo() + 1);
-      }
-
-      // Strip off a leading diagnostic code if there is one.
-      StringRef Msg = Err.getMessage();
-      if (Msg.startswith("error: "))
-        Msg = Msg.substr(7);
-
-      unsigned DiagID =
-          CI.getDiagnostics().getCustomDiagID(DiagnosticsEngine::Error, "%0");
-
-      CI.getDiagnostics().Report(Loc, DiagID) << Msg;
-      return;
-    }
     const TargetOptions &TargetOpts = CI.getTargetOpts();
     if (TheModule->getTargetTriple() != TargetOpts.Triple) {
       CI.getDiagnostics().Report(SourceLocation(),
@@ -769,3 +866,150 @@ EmitCodeGenOnlyAction::EmitCodeGenOnlyAction(llvm::LLVMContext *_VMContext)
 void EmitObjAction::anchor() { }
 EmitObjAction::EmitObjAction(llvm::LLVMContext *_VMContext)
   : CodeGenAction(Backend_EmitObj, _VMContext) {}
+
+void EmitMultiObjAction::anchor() { }
+EmitMultiObjAction::EmitMultiObjAction(llvm::LLVMContext *_VMContext)
+  : CodeGenAction(Backend_EmitMultiObj, _VMContext) {}
+
+static std::string AppendArchName(StringRef File, const TargetInfo *TI) {
+  size_t dot = File.rfind('.');
+  std::string NewFile = File.substr(0, dot);
+  NewFile += "_";
+  NewFile += TI->getTriple().getArchName();
+  NewFile += File.substr(dot, StringRef::npos);
+  return NewFile;
+}
+
+static llvm::SmallVector<std::string, 2>
+dedupTargets(llvm::SmallVector<std::string, 2> &Requested) {
+  std::set<std::string> Targets;
+  for(auto &Target : Requested) Targets.insert(Target);
+  llvm::SmallVector<std::string, 2> Ret;
+  for(auto Target : Targets) Ret.push_back(Target);
+  return Ret;
+}
+
+/// Create target information & output streams for each target.
+bool EmitMultiObjAction::InitializeTargets(CompilerInstance &CI,
+                                           StringRef InFile) {
+  // Note: remove output filename from frontend args, as it will override any
+  // special names we try to specify.
+  std::string OutFile(InFile);
+  if(CI.getFrontendOpts().OutputFile != "") {
+    OutFile = CI.getFrontendOpts().OutputFile;
+    CI.getFrontendOpts().OutputFile = "";
+  }
+
+  // Populate list of targets requested.  If none are requested, default to all
+  // supported targets.
+  std::vector<std::string> &Requested = CI.getCodeGenOpts().PopcornTargets;
+  if(Requested.size()) {
+    for(auto &Target : Requested) {
+      if(!Popcorn::SupportedTarget(Target)) {
+        std::string Msg("Popcorn: unsupported target '" + Target + "'");
+        unsigned DiagID =
+            CI.getDiagnostics().getCustomDiagID(DiagnosticsEngine::Error, "%0");
+        CI.getDiagnostics().Report(DiagID) << Msg;
+        return false;
+      }
+      Targets.push_back(Target);
+    }
+    Targets = dedupTargets(Targets);
+  }
+  else Popcorn::GetAllTargets(this->Targets);
+
+  BackendAction BA = Backend_EmitMultiObj;
+  if(Targets.size() > 1) {
+    for(size_t i = 0; i < Targets.size(); i++) {
+      std::shared_ptr<TargetOptions> Opts = Popcorn::GetPopcornTargetOpts(Targets[i]);
+      TargetOpts.push_back(Opts);
+      TargetInfos.push_back(TargetInfo::CreateTargetInfo(CI.getDiagnostics(), Opts));
+
+      raw_pwrite_stream *OS;
+      OS = GetOutputStream(CI, AppendArchName(OutFile, TargetInfos[i]), BA);
+      if (!OS) return false;
+      OutFiles.push_back(OS);
+    }
+  }
+  else {
+    std::shared_ptr<TargetOptions> Opts = Popcorn::GetPopcornTargetOpts(Targets[0]);
+    TargetOpts.push_back(Opts);
+    TargetInfos.push_back(TargetInfo::CreateTargetInfo(CI.getDiagnostics(), Opts));
+
+    raw_pwrite_stream *OS = GetOutputStream(CI, OutFile, BA);
+    if (!OS) return false;
+    OutFiles.push_back(OS);
+  }
+  CI.getFrontendOpts().OutputFile = OutFile;
+
+  return true;
+}
+
+/// Initialize target information & open output streams for each target.
+std::unique_ptr<ASTConsumer>
+EmitMultiObjAction::CreateASTConsumer(CompilerInstance &CI,
+                                      StringRef InFile) {
+  if(!InitializeTargets(CI, InFile)) return nullptr;
+  llvm::Module *LinkModuleToUse = getLinkModuleToUse(CI);
+  CoverageSourceInfo *CoverageInfo = getCoverageInfo(CI);
+
+  // Create a MultiBackendConsumer, which is identical to a BackendConsumer
+  // except that it runs the generated IR through multiple backends.
+  std::unique_ptr<MultiBackendConsumer> Result(new MultiBackendConsumer(
+      CI.getDiagnostics(), CI.getHeaderSearchOpts(),
+      CI.getPreprocessorOpts(), CI.getCodeGenOpts(), CI.getCodeGenNoOpts(),
+      TargetOpts, CI.getLangOpts(), CI.getFrontendOpts().ShowTimers, InFile,
+      LinkModuleToUse, OutFiles, *VMContext, TargetInfos, CoverageInfo));
+  BEConsumer = Result.get();
+  return std::move(Result);
+}
+
+void EmitMultiObjAction::ExecuteAction() {
+  // If this is an IR file, we have to treat it specially.
+  if (getCurrentFileKind() == IK_LLVM_IR) {
+    // Initialize targets here because we never called CreateASTConsumer
+    CompilerInstance &CI = getCompilerInstance();
+    StringRef OutFile = getCurrentFile();
+    BackendAction BA = Backend_EmitMultiObj;
+
+    if(!InitializeTargets(CI, OutFile)) {
+      // TODO add diagnostics saying we couldn't initialize targets
+      return;
+    }
+
+    if(ExecuteActionIRCommon(BA, CI)) {
+      // TODO add diagnostics saying we couldn't do common IR work
+      return;
+    }
+
+    // Apply IR optimizations, but strip target-specific attributes from all
+    // functions added by analyses
+    std::shared_ptr<TargetOptions> IRTargetOpts =
+      Popcorn::GetPopcornTargetOpts(TheModule->getTargetTriple());
+    ApplyIROptimizations(CI.getDiagnostics(), CI.getCodeGenOpts(),
+                         *IRTargetOpts, CI.getLangOpts(),
+                         TheModule.get(), BA, nullptr);
+    Popcorn::StripTargetAttributes(*TheModule);
+
+    // Emit machine code for all specified architectures
+    for(size_t i = 0; i < Targets.size(); i++) {
+      llvm::Module *ArchModule = CloneModule(TheModule.get());
+      ArchModule->setTargetTriple(Targets[i]);
+      ArchModule->setDataLayout(TargetInfos[i]->getTargetDescription());
+      Popcorn::AddArchSpecificTargetFeatures(*ArchModule, TargetOpts[i]);
+      LLVMContext &Ctx = ArchModule->getContext();
+      Ctx.setInlineAsmDiagnosticHandler(BitcodeInlineAsmDiagHandler);
+      CodegenBackendOutput(CI.getDiagnostics(), CI.getCodeGenNoOpts(),
+                           *TargetOpts[i], CI.getLangOpts(),
+                           TargetInfos[i]->getTargetDescription(),
+                           ArchModule, BA, OutFiles[i]);
+      delete ArchModule;
+    }
+
+    return;
+  }
+
+  // Otherwise follow the normal AST path.
+  this->ASTFrontendAction::ExecuteAction();
+}
+
diff --git a/clang/lib/CodeGen/CodeGenFunction.h b/clang/lib/CodeGen/CodeGenFunction.h
index f2bc402f8b2..dfc3ccb02fe 100644
--- a/clang/lib/CodeGen/CodeGenFunction.h
+++ b/clang/lib/CodeGen/CodeGenFunction.h
@@ -913,6 +913,15 @@ private:
   llvm::MDNode *createProfileWeightsForLoop(const Stmt *Cond,
                                             uint64_t LoopCount);
 
+  /// \brief Locals that were offloaded to global memory for a captured
+  /// statement and need to be restored after the statement has executed.
+  /// First declaration in the pair is the local, second is the global.
+  /// Maintain a mapping of captured statements & their offloaded locals.
+  typedef std::pair<ValueDecl *, ValueDecl *> OffloadPair;
+  typedef llvm::SmallVector<OffloadPair, 4> OffloadList;
+  typedef llvm::DenseMap<const CapturedStmt *, OffloadList> OffloadMap;
+  OffloadMap OffloadedLocals;
+
 public:
   /// Increment the profiler's counter for the given statement.
   void incrementProfileCounter(const Stmt *S) {
@@ -2084,6 +2093,17 @@ public:
   void EmitCXXForRangeStmt(const CXXForRangeStmt &S,
                            ArrayRef<const Attr *> Attrs = None);
 
+  /// \brief Offload/restore capabilities for local variables captured in
+  /// capture statements.
+  void addOffloaded(const CapturedStmt *S, ValueDecl *L, ValueDecl *G);
+  const OffloadList &getOffloaded(const CapturedStmt *S) const;
+  VarDecl *CreateOffloadedGlobal(const Stmt &S, QualType Ty, std::string &Name);
+  VarDecl *CreateOffloadedGlobal(const Stmt &S, const Expr *I);
+  LValue GetVarDeclLValue(QualType SrcType, VarDecl *VD);
+  LValue GetVarDeclLValue(const Expr *I, VarDecl *VD);
+  DeclRefExpr *GetDeclRefForOffload(ValueDecl *VD);
+  void RestoreOffloadedLocals(const CapturedStmt *S);
+
   LValue InitCapturedStruct(const CapturedStmt &S);
   llvm::Function *EmitCapturedStmt(const CapturedStmt &S, CapturedRegionKind K);
   void GenerateCapturedStmtFunctionProlog(const CapturedStmt &S);
@@ -2191,6 +2211,10 @@ public:
   ///
   /// \param D Directive (possibly) with the 'linear' clause.
   void EmitOMPLinearClauseInit(const OMPLoopDirective &D);
+  /// \brief Emit prefetching requests for loop directive.
+  ///
+  /// \param D Directive (possibly) with the 'prefetch' clause.
+  void EmitOMPPrefetchClauses(const OMPLoopDirective &D);
 
   void EmitOMPParallelDirective(const OMPParallelDirective &S);
   void EmitOMPSimdDirective(const OMPSimdDirective &S);
diff --git a/clang/lib/CodeGen/PopcornUtil.cpp b/clang/lib/CodeGen/PopcornUtil.cpp
new file mode 100644
index 00000000000..9bda4e2e11f
--- /dev/null
+++ b/clang/lib/CodeGen/PopcornUtil.cpp
@@ -0,0 +1,103 @@
+//===--- PopcornUtil.cpp - LLVM Popcorn Linux Utilities -------------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include <clang/CodeGen/PopcornUtil.h>
+#include <llvm/ADT/Triple.h>
+#include <llvm/ADT/SmallVector.h>
+
+using namespace clang;
+using namespace llvm;
+
+const static std::vector<std::string> PopcornSupported = {
+  "aarch64-linux-gnu",
+  "x86_64-linux-gnu"
+};
+
+bool Popcorn::SupportedTarget(const StringRef Target) {
+  for(auto SupportedTarget : PopcornSupported)
+    if(Target == SupportedTarget) return true;
+  return false;
+}
+
+void Popcorn::GetAllTargets(SmallVector<std::string, 2> &Targets) {
+  Targets.clear();
+  for(auto Target : PopcornSupported) Targets.push_back(Target);
+}
+
+typedef std::shared_ptr<TargetOptions> TargetOptionsPtr;
+
+TargetOptionsPtr Popcorn::GetPopcornTargetOpts(const StringRef TripleStr) {
+  Triple Triple(Triple::normalize(TripleStr));
+  assert(!Triple.getTriple().empty() && "Invalid target triple");
+
+  TargetOptionsPtr Opts(new TargetOptions);
+  Opts->Triple = Triple.getTriple();
+  Opts->ABI = "";
+  Opts->FPMath = "";
+  Opts->FeaturesAsWritten.clear();
+  Opts->LinkerVersion = "";
+  Opts->Reciprocals.clear();
+
+  // TODO need to make CPU selectable & add target features according to CPU
+
+  switch(Triple.getArch()) {
+  case Triple::ArchType::aarch64:
+    Opts->ABI = "aapcs";
+    Opts->CPU = "generic";
+    Opts->FeaturesAsWritten.push_back("+neon");
+    break;
+  case Triple::ArchType::x86_64:
+    Opts->CPU = "x86-64";
+    Opts->FPMath = "sse";
+    Opts->FeaturesAsWritten.push_back("+sse");
+    Opts->FeaturesAsWritten.push_back("+sse2");
+    Opts->FeaturesAsWritten.push_back("+rtm");
+    break;
+  default: llvm_unreachable("Triple not currently supported on Popcorn");
+  }
+
+  return Opts;
+}
+
+void Popcorn::StripTargetAttributes(Module &M) {
+  /// Target-specific function attributes
+  static SmallVector<std::string, 2> TargetAttributes = {
+    "target-cpu",
+    "target-features"
+  };
+
+  for(Function &F : M) {
+    AttrBuilder AB(F.getAttributes(), AttributeSet::FunctionIndex);
+    for(std::string &Attr : TargetAttributes) {
+      if(F.hasFnAttribute(Attr))
+        AB.removeAttribute(Attr);
+    }
+    F.setAttributes(
+      AttributeSet::get(F.getContext(), AttributeSet::FunctionIndex, AB));
+  }
+}
+
+void Popcorn::AddArchSpecificTargetFeatures(Module &M,
+                                            TargetOptionsPtr TargetOpts) {
+  static const char *TF = "target-features";
+  std::string AllFeatures("");
+
+  for(auto &Feature : TargetOpts->FeaturesAsWritten)
+    AllFeatures += Feature + ",";
+  AllFeatures = AllFeatures.substr(0, AllFeatures.length() - 1);
+
+  for(Function &F : M) {
+    AttrBuilder AB(F.getAttributes(), AttributeSet::FunctionIndex);
+    assert(!F.hasFnAttribute(TF) && "Target features weren't stripped");
+    AB.addAttribute(TF, AllFeatures);
+    F.setAttributes(
+      AttributeSet::get(F.getContext(), AttributeSet::FunctionIndex, AB));
+  }
+}
+
diff --git a/clang/lib/CodeGen/PrefetchBuilder.cpp b/clang/lib/CodeGen/PrefetchBuilder.cpp
new file mode 100644
index 00000000000..c5413a8e9d2
--- /dev/null
+++ b/clang/lib/CodeGen/PrefetchBuilder.cpp
@@ -0,0 +1,111 @@
+//=- Prefetch.cpp - Prefetching Analysis for Structured Blocks -----------*-==//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements prefetching analysis for structured blocks.  The
+// analysis traverses the AST to determine how arrays are accessed in structured
+// blocks and generates expressions defining ranges of elements accessed.
+//
+//===----------------------------------------------------------------------===//
+
+#include "clang/CodeGen/PrefetchBuilder.h"
+#include "llvm/IR/CallSite.h"
+#include "llvm/Support/Debug.h"
+
+using namespace clang;
+
+//===----------------------------------------------------------------------===//
+// Prefetch builder API
+//
+
+void PrefetchBuilder::EmitPrefetchCallDeclarations() {
+  using namespace clang::CodeGen;
+  std::vector<llvm::Type *> ParamTypes;
+  llvm::FunctionType *FnType;
+
+  // declare void @popcorn_prefetch(i32, i8*, i8*)
+  ParamTypes = { CGF.Int32Ty, CGF.Int8PtrTy, CGF.Int8PtrTy };
+  FnType = llvm::FunctionType::get(CGF.VoidTy, ParamTypes, false);
+  Prefetch = CGM.CreateRuntimeFunction(FnType, "popcorn_prefetch");
+
+  // declare i64 @popcorn_prefetch_execute()
+  ParamTypes.clear();
+  FnType = llvm::FunctionType::get(CGF.Int64Ty, ParamTypes, false);
+  Execute = CGM.CreateRuntimeFunction(FnType, "popcorn_prefetch_execute");
+}
+
+static llvm::Constant *getPrefetchKind(CodeGen::CodeGenFunction &CGF,
+                                       enum PrefetchRange::Type Perm) {
+  llvm::Type *Ty = llvm::Type::getInt32Ty(CGF.CurFn->getContext());
+  switch(Perm) {
+  case PrefetchRange::Read: return llvm::ConstantInt::get(Ty, 0);
+  case PrefetchRange::Write: return llvm::ConstantInt::get(Ty, 1);
+  default: llvm_unreachable("Invalid prefetch type\n"); return nullptr;
+  }
+}
+
+Expr *PrefetchBuilder::buildArrayIndex(VarDecl *Base, Expr *Subscript) {
+  // Build DeclRefExpr for variable representing base
+  QualType Ty = Base->getType(), ElemTy;
+  DeclRefExpr *DRE = DeclRefExpr::Create(Ctx, NestedNameSpecifierLoc(),
+                                         SourceLocation(), Base, false,
+                                         Base->getSourceRange().getBegin(),
+                                         Ty, VK_LValue);
+
+  // Get an array subscript, e.g., arr[idx]
+  Ty = Ty.getDesugaredType(Ctx);
+  if(isa<ArrayType>(Ty)) ElemTy = cast<ArrayType>(ElemTy)->getElementType();
+  else ElemTy = cast<PointerType>(Ty)->getPointeeType();
+  return new (Ctx) ArraySubscriptExpr(DRE, Subscript, ElemTy, VK_RValue,
+                                      OK_Ordinary, SourceLocation());
+}
+
+Expr *PrefetchBuilder::buildAddrOf(Expr *ArrSub) {
+  // Get the address of the array index, e.g., &arr[idx]
+  QualType RePtrTy = Ctx.getPointerType(ArrSub->getType());
+  UnaryOperator *Addr = new (Ctx) UnaryOperator(ArrSub, UO_AddrOf, RePtrTy,
+                                                VK_LValue, OK_Ordinary,
+                                                SourceLocation());
+
+  // Finally, cast it to a void *, e.g., (void *)&arr[idx]
+  QualType VoidPtrTy = Ctx.getPointerType(Ctx.VoidTy.withConst());
+  return ImplicitCastExpr::Create(Ctx, VoidPtrTy, CK_BitCast, Addr, nullptr,
+                                  VK_RValue);
+}
+
+void PrefetchBuilder::EmitPrefetchCall(const PrefetchRange &P) {
+  Expr *StartAddr, *EndAddr;
+  CodeGen::RValue LoweredStart, LoweredEnd;
+  std::vector<llvm::Value *> Params;
+  VarDecl *Array = P.getArray();
+
+  // TODO this assumes we're only prefetching arrays!
+
+  StartAddr = P.getStart();
+  if(!isa<ArraySubscriptExpr>(StartAddr))
+    StartAddr = buildArrayIndex(Array, StartAddr);
+  StartAddr = buildAddrOf(StartAddr);
+
+  EndAddr = P.getEnd();
+  if(!isa<ArraySubscriptExpr>(EndAddr))
+    EndAddr = buildArrayIndex(Array, EndAddr);
+  EndAddr = buildAddrOf(EndAddr);
+
+  LoweredStart = CGF.EmitAnyExpr(StartAddr);
+  LoweredEnd = CGF.EmitAnyExpr(EndAddr);
+  Params = { getPrefetchKind(CGF, P.getType()),
+             LoweredStart.getScalarVal(),
+             LoweredEnd.getScalarVal() };
+  CGF.EmitCallOrInvoke(Prefetch, Params);
+}
+
+void PrefetchBuilder::EmitPrefetchExecuteCall() {
+  std::vector<llvm::Value *> Params;
+  CGF.EmitCallOrInvoke(Execute, Params);
+}
+
diff --git a/clang/lib/Driver/Tools.cpp b/clang/lib/Driver/Tools.cpp
index b801705a8f5..a7c21251099 100644
--- a/clang/lib/Driver/Tools.cpp
+++ b/clang/lib/Driver/Tools.cpp
@@ -4934,6 +4934,49 @@ void Clang::ConstructJob(Compilation &C, const JobAction &JA,
       A->render(Args, CmdArgs);
   }
 
+  // Forward Popcorn & other standard compiler flags to -cc1 to generate
+  // multi-ISA binaries
+  if(Args.hasArg(options::OPT_popcorn_migratable) ||
+     Args.hasArg(options::OPT_popcorn_metadata)) {
+    // Full-blown Popcorn migration capabilities, including adding migration
+    // points, symbol alignment and generating stack transformation metadata
+    CmdArgs.push_back("-ffunction-sections");
+    CmdArgs.push_back("-fdata-sections");
+    CmdArgs.push_back("-popcorn-alignment");
+    CmdArgs.push_back("-popcorn-migratable");
+    CmdArgs.push_back("-mllvm");
+    if(Args.hasArg(options::OPT_popcorn_migratable))
+      CmdArgs.push_back("-popcorn-instrument=migration");
+    else
+      CmdArgs.push_back("-popcorn-instrument=metadata");
+    CmdArgs.push_back("-mllvm");
+    CmdArgs.push_back("-optimize-regalloc");
+    CmdArgs.push_back("-mllvm");
+    CmdArgs.push_back("-fast-isel=false");
+    for(auto Target : Args.getAllArgValues(options::OPT_popcorn_target)) {
+      std::string combined("-popcorn-target=" + Target);
+      CmdArgs.push_back(Args.MakeArgString(combined));
+    }
+  }
+  else if(Args.hasArg(options::OPT_popcorn_libc)) {
+    // Symbol alignment for libc & generate stack transformation metadata for
+    // libc thread start functions
+    CmdArgs.push_back("-ffunction-sections");
+    CmdArgs.push_back("-fdata-sections");
+    CmdArgs.push_back("-popcorn-alignment");
+    CmdArgs.push_back("-mllvm");
+    CmdArgs.push_back("-popcorn-instrument=libc");
+  }
+  else if(Args.hasArg(options::OPT_popcorn_alignment)) {
+    // Only symbol alignment
+    CmdArgs.push_back("-ffunction-sections");
+    CmdArgs.push_back("-fdata-sections");
+    CmdArgs.push_back("-popcorn-alignment");
+  }
+
+  if(Args.hasArg(options::OPT_distributed_omp))
+    CmdArgs.push_back("-distributed-omp");
+
   // With -save-temps, we want to save the unoptimized bitcode output from the
   // CompileJobAction, so disable optimizations if they are not already
   // disabled.
diff --git a/clang/lib/Frontend/CompilerInvocation.cpp b/clang/lib/Frontend/CompilerInvocation.cpp
index fbeba09e1cf..0f01e69f07b 100644
--- a/clang/lib/Frontend/CompilerInvocation.cpp
+++ b/clang/lib/Frontend/CompilerInvocation.cpp
@@ -358,22 +358,11 @@ static void parseSanitizerKinds(StringRef FlagName,
 
 static bool ParseCodeGenArgs(CodeGenOptions &Opts, ArgList &Args, InputKind IK,
                              DiagnosticsEngine &Diags,
-                             const TargetOptions &TargetOpts) {
+                             const TargetOptions &TargetOpts,
+                             unsigned OptimizationLevel) {
   using namespace options;
   bool Success = true;
 
-  unsigned OptimizationLevel = getOptimizationLevel(Args, IK, Diags);
-  // TODO: This could be done in Driver
-  unsigned MaxOptLevel = 3;
-  if (OptimizationLevel > MaxOptLevel) {
-    // If the optimization level is not supported, fall back on the default
-    // optimization
-    Diags.Report(diag::warn_drv_optimization_value)
-        << Args.getLastArg(OPT_O)->getAsString(Args) << "-O" << MaxOptLevel;
-    OptimizationLevel = MaxOptLevel;
-  }
-  Opts.OptimizationLevel = OptimizationLevel;
-
   // We must always run at least the always inlining pass.
   Opts.setInlining(
     (Opts.OptimizationLevel > 1) ? CodeGenOptions::NormalInlining
@@ -675,9 +664,41 @@ static bool ParseCodeGenArgs(CodeGenOptions &Opts, ArgList &Args, InputKind IK,
   Opts.CudaGpuBinaryFileNames =
       Args.getAllArgValues(OPT_fcuda_include_gpubinary);
 
+  Opts.PopcornAlignment = Args.hasArg(OPT_popcorn_alignment);
+  Opts.PopcornMigratable = Args.hasArg(OPT_popcorn_migratable);
+
+  // TODO: the Popcorn compiler doesn't currently support vectorization
+  if(Opts.PopcornMigratable || Args.hasArg(OPT_popcorn_libc)) {
+    Opts.VectorizeBB = 0;
+    Opts.VectorizeLoop = 0;
+    Opts.VectorizeSLP = 0;
+  }
+
+  if(Opts.PopcornMigratable)
+    for(auto Target : Args.getAllArgValues(OPT_popcorn_target))
+      Opts.PopcornTargets.push_back(Target);
+
   return Success;
 }
 
+static bool ParseCodeGenArgs(CodeGenOptions &Opts, ArgList &Args, InputKind IK,
+                             DiagnosticsEngine &Diags,
+                             const TargetOptions &TargetOpts) {
+  unsigned OptimizationLevel = getOptimizationLevel(Args, IK, Diags);
+  // TODO: This could be done in Driver
+  unsigned MaxOptLevel = 3;
+  if (OptimizationLevel > MaxOptLevel) {
+    // If the optimization level is not supported, fall back on the default
+    // optimization
+    Diags.Report(diag::warn_drv_optimization_value)
+        << Args.getLastArg(OPT_O)->getAsString(Args) << "-O" << MaxOptLevel;
+    OptimizationLevel = MaxOptLevel;
+  }
+  Opts.OptimizationLevel = OptimizationLevel;
+  return ParseCodeGenArgs(Opts, Args, IK, Diags, TargetOpts,
+                          OptimizationLevel);
+}
+
 static void ParseDependencyOutputArgs(DependencyOutputOptions &Opts,
                                       ArgList &Args) {
   using namespace options;
@@ -856,7 +877,11 @@ static InputKind ParseFrontendArgs(FrontendOptions &Opts, ArgList &Args,
     case OPT_emit_codegen_only:
       Opts.ProgramAction = frontend::EmitCodeGenOnly; break;
     case OPT_emit_obj:
-      Opts.ProgramAction = frontend::EmitObj; break;
+      if(Args.hasArg(OPT_popcorn_migratable))
+        Opts.ProgramAction = frontend::EmitMultiObj;
+      else
+        Opts.ProgramAction = frontend::EmitObj;
+      break;
     case OPT_fixit_EQ:
       Opts.FixItSuffix = A->getValue();
       // fall-through!
@@ -1704,6 +1729,8 @@ static void ParseLangArgs(LangOptions &Opts, ArgList &Args, InputKind IK,
   Opts.SanitizeAddressFieldPadding =
       getLastArgIntValue(Args, OPT_fsanitize_address_field_padding, 0, Diags);
   Opts.SanitizerBlacklistFiles = Args.getAllArgValues(OPT_fsanitize_blacklist);
+
+  Opts.DistributedOmp = Args.hasArg(OPT_distributed_omp);
 }
 
 static void ParsePreprocessorArgs(PreprocessorOptions &Opts, ArgList &Args,
@@ -1803,6 +1830,7 @@ static void ParsePreprocessorOutputArgs(PreprocessorOutputOptions &Opts,
   case frontend::EmitLLVMOnly:
   case frontend::EmitCodeGenOnly:
   case frontend::EmitObj:
+  case frontend::EmitMultiObj:
   case frontend::FixIt:
   case frontend::GenerateModule:
   case frontend::GeneratePCH:
@@ -1890,6 +1918,11 @@ bool CompilerInvocation::CreateFromArgs(CompilerInvocation &Res,
   ParseTargetArgs(Res.getTargetOpts(), Args);
   Success &= ParseCodeGenArgs(Res.getCodeGenOpts(), Args, DashX, Diags,
                               Res.getTargetOpts());
+  // TODO Popcorn: until we can limit optimizations across migration points, we
+  // need to turn off backend optimizations
+  if(Args.hasArg(OPT_popcorn_migratable) || Args.hasArg(OPT_popcorn_metadata))
+    Success &= ParseCodeGenArgs(Res.getCodeGenNoOpts(), Args, DashX, Diags,
+                                Res.getTargetOpts(), 0);
   ParseHeaderSearchArgs(Res.getHeaderSearchOpts(), Args);
   if (DashX != IK_AST && DashX != IK_LLVM_IR) {
     ParseLangArgs(*Res.getLangOpts(), Args, DashX, Diags);
diff --git a/clang/lib/FrontendTool/ExecuteCompilerInvocation.cpp b/clang/lib/FrontendTool/ExecuteCompilerInvocation.cpp
index 79cf0049a7b..2c001a318eb 100644
--- a/clang/lib/FrontendTool/ExecuteCompilerInvocation.cpp
+++ b/clang/lib/FrontendTool/ExecuteCompilerInvocation.cpp
@@ -50,6 +50,7 @@ static FrontendAction *CreateFrontendBaseAction(CompilerInstance &CI) {
   case EmitLLVMOnly:           return new EmitLLVMOnlyAction();
   case EmitCodeGenOnly:        return new EmitCodeGenOnlyAction();
   case EmitObj:                return new EmitObjAction();
+  case EmitMultiObj:           return new EmitMultiObjAction();
   case FixIt:                  return new FixItAction();
   case GenerateModule:         return new GenerateModuleAction;
   case GeneratePCH:            return new GeneratePCHAction;
diff --git a/clang/lib/Parse/CMakeLists.txt b/clang/lib/Parse/CMakeLists.txt
index b868696eb6b..241be63f028 100644
--- a/clang/lib/Parse/CMakeLists.txt
+++ b/clang/lib/Parse/CMakeLists.txt
@@ -23,6 +23,7 @@ add_clang_library(clangParse
 
   LINK_LIBS
   clangAST
+  clangAnalysis
   clangBasic
   clangLex
   clangSema
diff --git a/clang/lib/Parse/ParseOpenMP.cpp b/clang/lib/Parse/ParseOpenMP.cpp
index 0113a3157c2..73a993ef628 100644
--- a/clang/lib/Parse/ParseOpenMP.cpp
+++ b/clang/lib/Parse/ParseOpenMP.cpp
@@ -14,6 +14,7 @@
 #include "RAIIObjectsForParser.h"
 #include "clang/AST/ASTConsumer.h"
 #include "clang/AST/ASTContext.h"
+#include "clang/AST/RecursiveASTVisitor.h"
 #include "clang/AST/StmtOpenMP.h"
 #include "clang/Parse/ParseDiagnostic.h"
 #include "clang/Parse/Parser.h"
@@ -25,6 +26,103 @@ using namespace clang;
 // OpenMP declarative directives.
 //===----------------------------------------------------------------------===//
 
+/// \brief Helper class to find variables declared or used in a statement's
+/// sub-tree.
+class VarFinder : public RecursiveASTVisitor<VarFinder> {
+public:
+  void reset() { Variables.clear(); }
+
+  bool VisitDeclStmt(DeclStmt *D) {
+    for(auto &Child : D->getDeclGroup()) {
+      if(isa<VarDecl>(Child))
+        Variables.insert(cast<VarDecl>(Child));
+    }
+    return true;
+  }
+
+  bool VisitDeclRefExpr(DeclRefExpr *D) {
+    ValueDecl *VD = D->getDecl();
+    if(isa<VarDecl>(VD)) Variables.insert(cast<VarDecl>(VD));
+    return true;
+  }
+
+  bool VisitVarDecl(VarDecl *D) { Variables.insert(D); return true; }
+
+  const llvm::SmallPtrSet<const VarDecl *, 2> &
+  getDeclaredOrReferencedVars() const { return Variables; }
+
+private:
+  llvm::SmallPtrSet<const VarDecl *, 2> Variables;
+};
+
+void Parser::CheckOpenMPPrefetchClauses(StmtResult Directive) {
+  VarFinder CapturedVarFinder, LoopVarFinder, UseFinder;
+
+  OMPLoopDirective *D = dyn_cast<OMPLoopDirective>(Directive.get());
+  if(D) {
+    // OpenMP Standard 4.5, Section 2.6:
+    // A loop has canonical loop form if it conforms to the following:
+    //    for (init-expr; test-expr; incr-expr) structured-block
+    //      init-expr     One of the following:
+    //                    var = lb
+    //                    integer-type var = lb
+    //                    random-access-iterator-type var = lb
+    //                    pointer-type var = lb
+    //
+    // Thus we can examine the initialization statement to find the loop
+    // iteration variable.
+    CapturedStmt *Captured = cast<CapturedStmt>(D->getAssociatedStmt());
+    ForStmt *For = cast<ForStmt>(Captured->getCapturedStmt());
+
+    // Find captured & loop iteration variables
+    CapturedVarFinder.TraverseStmt(Captured);
+    const llvm::SmallPtrSet<const VarDecl *, 2> &CapturedVars =
+      CapturedVarFinder.getDeclaredOrReferencedVars();
+    LoopVarFinder.TraverseStmt(For->getInit());
+    const llvm::SmallPtrSet<const VarDecl *, 2> &LoopVars =
+      LoopVarFinder.getDeclaredOrReferencedVars();
+
+    for(auto &&I = D->getClausesOfKind(OMPC_prefetch); I; ++I) {
+      auto *C = cast<OMPPrefetchClause>(*I);
+
+      if(C->getPrefetchKind() == OMPC_PREFETCH_smart &&
+         !For->prefetchEnabled()) {
+        // Analyse for-loop to determine what variables to prefetch
+        ASTContext &Ctx = getActions().getASTContext();
+        For->setPrefetchEnabled(true);
+        PrefetchAnalysis PA(&Ctx, For);
+        PA.analyzeStmt();
+        PA.calculatePrefetchRanges();
+        // TODO remove
+        PA.dump();
+        Ctx.addPrefetchAnalysis(For, PA);
+      }
+      else {
+        // Verify we're only prefetching captured variables
+        for(auto E : C->varlists()) {
+          UseFinder.reset();
+          // TODO this cast is nasty
+          UseFinder.TraverseStmt((Expr *)E);
+          for(const auto &V : UseFinder.getDeclaredOrReferencedVars())
+            if(!CapturedVars.count(V))
+              Diag(E->getExprLoc(), diag::err_omp_invalid_prefetch_capture);
+        }
+  
+        // Verify loop iteration variable use
+        Expr *Start = C->getStartOfRange(), *End = C->getEndOfRange();
+        if(Start && !End) {
+          UseFinder.reset();
+          UseFinder.TraverseStmt(Start);
+          for(const auto &V : UseFinder.getDeclaredOrReferencedVars())
+            if(!LoopVars.count(V))
+              Diag(C->getFirstColonLoc(),
+                   diag::err_omp_invalid_prefetch_loop_var);
+        }
+      }
+    }
+  }
+}
+
 static OpenMPDirectiveKind ParseOpenMPDirectiveKind(Parser &P) {
   // Array of foldings: F[i][0] F[i][1] ===> F[i][2].
   // E.g.: OMPD_for OMPD_simd ===> OMPD_for_simd
@@ -294,6 +392,21 @@ Parser::ParseOpenMPDeclarativeOrExecutableDirective(bool StandAloneAllowed) {
           DKind, DirName, CancelRegion, Clauses, AssociatedStmt.get(), Loc,
           EndLoc);
 
+    // Enable optimizations for Popcorn Linux, if requested.
+    if(PP.getLangOpts().DistributedOmp) {
+      if((DKind == OMPD_parallel || DKind == OMPD_parallel_for ||
+          DKind == OMPD_parallel_for_simd || DKind == OMPD_parallel_sections)) {
+        CapturedStmt *CS = cast<CapturedStmt>(AssociatedStmt.get());
+        CS->setOffloadShared(true);
+      }
+
+      if(DKind == OMPD_for || DKind == OMPD_parallel_for ||
+         DKind == OMPD_parallel_for_simd) {
+        cast<OMPExecutableDirective>(Directive.get())->setPrefetching(true);
+        CheckOpenMPPrefetchClauses(Directive);
+      }
+    }
+
     // Exit scope.
     Actions.EndOpenMPDSABlock(Directive.get());
     OMPDirectiveScope.Exit();
@@ -385,7 +498,7 @@ bool Parser::ParseOpenMPSimpleVarList(OpenMPDirectiveKind Kind,
 ///       lastprivate-clause | reduction-clause | proc_bind-clause |
 ///       schedule-clause | copyin-clause | copyprivate-clause | untied-clause |
 ///       mergeable-clause | flush-clause | read-clause | write-clause |
-///       update-clause | capture-clause | seq_cst-clause
+///       update-clause | capture-clause | seq_cst-clause | prefetch-clause
 ///
 OMPClause *Parser::ParseOpenMPClause(OpenMPDirectiveKind DKind,
                                      OpenMPClauseKind CKind, bool FirstClause) {
@@ -479,6 +592,7 @@ OMPClause *Parser::ParseOpenMPClause(OpenMPDirectiveKind DKind,
   case OMPC_copyprivate:
   case OMPC_flush:
   case OMPC_depend:
+  case OMPC_prefetch:
     Clause = ParseOpenMPVarListClause(CKind);
     break;
   case OMPC_unknown:
@@ -616,7 +730,7 @@ OMPClause *Parser::ParseOpenMPSingleExprWithArgClause(OpenMPClauseKind Kind) {
 
   if (Kind == OMPC_schedule &&
       (Type == OMPC_SCHEDULE_static || Type == OMPC_SCHEDULE_dynamic ||
-       Type == OMPC_SCHEDULE_guided) &&
+       Type == OMPC_SCHEDULE_guided || Type == OMPC_SCHEDULE_hetprobe) &&
       Tok.is(tok::comma)) {
     CommaLoc = ConsumeAnyToken();
     ExprResult LHS(ParseCastExpression(false, false, NotTypeCast));
@@ -702,17 +816,22 @@ static bool ParseReductionId(Parser &P, CXXScopeSpec &ReductionIdScopeSpec,
 ///       'flush' '(' list ')'
 ///    depend-clause:
 ///       'depend' '(' in | out | inout : list ')'
+///    prefetch-clause:
+///       'prefetch' '(' read | write ':' list [ ':' start ':' end ] ')'
+///                          or
+///                  '(' smart ')'
 ///
 OMPClause *Parser::ParseOpenMPVarListClause(OpenMPClauseKind Kind) {
   SourceLocation Loc = Tok.getLocation();
   SourceLocation LOpen = ConsumeToken();
-  SourceLocation ColonLoc = SourceLocation();
+  SourceLocation ColonLoc = SourceLocation(), EndColonLoc = SourceLocation();
   // Optional scope specifier and unqualified id for reduction identifier.
   CXXScopeSpec ReductionIdScopeSpec;
   UnqualifiedId ReductionId;
   bool InvalidReductionId = false;
   OpenMPDependClauseKind DepKind = OMPC_DEPEND_unknown;
-  SourceLocation DepLoc;
+  OpenMPPrefetchClauseKind PrefKind = OMPC_PREFETCH_unknown;
+  SourceLocation DepLoc, PrefLoc;
 
   // Parse '('.
   BalancedDelimiterTracker T(*this, tok::l_paren, tok::annot_pragma_openmp_end);
@@ -755,13 +874,38 @@ OMPClause *Parser::ParseOpenMPVarListClause(OpenMPClauseKind Kind) {
     } else {
       Diag(Tok, diag::warn_pragma_expected_colon) << "dependency type";
     }
+  } else if (Kind == OMPC_prefetch) {
+  // Handle permission type for prefetch clause.
+    ColonProtectionRAIIObject ColonRAII(*this);
+    PrefKind = static_cast<OpenMPPrefetchClauseKind>(getOpenMPSimpleClauseType(
+        Kind, Tok.is(tok::identifier) ? PP.getSpelling(Tok) : ""));
+    PrefLoc = Tok.getLocation();
+
+    if (PrefKind == OMPC_PREFETCH_unknown) {
+      SkipUntil(tok::r_paren, tok::annot_pragma_openmp_end, StopBeforeMatch);
+      Diag(PrefLoc, diag::err_omp_invalid_prefetch_kind) << true;
+    } else {
+      ConsumeToken(); // Keyword
+      if(PrefKind == OMPC_PREFETCH_smart) {
+        // The smart prefetcher automatically generates list of variables to
+        // prefetch; nothing else is needed in the clause.
+        SmallVector<Expr *, 5> dummy;
+        T.consumeClose();
+        return Actions.ActOnOpenMPVarListClause(
+            Kind, dummy, nullptr, nullptr, Loc, LOpen, ColonLoc, EndColonLoc,
+            Tok.getLocation(), ReductionIdScopeSpec, DeclarationNameInfo(),
+            DepKind, DepLoc, PrefKind, PrefLoc);
+      }
+      else ConsumeToken(); // Colon
+    }
   }
 
   SmallVector<Expr *, 5> Vars;
   bool IsComma = ((Kind != OMPC_reduction) && (Kind != OMPC_depend)) ||
                  ((Kind == OMPC_reduction) && !InvalidReductionId) ||
                  ((Kind == OMPC_depend) && DepKind != OMPC_DEPEND_unknown);
-  const bool MayHaveTail = (Kind == OMPC_linear || Kind == OMPC_aligned);
+  const bool MayHaveTail = (Kind == OMPC_linear || Kind == OMPC_aligned ||
+                            Kind == OMPC_prefetch);
   while (IsComma || (Tok.isNot(tok::r_paren) && Tok.isNot(tok::colon) &&
                      Tok.isNot(tok::annot_pragma_openmp_end))) {
     ColonProtectionRAIIObject ColonRAII(*this, MayHaveTail);
@@ -787,9 +931,9 @@ OMPClause *Parser::ParseOpenMPVarListClause(OpenMPClauseKind Kind) {
           << (Kind == OMPC_flush);
   }
 
-  // Parse ':' linear-step (or ':' alignment).
+  // Parse ':' linear-step, ':' alignment, or ':' start.
   Expr *TailExpr = nullptr;
-  const bool MustHaveTail = MayHaveTail && Tok.is(tok::colon);
+  const bool MustHaveTail = (MayHaveTail && Tok.is(tok::colon));
   if (MustHaveTail) {
     ColonLoc = Tok.getLocation();
     ConsumeToken();
@@ -802,6 +946,20 @@ OMPClause *Parser::ParseOpenMPVarListClause(OpenMPClauseKind Kind) {
                 StopBeforeMatch);
   }
 
+  // Parse ':' end.
+  Expr *EndExpr = nullptr;
+  if (Kind == OMPC_prefetch && Tok.is(tok::colon)) {
+    EndColonLoc = Tok.getLocation();
+    ConsumeToken();
+    ExprResult Tail =
+        Actions.CorrectDelayedTyposInExpr(ParseAssignmentExpression());
+    if (Tail.isUsable())
+      EndExpr = Tail.get();
+    else
+      SkipUntil(tok::comma, tok::r_paren, tok::annot_pragma_openmp_end,
+                StopBeforeMatch);
+  }
+
   // Parse ')'.
   T.consumeClose();
   if ((Kind == OMPC_depend && DepKind != OMPC_DEPEND_unknown && Vars.empty()) ||
@@ -810,10 +968,10 @@ OMPClause *Parser::ParseOpenMPVarListClause(OpenMPClauseKind Kind) {
     return nullptr;
 
   return Actions.ActOnOpenMPVarListClause(
-      Kind, Vars, TailExpr, Loc, LOpen, ColonLoc, Tok.getLocation(),
-      ReductionIdScopeSpec,
+      Kind, Vars, TailExpr, EndExpr, Loc, LOpen, ColonLoc, EndColonLoc,
+      Tok.getLocation(), ReductionIdScopeSpec,
       ReductionId.isValid() ? Actions.GetNameFromUnqualifiedId(ReductionId)
                             : DeclarationNameInfo(),
-      DepKind, DepLoc);
+      DepKind, DepLoc, PrefKind, PrefLoc);
 }
 
diff --git a/clang/lib/Parse/ParsePragma.cpp b/clang/lib/Parse/ParsePragma.cpp
index 892d3c6a52c..86cfce77df7 100644
--- a/clang/lib/Parse/ParsePragma.cpp
+++ b/clang/lib/Parse/ParsePragma.cpp
@@ -156,6 +156,23 @@ struct PragmaUnrollHintHandler : public PragmaHandler {
                     Token &FirstToken) override;
 };
 
+struct PragmaNoPopcornHandler: public PragmaHandler {
+  PragmaNoPopcornHandler() : PragmaHandler("popcorn") {}
+  void HandlePragma(Preprocessor &PP,  PragmaIntroducerKind Introducer,
+                    Token &FirstToken) override;
+};
+
+struct PragmaPopcornHandler : public PragmaHandler {
+  enum Type {
+    Prefetch, // Prefetch for the statement following the pragma
+    None      // Unknown pragma type
+  };
+
+  PragmaPopcornHandler() : PragmaHandler("popcorn") {}
+  void HandlePragma(Preprocessor &PP, PragmaIntroducerKind Introducer,
+                    Token &FirstToken) override;
+};
+
 }  // end namespace
 
 void Parser::initializePragmaHandlers() {
@@ -235,6 +252,12 @@ void Parser::initializePragmaHandlers() {
 
   NoUnrollHintHandler.reset(new PragmaUnrollHintHandler("nounroll"));
   PP.AddPragmaHandler(NoUnrollHintHandler.get());
+
+  if (getLangOpts().DistributedOmp)
+    PopcornHandler.reset(new PragmaPopcornHandler());
+  else
+    PopcornHandler.reset(new PragmaNoPopcornHandler());
+  PP.AddPragmaHandler(PopcornHandler.get());
 }
 
 void Parser::resetPragmaHandlers() {
@@ -304,6 +327,9 @@ void Parser::resetPragmaHandlers() {
 
   PP.RemovePragmaHandler(NoUnrollHintHandler.get());
   NoUnrollHintHandler.reset();
+
+  PP.RemovePragmaHandler(PopcornHandler.get());
+  PopcornHandler.reset();
 }
 
 /// \brief Handle the annotation token produced for #pragma unused(...)
@@ -868,6 +894,109 @@ bool Parser::HandlePragmaLoopHint(LoopHint &Hint) {
   return true;
 }
 
+enum PopcornClauseKind {
+  PC_PrefetchIgnore, // Ignore array/pointer variable during prefetch analysis
+  PC_Unknown         // Unknown clause type
+};
+
+static const char *getPopcornClauseName(enum PopcornClauseKind K) {
+  switch(K) {
+  default: return "unknown";
+  case PC_PrefetchIgnore: return "ignore";
+  }
+}
+
+static enum PopcornClauseKind getPopcornClauseKind(llvm::StringRef Name) {
+  return llvm::StringSwitch<PopcornClauseKind>(Name)
+    .Case("ignore", PC_PrefetchIgnore)
+    .Default(PC_Unknown);
+}
+
+void Parser::ParseVarList(llvm::SmallPtrSet<VarDecl *, 4> &Vars) {
+  bool isComma = false;
+  DeclRefExpr *D;
+  VarDecl *VD;
+
+  while(isComma || (Tok.isNot(tok::r_paren) &&
+                    Tok.isNot(tok::annot_pragma_popcorn_prefetch_end))) {
+    // Parse variable
+    ExprResult VarExpr =
+      Actions.CorrectDelayedTyposInExpr(ParseAssignmentExpression());
+
+    if(VarExpr.isUsable()) {
+      Expr *E = VarExpr.get();
+      if((D = dyn_cast<DeclRefExpr>(E)) &&
+         (VD = dyn_cast<VarDecl>(D->getDecl())))
+        Vars.insert(VD);
+      else {
+        PP.Diag(E->getLocStart(), diag::err_pragma_popcorn_expected_var_name);
+        SkipUntil(tok::comma, tok::r_paren,
+                  tok::annot_pragma_popcorn_prefetch_end);
+      }
+    }
+
+    // Parse ',' if any
+    isComma = Tok.is(tok::comma);
+    if(isComma) ConsumeToken();
+  }
+}
+
+StmtResult Parser::HandlePragmaPopcorn() {
+  llvm::SmallPtrSet<VarDecl *, 4> Ignore;
+
+  assert(Tok.is(tok::annot_pragma_popcorn_prefetch));
+  ConsumeToken(); // The annotation token.
+
+  while(Tok.isNot(tok::annot_pragma_popcorn_prefetch_end)) {
+    // Parse clause type.
+    enum PopcornClauseKind Kind = getPopcornClauseKind(PP.getSpelling(Tok));
+    if(Kind == PC_Unknown) {
+      PP.Diag(Tok.getLocation(),
+              diag::err_pragma_popcorn_invalid_clause) << PP.getSpelling(Tok);
+      SkipUntil(tok::r_paren, tok::annot_pragma_popcorn_prefetch_end,
+                Parser::StopBeforeMatch);
+      continue;
+    }
+    ConsumeToken();
+
+    // Parse '('.
+    BalancedDelimiterTracker T(*this, tok::l_paren,
+                               tok::annot_pragma_popcorn_prefetch_end);
+    if(T.expectAndConsume(diag::err_expected_lparen_after,
+                          getPopcornClauseName(Kind)))
+      return StmtError();
+
+    switch(Kind) {
+    default: llvm_unreachable("Unknown '#pragma popcorn' clause type"); break;
+    case PC_PrefetchIgnore: ParseVarList(Ignore); break;
+    }
+
+    // Parse ')'.
+    T.consumeClose();
+  }
+  ConsumeToken(); // Consume final token.
+
+  StmtResult R = ParseStatement();
+
+  if(R.isInvalid()) return StmtError();
+  else if(isa<ForStmt>(R.get())) {
+    ASTContext &Ctx = getActions().getASTContext();
+    ForStmt *S = cast<ForStmt>(R.get());
+    S->setPrefetchEnabled(true);
+    PrefetchAnalysis PA(&Ctx, R.get());
+    PA.ignoreVars(Ignore);
+    PA.analyzeStmt();
+    PA.calculatePrefetchRanges();
+    // TODO remove
+    PA.dump();
+    Ctx.addPrefetchAnalysis(R.get(), PA);
+  }
+  else PP.Diag(R.get()->getLocStart(),
+               diag::warn_pragma_popcorn_prefetch_invalid_stmt);
+
+  return R;
+}
+
 // #pragma GCC visibility comes in two variants:
 //   'push' '(' [visibility] ')'
 //   'pop'
@@ -1458,10 +1587,10 @@ void
 PragmaNoOpenMPHandler::HandlePragma(Preprocessor &PP,
                                     PragmaIntroducerKind Introducer,
                                     Token &FirstTok) {
-  if (!PP.getDiagnostics().isIgnored(diag::warn_pragma_omp_ignored,
+  if (!PP.getDiagnostics().isIgnored(diag::warn_pragma_popcorn_ignored,
                                      FirstTok.getLocation())) {
-    PP.Diag(FirstTok, diag::warn_pragma_omp_ignored);
-    PP.getDiagnostics().setSeverity(diag::warn_pragma_omp_ignored,
+    PP.Diag(FirstTok, diag::warn_pragma_popcorn_ignored);
+    PP.getDiagnostics().setSeverity(diag::warn_pragma_popcorn_ignored,
                                     diag::Severity::Ignored, SourceLocation());
   }
   PP.DiscardUntilEndOfDirective();
@@ -2119,3 +2248,66 @@ void PragmaUnrollHintHandler::HandlePragma(Preprocessor &PP,
   PP.EnterTokenStream(TokenArray, 1, /*DisableMacroExpansion=*/false,
                       /*OwnsTokens=*/true);
 }
+
+void PragmaNoPopcornHandler::HandlePragma(Preprocessor &PP,
+                                          PragmaIntroducerKind Introducer,
+                                          Token &Tok) {
+  if (!PP.getDiagnostics().isIgnored(diag::warn_pragma_omp_ignored,
+                                     Tok.getLocation())) {
+    PP.Diag(Tok, diag::warn_pragma_omp_ignored);
+    PP.getDiagnostics().setSeverity(diag::warn_pragma_omp_ignored,
+                                    diag::Severity::Ignored, SourceLocation());
+  }
+  PP.DiscardUntilEndOfDirective();
+}
+
+void PragmaPopcornHandler::HandlePragma(Preprocessor &PP,
+                                        PragmaIntroducerKind Introducer,
+                                        Token &Tok) {
+  // Incoming token is "popcorn" for "#pragma popcorn".
+  PP.Lex(Tok);
+  if (Tok.isNot(tok::identifier)) {
+    PP.Diag(Tok.getLocation(), diag::warn_pragma_popcorn_no_arg);
+    return;
+  }
+
+  IdentifierInfo *OptionInfo = Tok.getIdentifierInfo();
+  enum PragmaPopcornHandler::Type Ty =
+    llvm::StringSwitch<enum PragmaPopcornHandler::Type>(OptionInfo->getName())
+                             .Case("prefetch", PragmaPopcornHandler::Prefetch)
+                             .Default(PragmaPopcornHandler::None);
+  if (Ty == PragmaPopcornHandler::None) {
+    PP.Diag(Tok.getLocation(), diag::warn_pragma_popcorn_invalid_option)
+        << OptionInfo->getName();
+    return;
+  }
+
+  switch(Ty) {
+  default: llvm_unreachable("Should have weeded out invalid types"); break;
+  case PragmaPopcornHandler::Prefetch: {
+    // Capture all tokens to be included for prefetching analysis.
+    SmallVector<Token, 16> Pragma;
+    Token NextTok;
+
+    NextTok.startToken();
+    NextTok.setKind(tok::annot_pragma_popcorn_prefetch);
+    NextTok.setLocation(Tok.getLocation());
+    while (NextTok.isNot(tok::eod)) {
+      Pragma.push_back(NextTok);
+      PP.Lex(NextTok);
+    }
+    SourceLocation EodLoc = NextTok.getLocation();
+    NextTok.startToken();
+    NextTok.setKind(tok::annot_pragma_popcorn_prefetch_end);
+    NextTok.setLocation(EodLoc);
+    Pragma.push_back(NextTok);
+
+    Token *Toks = new Token[Pragma.size()];
+    std::copy(Pragma.begin(), Pragma.end(), Toks);
+    PP.EnterTokenStream(Toks, Pragma.size(),
+                        /*DisableMacroExpansion=*/false, /*OwnsTokens=*/true);
+    break;
+  }
+  }
+}
+
diff --git a/clang/lib/Parse/ParseStmt.cpp b/clang/lib/Parse/ParseStmt.cpp
index b658cef234e..5cf67ab60c6 100644
--- a/clang/lib/Parse/ParseStmt.cpp
+++ b/clang/lib/Parse/ParseStmt.cpp
@@ -357,6 +357,9 @@ Retry:
   case tok::annot_pragma_loop_hint:
     ProhibitAttributes(Attrs);
     return ParsePragmaLoopHint(Stmts, OnlyStatement, TrailingElseLoc, Attrs);
+
+  case tok::annot_pragma_popcorn_prefetch:
+    return HandlePragmaPopcorn();
   }
 
   // If we reached this code, the statement must end in a semicolon.
diff --git a/clang/lib/Sema/CMakeLists.txt b/clang/lib/Sema/CMakeLists.txt
index 4a772d8972a..45a78b986e9 100644
--- a/clang/lib/Sema/CMakeLists.txt
+++ b/clang/lib/Sema/CMakeLists.txt
@@ -11,6 +11,9 @@ add_clang_library(clangSema
   IdentifierResolver.cpp
   JumpDiagnostics.cpp
   MultiplexExternalSemaSource.cpp
+  PrefetchAnalysis.cpp
+  PrefetchDataflow.cpp
+  PrefetchExprBuilder.cpp
   Scope.cpp
   ScopeInfo.cpp
   Sema.cpp
diff --git a/clang/lib/Sema/PrefetchAnalysis.cpp b/clang/lib/Sema/PrefetchAnalysis.cpp
new file mode 100644
index 00000000000..738ceb7b9bb
--- /dev/null
+++ b/clang/lib/Sema/PrefetchAnalysis.cpp
@@ -0,0 +1,937 @@
+//=- PrefetchAnalysis.cpp - Prefetching Analysis for Structured Blocks ---*-==//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements prefetching analysis for structured blocks.  The
+// analysis traverses the AST to determine how arrays are accessed in structured
+// blocks and generates expressions defining ranges of elements accessed.
+//
+//===----------------------------------------------------------------------===//
+
+#include "clang/AST/ASTContext.h"
+#include "clang/AST/RecursiveASTVisitor.h"
+#include "clang/Sema/PrefetchAnalysis.h"
+#include "clang/Sema/PrefetchDataflow.h"
+#include "clang/Sema/PrefetchExprBuilder.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/Support/Debug.h"
+
+using namespace clang;
+
+//===----------------------------------------------------------------------===//
+// PrefetchRange API
+//
+
+bool PrefetchRange::equalExceptType(const PrefetchRange &RHS) {
+  if(Array != RHS.Array) return false;
+  else if(!PrefetchExprEquality::exprEqual(Start, RHS.Start)) return false;
+  else if(!PrefetchExprEquality::exprEqual(End, RHS.End)) return false;
+  else return true;
+}
+
+bool PrefetchRange::operator==(const PrefetchRange &RHS) {
+  if(Ty == RHS.Ty && equalExceptType(RHS)) return true;
+  else return false;
+}
+
+//===----------------------------------------------------------------------===//
+// Common utilities
+//
+
+/// Return whether a type is both scalar and integer.
+bool PrefetchAnalysis::isScalarIntType(const QualType &Ty) {
+  return Ty->isIntegerType() && Ty->isScalarType();
+}
+
+/// Get the size in bits for builtin integer types.
+unsigned PrefetchAnalysis::getTypeSize(BuiltinType::Kind K) {
+  switch(K) {
+  case BuiltinType::Bool:
+  case BuiltinType::Char_U: case BuiltinType::UChar:
+  case BuiltinType::Char_S: case BuiltinType::SChar:
+    return 8;
+  case BuiltinType::WChar_U: case BuiltinType::Char16:
+  case BuiltinType::UShort:
+  case BuiltinType::WChar_S:
+  case BuiltinType::Short:
+    return 16;
+  case BuiltinType::Char32:
+  case BuiltinType::UInt:
+  case BuiltinType::Int:
+    return 32;
+  case BuiltinType::ULong:
+  case BuiltinType::ULongLong:
+  case BuiltinType::Long:
+  case BuiltinType::LongLong:
+    return 64;
+  case BuiltinType::UInt128:
+  case BuiltinType::Int128:
+    return 128;
+  default: return UINT32_MAX;
+  }
+}
+
+/// Return the variable declaration if the declared value is a variable and if
+/// it is a scalar integer type or nullptr otherwise.
+VarDecl *PrefetchAnalysis::getVarIfScalarInt(ValueDecl *VD) {
+  if(isa<VarDecl>(VD)) {
+    VarDecl *Var = cast<VarDecl>(VD);
+    if(isScalarIntType(Var->getType())) return Var;
+  }
+  return nullptr;
+}
+
+// Filter functions to only select appropriate operator types.  Return true if
+// the operator is of a type that should be analyzed, or false otherwise.
+typedef bool (*UnaryOpFilter)(UnaryOperator::Opcode);
+typedef bool (*BinaryOpFilter)(BinaryOperator::Opcode);
+
+// Don't analyze *any* operation types.
+bool NoUnaryOp(UnaryOperator::Opcode Op) { return false; }
+bool NoBinaryOp(BinaryOperator::Opcode Op) { return false; }
+
+// Filter out non-assignment binary operations.
+static bool FilterAssignOp(BinaryOperator::Opcode Op) {
+  switch(Op) {
+  case BO_Assign: case BO_MulAssign: case BO_DivAssign: case BO_RemAssign:
+  case BO_AddAssign: case BO_SubAssign: case BO_ShlAssign: case BO_ShrAssign:
+  case BO_AndAssign: case BO_XorAssign: case BO_OrAssign:
+    return true;
+  default: return false;
+  }
+}
+
+// Filter out non-relational binary operations.
+static bool FilterRelationalOp(BinaryOperator::Opcode Op) {
+  switch(Op) {
+  case BO_LT: case BO_GT: case BO_LE: case BO_GE: case BO_EQ: case BO_NE:
+    return true;
+  default: return false;
+  }
+}
+
+// Filter out non-math/logic binary operations.
+static bool FilterMathLogicOp(BinaryOperator::Opcode Op) {
+  switch(Op) {
+  case BO_Mul: case BO_Div: case BO_Rem: case BO_Add: case BO_Sub:
+  case BO_Shl: case BO_Shr: case BO_And: case BO_Xor: case BO_Or:
+    return true;
+  default: return false;
+  }
+}
+
+// Filter out non-math unary operations.
+static bool FilterMathOp(UnaryOperator::Opcode Op) {
+  switch(Op) {
+  case UO_PostInc: case UO_PostDec: case UO_PreInc: case UO_PreDec:
+    return true;
+  default: return false;
+  }
+}
+
+/// Return the statement if it is a scoping statement (e.g., for-loop) or
+/// nullptr otherwise.
+static bool isScopingStmt(Stmt *S) {
+  if(isa<CapturedStmt>(S) || isa<CompoundStmt>(S) || isa<CXXCatchStmt>(S) ||
+     isa<CXXForRangeStmt>(S) || isa<CXXTryStmt>(S) || isa<DoStmt>(S) ||
+     isa<ForStmt>(S) || isa<IfStmt>(S) || isa<OMPExecutableDirective>(S) ||
+     isa<SwitchStmt>(S) || isa<WhileStmt>(S)) return true;
+  else return false;
+}
+
+/// A vector of variable declarations.
+typedef llvm::SmallVector<VarDecl *, 4> VarVec;
+
+//===----------------------------------------------------------------------===//
+// Prefetch analysis -- array accesses
+//
+
+/// Scoping information for array analyses.  A node in a singly-linked list
+/// which allows traversal from innermost scope outwards.  Nodes are reference
+/// counted, so when array accesses which reference the scope (if any) are
+/// deleted, the scoping chain itself gets deleted.
+// TODO verify the chain gets freed correctly
+struct ScopeInfo {
+  Stmt *ScopeStmt;                        // Statement providing scope
+  std::shared_ptr<ScopeInfo> ParentScope; // The parent in the scope chain
+  ScopeInfo(Stmt *ScopeStmt, std::shared_ptr<ScopeInfo> &ParentScope)
+    : ScopeStmt(ScopeStmt), ParentScope(ParentScope) {}
+};
+typedef std::shared_ptr<ScopeInfo> ScopeInfoPtr;
+
+/// An array access.
+class ArrayAccess {
+public:
+  ArrayAccess(PrefetchRange::Type Ty, ArraySubscriptExpr *S,
+              const ScopeInfoPtr &AccessScope)
+    : Valid(true), Ty(Ty), S(S), Base(nullptr), Idx(S),
+      AccessScope(AccessScope) {
+
+    DeclRefExpr *DR;
+    VarDecl *VD;
+
+    // Drill down into subscripts for multi-dimensional arrays, e.g., a[i][j]
+    while(isa<ArraySubscriptExpr>(S->getBase()->IgnoreImpCasts()))
+      S = cast<ArraySubscriptExpr>(S->getBase()->IgnoreImpCasts());
+
+    if(!(DR = dyn_cast<DeclRefExpr>(S->getBase()->IgnoreImpCasts()))) {
+      Valid = false;
+      return;
+    }
+
+    if(!(VD = dyn_cast<VarDecl>(DR->getDecl()))) {
+      Valid = false;
+      return;
+    }
+
+    Base = VD;
+  }
+
+  bool isValid() const { return Valid; }
+  Stmt *getStmt() const { return S; }
+  PrefetchRange::Type getAccessType() const { return Ty; }
+  VarDecl *getBase() const { return Base; }
+  Expr *getIndex() const { return Idx; }
+  const VarVec &getVarsInIdx() const { return VarsInIdx; }
+  const ScopeInfoPtr &getScope() const { return AccessScope; }
+
+  void setInvalid() { Valid = false; }
+  void addVarInIdx(VarDecl *V) { if(V != Base) VarsInIdx.push_back(V); }
+
+  void print(llvm::raw_ostream &O, PrintingPolicy &Policy) const {
+    O << "Array: " << Base->getName() << "\nIndex expression: ";
+    Idx->printPretty(O, nullptr, Policy);
+    O << "\nScoping statement:\n";
+    AccessScope->ScopeStmt->printPretty(O, nullptr, Policy);
+    O << "\nVariables used in index calculation:";
+    for(auto Var : VarsInIdx) O << " " << Var->getName();
+    O << "\n";
+  }
+  void dump(PrintingPolicy &Policy) const { print(llvm::dbgs(), Policy); }
+
+private:
+  bool Valid;               // Is the access valid?
+  PrefetchRange::Type Ty;   // The type of access
+  Stmt *S;                  // The entire array access statement
+  VarDecl *Base;            // The array base
+  Expr *Idx;                // Expression used to calculate index
+  VarVec VarsInIdx;         // Variables used in index calculation
+  ScopeInfoPtr AccessScope; // Scope of the array access
+};
+
+/// Traverse a statement looking for array accesses.
+// TODO *** NEED TO LIMIT TO AFFINE ACCESSES ***
+class ArrayAccessPattern : public RecursiveASTVisitor<ArrayAccessPattern> {
+public:
+  ArrayAccessPattern(llvm::SmallPtrSet<VarDecl *, 4> &Ignore)
+    : Ignore(Ignore) {}
+
+  void InitTraversal() {
+    AssignSide.push_back(TS_RHS);
+    SubscriptSide.push_back(AS_Index);
+    CurAccess.push_back(nullptr);
+  }
+
+  /// Traverse a binary operator & maintain traversal structure to determine if
+  /// we're reading or writing in the array access.  Left-hand side == writing
+  /// and right-hande side == reading.
+  bool TraverseBinaryOperator(BinaryOperator *B) {
+    if(FilterAssignOp(B->getOpcode())) {
+      AssignSide.push_back(TS_LHS);
+      TraverseStmt(B->getLHS());
+      AssignSide.pop_back();
+      AssignSide.push_back(TS_RHS);
+      TraverseStmt(B->getRHS());
+      AssignSide.pop_back();
+    }
+    else RecursiveASTVisitor::TraverseStmt(B);
+    return true;
+  }
+
+  /// Traverse an array subscript & maintain traversal structure to determine if
+  /// we're exploring the base or index of the access.  Don't record subscript
+  /// expressions if we're currently exploring the base of another subscript, as
+  /// it's part of a multi-dimensional access, e.g., a[i][j].
+  bool TraverseArraySubscriptExpr(ArraySubscriptExpr *AS) {
+    // Record array access if we're not exploring a higher-level access' base
+    if(SubscriptSide.back() != AS_Base) VisitArraySubscriptExpr(AS);
+
+    SubscriptSide.push_back(AS_Base);
+    TraverseStmt(AS->getBase());
+    SubscriptSide.pop_back();
+    SubscriptSide.push_back(AS_Index);
+    TraverseStmt(AS->getIdx());
+    SubscriptSide.pop_back();
+
+    // Don't record any more variables for this access
+    if(SubscriptSide.back() != AS_Base) CurAccess.pop_back();
+
+    return true;
+  }
+
+  /// Traverse a statement.  Record scoping information where applicable.
+  bool TraverseStmt(Stmt *S) {
+    bool isScope;
+    BinaryOperator *B;
+
+    if(!S) return true;
+
+    isScope = isScopingStmt(S);
+    if(isScope) CurScope = ScopeInfoPtr(new ScopeInfo(S, CurScope));
+
+    // For some reason RecursiveASTVisitor doesn't redirect binary operations
+    // to TraverseBinaryOperator but instead to individual operation functions
+    // (e.g., TraverseBinAssign).  Instead, redirect those here.
+    if((B = dyn_cast<BinaryOperator>(S))) TraverseBinaryOperator(B);
+    else RecursiveASTVisitor<ArrayAccessPattern>::TraverseStmt(S);
+
+    if(isScope) CurScope = CurScope->ParentScope;
+
+    return true;
+  }
+
+  /// Analyze an array access.
+  bool VisitArraySubscriptExpr(ArraySubscriptExpr *Sub) {
+    PrefetchRange::Type Ty =
+      AssignSide.back() == TS_LHS ? PrefetchRange::Write :
+                                    PrefetchRange::Read;
+    ArrayAccesses.emplace_back(Ty, Sub, CurScope);
+    CurAccess.push_back(&ArrayAccesses.back());
+
+    return true;
+  }
+
+  /// Record variables seen during traversal used to construct indices.
+  bool VisitDeclRefExpr(DeclRefExpr *DR) {
+    ArrayAccess *Back = CurAccess.back();
+    if(Back && Back->isValid()) {
+      VarDecl *VD = dyn_cast<VarDecl>(DR->getDecl());
+      if(VD) Back->addVarInIdx(VD);
+      else Back->setInvalid(); // Can't analyze if decl != variable
+    }
+    return true;
+  }
+
+  /// Rather than removing invalid accesses during traversal (which complicates
+  /// traversal state handling), prune them in one go at the end.
+  void PruneInvalidOrIgnoredAccesses() {
+    llvm::SmallVector<ArrayAccess, 8> Pruned;
+
+    for(auto &Access : ArrayAccesses) {
+      if(Access.isValid() && !Ignore.count(Access.getBase()))
+        Pruned.push_back(Access);
+    }
+    ArrayAccesses = Pruned;
+  }
+
+  const llvm::SmallVector<ArrayAccess, 8> &getArrayAccesses() const
+  { return ArrayAccesses; }
+
+  llvm::SmallVector<ArrayAccess, 8> &getArrayAccesses()
+  { return ArrayAccesses; }
+
+private:
+  /// Which sub-tree of a binary operator we're traversing.  This determines
+  /// whether we're reading or writing the array.
+  enum TraverseStructure { TS_LHS, TS_RHS };
+
+  /// Which part of an array subscript expression we're traversing.
+  enum ArraySubscriptSide { AS_Base, AS_Index };
+
+  llvm::SmallVector<ArrayAccess, 8> ArrayAccesses;
+  ScopeInfoPtr CurScope;
+  llvm::SmallPtrSet<VarDecl *, 4> &Ignore;
+
+  // Traversal state
+  llvm::SmallVector<enum TraverseStructure, 8> AssignSide;
+  llvm::SmallVector<enum ArraySubscriptSide, 8> SubscriptSide;
+  llvm::SmallVector<ArrayAccess *, 8> CurAccess;
+};
+
+void PrefetchAnalysis::pruneArrayAccesses() {
+  llvm::SmallVector<ArrayAccess, 8> &Accesses = ArrAccesses->getArrayAccesses();
+  llvm::SmallVector<ArrayAccess, 8>::iterator Cur, Next;
+  for(Cur = Accesses.begin(); Cur != Accesses.end(); Cur++) {
+    if(!Cur->isValid()) continue;
+    for(Next = Cur + 1; Next != Accesses.end(); Next++) {
+      if(!Next->isValid()) continue;
+      if(Cur->getBase() == Next->getBase() &&
+         PrefetchExprEquality::exprEqual(Cur->getIndex(), Next->getIndex()))
+        Next->setInvalid();
+    }
+  }
+}
+
+void PrefetchAnalysis::mergePrefetchRanges() {
+  // TODO!
+}
+
+void PrefetchAnalysis::prunePrefetchRanges()
+{
+  // TODO we could prevent a bunch of copying if we used a linked list instead
+  // of a vector for ToPrefetch
+  llvm::SmallVector<PrefetchRange, 8>::iterator Cur, Next;
+  for(Cur = ToPrefetch.begin(); Cur != ToPrefetch.end(); Cur++)
+    if(PrefetchExprEquality::exprEqual(Cur->getStart(), Cur->getEnd()))
+      Cur = ToPrefetch.erase(Cur) - 1;
+
+  for(Cur = ToPrefetch.begin(); Cur != ToPrefetch.end(); Cur++) {
+    for(Next = Cur + 1; Next != ToPrefetch.end(); Next++) {
+      if(*Cur == *Next) Next = ToPrefetch.erase(Next) - 1;
+      else if(Cur->equalExceptType(*Next)) {
+        Cur->setType(Cur->getType() > Next->getType() ? Cur->getType() :
+                                                        Next->getType());
+        Next = ToPrefetch.erase(Next) - 1;
+      }
+    }
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// Prefetch analysis -- ForStmts
+//
+
+/// An induction variable and expressions describing its range.
+class InductionVariable {
+public:
+  /// The direction of change for the induction variable
+  enum Direction {
+    Increases, // Update changes variable from lower to higher values
+    Decreases, // Update changes variable from higher to lower values
+    Unknown // Update has an unknown effect, e.g., container interators
+  };
+
+  InductionVariable() : Var(nullptr), Init(nullptr), Cond(nullptr),
+                        Update(nullptr), Dir(Unknown), LowerB(nullptr),
+                        UpperB(nullptr) {}
+
+  InductionVariable(VarDecl *Var, Expr *Init, Expr *Cond, Expr *Update,
+                    ASTContext *Ctx)
+    : Var(Var), Init(Init), Cond(Cond), Update(Update), Dir(Unknown),
+      LowerB(nullptr), UpperB(nullptr) {
+
+    PrefetchExprBuilder::Modifier UpperMod, LowerMod;
+    const UnaryOperator *Unary;
+
+    assert(PrefetchAnalysis::isScalarIntType(Var->getType()) &&
+           "Invalid induction variable");
+
+    // Try to classify update direction to determine which expression specifies
+    // lower and upper bounds
+    if((Unary = dyn_cast<UnaryOperator>(Update)))
+      Dir = classifyUnaryOpDirection(Unary->getOpcode());
+
+    if(Dir == Increases) {
+      LowerMod.ClassifyModifier(Init, Ctx);
+      UpperMod.ClassifyModifier(Cond, Ctx);
+      LowerB = stripInductionVar(Init);
+      UpperB = stripInductionVar(Cond);
+    }
+    else if(Dir == Decreases) {
+      LowerMod.ClassifyModifier(Cond, Ctx);
+      UpperMod.ClassifyModifier(Init, Ctx);
+      LowerB = stripInductionVar(Cond);
+      UpperB = stripInductionVar(Init);
+    }
+
+    if(LowerB && UpperB) {
+      LowerB = PrefetchExprBuilder::cloneAndModifyExpr(LowerB, LowerMod, Ctx);
+      UpperB = PrefetchExprBuilder::cloneAndModifyExpr(UpperB, UpperMod, Ctx);
+    }
+  }
+
+  VarDecl *getVariable() const { return Var; }
+  Expr *getInit() const { return Init; }
+  Expr *getCond() const { return Cond; }
+  Expr *getUpdate() const { return Update; }
+  enum Direction getUpdateDirection() const { return Dir; }
+  Expr *getLowerBound() const { return LowerB; }
+  Expr *getUpperBound() const { return UpperB; }
+
+  void print(llvm::raw_ostream &O, PrintingPolicy &Policy) const {
+    O << "Induction Variable: " << Var->getName() << "\nDirection: ";
+    switch(Dir) {
+    case Increases: O << "increases\n"; break;
+    case Decreases: O << "decreases\n"; break;
+    case Unknown: O << "unknown update direction\n"; break;
+    }
+    if(LowerB && UpperB) {
+      O << "Lower bound: ";
+      LowerB->printPretty(O, nullptr, Policy);
+      O << "\nUpper bound: ";
+      UpperB->printPretty(O, nullptr, Policy);
+    }
+    else O << "-> Could not determine bounds <-";
+    O << "\n";
+  }
+
+  void dump(PrintingPolicy &Policy) const { print(llvm::dbgs(), Policy); }
+
+private:
+  VarDecl *Var;
+  Expr *Init, *Cond, *Update;
+
+  /// Expressions describing the lower & upper bounds of the induction variable
+  /// and its update direction.
+  enum Direction Dir;
+  Expr *LowerB, *UpperB;
+
+  /// Try to classify the induction variable's update direction based on the
+  /// unary operation type.
+  static enum Direction classifyUnaryOpDirection(UnaryOperator::Opcode Op) {
+    switch(Op) {
+    case UO_PostInc:
+    case UO_PreInc:
+      return Increases;
+    case UO_PostDec:
+    case UO_PreDec:
+      return Decreases;
+    default: return Unknown;
+    }
+  }
+
+  Expr *stripInductionVarFromBinOp(BinaryOperator *B) {
+    DeclRefExpr *D;
+    VarDecl *VD;
+
+    D = dyn_cast<DeclRefExpr>(B->getLHS()->IgnoreImpCasts());
+    if(!D) return nullptr;
+    VD = dyn_cast<VarDecl>(D->getDecl());
+    if(!VD) return nullptr;
+    if(VD == Var) return B->getRHS();
+    return nullptr;
+  }
+
+  Expr *stripInductionVarFromExpr(Expr *E) {
+    DeclRefExpr *D;
+    VarDecl *VD;
+
+    D = dyn_cast<DeclRefExpr>(E->IgnoreImpCasts());
+    if(!D) return nullptr;
+    VD = dyn_cast<VarDecl>(D->getDecl());
+    if(!VD) return nullptr;
+    if(VD != Var) return D;
+    return nullptr;
+  }
+
+  /// Remove the induction variable & operator from the expression, leaving
+  /// only a bounds expression.
+  Expr *stripInductionVar(Expr *E) {
+    BinaryOperator *B;
+    IntegerLiteral *L;
+
+    if((B = dyn_cast<BinaryOperator>(E))) return stripInductionVarFromBinOp(B);
+    else if((L = dyn_cast<IntegerLiteral>(E))) return L;
+    else if(E) return stripInductionVarFromExpr(E);
+    else return nullptr;
+  }
+};
+
+/// Syntactic sugar for InductionVariable containers.
+typedef std::shared_ptr<InductionVariable> InductionVariablePtr;
+typedef std::pair<VarDecl *, InductionVariablePtr> IVPair;
+typedef llvm::DenseMap<VarDecl *, InductionVariablePtr> IVMap;
+
+/// Map an induction variable to an expression describing a bound.
+typedef llvm::DenseMap<VarDecl *, Expr *> IVBoundMap;
+typedef std::pair<VarDecl *, Expr *> IVBoundPair;
+
+/// Traversal to find induction variables in loop initialization, condition and
+/// update expressions.
+template<UnaryOpFilter UnaryFilt, BinaryOpFilter BinaryFilt>
+class IVFinder : public RecursiveASTVisitor<IVFinder<UnaryFilt, BinaryFilt>> {
+public:
+  // Visit binary operators to find induction variables.
+  bool VisitBinaryOperator(BinaryOperator *B) {
+    Expr *LHS;
+    DeclRefExpr *DR;
+    VarDecl *Var;
+
+    // Filter out irrelevant operation types
+    if(!BinaryFilt(B->getOpcode())) return true;
+
+    // Look for DeclRefExprs of scalar integer type -- these reference
+    // induction variables
+    LHS = B->getLHS();
+    if(!PrefetchAnalysis::isScalarIntType(LHS->getType())) return true;
+    DR = dyn_cast<DeclRefExpr>(LHS->IgnoreImpCasts());
+    if(!DR) return true;
+
+    // Make sure the expression acting on the induction variable is a scalar
+    // integer (casts may change types)
+    Var = PrefetchAnalysis::getVarIfScalarInt(DR->getDecl());
+    if(!Var) return true;
+    InductionVars[Var] = B;
+    return true;
+  }
+
+  // Visit unary operators to find induction variables.
+  bool VisitUnaryOperator(UnaryOperator *U) {
+    Expr *SubExpr;
+    DeclRefExpr *DR;
+    VarDecl *Var;
+
+    // Filter out irrelevant operation types
+    if(!UnaryFilt(U->getOpcode())) return true;
+
+    // Look for DeclRefExprs of scalar integer type -- these reference
+    // induction variables
+    SubExpr = U->getSubExpr();
+    if(!PrefetchAnalysis::isScalarIntType(SubExpr->getType())) return true;
+    DR = dyn_cast<DeclRefExpr>(SubExpr->IgnoreImpCasts());
+    if(!DR) return true;
+
+    // Make sure the expression acting on the induction variable is a scalar
+    // integer (casts may change types)
+    Var = PrefetchAnalysis::getVarIfScalarInt(DR->getDecl());
+    if(!Var) return true;
+    InductionVars[Var] = U;
+    return true;
+  }
+
+  bool VisitDeclStmt(DeclStmt *D) {
+    VarDecl *Var;
+    for(auto &Child : D->getDeclGroup()) {
+      Var = PrefetchAnalysis::getVarIfScalarInt(dyn_cast<VarDecl>(Child));
+      if(!Var || !Var->hasInit()) continue;
+      InductionVars[Var] = Var->getInit();
+    }
+    return true;
+  }
+
+  /// Return all induction variables found.
+  const IVBoundMap &getInductionVars() const { return InductionVars; }
+
+  /// Return the bounds expression for a given induction variable, or nullptr
+  /// if none was found.
+  Expr *getVarBound(VarDecl *Var) {
+    IVBoundMap::iterator it = InductionVars.find(Var);
+    if(it != InductionVars.end()) return it->second;
+    else return nullptr;
+  }
+
+private:
+  IVBoundMap InductionVars;
+};
+
+/// Structural information about a for-loop, including induction variables and
+/// parent/child loops.
+class ForLoopInfo {
+public:
+  ForLoopInfo(ForStmt *Loop, std::shared_ptr<ForLoopInfo> &Parent, int Level)
+    : Loop(Loop), Parent(Parent), Level(Level) {}
+
+  /// Add an induction variable.
+  void addInductionVar(const InductionVariablePtr &IV)
+  { InductionVars.insert(IVPair(IV->getVariable(), IV)); }
+
+  /// Remove an induction variable if present.  Return true if removed or false
+  /// if we don't have the variable.
+  bool removeInductionVar(const InductionVariablePtr &IV) {
+    IVMap::iterator it = InductionVars.find(IV->getVariable());
+    if(it != InductionVars.end()) {
+      InductionVars.erase(it);
+      return true;
+    }
+    else return false;
+  }
+
+  /// Add a child loop.
+  void addChildLoop(std::shared_ptr<ForLoopInfo> &S) { Children.push_back(S); }
+
+  ForStmt *getLoop() const { return Loop; }
+  const std::shared_ptr<ForLoopInfo> &getParent() const { return Parent; }
+  int getLevel() const { return Level; }
+  const IVMap &getInductionVars() const { return InductionVars; }
+  const llvm::SmallVector<std::shared_ptr<ForLoopInfo>, 4> &getChildren() const
+  { return Children; }
+
+  void print(llvm::raw_ostream &O, PrintingPolicy &Policy) const {
+    O << "Loop @ " << this << "\nDepth: " << Level
+      << "\nParent: " << Parent.get();
+    if(Children.size()) {
+      O << "\nChildren:";
+      for(auto &Child : Children) O << " " << Child.get();
+    }
+    O << "\n";
+    for(auto &IV : InductionVars) IV.second->dump(Policy);
+    O << "\n";
+    Loop->printPretty(O, nullptr, Policy);
+    O << "\n";
+  }
+  void dump(PrintingPolicy &Policy) const { print(llvm::dbgs(), Policy); }
+
+private:
+  ForStmt *Loop;
+  std::shared_ptr<ForLoopInfo> Parent;
+  size_t Level;
+
+  IVMap InductionVars;
+  llvm::SmallVector<std::shared_ptr<ForLoopInfo>, 4> Children;
+};
+typedef std::shared_ptr<ForLoopInfo> ForLoopInfoPtr;
+
+/// Search a sub-tree for loops, calculating induction variables found in any
+/// loops along the way.  We *must* construct tree structural information in
+/// order to correctly handle complex loop nests, e.g.:
+///
+/// int a, b;
+/// for(a = ...; a < ...; a++) {
+///   for(b = 0; b < 10; b++) {
+///     ...
+///   }
+///
+///   for(b = 10; b < 20; b++) {
+///
+///   }
+/// }
+///
+/// In this example, induction variable 'b' has different ranges in each of the
+/// nested loops.
+class LoopNestTraversal : public RecursiveASTVisitor<LoopNestTraversal> {
+public:
+  LoopNestTraversal(ASTContext *Ctx) : Ctx(Ctx) {}
+
+  void InitTraversal() { if(!LoopNest.size()) LoopNest.emplace_back(nullptr); }
+
+  bool VisitForStmt(ForStmt *S) {
+    Expr *InitExpr, *CondExpr, *UpdateExpr;
+    IVFinder<NoUnaryOp, FilterAssignOp> Init;
+    IVFinder<NoUnaryOp, FilterRelationalOp> Cond;
+    IVFinder<FilterMathOp, FilterMathLogicOp> Update;
+
+    // Set up data & tree structure information.
+    LoopNest.emplace_back(
+      new ForLoopInfo(S, LoopNest.back(), LoopNest.size() - 1));
+    ForLoopInfoPtr &Cur = LoopNest.back();
+    Loops[S] = Cur;
+    if(Cur->getParent()) Cur->getParent()->addChildLoop(Cur);
+
+    // Find the induction variables in the loop expressions.
+    Init.TraverseStmt(S->getInit());
+    Cond.TraverseStmt(S->getCond());
+    Update.TraverseStmt(S->getInc());
+
+    // Find induction variables which are referenced in all three parts of the
+    // for-loop header.
+    const IVBoundMap &InitVars = Init.getInductionVars();
+    for(auto Var = InitVars.begin(), E = InitVars.end(); Var != E; Var++) {
+      InitExpr = Var->second;
+      CondExpr = Cond.getVarBound(Var->first),
+      UpdateExpr = Update.getVarBound(Var->first);
+      if(InitExpr && CondExpr && UpdateExpr) {
+        InductionVariablePtr IV(
+          new InductionVariable(Var->first, InitExpr, CondExpr,
+                                UpdateExpr, Ctx));
+        Cur->addInductionVar(std::move(IV));
+      }
+    }
+
+    return true;
+  }
+
+  bool TraverseStmt(Stmt *S) {
+    if(!S) return true;
+    RecursiveASTVisitor<LoopNestTraversal>::TraverseStmt(S);
+    if(isa<ForStmt>(S)) LoopNest.pop_back();
+    return true;
+  }
+
+  /// Prune induction variables so each each loop only maintains its own
+  /// induction variables and not those of any nested loops.
+  // TODO this may not be necessary...
+  void PruneInductionVars() {
+    // Each loop nest is a tree in a forest of all loop nests
+    for(auto &Info : Loops)
+      if(Info.second->getLevel() == 0)
+        PruneInductionVars(Info.second);
+  }
+
+  /// Get all loops discovered during the tree traversal.
+  const llvm::DenseMap<ForStmt *, ForLoopInfoPtr> &getLoops() const
+  { return Loops; }
+
+  /// Get the enclosing loop's information for an array access.
+  const ForLoopInfoPtr getEnclosingLoop(const ArrayAccess &A) const {
+    ScopeInfoPtr S = A.getScope();
+    while(S && !isa<ForStmt>(S->ScopeStmt)) S = S->ParentScope;
+    if(!S) return ForLoopInfoPtr(nullptr);
+
+    llvm::DenseMap<ForStmt *, ForLoopInfoPtr>::const_iterator it
+      = Loops.find(cast<ForStmt>(S->ScopeStmt));
+    if(it != Loops.end()) return it->second;
+    else return ForLoopInfoPtr(nullptr);
+  }
+
+private:
+  ASTContext *Ctx;
+
+  // A stack of nested loops to provide induction variable scoping information.
+  llvm::SmallVector<ForLoopInfoPtr, 4> LoopNest;
+
+  // Map loop statements to information gathered during traversal.
+  llvm::DenseMap<ForStmt *, ForLoopInfoPtr> Loops;
+
+  // Recursively prune induction variables in a bottom-up fashion (post-order
+  // traversal).
+  void PruneInductionVars(ForLoopInfoPtr Loop) {
+    for(auto &Child : Loop->getChildren()) {
+      PruneInductionVars(Child);
+      for(auto &IV : Child->getInductionVars())
+        Loop->removeInductionVar(IV.second);
+    }
+  }
+};
+
+/// Get all induction variables for a scope, including induction variables from
+/// any enclosing scopes.
+static void getAllInductionVars(const ForLoopInfoPtr &Scope, IVMap &IVs) {
+  assert(Scope && "Invalid arguments");
+
+  ForLoopInfoPtr TmpScope = Scope;
+  do {
+    const IVMap &LoopIVs = TmpScope->getInductionVars();
+    for(auto IV : LoopIVs) IVs[IV.first] = IV.second;
+    TmpScope = TmpScope->getParent();
+  } while(TmpScope);
+}
+
+/// A set of variable declarations.
+typedef PrefetchDataflow::VarSet VarSet;
+
+/// Search a for-loop statement for array access patterns based on loop
+/// induction variables that can be prefetched at runtime.
+void PrefetchAnalysis::analyzeForStmt() {
+  // Gather loop nest information, including induction variables
+  Loops->InitTraversal();
+  Loops->TraverseStmt(S);
+  Loops->PruneInductionVars();
+
+  // Find array/pointer accesses.
+  ArrAccesses->InitTraversal();
+  ArrAccesses->TraverseStmt(S);
+  ArrAccesses->PruneInvalidOrIgnoredAccesses();
+}
+
+//===----------------------------------------------------------------------===//
+// Prefetch analysis API
+//
+
+void PrefetchAnalysis::analyzeStmt() {
+  if(!Ctx || !S) return;
+
+  Loops = std::make_shared<LoopNestTraversal>(Ctx);
+  ArrAccesses = std::make_shared<ArrayAccessPattern>(Ignore);
+
+  // TODO other types of statements
+  if(isa<ForStmt>(S)) analyzeForStmt();
+
+  pruneArrayAccesses();
+}
+
+void PrefetchAnalysis::calculatePrefetchRanges() {
+  PrefetchDataflow Dataflow(Ctx);
+  IVMap AllIVs;
+  IVMap::const_iterator IVIt;
+  VarSet VarsToTrack;
+  ExprList VarExprs;
+  ReplaceMap LowerBounds, UpperBounds;
+  Expr *UpperBound, *LowerBound;
+  PrefetchExprBuilder::BuildInfo LowerBuild(Ctx, LowerBounds, true),
+                                 UpperBuild(Ctx, UpperBounds, true);
+
+  if(!Ctx || !S || !Loops || !ArrAccesses) return;
+
+  // TODO the following could probably be optimized to reduce re-computing
+  // induction variable sets.
+
+  // Run the dataflow analysis.  Collect all non-induction variables used to
+  // construct array indices to see if induction variables are used in any
+  // assignment expressions.
+  for(auto &Access : ArrAccesses->getArrayAccesses()) {
+    AllIVs.clear();
+    ForLoopInfoPtr Scope = Loops->getEnclosingLoop(Access);
+    getAllInductionVars(Scope, AllIVs);
+    for(auto &Var : Access.getVarsInIdx()) {
+      if(!AllIVs.count(Var)) VarsToTrack.insert(Var);
+    }
+  }
+
+  Dataflow.runDataflow(cast<ForStmt>(S)->getBody(), VarsToTrack);
+
+  // Reconstruct array subscript expressions with induction variable references
+  // replaced by their bounds.  This includes variables defined using
+  // expressions containing induction variables.
+  for(auto &Access : ArrAccesses->getArrayAccesses()) {
+    LowerBuild.reset();
+    UpperBuild.reset();
+    AllIVs.clear();
+
+    // Get the expressions for replacing upper & lower bounds of induction
+    // variables.  Note that we *must* add all induction variables even if
+    // they're not directly used, as other variables used in the index
+    // calculation may be defined based on induction variables.  For example:
+    //
+    // for(int i = ...; i < ...; i++) {
+    //   int j = i + offset;
+    //   ...
+    //   arr[j] = ...
+    // }
+    //
+    // In this example, 'i' is not directly used in addressing but the dataflow
+    // analysis determines that 'j' is defined based on 'i', and hence we need
+    // to replace 'j' with induction variable bounds expressions.
+    ForLoopInfoPtr Scope = Loops->getEnclosingLoop(Access);
+    getAllInductionVars(Scope, AllIVs);
+    for(auto &Pair : AllIVs) {
+      const InductionVariablePtr &IV = Pair.second;
+      LowerBounds.insert(ReplacePair(IV->getVariable(), IV->getLowerBound()));
+      UpperBounds.insert(ReplacePair(IV->getVariable(), IV->getUpperBound()));
+    }
+
+    // Add other variables used in array calculation that may be defined using
+    // induction variable expressions.
+    for(auto &Var : Access.getVarsInIdx()) {
+      IVIt = AllIVs.find(Var);
+      if(IVIt == AllIVs.end()) {
+        Dataflow.getVariableValues(Var, Access.getStmt(), VarExprs);
+        // TODO currently if a variable used in an index calculation can take
+        // on more than one value due to control flow, we just avoid inserting
+        // prefetch expressions due to all the possible permutations.
+        if(VarExprs.size() == 1) {
+          LowerBounds.insert(ReplacePair(Var, *VarExprs.begin()));
+          UpperBounds.insert(ReplacePair(Var, *VarExprs.begin()));
+        }
+      }
+    }
+
+    // Create array access bounds expressions
+    LowerBound =
+      PrefetchExprBuilder::cloneWithReplacement(Access.getIndex(), LowerBuild),
+    UpperBound =
+      PrefetchExprBuilder::cloneWithReplacement(Access.getIndex(), UpperBuild);
+    if(LowerBound && UpperBound)
+      ToPrefetch.emplace_back(Access.getAccessType(), Access.getBase(),
+                              LowerBound, UpperBound);
+  }
+
+  mergePrefetchRanges();
+  prunePrefetchRanges();
+}
+
+void PrefetchAnalysis::print(llvm::raw_ostream &O) const {
+  PrintingPolicy Policy(Ctx->getLangOpts());
+  for(auto &Range : ToPrefetch) {
+    O << "Array '" << Range.getArray()->getName() << "': ";
+    Range.getStart()->printPretty(O, nullptr, Policy);
+    O << " to ";
+    Range.getEnd()->printPretty(O, nullptr, Policy);
+    O << " (" << Range.getTypeName() << ")\n";
+  }
+}
+
diff --git a/clang/lib/Sema/PrefetchDataflow.cpp b/clang/lib/Sema/PrefetchDataflow.cpp
new file mode 100644
index 00000000000..3d1f944bf1c
--- /dev/null
+++ b/clang/lib/Sema/PrefetchDataflow.cpp
@@ -0,0 +1,290 @@
+//=- PrefetchDataflow.cpp - Dataflow analysis for prefetching ------------*-==//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements the dataflow of expressions as required for prefetching
+// analysis.  This is required to correctly discover how variables are used in
+// memory accesses in order to construct memory access ranges.
+//
+//===----------------------------------------------------------------------===//
+
+#include "clang/AST/ASTContext.h"
+#include "clang/AST/ParentMap.h"
+#include "clang/AST/RecursiveASTVisitor.h"
+#include "clang/Sema/PrefetchDataflow.h"
+#include "clang/Sema/PrefetchExprBuilder.h"
+#include "llvm/Support/Debug.h"
+#include <queue>
+
+using namespace clang;
+
+//===----------------------------------------------------------------------===//
+// Common utilities
+//
+
+/// Return whether a statement is a loop construct.
+static inline bool isLoopStmt(const Stmt *S) {
+  if(isa<DoStmt>(S) || isa<ForStmt>(S) || isa<WhileStmt>(S)) return true;
+  else return false;
+}
+
+/// Return whether a binary operator is an assign expression.
+static inline bool isAssign(const BinaryOperator *B) {
+  if(B->getOpcode() == BO_Assign) return true;
+  return false;
+}
+
+/// Return whether a binary operator is an operation + assign expression.
+static inline bool isMathAssign(const BinaryOperator *B) {
+  switch(B->getOpcode()) {
+  case BO_MulAssign: case BO_DivAssign: case BO_RemAssign:
+  case BO_AddAssign: case BO_SubAssign: case BO_ShlAssign:
+  case BO_ShrAssign: case BO_AndAssign: case BO_XorAssign:
+  case BO_OrAssign:
+    return true;
+  default: return false;
+  }
+}
+
+/// Return the variable referenced by the expression E, or a nullptr if none
+/// were referenced.
+static const VarDecl *getVariableIfReference(const Expr *E) {
+  const DeclRefExpr *DR;
+  const VarDecl *VD;
+
+  DR = dyn_cast<DeclRefExpr>(E->IgnoreImpCasts());
+  if(!DR) return nullptr;
+  VD = dyn_cast<VarDecl>(DR->getDecl());
+  return VD;
+}
+
+//===----------------------------------------------------------------------===//
+// Expression dataflow API
+//
+
+PrefetchDataflow::PrefetchDataflow() : Ctx(nullptr) {}
+PrefetchDataflow::PrefetchDataflow(ASTContext *Ctx) : Ctx(Ctx) {}
+PrefetchDataflow::PrefetchDataflow(const PrefetchDataflow &RHS)
+  : Ctx(RHS.Ctx) {}
+
+PrefetchDataflow &PrefetchDataflow::operator=(const PrefetchDataflow &RHS) {
+  Ctx = RHS.Ctx;
+  return *this;
+}
+
+/// Analyze a statement to determine if we're defining a relevant variable.  If
+/// so, clone & store the defining expression.
+static void checkAndUpdateVarDefs(ASTContext *Ctx, const Stmt *S,
+                                  const PrefetchDataflow::VarSet &VarsToTrack,
+                                  SymbolicValueMap &VarExprs) {
+  Expr *Clone;
+  const DeclStmt *DS;
+  const BinaryOperator *BO;
+  const VarDecl *VD;
+
+  // Check for variable declarations with initializers, the initial definition.
+  if((DS = dyn_cast<DeclStmt>(S))) {
+    for(auto D : DS->getDeclGroup()) {
+      VD = dyn_cast<VarDecl>(D);
+      if(VD && VD->hasInit() && VarsToTrack.count(VD)) {
+        // TODO unfortunately CFG & accompanying classes expose statements
+        // & expressions with const qualifiers.  But, we *really* need them
+        // to not be const qualified in order to clone them (in particular,
+        // cloning DeclRefExprs becomes a headache).
+        Clone = PrefetchExprBuilder::clone((Expr *)VD->getInit(), Ctx);
+        if(Clone) VarExprs[VD].insert(Clone);
+      }
+    }
+    return;
+  }
+
+  BO = dyn_cast<BinaryOperator>(S);
+  if(!BO) return;
+
+  // Check for assignment operation to a relevant variable.  If we had
+  // previous expression(s) describing the variable's value the assignment
+  // overwrites them.
+  if(isAssign(BO)) {
+    VD = getVariableIfReference(BO->getLHS());
+    if(VD && VarsToTrack.count(VD)) {
+      ExprList &Exprs = VarExprs[VD];
+      Exprs.clear();
+      Clone = PrefetchExprBuilder::clone(BO->getRHS(), Ctx);
+      if(Clone) Exprs.insert(Clone);
+    }
+  }
+  // TODO we currently don't handle math + assign operations, so the
+  // dataflow analysis clamps to 'unknown' (i.e., no expressions).
+  else if(isMathAssign(BO)) {
+    VD = getVariableIfReference(BO->getLHS());
+    if(VD && VarsToTrack.count(VD)) VarExprs.erase(VD);
+  }
+}
+
+void PrefetchDataflow::runDataflow(Stmt *S, VarSet &VarsToTrack) {
+  const CFGBlock *Block;
+  CFGBlock::const_succ_iterator Succ, SE;
+  CFGBlockSet Seen;
+  std::queue<const CFGBlock *> Work;
+  Optional<CFGStmt> StmtNode;
+  BlockValuesMap::iterator BVIt;
+  SymbolicValueMap CurMap;
+
+  if(!VarsToTrack.size()) return;
+  this->S = S;
+  TheCFG = CFG::buildCFG(nullptr, S, Ctx, CFG::BuildOptions());
+
+  Work.push(&TheCFG->getEntry());
+  while(!Work.empty()) {
+    Block = Work.front();
+    Work.pop();
+    Seen.insert(Block);
+
+    // Find assignment operations within the block.  Because of the forward
+    // dataflow algorithm, predecessors should have already pushed dataflow
+    // expressions, if any, to this block.
+    CurMap = VarValues[Block];
+    for(auto &Elem : *Block) {
+      StmtNode = Elem.getAs<CFGStmt>();
+      if(!StmtNode) continue;
+      checkAndUpdateVarDefs(Ctx, StmtNode->getStmt(), VarsToTrack, CurMap);
+    }
+
+    // Push dataflow expressions to successors & add not-yet visited blocks to
+    // the work queue.
+    for(Succ = Block->succ_begin(), SE = Block->succ_end();
+        Succ != SE; Succ++) {
+      if(!Succ->isReachable() || Seen.count(*Succ)) continue;
+      else {
+        SymbolicValueMap &SuccMap = VarValues[*Succ];
+        for(auto &Pair : CurMap) {
+          ExprList &VarExprs = SuccMap[Pair.first];
+          for(auto Expr : Pair.second) VarExprs.insert(Expr);
+        }
+        Work.push(*Succ);
+      }
+    }
+
+    // TODO do we need to treat sub-scopes, e.g., loops, differently?
+  }
+
+  // Make it easier to look up analysis for statements
+  PMap.reset(new ParentMap(S));
+  StmtToBlock.reset(CFGStmtMap::Build(TheCFG.get(), PMap.get()));
+}
+
+/// Search for statements in sub-trees.
+class StmtFinder : public RecursiveASTVisitor<StmtFinder> {
+public:
+  void initialize(const Stmt *TheStmt) {
+    this->TheStmt = TheStmt;
+    Found = false;
+  }
+
+  bool TraverseStmt(Stmt *S) {
+    if(S == TheStmt) {
+      Found = true;
+      return false;
+    }
+    else return RecursiveASTVisitor::TraverseStmt(S);
+  }
+
+  bool foundStmt() const { return Found; }
+
+private:
+  const Stmt *TheStmt;
+  bool Found;
+};
+
+void PrefetchDataflow::getVariableValues(VarDecl *Var,
+                                         const Stmt *Use,
+                                         ExprList &Exprs) const {
+  SymbolicValueMap TmpMap;
+  VarSet VarsToTrack;
+  Optional<CFGStmt> StmtNode;
+  StmtFinder Finder;
+
+  Exprs.clear();
+
+  // Find analysis for the given variable, if any, at the start of the block
+  // containing the statement.
+  if(!StmtToBlock) return;
+  const CFGBlock *B = StmtToBlock->getBlock(Use);
+  if(!B) return;
+  BlockValuesMap::const_iterator ValIt = VarValues.find(B);
+  if(ValIt == VarValues.end()) return;
+  const SymbolicValueMap &Values = ValIt->second;
+  SymbolicValueMap::const_iterator SymIt = Values.find(Var);
+
+  // Walk through the block to the statement, searching for definitions between
+  // the start of the block and the statement argument
+  VarsToTrack.insert(Var);
+  if(SymIt != Values.end()) TmpMap[Var] = SymIt->second;
+  else TmpMap[Var] = ExprList();
+  for(auto &Elem : *B) {
+    StmtNode = Elem.getAs<CFGStmt>();
+    if(!StmtNode) continue;
+
+    // TODO CFG exposes statements with const qualifiers while the
+    // RecursiveASTVisitor requires non-const qualified statements.
+    Finder.initialize(Use);
+    Finder.TraverseStmt((Stmt *)StmtNode->getStmt());
+    if(Finder.foundStmt()) {
+      Exprs = TmpMap[Var];
+      return;
+    }
+
+    checkAndUpdateVarDefs(Ctx, StmtNode->getStmt(), VarsToTrack, TmpMap);
+  }
+}
+
+void PrefetchDataflow::reset() {
+  S = nullptr;
+  StmtToBlock.reset();
+  PMap.reset();
+  TheCFG.reset();
+  VarValues.clear();
+}
+
+void PrefetchDataflow::print(llvm::raw_ostream &O) const {
+  if(!S) {
+    O << "<Prefetch Dataflow> No analysis -- did you run with runDataflow()?\n";
+    return;
+  }
+
+  if(!TheCFG) {
+    O << "<Prefetch Dataflow> No variables to track\n";
+    return;
+  }
+
+  if(!VarValues.size()) {
+    O << "<Prefetch Dataflow> No symbolic expressions detected\n";
+    return;
+  }
+
+  O << "<Prefetch Dataflow> Analysis results:\n";
+  PrintingPolicy PP(Ctx->getLangOpts());
+  for(auto Node : *TheCFG) {
+    Node->print(O, TheCFG.get(), Ctx->getLangOpts(), true);
+    O << "\n";
+    BlockValuesMap::const_iterator BVIt = VarValues.find(Node);
+    if(BVIt != VarValues.end()) {
+      for(auto VarValPair : BVIt->second) {
+        O << "Values for '" << VarValPair.first->getName() << "':\n";
+        for(auto E : VarValPair.second) {
+          E->printPretty(O, nullptr, PP);
+          O << "\n";
+        }
+      }
+    }
+    else O << "\n-> No dataflow values <-\n";
+  }
+}
+
+void PrefetchDataflow::dump() const { print(llvm::dbgs()); }
+
diff --git a/clang/lib/Sema/PrefetchExprBuilder.cpp b/clang/lib/Sema/PrefetchExprBuilder.cpp
new file mode 100644
index 00000000000..a98ce7f6cff
--- /dev/null
+++ b/clang/lib/Sema/PrefetchExprBuilder.cpp
@@ -0,0 +1,306 @@
+//=- PrefetchExprBuilder.cpp - Prefetching expression builder ------------*-==//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file defines a set of utilities for building expressions for
+// prefetching.
+//
+//===----------------------------------------------------------------------===//
+
+#include "clang/AST/ASTContext.h"
+#include "clang/Sema/PrefetchAnalysis.h"
+#include "clang/Sema/PrefetchExprBuilder.h"
+#include "llvm/Support/Debug.h"
+
+using namespace clang;
+
+//===----------------------------------------------------------------------===//
+// Prefetch expression comparison definitions
+//
+
+static bool
+BinaryOperatorEqual(const BinaryOperator *A, const BinaryOperator *B) {
+  if(A->getOpcode() != B->getOpcode()) return false;
+  else return PrefetchExprEquality::exprEqual(A->getLHS(), B->getLHS()) &&
+              PrefetchExprEquality::exprEqual(A->getRHS(), B->getRHS());
+}
+
+static bool UnaryOperatorEqual(const UnaryOperator *A, const UnaryOperator *B) {
+  if(A->getOpcode() != B->getOpcode()) return false;
+  else return PrefetchExprEquality::exprEqual(A->getSubExpr(), B->getSubExpr());
+}
+
+static bool ArraySubscriptExprEqual(const ArraySubscriptExpr *A,
+                                    const ArraySubscriptExpr *B) {
+  return PrefetchExprEquality::exprEqual(A->getLHS(), B->getLHS()) &&
+         PrefetchExprEquality::exprEqual(A->getRHS(), B->getRHS());
+}
+
+static bool DeclRefExprEqual(const DeclRefExpr *A, const DeclRefExpr *B) {
+  if(A->getDecl() == B->getDecl()) return true;
+  else return false;
+}
+
+static bool
+ImplicitCastExprEqual(const ImplicitCastExpr *A, const ImplicitCastExpr *B) {
+  if(A->getCastKind() != B->getCastKind()) return false;
+  else return PrefetchExprEquality::exprEqual(A->getSubExpr(), B->getSubExpr());
+}
+
+static bool
+IntegerLiteralEqual(const IntegerLiteral *A, const IntegerLiteral *B) {
+  return A->getValue() == B->getValue();
+}
+
+bool PrefetchExprEquality::exprEqual(const Expr *A, const Expr *B) {
+  const BinaryOperator *B_A, *B_B;
+  const UnaryOperator *U_A, *U_B;
+  const ArraySubscriptExpr *A_A, *A_B;
+  const DeclRefExpr *D_A, *D_B;
+  const ImplicitCastExpr *C_A, *C_B;
+  const IntegerLiteral *I_A, *I_B;
+
+  if(!A || ! B) return false;
+
+  // Check common characteristics.  Note that by checking the statement class,
+  // we know the expressions are of the same type and can use cast<> below.
+  if(A->getStmtClass() != B->getStmtClass() ||
+     A->getType() != B->getType() ||
+     A->getValueKind() != B->getValueKind() ||
+     A->getObjectKind() != B->getObjectKind()) return false;
+
+  // TODO better way to switch on type?
+  if((B_A = dyn_cast<BinaryOperator>(A))) {
+    B_B = cast<BinaryOperator>(B);
+    return BinaryOperatorEqual(B_A, B_B);
+  }
+  else if((U_A = dyn_cast<UnaryOperator>(A))) {
+    U_B = cast<UnaryOperator>(B);
+    return UnaryOperatorEqual(U_A, U_B);
+  }
+  else if((A_A = dyn_cast<ArraySubscriptExpr>(A))) {
+    A_B = cast<ArraySubscriptExpr>(B);
+    return ArraySubscriptExprEqual(A_A, A_B);
+  }
+  else if((D_A = dyn_cast<DeclRefExpr>(A))) {
+    D_B = cast<DeclRefExpr>(B);
+    return DeclRefExprEqual(D_A, D_B);
+  }
+  else if((C_A = dyn_cast<ImplicitCastExpr>(A))) {
+    C_B = cast<ImplicitCastExpr>(B);
+    return ImplicitCastExprEqual(C_A, C_B);
+  }
+  else if((I_A = dyn_cast<IntegerLiteral>(A))) {
+    I_B = cast<IntegerLiteral>(B);
+    return IntegerLiteralEqual(I_A, I_B);
+  }
+  else return false;
+}
+
+//===----------------------------------------------------------------------===//
+// Modifier class definitions
+//
+
+void PrefetchExprBuilder::Modifier::ClassifyModifier(const Expr *E,
+                                                     const ASTContext *Ctx) {
+  unsigned Bits;
+  const DeclRefExpr *DR;
+  const BinaryOperator *B;
+  const IntegerLiteral *L;
+  QualType BaseTy;
+
+  Ty = Unknown;
+  if(!E) return;
+
+  E = E->IgnoreImpCasts();
+  if((B = dyn_cast<BinaryOperator>(E))) {
+    // Note: both operands *must* have same type
+    BaseTy = B->getLHS()->getType().getDesugaredType(*Ctx);
+    assert(PrefetchAnalysis::isScalarIntType(BaseTy) &&
+           "Invalid expression type");
+    Bits = PrefetchAnalysis::getTypeSize(cast<BuiltinType>(BaseTy)->getKind());
+
+    switch(B->getOpcode()) {
+    default: Ty = None; break;
+    case BO_LT:
+      Ty = Sub;
+      Val = llvm::APInt(Bits, 1, false);
+      break;
+    case BO_GT:
+      Ty = Add;
+      Val = llvm::APInt(Bits, 1, false);
+      break;
+    // TODO hybrid math/assign operations
+    }
+  }
+  else if((DR = dyn_cast<DeclRefExpr>(E))) Ty = None;
+  else if((L = dyn_cast<IntegerLiteral>(E))) Ty = None;
+}
+
+//===----------------------------------------------------------------------===//
+// Prefetch expression builder definitions
+//
+
+typedef PrefetchExprBuilder::BuildInfo BuildInfo;
+
+Expr *PrefetchExprBuilder::cloneWithReplacement(Expr *E, BuildInfo &Info) {
+  BinaryOperator *B;
+  UnaryOperator *U;
+  ArraySubscriptExpr *A;
+  DeclRefExpr *D;
+  ImplicitCastExpr *C;
+  IntegerLiteral *I;
+
+  if(!E) return nullptr;
+
+  // TODO better way to switch on type?
+  if((B = dyn_cast<BinaryOperator>(E)))
+    return cloneBinaryOperator(B, Info);
+  else if((U = dyn_cast<UnaryOperator>(E)))
+    return cloneUnaryOperator(U, Info);
+  else if((A = dyn_cast<ArraySubscriptExpr>(E)))
+    return cloneArraySubscriptExpr(A, Info);
+  else if((D = dyn_cast<DeclRefExpr>(E)))
+    return cloneDeclRefExpr(D, Info);
+  else if((C = dyn_cast<ImplicitCastExpr>(E)))
+    return cloneImplicitCastExpr(C, Info);
+  else if((I = dyn_cast<IntegerLiteral>(E)))
+    return cloneIntegerLiteral(I, Info);
+  else {
+    // TODO delete
+    llvm::dbgs() << "Unhandled expression:\n";
+    if(Info.dumpInColor) E->dumpColor();
+    else E->dump();
+  }
+
+  return nullptr;
+}
+
+Expr *PrefetchExprBuilder::clone(Expr *E, ASTContext *Ctx) {
+  ReplaceMap Dummy; // No variables, don't replace any DeclRefExprs
+  BuildInfo DummyInfo(Ctx, Dummy, true);
+  return cloneWithReplacement(E, DummyInfo);
+}
+
+Expr *PrefetchExprBuilder::cloneBinaryOperator(BinaryOperator *B,
+                                               BuildInfo &Info) {
+  Expr *LHS = cloneWithReplacement(B->getLHS(), Info),
+       *RHS = cloneWithReplacement(B->getRHS(), Info);
+  if(!LHS || !RHS) return nullptr;
+  return new (*Info.Ctx) BinaryOperator(LHS, RHS, B->getOpcode(),
+                                        B->getType(),
+                                        B->getValueKind(),
+                                        B->getObjectKind(),
+                                        SourceLocation(),
+                                        B->isFPContractable());
+}
+
+Expr *PrefetchExprBuilder::cloneUnaryOperator(UnaryOperator *U,
+                                              BuildInfo &Info) {
+  Expr *Sub = cloneWithReplacement(U->getSubExpr(), Info);
+  if(!Sub) return nullptr;
+  return new (*Info.Ctx) UnaryOperator(Sub, U->getOpcode(),
+                                       U->getType(),
+                                       U->getValueKind(),
+                                       U->getObjectKind(),
+                                       SourceLocation());
+}
+
+Expr *PrefetchExprBuilder::cloneArraySubscriptExpr(ArraySubscriptExpr *A,
+                                                   BuildInfo &Info) {
+  Expr *LHS = cloneWithReplacement(A->getLHS(), Info),
+       *RHS = cloneWithReplacement(A->getRHS(), Info);
+  if(!LHS || !RHS) return nullptr;
+  return new (*Info.Ctx) ArraySubscriptExpr(LHS, RHS, A->getType(),
+                                            A->getValueKind(),
+                                            A->getObjectKind(),
+                                            SourceLocation());
+}
+
+Expr *PrefetchExprBuilder::cloneDeclRefExpr(DeclRefExpr *D,
+                                            BuildInfo &Info) {
+  Expr *Clone = nullptr;
+  VarDecl *VD;
+  ReplaceMap::const_iterator it;
+
+  // If the variable is relevant and we haven't replaced it before, replace it
+  // with the specified expression.
+  if((VD = dyn_cast<VarDecl>(D->getDecl())) &&
+     (it = Info.VarReplace.find(VD)) != Info.VarReplace.end() &&
+     !Info.SeenVars.count(VD)) {
+    Info.SeenVars.insert(VD);
+    Clone = cloneWithReplacement(it->second, Info);
+    Info.SeenVars.erase(VD);
+    return Clone;
+  }
+
+  // Clone the DeclRefExpr if the variable isn't relevant or if cloning the
+  // replacement failed.
+  return new (*Info.Ctx) DeclRefExpr(D->getDecl(),
+                                     D->refersToEnclosingVariableOrCapture(),
+                                     D->getType(),
+                                     D->getValueKind(),
+                                     SourceLocation(),
+                                     D->getNameInfo().getInfo());
+}
+
+Expr *PrefetchExprBuilder::cloneImplicitCastExpr(ImplicitCastExpr *C,
+                                                 BuildInfo &Info) {
+  Expr *Sub = cloneWithReplacement(C->getSubExpr(), Info);
+  if(!Sub) return nullptr;
+
+  // Avoid the situation that when replacing an induction variable with another
+  // expression we accidentally chain together 2 implicit casts (which causes
+  // CodeGen to choke).
+  if(C->getCastKind() == CastKind::CK_LValueToRValue &&
+     Sub->getValueKind() == VK_RValue)
+    return Sub;
+  else
+    return new (*Info.Ctx) ImplicitCastExpr(ImplicitCastExpr::OnStack,
+                                            C->getType(),
+                                            C->getCastKind(),
+                                            Sub,
+                                            C->getValueKind());
+}
+
+Expr *PrefetchExprBuilder::cloneIntegerLiteral(IntegerLiteral *L,
+                                               BuildInfo &Info) {
+  return new (*Info.Ctx) IntegerLiteral(*Info.Ctx, L->getValue(),
+                                        L->getType(),
+                                        SourceLocation());
+}
+
+Expr *PrefetchExprBuilder::cloneAndModifyExpr(Expr *E,
+                                              const Modifier &Mod,
+                                              ASTContext *Ctx) {
+  BinaryOperator::Opcode Op;
+  IntegerLiteral *RHS;
+
+  E = clone(E, Ctx);
+  if(!E) return nullptr;
+
+  switch(Mod.getType()) {
+  case Modifier::Add: Op = BO_Add; break;
+  case Modifier::Sub: Op = BO_Sub; break;
+  case Modifier::Mul: Op = BO_Mul; break;
+  case Modifier::Div: Op = BO_Div; break;
+  case Modifier::None: return E; // Nothing to do
+  case Modifier::Unknown: return nullptr; // Couldn't classify
+  }
+
+  RHS = new (*Ctx) IntegerLiteral(*Ctx, Mod.getVal(),
+                                  E->getType(),
+                                  SourceLocation());
+  return new (*Ctx) BinaryOperator(E, RHS, Op,
+                                   E->getType(),
+                                   E->getValueKind(),
+                                   E->getObjectKind(),
+                                   SourceLocation(),
+                                   false);
+}
+
diff --git a/clang/lib/Sema/SemaOpenMP.cpp b/clang/lib/Sema/SemaOpenMP.cpp
index ca67a1c0bf2..80b00ca0c96 100644
--- a/clang/lib/Sema/SemaOpenMP.cpp
+++ b/clang/lib/Sema/SemaOpenMP.cpp
@@ -4616,6 +4616,7 @@ OMPClause *Sema::ActOnOpenMPSingleExprClause(OpenMPClauseKind Kind, Expr *Expr,
   case OMPC_seq_cst:
   case OMPC_depend:
   case OMPC_unknown:
+  case OMPC_prefetch:
     llvm_unreachable("Clause is not allowed.");
   }
   return Res;
@@ -4831,6 +4832,7 @@ OMPClause *Sema::ActOnOpenMPSimpleClause(
   case OMPC_seq_cst:
   case OMPC_depend:
   case OMPC_unknown:
+  case OMPC_prefetch:
     llvm_unreachable("Clause is not allowed.");
   }
   return Res;
@@ -4952,6 +4954,7 @@ OMPClause *Sema::ActOnOpenMPSingleExprWithArgClause(
   case OMPC_seq_cst:
   case OMPC_depend:
   case OMPC_unknown:
+  case OMPC_prefetch:
     llvm_unreachable("Clause is not allowed.");
   }
   return Res;
@@ -5075,6 +5078,7 @@ OMPClause *Sema::ActOnOpenMPClause(OpenMPClauseKind Kind,
   case OMPC_flush:
   case OMPC_depend:
   case OMPC_unknown:
+  case OMPC_prefetch:
     llvm_unreachable("Clause is not allowed.");
   }
   return Res;
@@ -5129,10 +5133,12 @@ OMPClause *Sema::ActOnOpenMPSeqCstClause(SourceLocation StartLoc,
 
 OMPClause *Sema::ActOnOpenMPVarListClause(
     OpenMPClauseKind Kind, ArrayRef<Expr *> VarList, Expr *TailExpr,
-    SourceLocation StartLoc, SourceLocation LParenLoc, SourceLocation ColonLoc,
+    Expr *EndExpr, SourceLocation StartLoc, SourceLocation LParenLoc,
+    SourceLocation ColonLoc, SourceLocation EndColonLoc,
     SourceLocation EndLoc, CXXScopeSpec &ReductionIdScopeSpec,
     const DeclarationNameInfo &ReductionId, OpenMPDependClauseKind DepKind,
-    SourceLocation DepLoc) {
+    SourceLocation DepLoc, OpenMPPrefetchClauseKind PrefKind,
+    SourceLocation PrefLoc) {
   OMPClause *Res = nullptr;
   switch (Kind) {
   case OMPC_private:
@@ -5172,6 +5178,11 @@ OMPClause *Sema::ActOnOpenMPVarListClause(
     Res = ActOnOpenMPDependClause(DepKind, DepLoc, ColonLoc, VarList, StartLoc,
                                   LParenLoc, EndLoc);
     break;
+  case OMPC_prefetch:
+    Res = ActOnOpenMPPrefetchClause(PrefKind, PrefLoc, VarList, TailExpr,
+                                    EndExpr, StartLoc, LParenLoc, ColonLoc,
+                                    EndColonLoc, EndLoc);
+    break;
   case OMPC_if:
   case OMPC_final:
   case OMPC_num_threads:
@@ -6785,3 +6796,58 @@ Sema::ActOnOpenMPDependClause(OpenMPDependClauseKind DepKind,
                                  DepLoc, ColonLoc, Vars);
 }
 
+OMPClause *
+Sema::ActOnOpenMPPrefetchClause(OpenMPPrefetchClauseKind PrefKind,
+                                SourceLocation PrefLoc,
+                                ArrayRef<Expr *> VarList,
+                                Expr *Start, Expr *End,
+                                SourceLocation StartLoc,
+                                SourceLocation LParenLoc,
+                                SourceLocation FirstColonLoc,
+                                SourceLocation SecondColonLoc,
+                                SourceLocation EndLoc) {
+  // Check validity of range expressions & variables which use them
+  if(Start) {
+    if(!Start->getType()->isIntegerType()) {
+      Diag(Start->getExprLoc(), diag::err_omp_invalid_prefetch_range_type);
+      return nullptr;
+    }
+
+    if(End) {
+      if(!End->getType()->isIntegerType()) {
+        Diag(End->getExprLoc(), diag::err_omp_invalid_prefetch_range_type);
+        return nullptr;
+      }
+    }
+
+    for(const auto &Var : VarList) {
+      QualType T = Var->getType();
+
+      if(!T->isArrayType() && !T->isPointerType()) {
+        Diag(Var->getExprLoc(), diag::err_omp_invalid_prefetch_var_type)
+          << "array or pointer";
+        return nullptr;
+      }
+    }
+  }
+  else {
+    for(const auto &Var : VarList) {
+      // Note: array types with known sizes may be decayed to pointer types.
+      // We want the original type (i.e., the array type) but we can't simply
+      // call T.getDesugaredType() as it will return the pointer type.
+      QualType T = Var->getType();
+      while(isa<DecayedType>(T)) T = cast<DecayedType>(T)->getOriginalType();
+
+      if(!T->isConstantArrayType()) {
+        Diag(Var->getExprLoc(), diag::err_omp_invalid_prefetch_var_type)
+          << "constant-sized array";
+        return nullptr;
+      }
+    }
+  }
+
+  return OMPPrefetchClause::Create(Context, PrefKind, PrefLoc, VarList, Start,
+                                   End, StartLoc, LParenLoc, FirstColonLoc,
+                                   SecondColonLoc, EndLoc);
+}
+
diff --git a/clang/lib/Sema/TreeTransform.h b/clang/lib/Sema/TreeTransform.h
index 6e193a3529c..ecd0494322f 100644
--- a/clang/lib/Sema/TreeTransform.h
+++ b/clang/lib/Sema/TreeTransform.h
@@ -7469,6 +7469,14 @@ TreeTransform<Derived>::TransformOMPDependClause(OMPDependClause *C) {
       C->getLocStart(), C->getLParenLoc(), C->getLocEnd());
 }
 
+template <typename Derived>
+OMPClause *
+TreeTransform<Derived>::TransformOMPPrefetchClause(OMPPrefetchClause *C) {
+  // TODO Rob
+  llvm_unreachable("not yet implmented");
+  return C;
+}
+
 //===----------------------------------------------------------------------===//
 // Expression transformation
 //===----------------------------------------------------------------------===//
diff --git a/clang/lib/Serialization/ASTReaderStmt.cpp b/clang/lib/Serialization/ASTReaderStmt.cpp
index 76e8334695f..c8e447f185c 100644
--- a/clang/lib/Serialization/ASTReaderStmt.cpp
+++ b/clang/lib/Serialization/ASTReaderStmt.cpp
@@ -2065,6 +2065,11 @@ void OMPClauseReader::VisitOMPDependClause(OMPDependClause *C) {
   C->setVarRefs(Vars);
 }
 
+void OMPClauseReader::VisitOMPPrefetchClause(OMPPrefetchClause *C) {
+  // TODO Rob
+  llvm_unreachable("not yet implemented");
+}
+
 //===----------------------------------------------------------------------===//
 // OpenMP Directives.
 //===----------------------------------------------------------------------===//
diff --git a/clang/lib/Serialization/ASTWriterStmt.cpp b/clang/lib/Serialization/ASTWriterStmt.cpp
index 0dd809036a3..ad41e310ba5 100644
--- a/clang/lib/Serialization/ASTWriterStmt.cpp
+++ b/clang/lib/Serialization/ASTWriterStmt.cpp
@@ -1916,6 +1916,11 @@ void OMPClauseWriter::VisitOMPDependClause(OMPDependClause *C) {
     Writer->Writer.AddStmt(VE);
 }
 
+void OMPClauseWriter::VisitOMPPrefetchClause(OMPPrefetchClause *C) {
+  // TODO Rob
+  llvm_unreachable("not yet implemented");
+}
+
 //===----------------------------------------------------------------------===//
 // OpenMP Directives.
 //===----------------------------------------------------------------------===//
diff --git a/clang/tools/libclang/CIndex.cpp b/clang/tools/libclang/CIndex.cpp
index 8225a6c4429..c987ce63867 100644
--- a/clang/tools/libclang/CIndex.cpp
+++ b/clang/tools/libclang/CIndex.cpp
@@ -2147,6 +2147,10 @@ void OMPClauseEnqueue::VisitOMPFlushClause(const OMPFlushClause *C) {
 void OMPClauseEnqueue::VisitOMPDependClause(const OMPDependClause *C) {
   VisitOMPClauseList(C);
 }
+void OMPClauseEnqueue::VisitOMPPrefetchClause(const OMPPrefetchClause *C) {
+  // TODO Rob
+  llvm_unreachable("not yet implemented");
+}
 }
 
 void EnqueueVisitor::EnqueueChildren(const OMPClause *S) {
diff --git a/compiler-rt/lib/sanitizer_common/sanitizer_linux.cc b/compiler-rt/lib/sanitizer_common/sanitizer_linux.cc
index 98e5d122a0f..33156a04341 100644
--- a/compiler-rt/lib/sanitizer_common/sanitizer_linux.cc
+++ b/compiler-rt/lib/sanitizer_common/sanitizer_linux.cc
@@ -544,8 +544,7 @@ uptr internal_prctl(int option, uptr arg2, uptr arg3, uptr arg4, uptr arg5) {
 }
 #endif
 
-uptr internal_sigaltstack(const struct sigaltstack *ss,
-                         struct sigaltstack *oss) {
+uptr internal_sigaltstack(const void *ss, void *oss) {
   return internal_syscall(SYSCALL(sigaltstack), (uptr)ss, (uptr)oss);
 }
 
diff --git a/compiler-rt/lib/sanitizer_common/sanitizer_linux.h b/compiler-rt/lib/sanitizer_common/sanitizer_linux.h
index e9fc4ad448d..3fdb976ea1d 100644
--- a/compiler-rt/lib/sanitizer_common/sanitizer_linux.h
+++ b/compiler-rt/lib/sanitizer_common/sanitizer_linux.h
@@ -21,7 +21,6 @@
 #include "sanitizer_platform_limits_posix.h"
 
 struct link_map;  // Opaque type returned by dlopen().
-struct sigaltstack;
 
 namespace __sanitizer {
 // Dirent structure for getdents(). Note that this structure is different from
@@ -30,8 +29,7 @@ struct linux_dirent;
 
 // Syscall wrappers.
 uptr internal_getdents(fd_t fd, struct linux_dirent *dirp, unsigned int count);
-uptr internal_sigaltstack(const struct sigaltstack* ss,
-                          struct sigaltstack* oss);
+uptr internal_sigaltstack(const void* ss, void* oss);
 uptr internal_sigprocmask(int how, __sanitizer_sigset_t *set,
     __sanitizer_sigset_t *oldset);
 void internal_sigfillset(__sanitizer_sigset_t *set);
diff --git a/compiler-rt/lib/sanitizer_common/sanitizer_platform_limits_posix.cc b/compiler-rt/lib/sanitizer_common/sanitizer_platform_limits_posix.cc
index aaa37ed02eb..31ce2c5c944 100644
--- a/compiler-rt/lib/sanitizer_common/sanitizer_platform_limits_posix.cc
+++ b/compiler-rt/lib/sanitizer_common/sanitizer_platform_limits_posix.cc
@@ -153,7 +153,6 @@
 # include <sys/procfs.h>
 #endif
 #include <sys/user.h>
-#include <sys/ustat.h>
 #include <linux/cyclades.h>
 #include <linux/if_eql.h>
 #include <linux/if_plip.h>
@@ -246,7 +245,19 @@ namespace __sanitizer {
 #endif  // SANITIZER_LINUX || SANITIZER_FREEBSD
 
 #if SANITIZER_LINUX && !SANITIZER_ANDROID
-  unsigned struct_ustat_sz = sizeof(struct ustat);
+  // Use pre-computed size of struct ustat to avoid <sys/ustat.h> which
+  // has been removed from glibc 2.28.
+#if defined(__aarch64__) || defined(__s390x__) || defined (__mips64) \
+  || defined(__powerpc64__) || defined(__arch64__) || defined(__sparcv9) \
+  || defined(__x86_64__)
+#define SIZEOF_STRUCT_USTAT 32
+#elif defined(__arm__) || defined(__i386__) || defined(__mips__) \
+  || defined(__powerpc__) || defined(__s390__)
+#define SIZEOF_STRUCT_USTAT 20
+#else
+#error Unknown size of struct ustat
+#endif
+  unsigned struct_ustat_sz = SIZEOF_STRUCT_USTAT;
   unsigned struct_rlimit64_sz = sizeof(struct rlimit64);
   unsigned struct_statvfs64_sz = sizeof(struct statvfs64);
 #endif  // SANITIZER_LINUX && !SANITIZER_ANDROID
diff --git a/compiler-rt/lib/sanitizer_common/sanitizer_stoptheworld_linux_libcdep.cc b/compiler-rt/lib/sanitizer_common/sanitizer_stoptheworld_linux_libcdep.cc
index 47b27e7e5c7..eabcc5afe69 100644
--- a/compiler-rt/lib/sanitizer_common/sanitizer_stoptheworld_linux_libcdep.cc
+++ b/compiler-rt/lib/sanitizer_common/sanitizer_stoptheworld_linux_libcdep.cc
@@ -269,7 +269,7 @@ static int TracerThread(void* argument) {
 
   // Alternate stack for signal handling.
   InternalScopedBuffer<char> handler_stack_memory(kHandlerStackSize);
-  struct sigaltstack handler_stack;
+  stack_t handler_stack;
   internal_memset(&handler_stack, 0, sizeof(handler_stack));
   handler_stack.ss_sp = handler_stack_memory.data();
   handler_stack.ss_size = kHandlerStackSize;
diff --git a/compiler-rt/lib/tsan/rtl/tsan_platform_linux.cc b/compiler-rt/lib/tsan/rtl/tsan_platform_linux.cc
index 1309058210c..f6c962d1b8b 100644
--- a/compiler-rt/lib/tsan/rtl/tsan_platform_linux.cc
+++ b/compiler-rt/lib/tsan/rtl/tsan_platform_linux.cc
@@ -369,7 +369,7 @@ bool IsGlobalVar(uptr addr) {
 int ExtractResolvFDs(void *state, int *fds, int nfd) {
 #if SANITIZER_LINUX
   int cnt = 0;
-  __res_state *statp = (__res_state*)state;
+  struct __res_state *statp = (struct __res_state*)state;
   for (int i = 0; i < MAXNS && cnt < nfd; i++) {
     if (statp->_u._ext.nsaddrs[i] && statp->_u._ext.nssocks[i] != -1)
       fds[cnt++] = statp->_u._ext.nssocks[i];
diff --git a/libcxx/include/__hash_table b/libcxx/include/__hash_table
index f3a20309df7..220d753198d 100644
--- a/libcxx/include/__hash_table
+++ b/libcxx/include/__hash_table
@@ -1130,11 +1130,12 @@ private:
 template <class _Tp, class _Hash, class _Equal, class _Alloc>
 inline _LIBCPP_INLINE_VISIBILITY
 __hash_table<_Tp, _Hash, _Equal, _Alloc>::__hash_table()
-    _NOEXCEPT_(
-        is_nothrow_default_constructible<__bucket_list>::value &&
-        is_nothrow_default_constructible<__first_node>::value &&
-        is_nothrow_default_constructible<hasher>::value &&
-        is_nothrow_default_constructible<key_equal>::value)
+        _NOEXCEPT_(
+            is_nothrow_default_constructible<__bucket_list>::value &&
+            is_nothrow_default_constructible<__first_node>::value &&
+            is_nothrow_default_constructible<__node_allocator>::value &&
+            is_nothrow_default_constructible<hasher>::value &&
+            is_nothrow_default_constructible<key_equal>::value)
     : __p2_(0),
       __p3_(1.0f)
 {
@@ -1201,6 +1202,7 @@ __hash_table<_Tp, _Hash, _Equal, _Alloc>::__hash_table(__hash_table&& __u)
         _NOEXCEPT_(
             is_nothrow_move_constructible<__bucket_list>::value &&
             is_nothrow_move_constructible<__first_node>::value &&
+            is_nothrow_move_constructible<__node_allocator>::value &&
             is_nothrow_move_constructible<hasher>::value &&
             is_nothrow_move_constructible<key_equal>::value)
     : __bucket_list_(_VSTD::move(__u.__bucket_list_)),
diff --git a/libcxx/include/__locale b/libcxx/include/__locale
index 19895582cac..c3494d1b50d 100644
--- a/libcxx/include/__locale
+++ b/libcxx/include/__locale
@@ -34,7 +34,7 @@
 # include <support/solaris/xlocale.h>
 #elif defined(_NEWLIB_VERSION)
 # include <support/newlib/xlocale.h>
-#elif (defined(__GLIBC__) || defined(__APPLE__)      || defined(__FreeBSD__) \
+#elif (defined(__APPLE__)      || defined(__FreeBSD__) \
     || defined(__EMSCRIPTEN__) || defined(__IBMCPP__))
 # include <xlocale.h>
 #endif // __GLIBC__ || __APPLE__ || __FreeBSD__ || __sun__ || __EMSCRIPTEN__ || __IBMCPP__
diff --git a/libcxx/include/string b/libcxx/include/string
index 6be21955b14..8420fc075c7 100644
--- a/libcxx/include/string
+++ b/libcxx/include/string
@@ -1936,6 +1936,11 @@ basic_string<_CharT, _Traits, _Allocator>::basic_string()
 template <class _CharT, class _Traits, class _Allocator>
 inline _LIBCPP_INLINE_VISIBILITY
 basic_string<_CharT, _Traits, _Allocator>::basic_string(const allocator_type& __a)
+#if _LIBCPP_STD_VER <= 14
+        _NOEXCEPT_(is_nothrow_copy_constructible<allocator_type>::value)
+#else
+        _NOEXCEPT
+#endif
     : __r_(__a)
 {
 #if _LIBCPP_DEBUG_LEVEL >= 2
diff --git a/llvm/include/llvm/ADT/SetVector.h b/llvm/include/llvm/ADT/SetVector.h
index a7fd408c854..c5fec81bded 100644
--- a/llvm/include/llvm/ADT/SetVector.h
+++ b/llvm/include/llvm/ADT/SetVector.h
@@ -85,6 +85,12 @@ public:
     return vector_.end();
   }
 
+  /// \brief Return the first element of the SetVector.
+  const T &front() const {
+    assert(!empty() && "Cannot call front() on empty SetVector!");
+    return vector_.front();
+  }
+
   /// \brief Return the last element of the SetVector.
   const T &back() const {
     assert(!empty() && "Cannot call back() on empty SetVector!");
diff --git a/llvm/include/llvm/ADT/StringSet.h b/llvm/include/llvm/ADT/StringSet.h
index 3e0cc200b6d..d64ea1fb218 100644
--- a/llvm/include/llvm/ADT/StringSet.h
+++ b/llvm/include/llvm/ADT/StringSet.h
@@ -23,6 +23,15 @@ namespace llvm {
   class StringSet : public llvm::StringMap<char, AllocatorTy> {
     typedef llvm::StringMap<char, AllocatorTy> base;
   public:
+    StringSet() : StringMap<char, AllocatorTy>() {}
+
+    StringSet(std::initializer_list<StringRef> List)
+      : StringMap<char, AllocatorTy>() {
+      for(auto Key : List) {
+        assert(!Key.empty());
+        base::insert(std::make_pair(Key, '\0'));
+      }
+    }
 
     std::pair<typename base::iterator, bool> insert(StringRef Key) {
       assert(!Key.empty());
diff --git a/llvm/include/llvm/ADT/ilist_node.h b/llvm/include/llvm/ADT/ilist_node.h
index 26d0b55e409..a8bc7bb4766 100644
--- a/llvm/include/llvm/ADT/ilist_node.h
+++ b/llvm/include/llvm/ADT/ilist_node.h
@@ -70,7 +70,7 @@ public:
     const NodeTy *Prev = this->getPrev();
 
     // Check for sentinel.
-    if (!Prev->getNext())
+    if (!Prev || !Prev->getNext())
       return nullptr;
 
     return Prev;
@@ -81,7 +81,7 @@ public:
     NodeTy *Next = getNext();
 
     // Check for sentinel.
-    if (!Next->getNext())
+    if (!Next || !Next->getNext())
       return nullptr;
 
     return Next;
diff --git a/llvm/include/llvm/Analysis/LiveValues.h b/llvm/include/llvm/Analysis/LiveValues.h
new file mode 100644
index 00000000000..c393325cc8e
--- /dev/null
+++ b/llvm/include/llvm/Analysis/LiveValues.h
@@ -0,0 +1,209 @@
+/*
+ * Calculate live-value sets for functions.
+ *
+ * Liveness-analysis is based on the non-iterative dataflow algorithm for
+ * reducible graphs by Brandner et. al in:
+ *
+ * "Computing Liveness Sets for SSA-Form Programs"
+ * URL: https://hal.inria.fr/inria-00558509v1/document
+ * Accessed: 5/19/2016
+ *
+ * Author: Rob Lyerly <rlyerly@vt.edu>
+ * Date: 5/19/2016
+ */
+
+#ifndef _LIVE_VALUES_H
+#define _LIVE_VALUES_H
+
+#include <map>
+#include <set>
+#include <list>
+#include "llvm/Pass.h"
+#include "llvm/Analysis/LoopNestingTree.h"
+#include "llvm/IR/Function.h"
+#include "llvm/Support/raw_ostream.h"
+
+namespace llvm {
+
+class LiveValues : public FunctionPass
+{
+public:
+  typedef std::pair<const BasicBlock *, const BasicBlock *> Edge;
+
+  static char ID;
+
+  /**
+   * Default constructor.
+   */
+  LiveValues(void);
+
+  /**
+   * Default destructor.
+   */
+  ~LiveValues(void) {}
+
+  /**
+   * Return whether or not a given type should be included in the analysis.
+   * @return true if the type is included in liveness sets, false otherwise
+   */
+  bool includeAsm(void) const { return inlineasm; }
+  bool includeBitcasts(void) const { return bitcasts; }
+  bool includeComparisons(void) const { return comparisons; }
+  bool includeConstants(void) const { return constants; }
+  bool includeMetadata(void) const { return metadata; }
+
+  /**
+   * Set whether or not to include the specified type in the analysis (all
+   * are set to false by default by the constructor).
+   * @param include true if it should be included, false otherwise
+   */
+  void includeAsm(bool include) { inlineasm = include; }
+  void includeBitcasts(bool include) { bitcasts = include; }
+  void includeComparisons(bool include) { comparisons = include; }
+  void includeConstants(bool include) { constants = include; }
+  void includeMetadata(bool include) { metadata = include; }
+
+  /**
+   * Register which analysis passes we need.
+   * @param AU an analysis usage object
+   */
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const;
+
+  /**
+   * Calculate liveness sets for a function.
+   * @param F a function for which to calculate live values.
+   * @return false, always
+   */
+  virtual bool runOnFunction(Function &F);
+
+  /**
+   * Get the human-readable name of the pass.
+   * @return the pass name
+   */
+  virtual const char *getPassName() const { return "Live value analysis"; }
+
+  /**
+   * Print a human-readable version of the analysis.
+   * @param O an output stream
+   * @param F the function for which to print analysis
+   */
+  virtual void print(raw_ostream &O, const Function *F) const;
+
+  /**
+   * Return the live-in set for a basic block.
+   * @param BB a basic block
+   * @return a set of live-in values for the basic block; this set must be
+   *         freed by the user.
+   */
+  std::set<const Value *> *getLiveIn(const BasicBlock *BB) const;
+
+  /**
+   * Return the live-out set for a basic block.
+   * @param BB a basic block
+   * @return a set of live-out values for the basic block; this set must be
+   *         freed by the user.
+   */
+  std::set<const Value *> *getLiveOut(const BasicBlock *BB) const;
+
+  /**
+   * Get the live values across a given instruction, i.e., values live right
+   * after the invocation of the instruction (excluding the value defined by
+   * the instruction itself).
+   * @param inst an instruction
+   * @return the set of values live directly before the instruction; this set
+   *         must be freed by the user.
+   */
+  std::set<const Value *> *
+  getLiveValues(const Instruction *inst) const;
+
+private:
+  /* Should values of each type be included? */
+  bool inlineasm;
+  bool bitcasts;
+  bool comparisons;
+  bool constants;
+  bool metadata;
+
+  /* A loop nesting forest composed of 0 or more loop nesting trees. */
+  typedef std::list<LoopNestingTree> LoopNestingForest;
+
+  /* Maps live values to a basic block. */
+  typedef std::map<const BasicBlock *, std::set<const Value *> > LiveVals;
+  typedef std::pair<const BasicBlock *, std::set<const Value *> > LiveValsPair;
+
+  /* Store analysis for all functions. */
+  std::map<const Function *, LiveVals> FuncBBLiveIn;
+  std::map<const Function *, LiveVals> FuncBBLiveOut;
+
+  /**
+   * Return whether or not a value is a variable that should be tracked.
+   * @param val a value
+   * @return true if the value is a variable to be tracked, false otherwise
+   */
+  bool includeVal(const Value *val) const;
+
+  /**
+   * Insert the values used in phi-nodes at the beginning of basic block S (as
+   * values live from B) into the set uses.
+   * @param B a basic block which passes live values into phi-nodes in S
+   * @param S a basic block, successor to B
+   * @param uses set in which to add values used in phi-nodes in B
+   * @return the number of values added to the set
+   */
+  unsigned phiUses(const BasicBlock *B,
+                   const BasicBlock *S,
+                   std::set<const Value *> &uses);
+
+  /**
+   * Insert the values defined by the phi-nodes at the beginning of basic block
+   * B into the set defs.
+   * @param B a basic block
+   * @param defs set in which to add values defined by phi-nodes in B
+   * @return the number of values added to the set
+   */
+  unsigned phiDefs(const BasicBlock *B,
+                   std::set<const Value *> &defs);
+
+  /**
+   * Do a post-order traversal of the control flow graph to calculate partial
+   * liveness sets.
+   * @param F a function for which to calculate per-basic block partial
+   *          liveness sets
+   * @param liveIn per-basic block live-in values
+   * @param liveOut per-basic block live-out values
+   */
+  void dagDFS(Function &F, LiveVals &liveIn, LiveVals &liveOut);
+
+  /**
+   * Construct the loop-nesting forest for a function.
+   * @param F a function for which to calculate the loop-nesting forest.
+   * @param LNF a loop nesting forest to populate with loop nesting trees.
+   */
+  void constructLoopNestingForest(Function &F, LoopNestingForest &LNF);
+
+  /**
+   * Propagate live values throughout the loop-nesting tree.
+   * @param loopNest a loop-nesting tree
+   * @param liveIn per-basic block live-in values
+   * @param liveOut per-basic block live-out values
+   */
+  void propagateValues(const LoopNestingTree &loopNest,
+                       LiveVals &liveIn,
+                       LiveVals &liveOut);
+
+  /**
+   * Propagate live values within loops for all loop-nesting trees in the
+   * function's loop-nesting forest.
+   * @param LNF a loop nesting forest
+   * @param liveIn per-basic block live-in values
+   * @param liveOut per-basic block live-out values
+   */
+  void loopTreeDFS(LoopNestingForest &LNF,
+                   LiveVals &liveIn,
+                   LiveVals &liveOut);
+};
+
+} /* llvm namespace */
+
+#endif /* _LIVE_VALUES_H */
+
diff --git a/llvm/include/llvm/Analysis/LoopNestingTree.h b/llvm/include/llvm/Analysis/LoopNestingTree.h
new file mode 100644
index 00000000000..0bc6e28f477
--- /dev/null
+++ b/llvm/include/llvm/Analysis/LoopNestingTree.h
@@ -0,0 +1,191 @@
+/*
+ * Loop-nesting tree for a loop.  The root of a loop-nesting tree is the loop
+ * header of the outermost loop.  The children of any given node (including the
+ * root) are the basic blocks contained within the loop and the loop headers of
+ * nested loops.
+ *
+ * Note: we assume that the control-flow graphs are reducible
+ *
+ * Author: Rob Lyerly <rlyerly@vt.edu>
+ * Date: 5/23/2016
+ */
+
+#ifndef _LOOP_NESTING_TREE_H
+#define _LOOP_NESTING_TREE_H
+
+#include <list>
+#include <vector>
+#include <queue>
+#include "llvm/IR/BasicBlock.h"
+#include "llvm/Analysis/LoopInfo.h"
+#include "llvm/Support/raw_ostream.h"
+
+class LoopNestingTree {
+private:
+  /*
+   * Tree node object.
+   */
+  class Node {
+  public:
+    /**
+     * Construct a node for a basic block.
+     * @param _bb a basic block
+     * @param _parent the parent of this node, i.e. the loop header of the
+     *                containing loop
+     * @param _isLoopHeader is the basic block a loop header?
+     */
+    Node(const llvm::BasicBlock *_bb, const Node *_parent, bool _isLoopHeader)
+      : bb(_bb), parent(_parent), isLoopHeader(_isLoopHeader) {}
+
+    /**
+     * Add a child to the node.
+     * @param child a child to add to the node
+     */
+    void addChild(Node *child) { children.push_back(child); }
+
+    const llvm::BasicBlock *bb; /* Basic block encapsulated by the node. */
+    const Node *parent; /* Parent node, i.e. header of containing loop. */
+    std::list<Node *> children; /* Regular child nodes in the tree. */
+    bool isLoopHeader; /* Is the basic block a loop header? */
+  };
+
+  unsigned _size; /* Number of nodes (i.e., basic blocks) in the tree. */
+  unsigned _depth; /* Number of nested loops in the tree. */
+  Node *_root; /* Root of the tree, i.e. loop header of outermost loop. */
+
+  /**
+   * Print a node & its children.  Recurses into nested loops.
+   * @param O an output stream on which to print the tree
+   * @param node a node to print
+   * @param depth the current depth
+   */
+  void print(llvm::raw_ostream &O, Node *node, unsigned depth) const;
+
+  /**
+   * Delete the node's children & the node itself.  Recurses into nested loops.
+   * @param node the node being deleted
+   */
+  void deleteRecursive(Node *node);
+
+public:
+  /**
+   * Construct a loop-nesting tree from a strongly-connected component of the
+   * control-flow graph.
+   * @param SCC a strongly-connected component of the control-flow graph
+   * @param LI analysis from the loop info pass
+   */
+  LoopNestingTree(const std::vector<llvm::BasicBlock *> &SCC,
+                  const llvm::LoopInfo &LI);
+
+  /**
+   * Destroy a loop-nesting tree.
+   */
+  ~LoopNestingTree() { deleteRecursive(this->_root); }
+
+  /**
+   * Return the size of the loop-nesting tree, that is the number of nodes in
+   * the loop (and all nested loops).
+   * @return the number of nodes in the tree
+   */
+  unsigned size() const { return this->_size; }
+
+  /**
+   * Return the depth of the loop-nesting tree, that is the number of nested
+   * loops.  A value of one indicates that there are no nested loops.
+   * @return the number of nested loops in the tree
+   */
+  unsigned depth() const { return this->_depth; }
+
+  /**
+   * Print the tree.
+   * @param O the output stream on which to print the tree
+   */
+  void print(llvm::raw_ostream &O) const { print(O, this->_root, 0); }
+
+  /*
+   * Loop-node iterator object.  Delivers loop nodes in breadth-first order.
+   */
+  class loop_iterator {
+  public:
+    typedef loop_iterator self_type;
+    typedef const llvm::BasicBlock *value_type;
+    typedef value_type& reference;
+    typedef value_type* pointer;
+    typedef std::forward_iterator_tag iterator_category;
+
+    self_type operator++(void);
+    self_type operator++(int junk);
+    reference operator*(void) { return cur->bb; }
+    pointer operator->(void) { return &cur->bb; }
+    bool operator==(const self_type& rhs) { return cur == rhs.cur; }
+    bool operator!=(const self_type& rhs) { return cur != rhs.cur; }
+
+    friend class LoopNestingTree;
+    friend class child_iterator;
+  private:
+    Node *cur;
+    std::queue<Node *> remaining;
+
+    loop_iterator(Node *start) : cur(start) { addLoopHeaders(); }
+    void addLoopHeaders(void);
+  };
+
+  /*
+   * Child iterator object.  Traverses children of tree nodes.
+   */
+  class child_iterator {
+  public:
+    typedef child_iterator self_type;
+    typedef const llvm::BasicBlock *value_type;
+    typedef value_type& reference;
+    typedef value_type* pointer;
+    typedef std::forward_iterator_tag iterator_category;
+    enum location { BEGIN, END };
+
+    self_type operator++(void)
+      { self_type me = *this; it.operator++(); return me; }
+    self_type operator++(int junk) { it.operator++(junk); return *this; }
+    reference operator*(void) { return (*it)->bb; }
+    pointer operator->(void) { return &(*it)->bb; }
+    bool operator==(const self_type& rhs) { return it == rhs.it; }
+    bool operator!=(const self_type& rhs) { return it != rhs.it; }
+
+    friend class LoopNestingTree;
+  private:
+    std::list<Node *>::const_iterator it;
+
+    child_iterator(loop_iterator &parent, enum location loc);
+  };
+
+  /**
+   * Return an iterator for traversing all loop nodes (i.e., loop header basic
+   * blocks) in the tree.  Delivers nodes in a breadth-first ordering.
+   * @return an iterator to traverse the loop nodes in the tree
+   */
+  loop_iterator loop_begin() const { loop_iterator it(_root); return it; };
+
+  /**
+   * Return an iterator marking the end of the loop nodes in the tree.
+   * @return an iterator marking the end of the traversal
+   */
+  loop_iterator loop_end() const { loop_iterator it(nullptr); return it; };
+
+  /**
+   * Return an iterator for traversing the children of a loop node.
+   * @param an iterator associated with a loop node
+   * @return an iterator to traverse the children of a loop node
+   */
+  child_iterator children_begin(loop_iterator &parent) const
+    { child_iterator it(parent, child_iterator::BEGIN); return it; }
+
+  /**
+   * Return an iterator marking the end of the children of a loop node.
+   * @param an iterator associated with a loop node
+   * @return an iterator marking the end of the traversal
+   */
+  child_iterator children_end(loop_iterator &parent) const
+    { child_iterator it(parent, child_iterator::END); return it; }
+};
+
+#endif /* _LOOP_NESTING_TREE_H */
+
diff --git a/llvm/include/llvm/Analysis/LoopPaths.h b/llvm/include/llvm/Analysis/LoopPaths.h
new file mode 100644
index 00000000000..65042a58ac5
--- /dev/null
+++ b/llvm/include/llvm/Analysis/LoopPaths.h
@@ -0,0 +1,286 @@
+//===- LoopPaths.h - Enumerate paths in loops -------------------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements analysis which enumerates paths in loops.  In
+// particular, this pass calculates all paths in loops which are of the
+// following form:
+//
+//  - Header to backedge, with no equivalence points on the path
+//  - Header to with equivalence point
+//  - Equivalence point to equivalence point
+//  - Equivalence point to backedge
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_ANALYSIS_LOOPPATHS_H
+#define LLVM_ANALYSIS_LOOPPATHS_H
+
+#include <set>
+#include <vector>
+#include <list>
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/ADT/SetVector.h"
+#include "llvm/Analysis/LoopInfo.h"
+#include "llvm/IR/BasicBlock.h"
+#include "llvm/Pass.h"
+#include "llvm/Support/Debug.h"
+
+namespace llvm {
+
+//===----------------------------------------------------------------------===//
+// Utilities
+//===----------------------------------------------------------------------===//
+
+/// Sort loops based on nesting depth, with deeper-nested loops coming first.
+/// If the depths are equal, sort based on pointer value so that distinct loops
+/// with equal depths are not considered equivalent during insertion.
+struct LoopNestCmp {
+  bool operator() (const Loop * const &A, const Loop * const &B) {
+    unsigned DepthA = A->getLoopDepth(), DepthB = B->getLoopDepth();
+    if(DepthA > DepthB) return true;
+    else if(DepthA < DepthB) return false;
+    else return (uint64_t)A < (uint64_t)B;
+  }
+};
+
+/// A loop nest, sorted by depth (deeper loops are first).
+typedef std::set<Loop *, LoopNestCmp> LoopNest;
+
+/// A set of basic blocks.
+typedef SmallPtrSet<const BasicBlock *, 16> BlockSet;
+
+namespace LoopPathUtilities {
+
+/// Populate a LoopNest by traversing the loop L and its children.  Does *not*
+/// traverse loops containing L (e.g., loops for which L is a child).
+void populateLoopNest(Loop *L, LoopNest &Nest);
+
+/// Get blocks contained in all sub-loops of a loop, including loops nested
+/// deeper than those in immediate sub-loops (e.g., blocks of loop depth 3
+/// inside loop depth 1).
+void getSubBlocks(Loop *L, BlockSet &SubBlocks);
+
+}
+
+//===----------------------------------------------------------------------===//
+// LoopPath helper class
+//===----------------------------------------------------------------------===//
+
+/// Nodes along a path in a loop, represented by a basic block.
+class PathNode {
+private:
+  /// The block encapsulated by the node.
+  const BasicBlock *Block;
+
+  /// Whether or not the block is an exiting block from a sub-loop inside of
+  /// the current path.
+  bool SubLoopExit;
+
+public:
+  PathNode() = delete;
+  PathNode(const BasicBlock *Block, bool SubLoopExit = false)
+    : Block(Block), SubLoopExit(SubLoopExit) {}
+
+  const BasicBlock *getBlock() const { return Block; }
+  bool isSubLoopExit() const { return SubLoopExit; }
+  bool operator<(const PathNode &RHS) const { return Block < RHS.Block; }
+};
+
+/// A path through the loop, which begins/ends either on the loop's header, the
+/// loop's backedge(s) or equivalence points.
+class LoopPath {
+private:
+  /// Nodes that comprise the path.  Iteration over the container is equivalent
+  /// to traversing the path, but container has set semantics for quick
+  /// existence checks.
+  // TODO use a DenseSet for the set template argument, which requires defining
+  // a custom comparator
+  SetVector<PathNode, std::vector<PathNode>, std::set<PathNode> > Nodes;
+
+  /// The path begins & ends on specific instructions.  Note that Start *must*
+  /// be inside the starting block and End *must* be inside the ending block.
+  const Instruction *Start, *End;
+
+  /// Does the path start at the loop header?  If not, it by definition starts
+  /// at an equivalence point.
+  bool StartsAtHeader;
+
+  /// Does the path end at a backedge?  If not, it by definition ends at an
+  /// equivalence point.
+  bool EndsAtBackedge;
+
+public:
+  LoopPath() = delete;
+  LoopPath(const std::vector<PathNode> &NodeVector,
+           const Instruction *Start, const Instruction *End,
+           bool StartsAtHeader, bool EndsAtBackedge);
+
+  bool contains(BasicBlock *BB) const { return Nodes.count(PathNode(BB)); }
+  bool contains(const BasicBlock *BB) const
+  { return Nodes.count(PathNode(BB)); }
+
+  /// Get the starting point of the path, guaranteed to be either the loop
+  /// header or an equivalence point.
+  const PathNode &startNode() const { return Nodes.front(); }
+  const Instruction *startInst() const { return Start; }
+
+  /// Get the the ending point of the path, guaranteed to be either an
+  /// equivalence point or a backedge.
+  const PathNode &endNode() const { return Nodes.back(); }
+  const Instruction *endInst() const { return End; }
+
+  /// Iterators over the path's blocks.
+  SetVector<PathNode>::iterator begin() { return Nodes.begin(); }
+  SetVector<PathNode>::iterator end() { return Nodes.end(); }
+  SetVector<PathNode>::const_iterator cbegin() const { return Nodes.begin(); }
+  SetVector<PathNode>::const_iterator cend() const { return Nodes.end(); }
+
+  /// Return whether the path starts at the loop header or equivalence point.
+  bool startsAtHeader() const { return StartsAtHeader; }
+
+  /// Return whether the path ends at a backedge block or equivalence point.
+  bool endsAtBackedge() const { return EndsAtBackedge; }
+
+  /// Return whether this is a spanning path.
+  bool isSpanningPath() const { return StartsAtHeader && EndsAtBackedge; }
+
+  /// Return whether this is an equivalence point path.
+  bool isEqPointPath() const { return !StartsAtHeader || !EndsAtBackedge; }
+
+  std::string toString() const;
+  void print(raw_ostream &O) const;
+  void dump() const { print(dbgs()); }
+};
+
+//===----------------------------------------------------------------------===//
+// Pass implementation
+//===----------------------------------------------------------------------===//
+
+/// Analyze all paths within a loop nest
+class EnumerateLoopPaths : public FunctionPass {
+private:
+  /// Loop information analysis.
+  LoopInfo *LI;
+
+  /// All calculated paths for each analyzed loop.
+  DenseMap<const Loop *, std::vector<LoopPath> > Paths;
+
+  /// Whether there are paths of each type through a basic block in a loop.
+  /// These are *only* maintained for the current loop, not any sub-loops.
+  DenseMap<const Loop *, DenseMap<const BasicBlock *, bool> >
+  HasSpPath, HasEqPointPath;
+
+  /// Information about the loop currently being analyzed
+  Loop *CurLoop;
+  SmallPtrSet<const BasicBlock *, 4> Latches;
+  BlockSet SubLoopBlocks;
+
+  /// Set if analysis had to bail out because there are too many paths through
+  /// the function.
+  bool TooManyPaths;
+
+  /// Set if analysis had to bail out because it found a non-loop cycle due to
+  /// complect control flow (usually due to goto's).
+  bool DetectedCycle;
+
+  /// Depth-first search information for the current path being explored.
+  struct LoopDFSInfo {
+  public:
+    const Instruction *Start;
+    std::vector<PathNode> PathNodes;
+    bool StartsAtHeader;
+  };
+
+  /// Empty all data structures.
+  void reset() {
+    Paths.clear();
+    HasSpPath.clear();
+    HasEqPointPath.clear();
+    CurLoop = nullptr;
+    Latches.clear();
+    SubLoopBlocks.clear();
+  }
+
+  /// Search exit blocks of the loop containing Successor.  Add the terminating
+  /// instruction of the block to either of the two vectors, depending if there
+  /// is a path of either type through the exit block.
+  inline void getSubLoopSuccessors(const BasicBlock *Successor,
+                                   std::vector<const Instruction *> &EqPoint,
+                                   std::vector<const Instruction *> &Spanning);
+
+  /// Run a depth-first search for paths in the loop starting at an instruction.
+  /// Any paths found are added to the vector of paths, and new paths to explore
+  /// are added to the search list.
+  bool loopDFS(const Instruction *I,
+               LoopDFSInfo &DFSI,
+               std::vector<LoopPath> &CurPaths,
+               std::list<const Instruction *> &NewPaths);
+
+  /// Enumerate all paths within a loop, stored in the vector argument.  Return
+  /// true if the analysis was successful or false otherwise.
+  bool analyzeLoop(Loop *L, std::vector<LoopPath> &CurPaths);
+
+public:
+  static char ID;
+  EnumerateLoopPaths() : FunctionPass(ID) {}
+
+  /// Pass interface implementation.
+  void getAnalysisUsage(AnalysisUsage &AU) const override;
+  bool runOnFunction(Function &F) override;
+
+  /// Re-run analysis to enumerate paths through a loop.  Invalidates all APIs
+  /// below which populate containers with paths (for this loop only).
+  void rerunOnLoop(Loop *L);
+
+  /// Query whether analysis failed.
+  bool analysisFailed() const { return TooManyPaths || DetectedCycle; }
+
+  /// Query whether there were too many paths to enumerate in the function.
+  bool tooManyPaths() const { return TooManyPaths; }
+
+  /// Query whether analysis detected a non-loop cycle.
+  bool detectedCycle() const { return DetectedCycle; }
+
+  bool hasPaths(const Loop *L) const { return Paths.count(L); }
+
+  /// Get all the paths through a loop.  Paths in the vector are ordered as
+  /// they were discovered in the depth-first traversal of the loop.
+  void getPaths(const Loop *L, std::vector<const LoopPath *> &P) const;
+
+  /// Get all paths through a loop that end at a backedge.
+  void getBackedgePaths(const Loop *L, std::vector<const LoopPath *> &P) const;
+  void getBackedgePaths(const Loop *L, std::set<const LoopPath *> &P) const;
+
+  /// Get all spanning paths through a loop, where a spanning path is defined
+  /// as starting at the first instruction of the header of the loop and ending
+  /// at the branch in a latch.
+  void getSpanningPaths(const Loop *L, std::vector<const LoopPath *> &P) const;
+  void getSpanningPaths(const Loop *L, std::set<const LoopPath *> &P) const;
+
+  /// Get all the paths through the loop that begin and/or end at an
+  /// equivalence point.
+  void getEqPointPaths(const Loop *L, std::vector<const LoopPath *> &P) const;
+  void getEqPointPaths(const Loop *L, std::set<const LoopPath *> &P) const;
+
+  /// Get all the paths through a loop that contain a given basic block.
+  void getPathsThroughBlock(const Loop *L, BasicBlock *BB,
+                            std::vector<const LoopPath *> &P) const;
+  void getPathsThroughBlock(const Loop *L, BasicBlock *BB,
+                            std::set<const LoopPath *> &P) const;
+
+  /// Return whether there is each type of path through a basic block.
+  bool spanningPathThroughBlock(const Loop *L, const BasicBlock *BB) const;
+  bool eqPointPathThroughBlock(const Loop *L, const BasicBlock *BB) const;
+};
+
+}
+
+#endif
+
diff --git a/llvm/include/llvm/Analysis/Passes.h b/llvm/include/llvm/Analysis/Passes.h
index d112ab1823b..0bdf4ddf4df 100644
--- a/llvm/include/llvm/Analysis/Passes.h
+++ b/llvm/include/llvm/Analysis/Passes.h
@@ -173,6 +173,33 @@ namespace llvm {
   //
   FunctionPass *createMemDerefPrinter();
 
+  //===--------------------------------------------------------------------===//
+  //
+  // createPopcornCompatibilityPass - This pass analyzes & warns users about
+  // code features not yet supported by the Popcorn compiler, runtime & OS.
+  //
+  FunctionPass *createPopcornCompatibilityPass();
+
+  //===--------------------------------------------------------------------===//
+  //
+  // createLiveValuesPass - This pass calculates live-value sets for basic
+  // blocks in a function.
+  //
+  FunctionPass *createLiveValuesPass();
+
+  //===--------------------------------------------------------------------===//
+  //
+  // createEnumerateLoopPathsPass - This pass calculates all paths between
+  // equivalence points within a loop.
+  //
+  FunctionPass *createEnumerateLoopPathsPass();
+
+  //===--------------------------------------------------------------------===//
+  //
+  // createSelectMigrationPointsPass - This pass analyzes and marks instructions
+  // inside of functions to be migration points.
+  //
+  FunctionPass *createSelectMigrationPointsPass();
 }
 
 #endif
diff --git a/llvm/include/llvm/Analysis/PopcornUtil.h b/llvm/include/llvm/Analysis/PopcornUtil.h
new file mode 100644
index 00000000000..01fddcddb90
--- /dev/null
+++ b/llvm/include/llvm/Analysis/PopcornUtil.h
@@ -0,0 +1,188 @@
+//===- LoopPaths.h - Enumerate paths in loops -------------------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file provides Popcorn-specific utility APIs.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_ANALYSIS_POPCORNUTIL_H
+#define LLVM_ANALYSIS_POPCORNUTIL_H
+
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/IR/CallSite.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/Module.h"
+
+namespace llvm {
+namespace Popcorn {
+
+#define POPCORN_META "popcorn"
+#define POPCORN_MIGPOINT "migpoint"
+#define POPCORN_HTM_BEGIN "htmbegin"
+#define POPCORN_HTM_END "htmend"
+
+/// Add named metadata node with string operand to an instruction.
+static inline void addMetadata(Instruction *I, StringRef name, StringRef op) {
+  SmallVector<Metadata *, 2> MetaOps;
+  LLVMContext &C = I->getContext();
+  MDNode *MetaNode = I->getMetadata(name);
+
+  if(MetaNode) {
+    for(auto &Op : MetaNode->operands()) {
+      if(isa<MDString>(Op) && cast<MDString>(Op)->getString() == op) return;
+      else MetaOps.push_back(Op);
+    }
+  }
+
+  MetaOps.push_back(MDString::get(C, op));
+  MetaNode = MDNode::get(C, MetaOps);
+  I->setMetadata(name, MetaNode);
+}
+
+/// Remove string operand from named metadata node.
+static inline void
+removeMetadata(Instruction *I, StringRef name, StringRef op) {
+  SmallVector<Metadata *, 2> MetaOps;
+  MDNode *MetaNode = I->getMetadata(name);
+
+  if(MetaNode) {
+    for(auto &Op : MetaNode->operands()) {
+      if(isa<MDString>(Op) && cast<MDString>(Op)->getString() == op) continue;
+      else MetaOps.push_back(Op);
+    }
+
+    if(MetaOps.size()) {
+      MetaNode = MDNode::get(I->getContext(), MetaOps);
+      I->setMetadata(name, MetaNode);
+    }
+    else I->setMetadata(name, nullptr);
+  }
+}
+
+/// Check to see if instruction has named metadata node with string operand.
+static inline bool
+hasMetadata(const Instruction *I, StringRef name, StringRef op) {
+  const MDNode *MetaNode = I->getMetadata(name);
+  if(MetaNode)
+    for(auto &Op : MetaNode->operands())
+      if(isa<MDString>(Op) && cast<MDString>(Op)->getString() == op)
+        return true;
+  return false;
+}
+
+/// Return whether the instruction is a "true" call site, i.e., not an LLVM
+/// IR-level intrinsic or inline assembly.
+static inline bool isCallSite(const Instruction *I) {
+  if((isa<CallInst>(I) || isa<InvokeInst>(I)) && !isa<IntrinsicInst>(I)) {
+    ImmutableCallSite CS(I);
+    if(!CS.isInlineAsm()) return true;
+  }
+  return false;
+}
+
+/// Add metadata to an instruction marking it as an equivalence point.
+static inline void addEquivalencePointMetadata(Instruction *I) {
+  addMetadata(I, POPCORN_META, POPCORN_MIGPOINT);
+}
+
+/// Remove metadata from an instruction marking it as an equivalence point.
+static inline void removeEquivalencePointMetadata(Instruction *I) {
+  removeMetadata(I, POPCORN_META, POPCORN_MIGPOINT);
+}
+
+static inline bool hasEquivalencePointMetadata(Instruction *I) {
+  return hasMetadata(I , POPCORN_META, POPCORN_MIGPOINT);
+}
+
+/// Return whether an instruction is an equivalence point.  The instruction must
+/// satisfy one of the following:
+///
+/// 1. Is a function call site (not an intrinsic function call)
+/// 2. Analysis has tagged the instruction with appropriate metadata
+static inline bool isEquivalencePoint(const Instruction *I) {
+  if(isCallSite(I)) return true;
+  else return hasMetadata(I, POPCORN_META, POPCORN_MIGPOINT);
+}
+
+/// Add metadata to an instruction marking it as an HTM begin point.
+static inline void addHTMBeginMetadata(Instruction *I) {
+  addMetadata(I, POPCORN_META, POPCORN_HTM_BEGIN);
+}
+
+/// Remove metadata from an instruction marking it as an HTM begin point.
+static inline void removeHTMBeginMetadata(Instruction *I) {
+  removeMetadata(I, POPCORN_META, POPCORN_HTM_BEGIN);
+}
+
+/// Return whether an instruction is an HTM begin point.
+static inline bool isHTMBeginPoint(Instruction *I) {
+  return hasMetadata(I, POPCORN_META, POPCORN_HTM_BEGIN);
+}
+
+/// Add metadata to an instruction marking it as an HTM end point.
+static inline void addHTMEndMetadata(Instruction *I) {
+  addMetadata(I, POPCORN_META, POPCORN_HTM_END);
+}
+
+/// Remove metadata from an instruction marking it as an HTM end point.
+static inline void removeHTMEndMetadata(Instruction *I) {
+  removeMetadata(I, POPCORN_META, POPCORN_HTM_END);
+}
+
+/// Return whether an instruction is an HTM end point.
+static inline bool isHTMEndPoint(Instruction *I) {
+  return hasMetadata(I, POPCORN_META, POPCORN_HTM_END);
+}
+
+#define POPCORN_INST_KEY "popcorn-inst-ty"
+
+/// Type of instrumentation.
+enum InstrumentType {
+  HTM = 0,
+  Cycles,
+  None,
+  NumVals // Don't use!
+};
+
+/// Mark the module as having a certain instrumentation type.
+static inline void
+setInstrumentationType(Module &M, enum InstrumentType Ty) {
+  switch(Ty) {
+  case HTM:
+    M.addModuleFlag(Module::Error, POPCORN_INST_KEY, HTM);
+    break;
+  case Cycles:
+    M.addModuleFlag(Module::Error, POPCORN_INST_KEY, Cycles);
+    break;
+  case None:
+    M.addModuleFlag(Module::Error, POPCORN_INST_KEY, None);
+    break;
+  default: llvm_unreachable("Unknown instrumentation type"); break;
+  }
+}
+
+/// Get the type of instrumentation.
+static inline enum InstrumentType getInstrumentationType(Module &M) {
+  Metadata *MD = M.getModuleFlag(POPCORN_INST_KEY);
+  if(MD) {
+    ConstantAsMetadata *Val = cast<ConstantAsMetadata>(MD);
+    ConstantInt *IntVal = cast<ConstantInt>(Val->getValue());
+    uint64_t RawVal = IntVal->getZExtValue();
+    assert(RawVal < NumVals && "Invalid instrumentation type");
+    return (enum InstrumentType)RawVal;
+  }
+  else return None;
+}
+
+}
+}
+
+#endif
+
diff --git a/llvm/include/llvm/CodeGen/AsmPrinter.h b/llvm/include/llvm/CodeGen/AsmPrinter.h
index fe7efae325c..39b122304ae 100644
--- a/llvm/include/llvm/CodeGen/AsmPrinter.h
+++ b/llvm/include/llvm/CodeGen/AsmPrinter.h
@@ -199,9 +199,10 @@ public:
 
   /// Emit the specified function out to the OutStreamer.
   bool runOnMachineFunction(MachineFunction &MF) override {
+    bool modified = TagCallSites(MF);
     SetupMachineFunction(MF);
     EmitFunctionBody();
-    return false;
+    return modified;
   }
 
   //===------------------------------------------------------------------===//
@@ -329,6 +330,13 @@ public:
   /// instructions in verbose mode.
   virtual void emitImplicitDef(const MachineInstr *MI) const;
 
+  /// Some machine instructions encapsulate a call with follow-on boilerplate
+  /// instructions, meaning labels emitted after the "instruction" do not
+  /// capture the call's true return address.  Return an offset for correcting
+  /// these labels to refer to the call's actual return address.
+  virtual int getCanonicalReturnAddr(const MachineInstr *Call) const
+  { return 0; }
+
   //===------------------------------------------------------------------===//
   // Symbol Lowering Routines.
   //===------------------------------------------------------------------===//
@@ -393,6 +401,14 @@ public:
     EmitLabelPlusOffset(Label, 0, Size, IsSectionRelative);
   }
 
+  /// Find the stackmap intrinsic associated with a function call
+  MachineInstr *FindStackMap(MachineBasicBlock &MBB,
+                             MachineInstr *MI) const;
+
+  /// Move stackmap intrinsics directly after calls to correctly capture
+  /// return addresses
+  bool TagCallSites(MachineFunction &MF);
+
   //===------------------------------------------------------------------===//
   // Dwarf Emission Helper Routines
   //===------------------------------------------------------------------===//
diff --git a/llvm/include/llvm/CodeGen/MachineFunction.h b/llvm/include/llvm/CodeGen/MachineFunction.h
index c15ee1c006c..9ff8865d9bb 100644
--- a/llvm/include/llvm/CodeGen/MachineFunction.h
+++ b/llvm/include/llvm/CodeGen/MachineFunction.h
@@ -20,6 +20,8 @@
 
 #include "llvm/ADT/ilist.h"
 #include "llvm/CodeGen/MachineBasicBlock.h"
+#include "llvm/CodeGen/StackTransformTypes.h"
+#include "llvm/IR/Instructions.h"
 #include "llvm/IR/DebugLoc.h"
 #include "llvm/IR/Metadata.h"
 #include "llvm/Support/Allocator.h"
@@ -145,6 +147,16 @@ class MachineFunction {
   /// True if the function includes any inline assembly.
   bool HasInlineAsm;
 
+  /// Information about changes in number of IR operands to number of machine
+  /// /// operands due to legalization.
+  SMToOpLegalizeMap SMOpLegalizeChanges;
+
+  /// Duplicate live value locations for stackmap operands
+  InstToOperands SMDuplicateLocs;
+
+  /// Architecture-specific live value locations for each stackmap
+  InstToArchLiveValues SMArchSpecificLocs;
+
   MachineFunction(const MachineFunction &) = delete;
   void operator=(const MachineFunction&) = delete;
 public:
@@ -457,6 +469,9 @@ public:
     return Mask;
   }
 
+  /// Is a register caller-saved?
+  bool isCallerSaved(unsigned Reg) const;
+
   /// allocateMemRefsArray - Allocate an array to hold MachineMemOperand
   /// pointers.  This array is owned by the MachineFunction.
   MachineInstr::mmo_iterator allocateMemRefsArray(unsigned long Num);
@@ -488,6 +503,46 @@ public:
   /// getPICBaseSymbol - Return a function-local symbol to represent the PIC
   /// base.
   MCSymbol *getPICBaseSymbol() const;
+
+  //===--------------------------------------------------------------------===//
+  // Additional stack transformation metadata
+  //
+
+  /// Add a note indicating that machine operand number OpNo was changed to Num
+  /// operands in stackmap SMID.
+  void addOpLegalizeChange(int64_t SMID, unsigned OpNo, unsigned Num);
+
+  /// Add an IR/architecture-specific location mapping for a stackmap operand
+  void addSMOpLocation(const CallInst *SM, const Value *Val,
+                       const MachineLiveLoc &MLL);
+  void addSMOpLocation(const CallInst *SM, unsigned Op,
+                       const MachineLiveLoc &MLL);
+
+  /// Add an architecture-specific live value & location for a stackmap
+  void addSMArchSpecificLocation(const CallInst *SM,
+                                 const MachineLiveLoc &MLL,
+                                 const MachineLiveVal &MLV);
+
+  /// Update stack slot references to new indexes after stack slot coloring
+  void updateSMStackSlotRefs(SmallDenseMap<int, int, 16> &Changes);
+
+  /// Return the number of machine operands corresponding to a given IR operand.
+  unsigned getNumLegalizedOps(int64_t SMID, unsigned OpNo) const;
+
+  /// Are there any architecture-specific locations for operand Val in stackmap
+  /// SM?
+  bool hasSMOpLocations(const CallInst *SM, const Value *Val) const;
+
+  /// Are there any architecture-specific locations for stackmap SM?
+  bool hasSMArchSpecificLocations(const CallInst *SM) const;
+
+  /// Return the architecture-specific locations for a stackmap operand.
+  const MachineLiveLocs &getSMOpLocations(const CallInst *SM,
+                                          const Value *Val) const;
+
+  /// Return the architecture-specific locations for a stackmap that are not
+  /// associated with any operand.
+  const ArchLiveValues &getSMArchSpecificLocations(const CallInst *SM) const;
 };
 
 //===--------------------------------------------------------------------===//
diff --git a/llvm/include/llvm/CodeGen/Passes.h b/llvm/include/llvm/CodeGen/Passes.h
index 5d829217447..1e020154f0e 100644
--- a/llvm/include/llvm/CodeGen/Passes.h
+++ b/llvm/include/llvm/CodeGen/Passes.h
@@ -123,6 +123,15 @@ protected:
   /// Default setting for -enable-shrink-wrap on this target.
   bool EnableShrinkWrap;
 
+  /// Add equivalence points into the application
+  bool AddMigrationPoints;
+
+  /// Add stackmaps at function call sites & equivalence points
+  bool AddStackMaps;
+
+  /// Add stackmaps describing stack state in libc thread start functions
+  bool AddLibcStackMaps;
+
 public:
   TargetPassConfig(TargetMachine *tm, PassManagerBase &pm);
   // Dummy constructor.
@@ -142,6 +151,9 @@ public:
 
   CodeGenOpt::Level getOptLevel() const { return TM->getOptLevel(); }
 
+  CodeGenOpt::Level getArchIROptLevel() const
+  { return TM->getArchIROptLevel(); }
+
   /// Set the StartAfter, StartBefore and StopAfter passes to allow running only
   /// a portion of the normal code-gen pass sequence.
   ///
@@ -193,10 +205,33 @@ public:
   /// Return true if shrink wrapping is enabled.
   bool getEnableShrinkWrap() const;
 
+  /// Return whether we should instrument the code with equivalence points.
+  bool addMigrationPoints() const { return AddMigrationPoints; }
+
+  /// Return whether we should emit stack transformation metadata by
+  /// instrumenting the code with IR-level StackMaps.
+  bool addStackMaps() const { return AddStackMaps; }
+
+  /// Return whether we should emit transformation metadata (via IR-level
+  /// StackMaps) for libc thread start functions.
+  bool addLibcStackMaps() const { return AddLibcStackMaps; }
+
+  /// \brief Enable/disable adding equivalence points.
+  void setAddMigrationPoints(bool Set) { AddMigrationPoints = Set; }
+
+  /// \brief Enable/disable adding StackMaps.
+  void setAddStackMaps(bool Set) { AddStackMaps = Set; }
+
+  /// \brief Enable/disable adding StackMaps to libc thread start function.
+  void setAddLibcStackMaps(bool Set) { AddLibcStackMaps = Set; }
+
   /// Return true if the default global register allocator is in use and
   /// has not be overriden on the command line with '-regalloc=...'
   bool usingDefaultRegAlloc() const;
 
+  /// Add Popcorn-specific IR passes for code generation.
+  void addPopcornPasses();
+
   /// Add common target configurable passes that perform LLVM IR to IR
   /// transforms following machine independent optimization.
   virtual void addIRPasses();
@@ -448,6 +483,10 @@ namespace llvm {
   // instruction and update the MachineFunctionInfo with that information.
   extern char &ShrinkWrapID;
 
+  /// Stack transformation metadata pass.  Gather additional stack
+  /// transformation metadata from machine functions.
+  extern char &StackTransformMetadataID;
+
   /// VirtRegRewriter pass. Rewrite virtual registers to physical registers as
   /// assigned in VirtRegMap.
   extern char &VirtRegRewriterID;
diff --git a/llvm/include/llvm/CodeGen/StackMaps.h b/llvm/include/llvm/CodeGen/StackMaps.h
index fdc1a9143ed..3026f3479c6 100644
--- a/llvm/include/llvm/CodeGen/StackMaps.h
+++ b/llvm/include/llvm/CodeGen/StackMaps.h
@@ -13,6 +13,8 @@
 #include "llvm/ADT/MapVector.h"
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/CodeGen/MachineInstr.h"
+#include "llvm/CodeGen/StackTransformTypes.h"
+#include "llvm/IR/Instructions.h"
 #include "llvm/Support/Debug.h"
 #include <map>
 #include <vector>
@@ -22,6 +24,7 @@ namespace llvm {
 class AsmPrinter;
 class MCExpr;
 class MCStreamer;
+class UnwindInfo;
 
 /// \brief MI-level patchpoint operands.
 ///
@@ -142,9 +145,20 @@ public:
     unsigned Size;
     unsigned Reg;
     int64_t Offset;
-    Location() : Type(Unprocessed), Size(0), Reg(0), Offset(0) {}
-    Location(LocationType Type, unsigned Size, unsigned Reg, int64_t Offset)
-        : Type(Type), Size(Size), Reg(Reg), Offset(Offset) {}
+    bool Ptr;
+    bool Alloca;
+    bool Duplicate;
+    bool Temporary;
+    unsigned AllocaSize;
+    Location() : Type(Unprocessed), Size(0), Reg(0), Offset(0),
+                 Ptr(false), Alloca(false), Duplicate(false),
+                 Temporary(false), AllocaSize(0) {}
+    Location(LocationType Type, unsigned Size, unsigned Reg, int64_t Offset,
+             bool Ptr, bool Alloca, bool Duplicate, bool Temporary,
+             unsigned AllocaSize)
+        : Type(Type), Size(Size), Reg(Reg), Offset(Offset), Ptr(Ptr),
+          Alloca(Alloca), Duplicate(Duplicate), Temporary(Temporary),
+          AllocaSize(AllocaSize) {}
   };
 
   struct LiveOutReg {
@@ -158,10 +172,29 @@ public:
         : Reg(Reg), DwarfRegNum(DwarfRegNum), Size(Size) {}
   };
 
+  struct Operation {
+    ValueGenInst::InstType InstType;
+    Location::LocationType OperandType;
+    unsigned Size;
+    unsigned DwarfReg;
+    int64_t Constant;
+    bool isGenerated;
+    bool isSymbol;
+    const MCSymbol *Symbol;
+    Operation()
+      : Size(0), DwarfReg(0), Constant(0), isGenerated(false),
+        isSymbol(false), Symbol(nullptr) {}
+  };
+
   // OpTypes are used to encode information about the following logical
   // operand (which may consist of several MachineOperands) for the
   // OpParser.
-  typedef enum { DirectMemRefOp, IndirectMemRefOp, ConstantOp } OpType;
+  typedef enum {
+    DirectMemRefOp,
+    IndirectMemRefOp,
+    ConstantOp,
+    TemporaryOp
+  } OpType;
 
   StackMaps(AsmPrinter &AP);
 
@@ -185,7 +218,7 @@ public:
   /// If there is any stack map data, create a stack map section and serialize
   /// the map info into it. This clears the stack map data structures
   /// afterwards.
-  void serializeToStackMapSection();
+  void serializeToStackMapSection(const UnwindInfo *UI = nullptr);
 
 private:
   static const char *WSMP;
@@ -193,17 +226,23 @@ private:
   typedef SmallVector<LiveOutReg, 8> LiveOutVec;
   typedef MapVector<uint64_t, uint64_t> ConstantPool;
   typedef MapVector<const MCSymbol *, uint64_t> FnStackSizeMap;
+  typedef std::pair<Location, Operation> ArchValue;
+  typedef SmallVector<ArchValue, 8> ArchValues;
 
   struct CallsiteInfo {
+    const MCSymbol *Func;
     const MCExpr *CSOffsetExpr;
     uint64_t ID;
     LocationVec Locations;
     LiveOutVec LiveOuts;
-    CallsiteInfo() : CSOffsetExpr(nullptr), ID(0) {}
-    CallsiteInfo(const MCExpr *CSOffsetExpr, uint64_t ID,
-                 LocationVec &&Locations, LiveOutVec &&LiveOuts)
-        : CSOffsetExpr(CSOffsetExpr), ID(ID), Locations(std::move(Locations)),
-          LiveOuts(std::move(LiveOuts)) {}
+    ArchValues Vals;
+    CallsiteInfo() : Func(nullptr), CSOffsetExpr(nullptr), ID(0) {}
+    CallsiteInfo(const MCSymbol *Func, const MCExpr *CSOffsetExpr,
+                 uint64_t ID, LocationVec &&Locations,
+                 LiveOutVec &&LiveOuts, ArchValues &&Vals)
+        : Func(Func), CSOffsetExpr(CSOffsetExpr), ID(ID),
+          Locations(std::move(Locations)), LiveOuts(std::move(LiveOuts)),
+          Vals(std::move(Vals)) {}
   };
 
   typedef std::vector<CallsiteInfo> CallsiteInfoList;
@@ -213,10 +252,22 @@ private:
   ConstantPool ConstPool;
   FnStackSizeMap FnStackSize;
 
+  /// Get stackmap information for register location
+  void getRegLocation(unsigned Phys, unsigned &Dwarf, unsigned &Offset) const;
+
+  /// Get pointer typing information for stackmap operand
+  void getPointerInfo(const Value *Op, const DataLayout &DL, bool &isPtr,
+                      bool &isAlloca, unsigned &AllocaSize) const;
+
+  /// Add duplicate target-specific locations for a stackmap operand
+  void addDuplicateLocs(const CallInst *StackMap, const Value *Oper,
+                        LocationVec &Locs, unsigned Size, bool Ptr,
+                        bool Alloca, unsigned AllocaSize) const;
+
   MachineInstr::const_mop_iterator
   parseOperand(MachineInstr::const_mop_iterator MOI,
                MachineInstr::const_mop_iterator MOE, LocationVec &Locs,
-               LiveOutVec &LiveOuts) const;
+               LiveOutVec &LiveOuts, User::const_op_iterator &Op) const;
 
   /// \brief Create a live-out register record for the given register @p Reg.
   LiveOutReg createLiveOutReg(unsigned Reg,
@@ -226,6 +277,15 @@ private:
   /// registers that need to be recorded in the stackmap.
   LiveOutVec parseRegisterLiveOutMask(const uint32_t *Mask) const;
 
+  /// Convert a list of instructions used to generate an architecture-specific
+  /// live value into multiple individual records.
+  void genArchValsFromInsts(ArchValues &AV,
+                            Location &Loc,
+                            const MachineLiveVal &MLV);
+
+  /// Add architecture-specific locations for the stackmap.
+  void addArchLiveVals(const CallInst *SM, ArchValues &AV);
+
   /// This should be called by the MC lowering code _immediately_ before
   /// lowering the MI to an MCInst. It records where the operands for the
   /// instruction are stored, and outputs a label to record the offset of
@@ -240,7 +300,7 @@ private:
   void emitStackmapHeader(MCStreamer &OS);
 
   /// \brief Emit the function frame record for each function.
-  void emitFunctionFrameRecords(MCStreamer &OS);
+  void emitFunctionFrameRecords(MCStreamer &OS, const UnwindInfo *UI);
 
   /// \brief Emit the constant pool.
   void emitConstantPoolEntries(MCStreamer &OS);
diff --git a/llvm/include/llvm/CodeGen/StackTransformTypes.def b/llvm/include/llvm/CodeGen/StackTransformTypes.def
new file mode 100644
index 00000000000..b800e1f1c45
--- /dev/null
+++ b/llvm/include/llvm/CodeGen/StackTransformTypes.def
@@ -0,0 +1,36 @@
+//===-- llvm/Target/StackTransformTypes.def - Generator Opcodes -*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Macros which define the set of available instructions for the ISA-agnostic
+// value generator.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_CODEGEN_STACKTRANSFORMTYPES_DEF
+#define LLVM_CODEGEN_STACKTRANSFORMTYPES_DEF
+
+// TODO generate using TableGen rather than X macros
+
+/// Each instruction is defined by a mnemonic and an operand (represented using
+/// the various ValueGenInst types).
+#define VALUE_GEN_INST \
+  X(Set)             /* Set the destination to another value */ \
+  X(Add)             /* Add a value to the destination */ \
+  X(Subtract)        /* Subtract a value from the destination */ \
+  X(Multiply)        /* Multiply the destination by another value */ \
+  X(Divide)          /* Divide the destination by another value */ \
+  X(LeftShift)       /* Left-shift the destination */ \
+  X(RightShiftLog)   /* Right-shift (logical) the destination */ \
+  X(RightShiftArith) /* Right-shift (arithmetic) the destination */ \
+  X(Mask)            /* Apply bit mask to the destination */ \
+  X(Load32)          /* Load 32 bits from memory */ \
+  X(Load64)          /* Load 64 bits from memory */
+
+#endif
+
diff --git a/llvm/include/llvm/CodeGen/StackTransformTypes.h b/llvm/include/llvm/CodeGen/StackTransformTypes.h
new file mode 100644
index 00000000000..8d437c2f5ed
--- /dev/null
+++ b/llvm/include/llvm/CodeGen/StackTransformTypes.h
@@ -0,0 +1,587 @@
+//===------- StackTransformTypes.h - Stack Transform Types ------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_CODEGEN_STACKTRANFORMTYPES_H
+#define LLVM_CODEGEN_STACKTRANFORMTYPES_H
+
+#include <cstdint>
+#include <string>
+#include <vector>
+#include <map>
+#include <memory>
+#include "llvm/CodeGen/MachineOperand.h"
+#include "llvm/CodeGen/StackTransformTypes.def"
+#include "llvm/ADT/SmallVector.h"
+
+namespace llvm {
+
+class AsmPrinter;
+class Instruction;
+class MachineInstr;
+class MCSymbol;
+class Value;
+
+//===----------------------------------------------------------------------===//
+// Types for generating more complex architecture-specific live values
+//
+
+#define INV_INST_TYPE "Invalid instruction type"
+
+/// ValueGenInst - an instruction for the transformation runtime which generates
+/// a live value.  These instructions specify a simple operation (e.g., add) and
+/// an operand.  These instructions, coupled together with a destination
+/// location (i.e., a register or stack slot), allow the runtime to construct
+/// more complex live values like bit-shifts or pointers into arrays of structs.
+class ValueGenInst {
+public:
+  virtual ~ValueGenInst() {}
+
+  /// Instruction types
+  enum InstType {
+#define X(type) type,
+    VALUE_GEN_INST
+#undef X
+  };
+  virtual InstType type() const = 0;
+
+  /// Operand types
+  enum OpType { Register, Immediate, Reference };
+  virtual OpType opType() const = 0;
+
+  /// Equivalence checking.  Depends on both instruction & operand type, and
+  /// any operand-specific information.
+  virtual bool operator==(const ValueGenInst &RHS) const = 0;
+
+  /// Get a human-readable name for the instruction type
+  static const char *getInstName(enum InstType Type);
+  static std::string getInstNameStr(enum InstType Type);
+
+  /// Get a human-readable description of the instruction & operand
+  virtual std::string str() const = 0;
+private:
+  static const char *InstTypeStr[];
+};
+
+/// RegInstructionBase - base class for register operand instructions.  The
+/// register is stored as an architecture-specific physical register.
+class RegInstructionBase : public ValueGenInst {
+public:
+  virtual OpType opType() const { return Register; }
+  unsigned getReg() const { return Reg; }
+  void setReg(unsigned Reg) { this->Reg = Reg; }
+
+protected:
+  /// The register used in the instruction
+  // Note: will be converted to DWARF during metadata emission
+  unsigned Reg;
+
+  RegInstructionBase(unsigned Reg) : Reg(Reg) {}
+};
+
+/// RegInstruction<T> - register-based instructions.  Instructions are
+/// specified via template argument.
+template<ValueGenInst::InstType Type>
+class RegInstruction : public RegInstructionBase {
+  static_assert(Type == Set || Type == Add || Type == Subtract ||
+                Type == Multiply || Type == Divide,
+                INV_INST_TYPE " for register instruction");
+public:
+  RegInstruction(unsigned Reg) : RegInstructionBase(Reg) {}
+
+  virtual InstType type() const { return Type; }
+
+  virtual bool operator==(const ValueGenInst &RHS) const {
+    if(RHS.type() == Type && RHS.opType() == Register) {
+      const RegInstruction<Type> &RI = (const RegInstruction<Type> &)RHS;
+      if(RI.Reg == Reg) return true;
+    }
+    return false;
+  }
+
+  virtual std::string str() const
+  { return getInstNameStr(Type) + " register " + std::to_string(Reg); }
+};
+
+/// ImmInstructionBase - base class for immediate operand instructions.
+class ImmInstructionBase : public ValueGenInst {
+public:
+  virtual OpType opType() const { return Immediate; }
+  unsigned getImmSize() const { return Size; }
+  int64_t getImm() const { return Imm; }
+  void setImm(unsigned Size, int64_t Imm)
+  { this->Size = Size; this->Imm = Imm; }
+
+protected:
+  unsigned Size; // in bytes
+  int64_t Imm;
+
+  ImmInstructionBase(unsigned Size, int64_t Imm) : Size(Size), Imm(Imm) {}
+};
+
+/// ImmInstruction<T> - immediate-based instructions.  Instructions are
+/// specified via template argument.
+template<ValueGenInst::InstType Type>
+class ImmInstruction : public ImmInstructionBase {
+public:
+  ImmInstruction(unsigned Size, int64_t Imm) : ImmInstructionBase(Size, Imm) {}
+
+  virtual InstType type() const { return Type; }
+
+  virtual bool operator==(const ValueGenInst &RHS) const {
+    if(RHS.type() == Type && RHS.opType() == Immediate) {
+      const ImmInstruction<Type> &II = (const ImmInstruction<Type> &)RHS;
+      if(II.Imm == Imm && II.Size == Size) return true;
+    }
+    return false;
+  }
+
+  virtual std::string str() const
+  { return getInstNameStr(Type) + " immediate " + std::to_string(Imm); }
+};
+
+/// ReferenceInstruction - references to symbols.  Only supports set
+/// instructions.
+class RefInstruction : public ValueGenInst {
+public:
+  RefInstruction(const MachineOperand &Symbol) : Symbol(Symbol) {}
+
+  virtual InstType type() const { return Set; }
+  virtual OpType opType() const { return Reference; }
+  MCSymbol *getReference(AsmPrinter &AP) const;
+  virtual bool operator==(const ValueGenInst &RHS) const {
+    if(RHS.type() == Set && RHS.opType() == Reference) {
+      const RefInstruction &RI = (const RefInstruction &)RHS;
+      if(&RI.Symbol == &Symbol) return true;
+    }
+    return false;
+  }
+  virtual std::string str() const;
+
+private:
+  // MCSymbols may not exist yet, so instead store the operand to look up the
+  // MCSymbol at metadata emission time.
+  // Note: store hard-copy (not reference) because optimizations may convert
+  // symbol reference to a different type, e.g., register
+  const MachineOperand Symbol;
+};
+
+/// Wrap raw pointers to ValueGenInst in smart pointers.  Use shared_ptr so we
+/// can use copy constructors for containers of these instructions.
+typedef std::shared_ptr<ValueGenInst> ValueGenInstPtr;
+
+/// A list of instructions used to generate a value
+typedef std::vector<ValueGenInstPtr> ValueGenInstList;
+
+#undef INV_INST_TYPE
+
+//===----------------------------------------------------------------------===//
+// Machine-specific live values
+//
+// These are the live values used to populate an architecture-specific location,
+// e.g., a reference to a global symbol or an immediate value
+
+/// MachineLiveVal - A machine-specific live value
+class MachineLiveVal {
+public:
+  /// Constructors & destructors.
+  // Note: create child class objects rather than objects of this class.
+  virtual ~MachineLiveVal() {}
+  virtual MachineLiveVal *copy() const = 0;
+
+  /// Possible live value types
+  enum Type { SymbolRef, ConstPoolRef, StackObject, Immediate, Generated };
+
+  /// Determine the live value's type
+  virtual enum Type getType() const = 0;
+  virtual bool isReference() const { return false; }
+  virtual bool isSymbolRef() const { return false; }
+  virtual bool isConstPoolRef() const { return false; }
+  virtual bool isStackObject() const { return false; }
+  virtual bool isImm() const { return false; }
+  virtual bool isGenerated() const { return false; }
+
+  /// Equivalence checking
+  virtual bool operator==(const MachineLiveVal &RHS) const = 0;
+
+  /// Generate a human-readable string describing the value
+  virtual std::string toString() const = 0;
+
+  /// Get the machine instruction which defines the live value
+  const MachineInstr *getDefiningInst() const { return DefMI; }
+
+  /// Return whether this value is a pointer
+  // Note: if the value *could* be a pointer, this should be set so the runtime
+  // can do pointer-to-stack checks
+  bool isPtr() const { return Ptr; }
+
+protected:
+  /// Defining instruction for live value
+  const MachineInstr *DefMI;
+
+  /// Is this a pointer?
+  // Note: if the value *could* be a pointer, this should be set so the runtime
+  // can do pointer-to-stack checks
+  bool Ptr;
+
+  MachineLiveVal(const MachineInstr *DefMI, bool Ptr)
+    : DefMI(DefMI), Ptr(Ptr) {}
+  MachineLiveVal(const MachineLiveVal &C) : DefMI(C.DefMI), Ptr(C.Ptr) {}
+};
+
+/// MachineReference - a reference to some binary object outside of thread
+/// local storage
+class MachineReference : public MachineLiveVal {
+public:
+  virtual bool isReference() const { return true; }
+
+  /// Get a symbol reference for label generation
+  virtual MCSymbol *getReference(AsmPrinter &AP) const = 0;
+
+  /// Return whether we are to load the reference's value
+  virtual bool isLoad() const { return false; }
+
+protected:
+  MachineReference(const MachineInstr *DefMI)
+    : MachineLiveVal(DefMI, true) {}
+  MachineReference(const MachineReference &C) : MachineLiveVal(C) {}
+};
+
+/// MachineSymbolRef - a reference to a global symbol
+class MachineSymbolRef : public MachineReference {
+public:
+  MachineSymbolRef(const MachineOperand &Symbol,
+                   bool Load,
+                   const MachineInstr *DefMI)
+    : MachineReference(DefMI), Symbol(Symbol), Load(Load) {}
+  MachineSymbolRef(const MachineSymbolRef &C)
+    : MachineReference(C), Symbol(C.Symbol), Load(C.Load) {}
+  virtual MachineLiveVal *copy() const { return new MachineSymbolRef(*this); }
+
+  virtual enum Type getType() const { return SymbolRef; }
+  virtual bool isSymbolRef() const { return true; }
+
+  virtual bool operator==(const MachineLiveVal &RHS) const;
+  virtual std::string toString() const;
+  virtual MCSymbol *getReference(AsmPrinter &AP) const;
+  virtual bool isLoad() const { return Load; }
+
+private:
+  // MCSymbols may not exist yet, so instead store the operand to look up the
+  // MCSymbol at metadata emission time.
+  // Note: store hard-copy (not reference) because optimizations may convert
+  // symbol reference to a different type, e.g., register
+  const MachineOperand Symbol;
+
+  // Should we load the reference's value?
+  bool Load;
+};
+
+/// MachineConstPoolRef - a reference to a constant pool entry
+class MachineConstPoolRef : public MachineReference {
+public:
+  MachineConstPoolRef(int Index, const MachineInstr *DefMI)
+    : MachineReference(DefMI), Index(Index) {}
+  MachineConstPoolRef(const MachineConstPoolRef &C)
+    : MachineReference(C), Index(C.Index) {}
+  virtual MachineLiveVal *copy() const
+  { return new MachineConstPoolRef(*this); }
+
+  virtual enum Type getType() const { return ConstPoolRef; }
+  virtual bool isConstPoolRef() const { return true; }
+
+  virtual bool operator==(const MachineLiveVal &RHS) const;
+
+  virtual std::string toString() const
+  { return "reference to constant pool index " + std::to_string(Index); }
+
+  virtual MCSymbol *getReference(AsmPrinter &AP) const;
+
+private:
+  int Index;
+};
+
+/// MachineStackObject - an object on the stack
+class MachineStackObject : public MachineLiveVal {
+public:
+  MachineStackObject(int Index,
+                     bool Load,
+                     const MachineInstr *DefMI,
+                     bool Ptr = false)
+    : MachineLiveVal(DefMI, Ptr), Index(Index), Load(Load) {}
+  MachineStackObject(const MachineStackObject &C)
+    : MachineLiveVal(C), Index(C.Index), Load(C.Load) {}
+  virtual MachineLiveVal *copy() const
+  { return new MachineStackObject(*this); }
+
+  /// Objects common across stack frames for all supported architectures
+  enum Common { None, ReturnAddr = INT32_MAX };
+
+  virtual enum Type getType() const { return StackObject; }
+  virtual bool isStackObject() const { return true; }
+  virtual enum Common getCommonObjectType() const { return None; }
+  virtual bool isCommonObject() const { return false; }
+
+  virtual bool operator==(const MachineLiveVal &RHS) const;
+
+  virtual std::string toString() const;
+
+  /// Return the object's offset from a base register (returned in BR)
+  virtual int getOffsetFromReg(AsmPrinter &AP, unsigned &BR) const;
+
+  int getIndex() const { return Index; }
+  void setIndex(int Index) { this->Index = Index; }
+  bool isLoad() const { return Load; }
+  void setLoad(bool Load) { this->Load = Load; }
+
+private:
+  /// The stack slot index of a stack object
+  int Index;
+
+  /// Are we generating a reference to a stack object or loading a value from
+  /// the stack slot?
+  bool Load;
+};
+
+/// ReturnAddress - the return address stored on the stack
+class ReturnAddress : public MachineStackObject {
+public:
+  ReturnAddress(const MachineInstr *DefMI)
+    : MachineStackObject(ReturnAddr, true, DefMI, false) {}
+  ReturnAddress(const ReturnAddress &C) : MachineStackObject(C) {}
+  virtual MachineLiveVal *copy() const { return new ReturnAddress(*this); }
+
+  virtual enum Common getCommonObjectType() const { return ReturnAddr; }
+  virtual bool isCommonObject() const { return true; }
+
+  virtual std::string toString() const
+  { return "function return address"; }
+
+  virtual int getOffsetFromReg(AsmPrinter &AP, unsigned &BR) const;
+};
+
+/// MachineImmediate - an immediate value
+class MachineImmediate : public MachineLiveVal {
+public:
+  MachineImmediate(unsigned Size,
+                   uint64_t Value,
+                   const MachineInstr *DefMI,
+                   bool Ptr = false);
+  MachineImmediate(const MachineImmediate &C)
+    : MachineLiveVal(C), Size(C.Size), Value(C.Value) {}
+  virtual MachineLiveVal *copy() const { return new MachineImmediate(*this); }
+
+  virtual enum Type getType() const { return Immediate; }
+  virtual bool isImm() const { return true; }
+
+  virtual bool operator==(const MachineLiveVal &RHS) const;
+
+  virtual std::string toString() const
+  { return "immediate value: " + std::to_string(Value); }
+
+  unsigned getSize() const { return Size; }
+  uint64_t getValue() const { return Value; }
+
+private:
+  unsigned Size; // in bytes
+  uint64_t Value;
+};
+
+/// MachineGeneratedVal - a value generated through a set of small operations
+class MachineGeneratedVal : public MachineLiveVal {
+public:
+  MachineGeneratedVal(const ValueGenInstList &VG,
+                      const MachineInstr *DefMI,
+                      bool Ptr)
+    : MachineLiveVal(DefMI, Ptr), VG(VG) {}
+  virtual MachineLiveVal *copy() const
+  { return new MachineGeneratedVal(*this); }
+
+  enum Type getType() const { return Generated; }
+  bool isGenerated() const { return true; }
+
+  virtual bool operator==(const MachineLiveVal &RHS) const;
+
+  virtual std::string toString() const
+  { return "generated value, " + std::to_string(VG.size()) + " instruction(s)"; }
+
+  const ValueGenInstList &getInstructions() const { return VG; }
+
+private:
+  ValueGenInstList VG;
+};
+
+// TODO add API to generate "Operation"
+
+//===----------------------------------------------------------------------===//
+// Machine-specific locations
+//
+// These are locations to be populated with the live values, e.g., a register or
+// stack slot.
+//
+// Note: these represent a live value's *destination*, not the live value
+// itself.  For example, don't confuse MachineStackObject above (a live value
+// to be copied from a stack slot) versus MachineLiveStackSlot below (the
+// location where a live value will be stored).
+
+/// MachineLiveLoc - an architecture-specific location for a live value
+class MachineLiveLoc {
+public:
+  /// Constructors & destructors.
+  // Note: create child class objects rather than objects of this class.
+  virtual ~MachineLiveLoc() {}
+  virtual MachineLiveLoc *copy() const = 0;
+  virtual bool operator==(const MachineLiveLoc &R) const = 0;
+
+  /// Determine the live value location type
+  virtual bool isReg() const { return false; }
+  virtual bool isStackAddr() const { return false; }
+  virtual bool isStackSlot() const { return false; }
+
+  virtual std::string toString() const = 0;
+};
+
+/// MachineLiveReg - a live value stored in a register.  Stores the register
+/// number as an architecture-specific physical register.
+class MachineLiveReg : public MachineLiveLoc {
+public:
+  MachineLiveReg(unsigned Reg) : Reg(Reg) {}
+  MachineLiveReg(const MachineLiveReg &C) : Reg(C.Reg) {}
+  virtual MachineLiveLoc *copy() const { return new MachineLiveReg(*this); }
+
+  virtual bool isReg() const { return true; }
+
+  virtual bool operator==(const MachineLiveLoc &RHS) const;
+
+  unsigned getReg() const { return Reg; }
+  void setReg(unsigned Reg) { this->Reg = Reg; }
+
+  virtual std::string toString() const
+  { return "live value in register " + std::to_string(Reg); }
+
+private:
+  unsigned Reg;
+};
+
+/// MachineLiveStackAddr - a live value stored at a known stack address.  Can
+/// be used for stack objects at hard-coded offsets, e.g., the TOC pointer save
+/// location for PowerPC/ELFv2 ABI.
+class MachineLiveStackAddr : public MachineLiveLoc {
+public:
+  MachineLiveStackAddr() : Offset(INT32_MAX), Reg(UINT32_MAX), Size(0) {}
+  MachineLiveStackAddr(int Offset, unsigned Reg, unsigned Size)
+    : Offset(Offset), Reg(Reg), Size(Size) {}
+  MachineLiveStackAddr(const MachineLiveStackAddr &C)
+    : Offset(C.Offset), Reg(C.Reg), Size(C.Size) {}
+  virtual MachineLiveLoc *copy() const
+  { return new MachineLiveStackAddr(*this); }
+
+  virtual bool isStackAddr() const { return true; }
+
+  virtual bool operator==(const MachineLiveLoc &RHS) const;
+
+  int getOffset() const { return Offset; }
+  void setOffset(int Offset) { this->Offset = Offset; }
+  unsigned getReg() const { return Reg; }
+  void setReg(unsigned Reg) { this->Reg = Reg; }
+  void setSize(unsigned Size) { this->Size = Size; }
+
+  // Calculate the final position of the stack object.  Return the object's
+  // location as an offset from a base pointer register.
+  virtual int calcAndGetRegOffset(const AsmPrinter &AP, unsigned &BP)
+  { BP = Reg; return Offset; }
+
+  // The size of a stack object may need to be determined by code emission
+  // metadata in child classes, hence the AsmPrinter argument
+  virtual unsigned getSize(const AsmPrinter &AP) { return Size; }
+
+  virtual std::string toString() const
+  {
+    return "live value at register " + std::to_string(Reg) +
+           " + " + std::to_string(Offset);
+  }
+
+protected:
+  // The object is referenced by an offset from a (physical) register's value.
+  int Offset;
+  unsigned Reg, Size;
+};
+
+/// MachineLiveStackSlot - a live value stored in a stack slot.  A more
+/// abstract version of MachineLiveStackAddr, where the value is in a virtual
+/// stack slot whose address won't be determined until instruction emission.
+class MachineLiveStackSlot : public MachineLiveStackAddr {
+public:
+  MachineLiveStackSlot(int Index) : Index(Index) {}
+  MachineLiveStackSlot(const MachineLiveStackSlot &C)
+    : MachineLiveStackAddr(C), Index(C.Index) {}
+  virtual MachineLiveLoc *copy() const
+  { return new MachineLiveStackSlot(*this); }
+
+  virtual bool isStackSlot() const { return true; }
+
+  virtual bool operator==(const MachineLiveLoc &RHS) const;
+
+  unsigned getStackSlot() const { return Index; }
+  void setStackSlot(int Index) { this->Index = Index; }
+  virtual int calcAndGetRegOffset(const AsmPrinter &AP, unsigned &BP);
+  virtual unsigned getSize(const AsmPrinter &AP);
+
+  virtual std::string toString() const
+  { return "live value in stack slot " + std::to_string(Index); }
+
+private:
+  int Index;
+};
+
+/// Useful typedefs for data structures needed to store additional stack
+/// transformation metadata not captured in the stackmap instructions.
+
+/// Tidy up objects defined above into smart pointers
+typedef std::unique_ptr<MachineLiveVal> MachineLiveValPtr;
+typedef std::unique_ptr<MachineLiveLoc> MachineLiveLocPtr;
+
+/// Map an IR operand number to the number of Machine IR operands in the lowered
+/// stackmap instruction.  Required as legalizing some types (e.g., i128) may
+/// require changing the IR value into more/fewer multiple machine values.
+typedef std::map<unsigned, unsigned> OpNumberMap;
+typedef std::pair<unsigned, unsigned> OpNumberPair;
+
+/// Map stackmaps to information about how operands changed during legalization.
+typedef std::map<int64_t, OpNumberMap> SMToOpLegalizeMap;
+typedef std::pair<int64_t, OpNumberMap> SMToOpLegalizePair;
+
+/// A vector of architecture-specific live value locations
+// Note: we could use a set instead (because we want unique live values), but
+// because we're using MachineLiveLoc pointers the set would only uniquify
+// based on the pointer, not the pointed-to value.
+typedef SmallVector<MachineLiveLocPtr, 4> MachineLiveLocs;
+
+/// Map IR value to a list of architecture-specific live value locations.
+/// Usually used to store duplicate locations for an IR value.
+typedef std::map<const Value *, MachineLiveLocs> IRToMachineLocs;
+typedef std::pair<const Value *, MachineLiveLocs> IRMachineLocPair;
+
+/// Map an IR instruction to the metadata about its IR operands (and their
+/// associated architecture-specific live values locations).
+typedef std::map<const Instruction *, IRToMachineLocs> InstToOperands;
+typedef std::pair<const Instruction *, IRToMachineLocs> InstOperandPair;
+
+/// A pair to couple an architecture-specific location to the value used to
+/// populate it, and a vector for storing several of them.
+typedef std::pair<MachineLiveLocPtr, MachineLiveValPtr> ArchLiveValue;
+typedef SmallVector<ArchLiveValue, 8> ArchLiveValues;
+
+// Map an IR instruction to architecture-specific live values
+typedef std::map<const Instruction *, ArchLiveValues> InstToArchLiveValues;
+typedef std::pair<const Instruction *, ArchLiveValues> InstArchLiveValuePair;
+
+}
+
+#endif
+
diff --git a/llvm/include/llvm/CodeGen/UnwindInfo.h b/llvm/include/llvm/CodeGen/UnwindInfo.h
new file mode 100644
index 00000000000..ce9d46166b5
--- /dev/null
+++ b/llvm/include/llvm/CodeGen/UnwindInfo.h
@@ -0,0 +1,112 @@
+//===----------------- UnwindInfo.h - UnwindInfo ---------------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Generate unwinding information for stack transformation runtime.  Note that
+// this is implemented assuming the function uses a frame base pointer (FBP).
+// This requirement is guaranteed to be satisfied if the function has a
+// stackmap, which are the only functions for which we want to generate
+// unwinding information.
+//
+//===---------------------------------------------------------------------===//
+
+#ifndef LLVM_CODEGEN_UNWINDINFO_H
+#define LLVM_CODEGEN_UNWINDINFO_H
+
+#include "llvm/CodeGen/AsmPrinter.h"
+#include "llvm/CodeGen/MachineFrameInfo.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/MC/MCContext.h"
+#include "llvm/MC/MCStreamer.h"
+#include "llvm/Support/Debug.h"
+#include <map>
+
+namespace llvm {
+
+class UnwindInfo {
+public:
+  /// Per-function unwinding metadata classes & typedefs
+  class FuncUnwindInfo {
+  public:
+    uint32_t SecOffset; // Offset into unwinding record section
+    uint32_t NumUnwindRecord; // Number of unwinding records
+
+    FuncUnwindInfo() : SecOffset(UINT32_MAX), NumUnwindRecord(0) {}
+    FuncUnwindInfo(uint32_t SecOffset, uint32_t NumUnwindRecord)
+      : SecOffset(SecOffset), NumUnwindRecord(NumUnwindRecord) {}
+  };
+
+  typedef std::pair<const MCSymbol *, FuncUnwindInfo> FuncUnwindPair;
+  typedef std::map<const MCSymbol *, FuncUnwindInfo> FuncUnwindMap;
+
+  /// Unwinding record classes & typedefs
+  class RegOffset {
+  public:
+    uint32_t DwarfReg;
+    int32_t Offset;
+
+    RegOffset() : DwarfReg(0), Offset(0) {}
+    RegOffset(uint32_t DwarfReg, int32_t Offset) :
+      DwarfReg(DwarfReg), Offset(Offset) {}
+  };
+
+  typedef SmallVector<RegOffset, 32> CalleeSavedRegisters;
+  typedef std::pair<const MCSymbol *, CalleeSavedRegisters> FuncCalleePair;
+  typedef std::map<const MCSymbol *, CalleeSavedRegisters> FuncCalleeMap;
+
+  /// \brief Constructors
+  UnwindInfo() = delete;
+  UnwindInfo(AsmPrinter &AP)
+    : AP(AP), OutContext(AP.OutStreamer->getContext()), Emitted(false) {};
+
+  /// \bried Clear all saved unwinding information
+  void reset() {
+    Emitted = false;
+    FuncCalleeSaved.clear();
+    FuncUnwindMetadata.clear();
+  }
+
+  /// \brief Store unwinding information for a function
+  void recordUnwindInfo(const MachineFunction &MF);
+
+  /// \brief Add a register restore offset for a function.  MachineReg will get
+  /// converted to a DWARF register internally.
+  void addRegisterUnwindInfo(const MachineFunction &MF,
+                             uint32_t MachineReg,
+                             int32_t Offset);
+
+  /// Create an unwinding information section and serialize the map info into
+  /// it.
+  ///
+  /// Note: unlike StackMaps::serializeToStackMapSection, this function *does
+  /// not* clear out the data structures.  This is so that the stack map
+  /// machinery can access per-function unwinding information.
+  void serializeToUnwindInfoSection();
+
+  /// Get unwinding section metadata for a function
+  const FuncUnwindInfo &getUnwindInfo(const MCSymbol *Func) const;
+
+private:
+  AsmPrinter &AP;
+  MCContext &OutContext;
+  FuncCalleeMap FuncCalleeSaved;
+  FuncUnwindMap FuncUnwindMetadata;
+  bool Emitted;
+
+  /// \brief Emit the unwind info for each function.
+  void emitUnwindInfo(MCStreamer &OS);
+
+  /// \brief Emit the address range info for each function.
+  void emitAddrRangeInfo(MCStreamer &OS);
+
+  void print(raw_ostream &OS);
+  void debug() { print(dbgs()); }
+};
+}
+
+#endif
diff --git a/llvm/include/llvm/IR/DiagnosticInfo.h b/llvm/include/llvm/IR/DiagnosticInfo.h
index f38313f82ea..fe7579e373d 100644
--- a/llvm/include/llvm/IR/DiagnosticInfo.h
+++ b/llvm/include/llvm/IR/DiagnosticInfo.h
@@ -57,6 +57,7 @@ enum DiagnosticKind {
   DK_OptimizationRemarkMissed,
   DK_OptimizationRemarkAnalysis,
   DK_OptimizationFailure,
+  DK_OptimizationError,
   DK_MIRParser,
   DK_FirstPluginKind
 };
@@ -461,6 +462,29 @@ public:
   bool isEnabled() const override;
 };
 
+/// Diagnostic information for show-stopping failures.
+class DiagnosticInfoOptimizationError
+    : public DiagnosticInfoOptimizationBase {
+public:
+  /// \p Fn is the function where the diagnostic is being emitted. \p DLoc is
+  /// the location information to use in the diagnostic. If line table
+  /// information is available, the diagnostic will include the source code
+  /// location. \p Msg is the message to show. Note that this class does not
+  /// copy this message, so this reference must be valid for the whole life time
+  /// of the diagnostic.
+  DiagnosticInfoOptimizationError(const Function &Fn, const DebugLoc &DLoc,
+                                  const Twine &Msg)
+      : DiagnosticInfoOptimizationBase(DK_OptimizationError, DS_Error,
+                                       nullptr, Fn, DLoc, Msg) {}
+
+  static bool classof(const DiagnosticInfo *DI) {
+    return DI->getKind() == DK_OptimizationError;
+  }
+
+  /// \see DiagnosticInfoOptimizationBase::isEnabled.
+  bool isEnabled() const override;
+};
+
 /// Emit a warning when loop vectorization is specified but fails. \p Fn is the
 /// function triggering the warning, \p DLoc is the debug location where the
 /// diagnostic is generated. \p Msg is the message string to use.
diff --git a/llvm/include/llvm/IR/ModuleSlotTracker.h b/llvm/include/llvm/IR/ModuleSlotTracker.h
index c37dcecf8e4..1d617819b84 100644
--- a/llvm/include/llvm/IR/ModuleSlotTracker.h
+++ b/llvm/include/llvm/IR/ModuleSlotTracker.h
@@ -61,6 +61,9 @@ public:
   /// Purge the currently incorporated function and incorporate \c F.  If \c F
   /// is currently incorporated, this is a no-op.
   void incorporateFunction(const Function &F);
+
+  /// Get a local value's slot.
+  int getLocalSlot(const Value *);
 };
 
 } // end namespace llvm
diff --git a/llvm/include/llvm/IR/ValueMap.h b/llvm/include/llvm/IR/ValueMap.h
index 4d00b637609..7fddef45d8d 100644
--- a/llvm/include/llvm/IR/ValueMap.h
+++ b/llvm/include/llvm/IR/ValueMap.h
@@ -99,7 +99,7 @@ public:
   explicit ValueMap(const ExtraData &Data, unsigned NumInitBuckets = 64)
       : Map(NumInitBuckets), Data(Data) {}
 
-  bool hasMD() const { return MDMap; }
+  bool hasMD() const { return bool(MDMap); }
   MDMapT &MD() {
     if (!MDMap)
       MDMap.reset(new MDMapT);
diff --git a/llvm/include/llvm/InitializePasses.h b/llvm/include/llvm/InitializePasses.h
index e3b9a95f0a3..d01de91a898 100644
--- a/llvm/include/llvm/InitializePasses.h
+++ b/llvm/include/llvm/InitializePasses.h
@@ -119,6 +119,7 @@ void initializeDominanceFrontierPass(PassRegistry&);
 void initializeDominatorTreeWrapperPassPass(PassRegistry&);
 void initializeEarlyIfConverterPass(PassRegistry&);
 void initializeEdgeBundlesPass(PassRegistry&);
+void initializeEnumerateLoopPathsPass(PassRegistry&);
 void initializeExpandPostRAPass(PassRegistry&);
 void initializeGCOVProfilerPass(PassRegistry&);
 void initializeInstrProfilingPass(PassRegistry&);
@@ -131,6 +132,7 @@ void initializeDataFlowSanitizerPass(PassRegistry&);
 void initializeScalarizerPass(PassRegistry&);
 void initializeEarlyCSELegacyPassPass(PassRegistry &);
 void initializeEliminateAvailableExternallyPass(PassRegistry&);
+void initializeMigrationPointsPass(PassRegistry&);
 void initializeExpandISelPseudosPass(PassRegistry&);
 void initializeFunctionAttrsPass(PassRegistry&);
 void initializeGCMachineCodeAnalysisPass(PassRegistry&);
@@ -146,6 +148,7 @@ void initializeIfConverterPass(PassRegistry&);
 void initializeInductiveRangeCheckEliminationPass(PassRegistry&);
 void initializeIndVarSimplifyPass(PassRegistry&);
 void initializeInlineCostAnalysisPass(PassRegistry&);
+void initializeInsertStackMapsPass(PassRegistry&);
 void initializeInstructionCombiningPassPass(PassRegistry&);
 void initializeInstCountPass(PassRegistry&);
 void initializeInstNamerPass(PassRegistry&);
@@ -153,6 +156,7 @@ void initializeInternalizePassPass(PassRegistry&);
 void initializeIntervalPartitionPass(PassRegistry&);
 void initializeJumpThreadingPass(PassRegistry&);
 void initializeLCSSAPass(PassRegistry&);
+void initializeLibcStackMapsPass(PassRegistry&);
 void initializeLICMPass(PassRegistry&);
 void initializeLazyValueInfoPass(PassRegistry&);
 void initializeLibCallAliasAnalysisPass(PassRegistry&);
@@ -162,6 +166,7 @@ void initializeLiveIntervalsPass(PassRegistry&);
 void initializeLiveRegMatrixPass(PassRegistry&);
 void initializeLiveStacksPass(PassRegistry&);
 void initializeLiveVariablesPass(PassRegistry&);
+void initializeLiveValuesPass(PassRegistry&);
 void initializeLoaderPassPass(PassRegistry&);
 void initializeLocalStackSlotPassPass(PassRegistry&);
 void initializeLoopDeletionPass(PassRegistry&);
@@ -208,6 +213,7 @@ void initializeMergedLoadStoreMotionPass(PassRegistry &);
 void initializeMetaRenamerPass(PassRegistry&);
 void initializeMergeFunctionsPass(PassRegistry&);
 void initializeModuleDebugInfoPrinterPass(PassRegistry&);
+void initializeNameStringLiteralsPass(PassRegistry&);
 void initializeNaryReassociatePass(PassRegistry&);
 void initializeNoAAPass(PassRegistry&);
 void initializeObjCARCAliasAnalysisPass(PassRegistry&);
@@ -222,6 +228,7 @@ void initializePEIPass(PassRegistry&);
 void initializePHIEliminationPass(PassRegistry&);
 void initializePartialInlinerPass(PassRegistry&);
 void initializePeepholeOptimizerPass(PassRegistry&);
+void initializePopcornCompatibilityPass(PassRegistry&);
 void initializePostDomOnlyPrinterPass(PassRegistry&);
 void initializePostDomOnlyViewerPass(PassRegistry&);
 void initializePostDomPrinterPass(PassRegistry&);
@@ -250,9 +257,11 @@ void initializeSROA_DTPass(PassRegistry&);
 void initializeSROA_SSAUpPass(PassRegistry&);
 void initializeScalarEvolutionAliasAnalysisPass(PassRegistry&);
 void initializeScalarEvolutionPass(PassRegistry&);
+void initializeSelectMigrationPointsPass(PassRegistry&);
 void initializeShrinkWrapPass(PassRegistry &);
 void initializeSimpleInlinerPass(PassRegistry&);
-void initializeShadowStackGCLoweringPass(PassRegistry&);  
+void initializeShadowStackGCLoweringPass(PassRegistry&);
+void initializeStaticVarSectionsPass(PassRegistry&);
 void initializeRegisterCoalescerPass(PassRegistry&);
 void initializeSingleLoopExtractorPass(PassRegistry&);
 void initializeSinkingPass(PassRegistry&);
@@ -263,6 +272,7 @@ void initializeSpeculativeExecutionPass(PassRegistry&);
 void initializeStackProtectorPass(PassRegistry&);
 void initializeStackColoringPass(PassRegistry&);
 void initializeStackSlotColoringPass(PassRegistry&);
+void initializeStackTransformMetadataPass(PassRegistry&);
 void initializeStraightLineStrengthReducePass(PassRegistry &);
 void initializeStripDeadDebugInfoPass(PassRegistry&);
 void initializeStripDeadPrototypesPassPass(PassRegistry&);
diff --git a/llvm/include/llvm/LinkAllPasses.h b/llvm/include/llvm/LinkAllPasses.h
index cea5530db3b..468367d1537 100644
--- a/llvm/include/llvm/LinkAllPasses.h
+++ b/llvm/include/llvm/LinkAllPasses.h
@@ -33,6 +33,7 @@
 #include "llvm/Transforms/Instrumentation.h"
 #include "llvm/Transforms/ObjCARC.h"
 #include "llvm/Transforms/Scalar.h"
+#include "llvm/Transforms/Utils.h"
 #include "llvm/Transforms/Utils/SymbolRewriter.h"
 #include "llvm/Transforms/Utils/UnifyFunctionExitNodes.h"
 #include "llvm/Transforms/Vectorize.h"
@@ -81,8 +82,10 @@ namespace {
       (void) llvm::createDomPrinterPass();
       (void) llvm::createDomOnlyViewerPass();
       (void) llvm::createDomViewerPass();
+      (void) llvm::createMigrationPointsPass();
       (void) llvm::createGCOVProfilerPass();
       (void) llvm::createInstrProfilingPass();
+      (void) llvm::createEnumerateLoopPathsPass();
       (void) llvm::createFunctionInliningPass();
       (void) llvm::createAlwaysInlinerPass();
       (void) llvm::createGlobalDCEPass();
@@ -92,11 +95,14 @@ namespace {
       (void) llvm::createIPSCCPPass();
       (void) llvm::createInductiveRangeCheckEliminationPass();
       (void) llvm::createIndVarSimplifyPass();
+      (void) llvm::createInsertStackMapsPass();
       (void) llvm::createInstructionCombiningPass();
       (void) llvm::createInternalizePass();
       (void) llvm::createLCSSAPass();
+      (void) llvm::createLibcStackMapsPass();
       (void) llvm::createLICMPass();
       (void) llvm::createLazyValueInfoPass();
+      (void) llvm::createLiveValuesPass();
       (void) llvm::createLoopExtractorPass();
       (void) llvm::createLoopInterchangePass();
       (void) llvm::createLoopSimplifyPass();
@@ -110,6 +116,7 @@ namespace {
       (void) llvm::createLowerInvokePass();
       (void) llvm::createLowerSwitchPass();
       (void) llvm::createNaryReassociatePass();
+      (void) llvm::createNameStringLiteralsPass();
       (void) llvm::createNoAAPass();
       (void) llvm::createObjCARCAliasAnalysisPass();
       (void) llvm::createObjCARCAPElimPass();
@@ -120,6 +127,7 @@ namespace {
       (void) llvm::createPromoteMemoryToRegisterPass();
       (void) llvm::createDemoteRegisterToMemoryPass();
       (void) llvm::createPruneEHPass();
+      (void) llvm::createPopcornCompatibilityPass();
       (void) llvm::createPostDomOnlyPrinterPass();
       (void) llvm::createPostDomPrinterPass();
       (void) llvm::createPostDomOnlyViewerPass();
@@ -134,6 +142,7 @@ namespace {
       (void) llvm::createSafeStackPass();
       (void) llvm::createScalarReplAggregatesPass();
       (void) llvm::createSingleLoopExtractorPass();
+      (void) llvm::createStaticVarSectionsPass();
       (void) llvm::createStripSymbolsPass();
       (void) llvm::createStripNonDebugSymbolsPass();
       (void) llvm::createStripDeadDebugInfoPass();
@@ -170,6 +179,7 @@ namespace {
       (void) llvm::createBBVectorizePass();
       (void) llvm::createPartiallyInlineLibCallsPass();
       (void) llvm::createScalarizerPass();
+      (void) llvm::createSelectMigrationPointsPass();
       (void) llvm::createSeparateConstOffsetFromGEPPass();
       (void) llvm::createSpeculativeExecutionPass();
       (void) llvm::createRewriteSymbolsPass();
diff --git a/llvm/include/llvm/MC/MCCodeGenInfo.h b/llvm/include/llvm/MC/MCCodeGenInfo.h
index 0a4744f1d0f..547b5c76e9a 100644
--- a/llvm/include/llvm/MC/MCCodeGenInfo.h
+++ b/llvm/include/llvm/MC/MCCodeGenInfo.h
@@ -32,6 +32,11 @@ class MCCodeGenInfo {
   ///
   CodeGenOpt::Level OptLevel;
 
+  /// ArchIROptLevel - Optimization level (architecture-specific IR
+  /// optimizations only).  Defaults to OptLevel.
+  ///
+  CodeGenOpt::Level ArchIROptLevel;
+
 public:
   void initMCCodeGenInfo(Reloc::Model RM = Reloc::Default,
                          CodeModel::Model CM = CodeModel::Default,
@@ -43,8 +48,12 @@ public:
 
   CodeGenOpt::Level getOptLevel() const { return OptLevel; }
 
+  CodeGenOpt::Level getArchIROptLevel() const { return ArchIROptLevel; }
+
   // Allow overriding OptLevel on a per-function basis.
   void setOptLevel(CodeGenOpt::Level Level) { OptLevel = Level; }
+
+  void setArchIROptLevel(CodeGenOpt::Level Level) { ArchIROptLevel = Level; }
 };
 } // namespace llvm
 
diff --git a/llvm/include/llvm/MC/MCObjectFileInfo.h b/llvm/include/llvm/MC/MCObjectFileInfo.h
index 99e3f92bfe2..1e9cfb5b9f2 100644
--- a/llvm/include/llvm/MC/MCObjectFileInfo.h
+++ b/llvm/include/llvm/MC/MCObjectFileInfo.h
@@ -135,6 +135,10 @@ protected:
   /// Null if this target doesn't support a BSS section. ELF and MachO only.
   MCSection *TLSBSSSection; // Defaults to ".tbss".
 
+  /// Unwinding address ranges & register location sections.
+  MCSection *UnwindAddrRangeSection;
+  MCSection *UnwindInfoSection;
+
   /// StackMap section.
   MCSection *StackMapSection;
 
@@ -267,6 +271,8 @@ public:
   const MCSection *getTLSDataSection() const { return TLSDataSection; }
   MCSection *getTLSBSSSection() const { return TLSBSSSection; }
 
+  MCSection *getUnwindInfoSection() const { return UnwindInfoSection; }
+  MCSection *getUnwindAddrRangeSection() const { return UnwindAddrRangeSection; }
   MCSection *getStackMapSection() const { return StackMapSection; }
   MCSection *getFaultMapSection() const { return FaultMapSection; }
 
diff --git a/llvm/include/llvm/Target/TargetFrameLowering.h b/llvm/include/llvm/Target/TargetFrameLowering.h
index 3af2227410f..132ab67623e 100644
--- a/llvm/include/llvm/Target/TargetFrameLowering.h
+++ b/llvm/include/llvm/Target/TargetFrameLowering.h
@@ -227,6 +227,14 @@ public:
     return 0;
   }
 
+  /// Same as above, except that the 'frame register' will always be the ISA's
+  /// frame pointer (which can be different from the variable 'frame register'
+  /// which may be the stack pointer, frame pointer, etc.)
+  virtual int getFrameIndexReferenceFromFP(const MachineFunction &MF, int FI,
+                                           unsigned &FrameReg) const {
+    return getFrameIndexReference(MF, FI, FrameReg);
+  }
+
   /// This method determines which of the registers reported by
   /// TargetRegisterInfo::getCalleeSavedRegs() should actually get saved.
   /// The default implementation checks populates the \p SavedRegs bitset with
diff --git a/llvm/include/llvm/Target/TargetMachine.h b/llvm/include/llvm/Target/TargetMachine.h
index f1e9d1718f5..f1a9183051a 100644
--- a/llvm/include/llvm/Target/TargetMachine.h
+++ b/llvm/include/llvm/Target/TargetMachine.h
@@ -171,6 +171,13 @@ public:
   /// \brief Overrides the optimization level.
   void setOptLevel(CodeGenOpt::Level Level) const;
 
+  /// Returns the architecture-specific IR optimization level: None, Less,
+  /// Default or Aggressive.
+  CodeGenOpt::Level getArchIROptLevel() const;
+
+  /// \brief Overrides the architecture-specific IR optimization level.
+  void setArchIROptLevel(CodeGenOpt::Level Level) const;
+
   void setFastISel(bool Enable) { Options.EnableFastISel = Enable; }
 
   bool shouldPrintMachineCode() const { return Options.PrintMachineCode; }
diff --git a/llvm/include/llvm/Target/TargetRegisterInfo.h b/llvm/include/llvm/Target/TargetRegisterInfo.h
index 0ee936a7621..547f15c1089 100644
--- a/llvm/include/llvm/Target/TargetRegisterInfo.h
+++ b/llvm/include/llvm/Target/TargetRegisterInfo.h
@@ -869,6 +869,17 @@ public:
   /// getFrameRegister - This method should return the register used as a base
   /// for values allocated in the current stack frame.
   virtual unsigned getFrameRegister(const MachineFunction &MF) const = 0;
+
+  /// getReturnAddrLoc - This method should return the location of the saved
+  /// return address on the stack, expressed as a base register (returned via
+  /// BaseReg) and an offset
+  virtual int getReturnAddrLoc(const MachineFunction &MF,
+                               unsigned &BaseReg) const
+  {
+    llvm_unreachable("Not implemented for target!");
+    BaseReg = 0;
+    return INT32_MAX;
+  }
 };
 
 
diff --git a/llvm/include/llvm/Target/TargetSubtargetInfo.h b/llvm/include/llvm/Target/TargetSubtargetInfo.h
index 07c0c66bfa1..1a091566c2b 100644
--- a/llvm/include/llvm/Target/TargetSubtargetInfo.h
+++ b/llvm/include/llvm/Target/TargetSubtargetInfo.h
@@ -32,6 +32,7 @@ class TargetRegisterClass;
 class TargetRegisterInfo;
 class TargetSchedModel;
 class TargetSelectionDAGInfo;
+class TargetValues;
 struct MachineSchedPolicy;
 template <typename T> class SmallVectorImpl;
 
@@ -95,6 +96,13 @@ public:
     return nullptr;
   }
 
+  /// getValues - Returns the value generator object for the target or specific
+  /// subtarget
+  ///
+  virtual const TargetValues *getValues() const {
+    return nullptr;
+  };
+
   /// Resolve a SchedClass at runtime, where SchedClass identifies an
   /// MCSchedClassDesc with the isVariant property. This may return the ID of
   /// another variant SchedClass, but repeated invocation must quickly terminate
diff --git a/llvm/include/llvm/Target/TargetValues.h b/llvm/include/llvm/Target/TargetValues.h
new file mode 100644
index 00000000000..07aec94b098
--- /dev/null
+++ b/llvm/include/llvm/Target/TargetValues.h
@@ -0,0 +1,93 @@
+//===----- llvm/Target/TargetValues.h - Value Properties ----*- C++ -----*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file provides an API for detecting properties of architecture-specific
+// values & for generating a series of simple metadata instructions for
+// reconstituting a value.  This is used by the stack transformation runtime to
+// set up architecture-specific live values.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_TARGET_TARGETVAL_H
+#define LLVM_TARGET_TARGETVAL_H
+
+#include <memory>
+#include "llvm/CodeGen/StackTransformTypes.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/VirtRegMap.h"
+#include "llvm/IR/Instructions.h"
+
+namespace llvm {
+
+/// A value created by the backend to satisfy the stackmap.  Note that this is
+/// not necessarily an error -- some backends may create and save the value
+/// whereas other backends may materialize the value as needed.
+struct TemporaryValue {
+  enum type {
+    StackSlotRef
+  };
+
+  enum type Type;
+  unsigned Size;
+
+  /// The virtual register used if the temporary is stored in a register
+  unsigned Vreg;
+
+  /// If the value is of type StackSlotRef, the stack slot and offset into the
+  /// stack slot being referenced
+  int StackSlot;
+  int64_t Offset;
+};
+
+typedef std::unique_ptr<TemporaryValue> TemporaryValuePtr;
+
+//===----------------------------------------------------------------------===//
+// Superclass for ISA-specific values
+//
+
+class TargetValues {
+public:
+  TargetValues(const TargetValues &) = delete;
+  void operator=(const TargetValues &) = delete;
+  virtual ~TargetValues() {};
+
+  /// The code generator may have materialized a temporary value solely to
+  /// satisfy the stackmap if the value is materialized as-needed elsewhere.
+  /// Return metadata describing the temporary value in this situation.
+  virtual TemporaryValuePtr getTemporaryValue(const MachineInstr *MI,
+                                              const VirtRegMap *VRM) const
+  { return nullptr; }
+
+  /// Return a machine-specific value generated by a machine instruction.
+  virtual MachineLiveValPtr getMachineValue(const MachineInstr *MI) const = 0;
+
+  /// Add any required architecture-specific live values, e.g., the TOC pointer
+  /// on PowerPC.
+  virtual void addRequiredArchLiveValues(MachineFunction *MF,
+                                         const MachineInstr *MIStackMap,
+                                         const CallInst *IRStackMap) const
+  { return; }
+
+protected:
+  TargetValues() {}
+
+  /// Return whether or not the operand is some type of symbol reference.
+  static bool isSymbolValue(const MachineOperand &MO)
+  { return MO.isGlobal() || MO.isSymbol() || MO.isMCSymbol(); }
+  static bool isSymbolValue(const MachineOperand *MO)
+  { return isSymbolValue(*MO); }
+  static bool isSymbolValueConstant(const MachineOperand &MO);
+  static bool isSymbolValueConstant(const MachineOperand *MO)
+  { return isSymbolValueConstant(*MO); }
+};
+
+} // End llvm namespace
+
+#endif
+
diff --git a/llvm/include/llvm/Transforms/Instrumentation.h b/llvm/include/llvm/Transforms/Instrumentation.h
index 250e3893cb1..5cfe2cf71ee 100644
--- a/llvm/include/llvm/Transforms/Instrumentation.h
+++ b/llvm/include/llvm/Transforms/Instrumentation.h
@@ -136,6 +136,17 @@ FunctionPass *createBoundsCheckingPass();
 /// protect against stack-based overflow vulnerabilities.
 FunctionPass *createSafeStackPass();
 
+/// \brief This pass inserts equivalence points into functions.
+FunctionPass *createMigrationPointsPass();
+
+/// \brief This pass inserts stack map intrinsics at equivalence points in
+/// order to record live value locations
+ModulePass *createInsertStackMapsPass();
+
+/// \brief This pass inserts stack map intrinsics similarly to InsertStackMaps,
+/// but only in thread start functions inside of libc
+ModulePass *createLibcStackMapsPass();
+
 } // End llvm namespace
 
 #endif
diff --git a/llvm/include/llvm/Transforms/Utils.h b/llvm/include/llvm/Transforms/Utils.h
new file mode 100644
index 00000000000..c489ef9c632
--- /dev/null
+++ b/llvm/include/llvm/Transforms/Utils.h
@@ -0,0 +1,16 @@
+namespace llvm {
+
+//===----------------------------------------------------------------------===//
+//
+// NameStringLiterals - Give symbol names to anonymous string literals so they
+// can be aligned at link-time
+//
+ModulePass *createNameStringLiteralsPass();
+
+//===----------------------------------------------------------------------===//
+//
+// StaticVarSections - Put static global variables into their own sections
+//
+ModulePass *createStaticVarSectionsPass();
+
+}
diff --git a/llvm/lib/Analysis/Analysis.cpp b/llvm/lib/Analysis/Analysis.cpp
index 842ff0a14e2..2911854f9fe 100644
--- a/llvm/lib/Analysis/Analysis.cpp
+++ b/llvm/lib/Analysis/Analysis.cpp
@@ -42,8 +42,10 @@ void llvm::initializeAnalysis(PassRegistry &Registry) {
   initializeDomViewerPass(Registry);
   initializeDomPrinterPass(Registry);
   initializeDomOnlyViewerPass(Registry);
+  initializeEnumerateLoopPathsPass(Registry);
   initializePostDomViewerPass(Registry);
   initializeDomOnlyPrinterPass(Registry);
+  initializePopcornCompatibilityPass(Registry);
   initializePostDomPrinterPass(Registry);
   initializePostDomOnlyViewerPass(Registry);
   initializePostDomOnlyPrinterPass(Registry);
@@ -53,6 +55,7 @@ void llvm::initializeAnalysis(PassRegistry &Registry) {
   initializeLazyValueInfoPass(Registry);
   initializeLibCallAliasAnalysisPass(Registry);
   initializeLintPass(Registry);
+  initializeLiveValuesPass(Registry);
   initializeLoopInfoWrapperPassPass(Registry);
   initializeMemDepPrinterPass(Registry);
   initializeMemDerefPrinterPass(Registry);
@@ -66,6 +69,7 @@ void llvm::initializeAnalysis(PassRegistry &Registry) {
   initializeRegionOnlyPrinterPass(Registry);
   initializeScalarEvolutionPass(Registry);
   initializeScalarEvolutionAliasAnalysisPass(Registry);
+  initializeSelectMigrationPointsPass(Registry);
   initializeTargetTransformInfoWrapperPassPass(Registry);
   initializeTypeBasedAliasAnalysisPass(Registry);
   initializeScopedNoAliasAAPass(Registry);
diff --git a/llvm/lib/Analysis/CMakeLists.txt b/llvm/lib/Analysis/CMakeLists.txt
index 3ec79adba57..51470f84fcd 100644
--- a/llvm/lib/Analysis/CMakeLists.txt
+++ b/llvm/lib/Analysis/CMakeLists.txt
@@ -34,10 +34,13 @@ add_llvm_library(LLVMAnalysis
   LibCallAliasAnalysis.cpp
   LibCallSemantics.cpp
   Lint.cpp
+  LiveValues.cpp
   Loads.cpp
   LoopAccessAnalysis.cpp
   LoopInfo.cpp
+  LoopNestingTree.cpp
   LoopPass.cpp
+  LoopPaths.cpp
   MemDepPrinter.cpp
   MemDerefPrinter.cpp
   MemoryBuiltins.cpp
@@ -46,6 +49,7 @@ add_llvm_library(LLVMAnalysis
   ModuleDebugInfoPrinter.cpp
   NoAliasAnalysis.cpp
   PHITransAddr.cpp
+  PopcornCompatibility.cpp
   PostDominators.cpp
   PtrUseVisitor.cpp
   RegionInfo.cpp
@@ -55,6 +59,7 @@ add_llvm_library(LLVMAnalysis
   ScalarEvolutionAliasAnalysis.cpp
   ScalarEvolutionExpander.cpp
   ScalarEvolutionNormalization.cpp
+  SelectMigrationPoints.cpp
   SparsePropagation.cpp
   TargetLibraryInfo.cpp
   TargetTransformInfo.cpp
diff --git a/llvm/lib/Analysis/LiveValues.cpp b/llvm/lib/Analysis/LiveValues.cpp
new file mode 100644
index 00000000000..771aa7aefdc
--- /dev/null
+++ b/llvm/lib/Analysis/LiveValues.cpp
@@ -0,0 +1,386 @@
+#include "llvm/Analysis/LiveValues.h"
+#include "llvm/IR/Metadata.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/CFG.h"
+#include "llvm/Analysis/CFG.h"
+#include "llvm/Analysis/LoopInfo.h"
+#include "llvm/ADT/PostOrderIterator.h"
+#include "llvm/ADT/SCCIterator.h"
+#include "llvm/Support/Debug.h"
+
+#define DEBUG_TYPE "live-values"
+
+using namespace llvm;
+
+char LiveValues::ID = 0;
+INITIALIZE_PASS_BEGIN(LiveValues, "live-values", 
+                    "Live-value set calculation", true, true)
+INITIALIZE_PASS_DEPENDENCY(LoopInfoWrapperPass)
+INITIALIZE_PASS_END(LiveValues, "live-values", 
+                    "Live-value set calculation", true, true)
+
+namespace llvm {
+  FunctionPass *createLiveValuesPass() { return new LiveValues(); }
+}
+
+///////////////////////////////////////////////////////////////////////////////
+// Public API
+///////////////////////////////////////////////////////////////////////////////
+
+LiveValues::LiveValues(void)
+  : FunctionPass(ID), inlineasm(false), bitcasts(true), comparisons(true),
+    constants(false), metadata(false) {}
+
+void LiveValues::getAnalysisUsage(AnalysisUsage &AU) const
+{
+  AU.addRequired<LoopInfoWrapperPass>();
+  AU.setPreservesAll();
+}
+
+bool LiveValues::runOnFunction(Function &F)
+{
+  if(FuncBBLiveIn.count(&F))
+  {
+    DEBUG(
+      errs() << "\nFound previous analysis for " << F.getName() << "\n\n";
+      print(errs(), &F);
+    );
+  }
+  else
+  {
+    DEBUG(errs() << "\n********** Beginning LiveValues **********\n"
+                 << "********** Function: " << F.getName() << " **********\n\n"
+                    "LiveValues: performing bottom-up dataflow analysis\n");
+
+    LoopNestingForest LNF;
+    FuncBBLiveIn.emplace(&F, LiveVals());
+    FuncBBLiveOut.emplace(&F, LiveVals());
+
+    /* 1. Compute partial liveness sets using a postorder traversal. */
+    dagDFS(F, FuncBBLiveIn[&F], FuncBBLiveOut[&F]);
+
+    DEBUG(errs() << "LiveValues: constructing loop-nesting forest\n");
+
+    /* 2. Construct loop-nesting forest. */
+    constructLoopNestingForest(F, LNF);
+
+    DEBUG(errs() << "LiveValues: propagating values within loop-nests\n");
+
+    /* 3. Propagate live variables within loop bodies. */
+    loopTreeDFS(LNF, FuncBBLiveIn[&F], FuncBBLiveOut[&F]);
+
+    DEBUG(
+      print(errs(), &F);
+      errs() << "LiveValues: finished analysis\n"
+    );
+  }
+
+  return false;
+}
+
+void
+LiveValues::print(raw_ostream &O, const Function *F) const
+{
+  LiveVals::const_iterator bbIt;
+  std::set<const Value *>::const_iterator valIt;
+  const Module *M = F->getParent();
+
+  O << "LiveValues: results of live-value analysis\n";
+
+  if(!FuncBBLiveIn.count(F) || !FuncBBLiveOut.count(F))
+  {
+    if(F->hasName())
+      O << "No liveness information for function " << F->getName() << "\n";
+    else
+      O << "No liveness information for requested function\n";
+  }
+  else
+  {
+    for(bbIt = FuncBBLiveIn.at(F).cbegin();
+        bbIt != FuncBBLiveIn.at(F).cend();
+        bbIt++)
+    {
+      const BasicBlock *bb = bbIt->first;
+      const std::set<const Value *> &liveInVals = bbIt->second;
+      const std::set<const Value *> &liveOutVals = FuncBBLiveOut.at(F).at(bb);
+
+      bb->printAsOperand(O, false, M);
+      O << "\n  Live-in:\n    ";
+      for(valIt = liveInVals.cbegin(); valIt != liveInVals.cend(); valIt++)
+      {
+        (*valIt)->printAsOperand(O, false, M);
+        O << " ";
+      }
+
+      O << "\n  Live-out:\n    ";
+      for(valIt = liveOutVals.cbegin(); valIt != liveOutVals.cend(); valIt++)
+      {
+        (*valIt)->printAsOperand(O, false, M);
+        O << " ";
+      }
+
+      O << "\n";
+    }
+  }
+}
+
+std::set<const Value *> *LiveValues::getLiveIn(const BasicBlock *BB) const
+{
+  const Function *F = BB->getParent();
+  return new std::set<const Value *>(FuncBBLiveIn.at(F).at(BB));
+}
+
+std::set<const Value *> *LiveValues::getLiveOut(const BasicBlock *BB) const
+{
+  const Function *F = BB->getParent();
+  return new std::set<const Value *>(FuncBBLiveOut.at(F).at(BB));
+}
+
+std::set<const Value *>
+*LiveValues::getLiveValues(const Instruction *inst) const
+{
+  const BasicBlock *BB = inst->getParent();
+  const Function *F = BB->getParent();
+  BasicBlock::const_reverse_iterator ri, rie;
+  std::set<const Value *> *live = nullptr;
+
+  // Note: some functions have unreachable basic blocks (e.g., functions that
+  // call exit and then return a value).  If we don't have analysis for the
+  // block, return an empty set.
+  const LiveVals &Blocks = FuncBBLiveOut.at(F);
+  if(Blocks.count(BB)) live = new std::set<const Value *>(Blocks.at(BB));
+  else return new std::set<const Value *>;
+
+  for(ri = BB->rbegin(), rie = BB->rend(); ri != rie; ri++)
+  {
+    if(&*ri == inst) break;
+
+    live->erase(&*ri);
+    for(User::const_op_iterator op = ri->op_begin();
+        op != ri->op_end();
+        op++)
+      if(includeVal(*op))
+        live->insert(*op);
+  }
+
+  live->erase(&*ri);
+  for(User::const_op_iterator op = ri->op_begin();
+      op != ri->op_end();
+      op++)
+    if(includeVal(*op))
+      live->insert(*op);
+
+  return live;
+}
+
+///////////////////////////////////////////////////////////////////////////////
+// Private API
+///////////////////////////////////////////////////////////////////////////////
+
+bool LiveValues::includeVal(const llvm::Value *val) const
+{
+  bool include = true;
+
+  // TODO other values that should be filtered out?
+  if(isa<BasicBlock>(val))
+    include = false;
+  else if(isa<InlineAsm>(val) && !inlineasm)
+    include = false;
+  else if(isa<BitCastInst>(val) && !bitcasts)
+    include = false;
+  else if(isa<CmpInst>(val) && !comparisons)
+    include = false;
+  else if(isa<Constant>(val) && !constants)
+    include = false;
+  else if(isa<MetadataAsValue>(val) && !metadata)
+    include = false;
+
+  return include;
+}
+
+unsigned LiveValues::phiUses(const BasicBlock *B,
+                             const BasicBlock *S,
+                             std::set<const Value *> &uses)
+{
+  const PHINode *phi;
+  unsigned added = 0;
+
+  for(BasicBlock::const_iterator it = S->begin(); it != S->end(); it++)
+  {
+    if((phi = dyn_cast<PHINode>(&*it))) {
+      for(unsigned i = 0; i < phi->getNumIncomingValues(); i++)
+        if(phi->getIncomingBlock(i) == B &&
+           includeVal(phi->getIncomingValue(i)))
+          if(uses.insert(phi->getIncomingValue(i)).second)
+            added++;
+    }
+    else break; // phi-nodes are always at the start of the basic block
+  }
+
+  return added;
+}
+
+unsigned LiveValues::phiDefs(const BasicBlock *B,
+                             std::set<const Value *> &uses)
+{
+  const PHINode *phi;
+  unsigned added = 0;
+
+  for(BasicBlock::const_iterator it = B->begin(); it != B->end(); it++)
+  {
+    if((phi = dyn_cast<PHINode>(&*it))) {
+      if(includeVal(phi))
+        if(uses.insert(&*it).second)
+          added++;
+    }
+    else break; // phi-nodes are always at the start of the basic block
+  }
+
+  return added;
+}
+
+void LiveValues::dagDFS(Function &F, LiveVals &liveIn, LiveVals &liveOut)
+{
+  std::set<const Value *> live, phiDefined;
+  std::set<Edge> loopEdges;
+  SmallVector<Edge, 16> loopEdgeVec;
+
+  /* Find loop edges & convert to set for existence checking. */
+  FindFunctionBackedges(F, loopEdgeVec);
+  for(SmallVectorImpl<Edge>::const_iterator eit = loopEdgeVec.begin();
+      eit != loopEdgeVec.end();
+      eit++)
+    loopEdges.insert(*eit);
+
+  /* Calculate partial liveness sets for CFG nodes. */
+  for(auto B = po_iterator<const BasicBlock *>::begin(&F.getEntryBlock());
+      B != po_iterator<const BasicBlock *>::end(&F.getEntryBlock());
+      B++)
+  {
+    /* Calculate live-out set (lines 4-7 of Algorithm 2). */
+    for(succ_const_iterator S = succ_begin(*B); S != succ_end(*B); S++)
+    {
+      // Note: skip self-loop-edges, as adding values from phi-uses of this
+      // block causes use-def violations, and LLVM will complain.  This
+      // shouldn't matter, as phi-defs will cover this case.
+      if(*S == *B) continue;
+
+      phiUses(*B, *S, live);
+      if(!loopEdges.count(Edge(*B, *S)))
+      {
+        phiDefs(*S, phiDefined);
+        for(std::set<const Value *>::const_iterator vi = liveIn[*S].begin();
+            vi != liveIn[*S].end();
+            vi++)
+          if(!phiDefined.count(*vi) && includeVal(*vi)) live.insert(*vi);
+        phiDefined.clear();
+      }
+    }
+    liveOut.insert(LiveValsPair(*B, std::set<const Value *>(live)));
+
+    /* Calculate live-in set (lines 8-11 of Algorithm 2). */
+    for(BasicBlock::const_reverse_iterator inst = (*B)->rbegin();
+        inst != (*B)->rend();
+        inst++)
+    {
+      if(isa<PHINode>(&*inst)) break;
+
+      live.erase(&*inst);
+      for(User::const_op_iterator op = inst->op_begin();
+          op != inst->op_end();
+          op++)
+        if(includeVal(*op)) live.insert(*op);
+    }
+    phiDefs(*B, live);
+    liveIn.insert(LiveValsPair(*B, std::set<const Value *>(live)));
+
+    live.clear();
+
+    DEBUG(
+      errs() << "  ";
+      (*B)->printAsOperand(errs(), false);
+      errs() << ":\n";
+      errs() << "    Live-in:\n      ";
+      std::set<const Value *>::const_iterator it;
+      for(it = liveIn[*B].begin(); it != liveIn[*B].end(); it++)
+      {
+        (*it)->printAsOperand(errs(), false);
+        errs() << " ";
+      }
+      errs() << "\n    Live-out:\n      ";
+      for(it = liveOut[*B].begin(); it != liveOut[*B].end(); it++)
+      {
+        (*it)->printAsOperand(errs(), false);
+        errs() << " ";
+      }
+      errs() << "\n";
+    );
+  }
+}
+
+void LiveValues::constructLoopNestingForest(Function &F,
+                                            LoopNestingForest &LNF)
+{
+  LoopInfo &LI = getAnalysis<LoopInfoWrapperPass>().getLoopInfo();
+
+  for(scc_iterator<Function *> scc = scc_begin(&F);
+      scc != scc_end(&F);
+      ++scc)
+  {
+    const std::vector<BasicBlock *> &SCC = *scc;
+    LNF.emplace_back(SCC, LI);
+
+    DEBUG(
+      errs() << "Loop nesting tree: "
+             << LNF.back().size() << " node(s), loop-nesting depth: "
+             << LNF.back().depth() << "\n";
+      LNF.back().print(errs());
+      errs() << "\n"
+    );
+  }
+}
+
+void LiveValues::propagateValues(const LoopNestingTree &loopNest,
+                                 LiveVals &liveIn,
+                                 LiveVals &liveOut)
+{
+  std::set<const Value *> liveLoop, phiDefined;
+
+  /* Iterate over all loop nodes. */
+  for(LoopNestingTree::loop_iterator loop = loopNest.loop_begin();
+      loop != loopNest.loop_end();
+      loop++)
+  {
+    /* Calculate LiveLoop (lines 3 & 4 of Algorithm 3). */
+    phiDefs(*loop, phiDefined);
+    for(std::set<const Value *>::const_iterator it = liveIn[*loop].begin();
+        it != liveIn[*loop].end();
+        it++)
+      if(!phiDefined.count(*it) && includeVal(*it))
+        liveLoop.insert(*it);
+
+    /* Propagate values to children (lines 5-8 of Algorithm 3). */
+    for(LoopNestingTree::child_iterator child = loopNest.children_begin(loop);
+        child != loopNest.children_end(loop);
+        child++) {
+      for(std::set<const Value *>::const_iterator it = liveLoop.begin();
+          it != liveLoop.end();
+          it++) {
+        liveIn[*child].insert(*it);
+        liveOut[*child].insert(*it);
+      }
+    }
+
+    liveLoop.clear();
+  }
+}
+
+void LiveValues::loopTreeDFS(LoopNestingForest &LNF,
+                             LiveVals &liveIn,
+                             LiveVals &liveOut)
+{
+  LoopNestingForest::const_iterator it;
+  for(it = LNF.begin(); it != LNF.end(); it++)
+    propagateValues(*it, liveIn, liveOut);
+}
+
diff --git a/llvm/lib/Analysis/LoopNestingTree.cpp b/llvm/lib/Analysis/LoopNestingTree.cpp
new file mode 100644
index 00000000000..10c7582eb78
--- /dev/null
+++ b/llvm/lib/Analysis/LoopNestingTree.cpp
@@ -0,0 +1,148 @@
+#include "llvm/Analysis/LoopNestingTree.h"
+#include "llvm/Support/raw_ostream.h"
+
+using namespace llvm;
+
+///////////////////////////////////////////////////////////////////////////////
+// Public API
+///////////////////////////////////////////////////////////////////////////////
+
+LoopNestingTree::LoopNestingTree(const std::vector<BasicBlock *> &SCC,
+                                 const LoopInfo &LI)
+  : _size(1), _depth(1), _root(nullptr)
+{
+  unsigned depth = 1, nodeDepth;
+  const Loop *loop = nullptr;
+  Node *loopHeader = nullptr, *newHeader = nullptr;
+  std::list<Node *> work;
+
+  /* Bootstrap by grabbing the loop of the first basic block encountered. */
+  loop = LI[SCC.front()];
+  if(!loop) // Is the SCC actually a loop?
+  {
+    _root = new Node(SCC.front(), nullptr, true);
+    return;
+  }
+
+  /* Get header of outermost loop, the tree's root. */
+  while(loop->getLoopDepth() > 1)
+    loop = loop->getParentLoop();
+  _root = new Node(loop->getHeader(), nullptr, true);
+  work.push_back(_root);
+
+  /* Parse the loop-headers of the SCC into the tree. */
+  while(work.size())
+  {
+    loopHeader = work.front();
+    work.pop_front();
+    loop = LI[loopHeader->bb];
+    depth = LI.getLoopDepth(loopHeader->bb);
+    _depth = (depth > _depth ? depth : _depth);
+
+    /* Add children of the loop header. */
+    for(auto bbi = loop->block_begin()++; bbi != loop->block_end(); bbi++)
+    {
+      nodeDepth = LI.getLoopDepth(*bbi);
+      if(nodeDepth == depth) // Regular child node
+      {
+        loopHeader->addChild(new Node(*bbi, loopHeader, false));
+        _size++;
+      }
+      else if(nodeDepth == (depth + 1) && // Header of nested loop
+              LI.isLoopHeader(*bbi))
+      {
+        newHeader = new Node(*bbi, loopHeader, true);
+        loopHeader->addChild(newHeader);
+        work.push_back(newHeader);
+        _size++;
+      }
+    }
+  }
+}
+
+LoopNestingTree::loop_iterator LoopNestingTree::loop_iterator::operator++()
+{
+  loop_iterator me = *this;
+  if(remaining.size())
+  {
+    cur = remaining.front();
+    remaining.pop();
+    addLoopHeaders();
+  }
+  else cur = nullptr;
+  return me;
+}
+
+LoopNestingTree::loop_iterator
+LoopNestingTree::loop_iterator::operator++(int junk)
+{
+  if(remaining.size())
+  {
+    cur = remaining.front();
+    remaining.pop();
+    addLoopHeaders();
+  }
+  else cur = nullptr;
+  return *this;
+}
+
+LoopNestingTree::child_iterator::child_iterator(loop_iterator &parent,
+                                                enum location loc)
+{
+  if(loc == BEGIN) it = parent.cur->children.begin();
+  else it = parent.cur->children.end();
+}
+
+///////////////////////////////////////////////////////////////////////////////
+// Private API
+///////////////////////////////////////////////////////////////////////////////
+
+void LoopNestingTree::print(raw_ostream &O, Node *node, unsigned depth) const
+{
+  for(unsigned i = 0; i < depth; i++) O << " ";
+  node->bb->printAsOperand(O, false);
+  O << "\n";
+  if(node->children.size())
+  {
+    for(unsigned i = 0; i < depth; i++) O << " ";
+    O << "\\\n";
+
+    for(std::list<Node *>::const_iterator it = node->children.begin();
+        it != node->children.end();
+        it++)
+    {
+      if((*it)->isLoopHeader) print(O, (*it), depth + 1);
+      else
+      {
+        for(unsigned i = 0; i < depth + 1; i++) O << " ";
+        (*it)->bb->printAsOperand(O, false);
+        O << "\n";
+      }
+    }
+  }
+}
+
+void LoopNestingTree::deleteRecursive(Node *node)
+{
+  for(std::list<Node *>::iterator it = node->children.begin();
+      it != node->children.end();
+      it++)
+  {
+    if((*it)->isLoopHeader) deleteRecursive(*it);
+    else delete *it;
+  }
+  delete node;
+}
+
+void LoopNestingTree::loop_iterator::addLoopHeaders()
+{
+  if(cur != nullptr)
+  {
+    for(std::list<Node *>::const_iterator it = cur->children.begin();
+        it != cur->children.end();
+        it++)
+      if((*it)->isLoopHeader)
+        remaining.push(*it);
+  }
+}
+
diff --git a/llvm/lib/Analysis/LoopPaths.cpp b/llvm/lib/Analysis/LoopPaths.cpp
new file mode 100644
index 00000000000..d74693afafc
--- /dev/null
+++ b/llvm/lib/Analysis/LoopPaths.cpp
@@ -0,0 +1,550 @@
+//===- LoopPaths.h - Enumerate paths in loops -------------------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements analysis which enumerates paths in loops.  In
+// particular, this pass calculates all paths in loops which are of the
+// following form:
+//
+//  - Header to backedge block, with no equivalence points on the path
+//  - Header to block with equivalence point
+//  - Block with equivalence point to block with equivalence point
+//  - Block with equivalence point to backedge block
+//
+// Note that backedge blocks may or may not also be exit blocks.
+//
+//===----------------------------------------------------------------------===//
+
+#include <queue>
+#include "llvm/Analysis/LoopPaths.h"
+#include "llvm/Analysis/PopcornUtil.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "looppaths"
+
+const static cl::opt<unsigned>
+MaxNumPaths("max-num-paths", cl::Hidden, cl::init(10000),
+  cl::desc("Max number of paths to analyze"));
+
+void LoopPathUtilities::populateLoopNest(Loop *L, LoopNest &Nest) {
+  std::queue<Loop *> ToVisit;
+  Nest.clear();
+  Nest.insert(L);
+  ToVisit.push(L);
+
+  while(ToVisit.size()) {
+    const Loop *Sub = ToVisit.front();
+    ToVisit.pop();
+    for(auto L : Sub->getSubLoops()) {
+      Nest.insert(L);
+      ToVisit.push(L);
+    }
+  }
+}
+
+void LoopPathUtilities::getSubBlocks(Loop *L, BlockSet &SubBlocks) {
+  SubBlocks.clear();
+  LoopNest Nest;
+
+  for(auto Sub : L->getSubLoops()) {
+    populateLoopNest(Sub, Nest);
+    for(auto Nested : Nest)
+      for(auto BB : Nested->getBlocks())
+        SubBlocks.insert(BB);
+  }
+}
+
+LoopPath::LoopPath(const std::vector<PathNode> &NodeVector,
+                   const Instruction *Start, const Instruction *End,
+                   bool StartsAtHeader, bool EndsAtBackedge)
+  : Start(Start), End(End), StartsAtHeader(StartsAtHeader),
+    EndsAtBackedge(EndsAtBackedge) {
+  assert(NodeVector.size() && "Trivial path");
+  assert(Start && Start->getParent() == NodeVector.front().getBlock() &&
+         "Invalid starting instruction");
+  assert(End && End->getParent() == NodeVector.back().getBlock() &&
+         "Invalid ending instruction");
+
+  for(auto Node : NodeVector) Nodes.insert(Node);
+}
+
+std::string LoopPath::toString() const {
+  std::string buf = "Path with " + std::to_string(Nodes.size()) + " nodes(s)\n";
+
+  buf += "  Start: ";
+  if(Start->hasName()) buf += Start->getName();
+  else buf += "<unnamed instruction>";
+  buf += "\n";
+
+  buf += "  End: ";
+  if(End->hasName()) buf += End->getName();
+  else buf += "<unnamed instruction>";
+  buf += "\n";
+
+  buf += "  Nodes:\n";
+  for(auto Node : Nodes) {
+    buf += "    ";
+    const BasicBlock *BB = Node.getBlock();
+    if(BB->hasName()) buf += BB->getName();
+    else buf += "<unnamed block>";
+    if(Node.isSubLoopExit()) buf += " (sub-loop exit)";
+    buf += "\n";
+  }
+
+  return buf;
+}
+
+void LoopPath::print(raw_ostream &O) const {
+  O << "    Path with " << std::to_string(Nodes.size()) << " nodes(s)\n";
+  O << "    Start:"; Start->print(O); O << "\n";
+  O << "    End:"; End->print(O); O << "\n";
+  O << "    Nodes:\n";
+  for(auto Node : Nodes) {
+    const BasicBlock *BB = Node.getBlock();
+    if(BB->hasName()) O << "      " << BB->getName();
+    else O << "      <unnamed block>";
+    if(Node.isSubLoopExit()) O << " (sub-loop exit)";
+    O << "\n";
+  }
+}
+
+void EnumerateLoopPaths::getAnalysisUsage(AnalysisUsage &AU) const {
+  AU.addRequired<LoopInfoWrapperPass>();
+  AU.setPreservesAll();
+}
+
+/// Search over the instructions in a basic block (starting at I) for
+/// equivalence points.  Return an equivalence point if found, or null
+/// otherwise.
+static const Instruction *hasEquivalencePoint(const Instruction *I) {
+  if(!I) return nullptr;
+  for(BasicBlock::const_iterator it(I), e = I->getParent()->end();
+      it != e; ++it)
+    if(Popcorn::isEquivalencePoint(it)) return it;
+  return nullptr;
+}
+
+/// Add a value to a list if it's not already contained in the list.  This is
+/// used to unique new instructions at which to start searches, as multiple
+/// paths may end at the same equivalence point (but we don't need to search
+/// it multiple times).
+static inline void pushIfNotPresent(const Instruction *I,
+                                    std::list<const Instruction *> &List) {
+  if(std::find(List.begin(), List.end(), I) == List.end()) List.push_back(I);
+}
+
+/// Return whether the current path contains a basic block.
+static inline bool pathContains(const std::vector<PathNode> &Path,
+                                const BasicBlock *BB) {
+  for(auto &Node : Path)
+    if(Node.getBlock() == BB) return true;
+  return false;
+}
+
+// TODO this needs to be converted to iteration rather than recursion
+
+void
+EnumerateLoopPaths::getSubLoopSuccessors(const BasicBlock *Successor,
+                                   std::vector<const Instruction *> &EqPoint,
+                                   std::vector<const Instruction *> &Spanning) {
+  const Instruction *Term;
+  const Loop *SubLoop;
+  SmallVector<BasicBlock *, 4> ExitBlocks;
+
+  EqPoint.clear();
+  Spanning.clear();
+  assert(CurLoop->contains(Successor) && SubLoopBlocks.count(Successor) &&
+         "Invalid sub-loop block");
+
+  SubLoop = LI->getLoopFor(Successor);
+  SubLoop->getExitingBlocks(ExitBlocks);
+  for(auto Exit : ExitBlocks) {
+    Term = Exit->getTerminator();
+    if(HasSpPath[SubLoop][Exit]) Spanning.push_back(Term);
+    if(HasEqPointPath[SubLoop][Exit]) EqPoint.push_back(Term);
+  }
+}
+
+static inline void printNewPath(raw_ostream &O, const LoopPath &Path) {
+  O << "Found path that start at ";
+  if(Path.startsAtHeader()) O << "the header";
+  else O << "an equivalence point";
+  O << " and ends at ";
+  if(Path.endsAtBackedge()) O << "a loop backedge";
+  else O << "an equivalence point";
+  Path.print(O);
+}
+
+bool EnumerateLoopPaths::loopDFS(const Instruction *I,
+                                 LoopDFSInfo &DFSI,
+                                 std::vector<LoopPath> &CurPaths,
+                                 std::list<const Instruction *> &NewPaths) {
+  const Instruction *EqPoint;
+  const BasicBlock *BB = I->getParent(), *PathBlock;
+  std::vector<const Instruction *> EqPointInsts, SpanningInsts;
+
+  if(!SubLoopBlocks.count(BB)) {
+    if(pathContains(DFSI.PathNodes, BB)) {
+      DetectedCycle = true;
+      return false;
+    }
+
+    DFSI.PathNodes.emplace_back(BB, false);
+
+    if((EqPoint = hasEquivalencePoint(I))) {
+      CurPaths.emplace_back(DFSI.PathNodes, DFSI.Start, EqPoint,
+                            DFSI.StartsAtHeader, false);
+
+      if(CurPaths.size() > MaxNumPaths) {
+        TooManyPaths = true;
+        return false;
+      }
+
+      for(auto Node : DFSI.PathNodes) {
+        PathBlock = Node.getBlock();
+        if(!SubLoopBlocks.count(PathBlock))
+          HasEqPointPath[CurLoop][PathBlock] = true;
+      }
+
+      // Add instruction after equivalence point (or at start of successor
+      // basic blocks if EqPoint is the last instruction in its block) as start
+      // of new equivalence point path to be searched.
+      if(!EqPoint->isTerminator())
+        pushIfNotPresent(EqPoint->getNextNode(), NewPaths);
+      else {
+        for(auto Succ : successors(BB)) {
+          if(!CurLoop->contains(Succ) || // Skip exit blocks & latches
+             Succ == CurLoop->getHeader()) continue;
+          else if(!SubLoopBlocks.count(Succ)) // Successor is in same outer loop
+            pushIfNotPresent(&Succ->front(), NewPaths);
+          else { // Successor is in sub-loop
+            getSubLoopSuccessors(Succ, EqPointInsts, SpanningInsts);
+            for(auto SLE : EqPointInsts) pushIfNotPresent(SLE, NewPaths);
+            for(auto SLE : SpanningInsts)
+              if(!loopDFS(SLE, DFSI, CurPaths, NewPaths)) return false;
+          }
+        }
+      }
+
+      DEBUG(printNewPath(dbgs(), CurPaths.back()));
+    }
+    else if(Latches.count(BB)) {
+      CurPaths.emplace_back(DFSI.PathNodes, DFSI.Start, BB->getTerminator(),
+                            DFSI.StartsAtHeader, true);
+
+      if(CurPaths.size() > MaxNumPaths) {
+        TooManyPaths = true;
+        return false;
+      }
+
+      if(DFSI.StartsAtHeader) {
+        for(auto Node : DFSI.PathNodes) {
+          PathBlock = Node.getBlock();
+          if(!SubLoopBlocks.count(PathBlock))
+            HasSpPath[CurLoop][PathBlock] = true;
+        }
+      }
+      else {
+        for(auto Node : DFSI.PathNodes) {
+          PathBlock = Node.getBlock();
+          if(!SubLoopBlocks.count(PathBlock))
+            HasEqPointPath[CurLoop][PathBlock] = true;
+        }
+      }
+
+      DEBUG(printNewPath(dbgs(), CurPaths.back()));
+    }
+    else {
+      for(auto Succ : successors(BB)) {
+        if(!CurLoop->contains(Succ)) continue;
+        else if(!SubLoopBlocks.count(Succ)) {
+          if(!loopDFS(&Succ->front(), DFSI, CurPaths, NewPaths))
+            return false;
+        }
+        else {
+          getSubLoopSuccessors(Succ, EqPointInsts, SpanningInsts);
+          for(auto SLE : EqPointInsts) {
+            // Rather than stopping the path at the equivalence point inside
+            // of a sub-loop, stop it at the end of the current block
+            // TODO this can create duplicates for a path that reaches a
+            // sub-loop with multiple exiting blocks, but the analysis in
+            // MigrationPoints doesn't care about paths that don't end at a
+            // backedge anyway
+            CurPaths.emplace_back(DFSI.PathNodes, DFSI.Start,
+                                  BB->getTerminator(),
+                                  DFSI.StartsAtHeader, false);
+
+            if(CurPaths.size() > MaxNumPaths) {
+              TooManyPaths = true;
+              return false;
+            }
+
+            pushIfNotPresent(SLE, NewPaths);
+            DEBUG(printNewPath(dbgs(), CurPaths.back()));
+          }
+          for(auto SLE : SpanningInsts)
+            if(!loopDFS(SLE, DFSI, CurPaths, NewPaths)) return false;
+        }
+      }
+    }
+    DFSI.PathNodes.pop_back();
+  }
+  else {
+    if(pathContains(DFSI.PathNodes, BB)) {
+      DetectedCycle = true;
+      return false;
+    }
+
+    // This is a sub-loop block; we only want to explore successors who are not
+    // contained in this sub-loop but are still contained in the current loop.
+    DFSI.PathNodes.emplace_back(BB, true);
+
+    const Loop *WeedOutLoop = LI->getLoopFor(BB);
+    for(auto Succ : successors(BB)) {
+      if(WeedOutLoop->contains(Succ) || !CurLoop->contains(Succ)) continue;
+      else if(!SubLoopBlocks.count(Succ)) {
+        if(!loopDFS(&Succ->front(), DFSI, CurPaths, NewPaths))
+          return false;
+      }
+      else {
+        getSubLoopSuccessors(Succ, EqPointInsts, SpanningInsts);
+        for(auto SLE : EqPointInsts) {
+          // TODO this can create duplicates for a path that reaches a sub-loop
+          // with multiple exiting blocks, but the analysis in MigrationPoints
+          // doesn't care about paths that don't end at a backedge anyway
+          CurPaths.emplace_back(DFSI.PathNodes, DFSI.Start,
+                                BB->getTerminator(),
+                                DFSI.StartsAtHeader, false);
+
+          if(CurPaths.size() > MaxNumPaths) {
+            TooManyPaths = true;
+            return false;
+          }
+
+          DEBUG(printNewPath(dbgs(), CurPaths.back()));
+          pushIfNotPresent(SLE, NewPaths);
+        }
+        for(auto SLE: SpanningInsts)
+          if(!loopDFS(SLE, DFSI, CurPaths, NewPaths)) return false;
+      }
+    }
+    DFSI.PathNodes.pop_back();
+  }
+
+  return true;
+}
+
+bool EnumerateLoopPaths::analyzeLoop(Loop *L, std::vector<LoopPath> &CurPaths) {
+  std::list<const Instruction *> NewPaths;
+  SmallVector<BasicBlock *, 4> LatchVec;
+  LoopDFSInfo DFSI;
+
+  CurPaths.clear();
+  HasSpPath[L].clear();
+  HasEqPointPath[L].clear();
+
+  DEBUG(
+    DebugLoc DL(L->getStartLoc());
+    dbgs() << "Enumerating paths";
+    if(DL) {
+      dbgs() << " for loop at ";
+      DL.print(dbgs());
+    }
+    dbgs() << ": "; L->dump();
+  );
+
+  // Store information about the current loop, it's backedges, and sub-loops
+  CurLoop = L;
+  Latches.clear();
+  L->getLoopLatches(LatchVec);
+  for(auto L : LatchVec) Latches.insert(L);
+  LoopPathUtilities::getSubBlocks(L, SubLoopBlocks);
+
+  assert(Latches.size() && "No backedges, not a loop?");
+  assert(!SubLoopBlocks.count(L->getHeader()) && "Header is in sub-loop?");
+
+  DFSI.Start = &L->getHeader()->front();
+  DFSI.StartsAtHeader = true;
+  if(!loopDFS(DFSI.Start, DFSI, CurPaths, NewPaths)) return false;
+  assert(DFSI.PathNodes.size() == 0 && "Invalid traversal");
+
+  DFSI.StartsAtHeader = false;
+  while(!NewPaths.empty()) {
+    DFSI.Start = NewPaths.front();
+    NewPaths.pop_front();
+    if(!loopDFS(DFSI.Start, DFSI, CurPaths, NewPaths)) return false;
+    assert(DFSI.PathNodes.size() == 0 && "Invalid traversal");
+  }
+
+  return true;
+}
+
+bool EnumerateLoopPaths::runOnFunction(Function &F) {
+  DEBUG(dbgs() << "\n********** ENUMERATE LOOP PATHS **********\n"
+               << "********** Function: " << F.getName() << "\n\n");
+
+  reset();
+  TooManyPaths = DetectedCycle = false;
+  LI = &getAnalysis<LoopInfoWrapperPass>().getLoopInfo();
+  std::vector<LoopNest> Nests;
+
+  // Discover all loop nests.
+  for(LoopInfo::iterator I = LI->begin(), E = LI->end(); I != E; ++I) {
+    if((*I)->getLoopDepth() != 1) continue;
+    Nests.push_back(LoopNest());
+    LoopPathUtilities::populateLoopNest(*I, Nests.back());
+  }
+
+  // Search all loops within all loop nests.
+  for(auto Nest : Nests) {
+    DEBUG(dbgs() << "Analyzing nest with " << std::to_string(Nest.size())
+                 << " loops\n");
+
+    for(auto L : Nest) {
+      std::vector<LoopPath> &CurPaths = Paths[L];
+      assert(CurPaths.size() == 0 && "Re-processing loop?");
+      if(!analyzeLoop(L, CurPaths)) break;
+    }
+
+    if(analysisFailed()) break;
+  }
+
+  if(TooManyPaths) {
+    DEBUG(dbgs() << "WARNING: too many paths, bailing on analysis\n");
+    reset();
+  }
+
+  if(DetectedCycle) {
+    DEBUG(dbgs() << "WARNING: detected a cycle, bailing on analysis\n");
+    reset();
+  }
+
+  return false;
+}
+
+void EnumerateLoopPaths::rerunOnLoop(Loop *L) {
+  // We *should* be analyzing a loop for the 2+ time
+  std::vector<LoopPath> &CurPaths = Paths[L];
+  DEBUG(if(!CurPaths.size()) dbgs() << "  -> No previous analysis?\n");
+  if(!analyzeLoop(L, CurPaths)) reset();
+}
+
+void EnumerateLoopPaths::getPaths(const Loop *L,
+                                  std::vector<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second) P.push_back(&Path);
+}
+
+void
+EnumerateLoopPaths::getBackedgePaths(const Loop *L,
+                                     std::vector<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.endsAtBackedge()) P.push_back(&Path);
+}
+
+void
+EnumerateLoopPaths::getBackedgePaths(const Loop *L,
+                                     std::set<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.endsAtBackedge()) P.insert(&Path);
+}
+
+void
+EnumerateLoopPaths::getSpanningPaths(const Loop *L,
+                                     std::vector<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.isSpanningPath()) P.push_back(&Path);
+}
+
+void
+EnumerateLoopPaths::getSpanningPaths(const Loop *L,
+                                     std::set<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.isSpanningPath()) P.insert(&Path);
+}
+
+void
+EnumerateLoopPaths::getEqPointPaths(const Loop *L,
+                                    std::vector<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.isEqPointPath()) P.push_back(&Path);
+}
+
+void
+EnumerateLoopPaths::getEqPointPaths(const Loop *L,
+                                    std::set<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.isEqPointPath()) P.insert(&Path);
+}
+
+void
+EnumerateLoopPaths::getPathsThroughBlock(const Loop *L, BasicBlock *BB,
+                                         std::vector<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  assert(L->contains(BB) && "Loop does not contain basic block");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.contains(BB)) P.push_back(&Path);
+}
+
+void
+EnumerateLoopPaths::getPathsThroughBlock(const Loop *L, BasicBlock *BB,
+                                         std::set<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  assert(L->contains(BB) && "Loop does not contain basic block");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.contains(BB)) P.insert(&Path);
+}
+
+bool EnumerateLoopPaths::spanningPathThroughBlock(const Loop *L,
+                                                  const BasicBlock *BB) const {
+  assert(hasPaths(L) && "No paths for loop");
+  assert(L->contains(BB) && "Loop does not contain basic block");
+  return HasSpPath.find(L)->second.find(BB)->second;
+}
+
+bool EnumerateLoopPaths::eqPointPathThroughBlock(const Loop *L,
+                                                 const BasicBlock *BB) const {
+  assert(hasPaths(L) && "No paths for loop");
+  assert(L->contains(BB) && "Loop does not contain basic block");
+  return HasEqPointPath.find(L)->second.find(BB)->second;
+}
+
+char EnumerateLoopPaths::ID = 0;
+INITIALIZE_PASS_BEGIN(EnumerateLoopPaths, "looppaths",
+                      "Enumerate paths in loops",
+                      false, true)
+INITIALIZE_PASS_DEPENDENCY(LoopInfoWrapperPass)
+INITIALIZE_PASS_END(EnumerateLoopPaths, "looppaths",
+                    "Enumerate paths in loops",
+                    false, true)
+
+
+namespace llvm {
+  FunctionPass *createEnumerateLoopPathsPass()
+  { return new EnumerateLoopPaths(); }
+}
+
diff --git a/llvm/lib/Analysis/PopcornCompatibility.cpp b/llvm/lib/Analysis/PopcornCompatibility.cpp
new file mode 100644
index 00000000000..803c7cf66f3
--- /dev/null
+++ b/llvm/lib/Analysis/PopcornCompatibility.cpp
@@ -0,0 +1,142 @@
+//===- PopcornCompatibility.cpp -------------------------------------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Looks for code features which are not currently handled by the Popcorn
+// compiler/stack transformation process.  These code features either *might*
+// cause issues during stack transformation (and hence the compiler will issue
+// a warning), or are guaranteed to not be handled correctly and will cause
+// compilation to abort.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Pass.h"
+#include "llvm/IR/CallSite.h"
+#include "llvm/IR/DiagnosticInfo.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/LLVMContext.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "popcorn-compat"
+
+namespace {
+
+class PopcornCompatibility : public FunctionPass
+{
+public:
+  static char ID;
+
+  PopcornCompatibility() : FunctionPass(ID) {
+    initializePopcornCompatibilityPass(*PassRegistry::getPassRegistry());
+  }
+  ~PopcornCompatibility() {}
+
+  virtual const char *getPassName() const
+  { return "Popcorn compatibility checking"; }
+
+  //===--------------------------------------------------------------------===//
+  // Warning & error printing
+  //===--------------------------------------------------------------------===//
+
+  /// Emit a warning message for a given location, denoted by an instruction.
+  static void warn(const Instruction *I, const std::string &Msg) {
+    const Function *F = I->getParent()->getParent();
+    std::string Warning("Popcorn compatibility");
+    if(F->hasName()) {
+      Warning += " in function '";
+      Warning += F->getName();
+      Warning += "'";
+    }
+    Warning += ": " + Msg;
+    DiagnosticInfoOptimizationFailure DI(*F, I->getDebugLoc(), Warning);
+    I->getContext().diagnose(DI);
+  }
+
+  /// Emit a warning message for a function.
+  static void warn(const Function &F, const std::string &Msg)
+  { warn(F.getEntryBlock().begin(), Msg); }
+
+  /// Emit an error message for a given location, denoted by an instruction.
+  static void error(const Instruction *I, const std::string &Msg) {
+    const Function *F = I->getParent()->getParent();
+    std::string Error("Popcorn compatibility");
+    if(F->hasName()) {
+      Error += " in function '";
+      Error += F->getName();
+      Error += "'";
+    }
+    Error += ": " + Msg;
+    DiagnosticInfoOptimizationError DI(*F, I->getDebugLoc(), Error);
+    I->getContext().diagnose(DI);
+  }
+
+  //===--------------------------------------------------------------------===//
+  // Properties of instructions
+  //===--------------------------------------------------------------------===//
+
+  /// Return whether the alloca is dynamically-sized.
+  static bool isVariableSizedAlloca(const Instruction &I) {
+    const AllocaInst *AI;
+    if((AI = dyn_cast<AllocaInst>(&I)) && !AI->isStaticAlloca()) return true;
+    else return false;
+  }
+
+  static bool isInlineAsm(const Instruction &I) {
+    if((isa<CallInst>(I) || isa<InvokeInst>(I)) && !isa<IntrinsicInst>(I)) {
+      ImmutableCallSite CS(&I);
+      if(CS.isInlineAsm()) return true;
+    }
+    return false;
+  }
+
+  //===--------------------------------------------------------------------===//
+  // The main show
+  //===--------------------------------------------------------------------===//
+
+  virtual bool runOnFunction(Function &F) {
+    std::string Msg;
+
+    if(!F.isDeclaration() && !F.isIntrinsic()) {
+      if(F.isVarArg()) warn(F, "function takes a variable number of arguments");
+      for(auto &BB : F) {
+        for(auto &I : BB) {
+          if(isVariableSizedAlloca(I)) {
+            Msg = "stack variable '";
+            Msg += I.getName();
+            Msg += "' is dynamically sized (will cause "
+                   "issues during code generation)";
+            error(&I, Msg);
+          }
+
+          if(isInlineAsm(I))
+            warn(&I, "inline assembly may have unanalyzable side-effects");
+
+          if(isa<VAArgInst>(I) || isa<VACopyInst>(I) || isa<VAEndInst>(I))
+            warn(&I, "va_arg not transformable across architectures");
+        }
+      }
+    }
+    return false;
+  }
+
+private:
+};
+
+} /* end anonymous namespace */
+
+char PopcornCompatibility::ID = 0;
+
+INITIALIZE_PASS(PopcornCompatibility, "popcorn-compat",
+                "Analyze code for compatibility issues", false, true)
+
+namespace llvm {
+  FunctionPass *createPopcornCompatibilityPass()
+  { return new PopcornCompatibility(); }
+}
+
diff --git a/llvm/lib/Analysis/SelectMigrationPoints.cpp b/llvm/lib/Analysis/SelectMigrationPoints.cpp
new file mode 100644
index 00000000000..cd8a116b0e5
--- /dev/null
+++ b/llvm/lib/Analysis/SelectMigrationPoints.cpp
@@ -0,0 +1,1837 @@
+//===- SelectMigrationPoints.cpp ------------------------------------------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Select code locations to instrument with migration points, which are
+// locations where threads make calls to invoke the migration process in
+// addition to any other instrumentation (e.g., hardware transactional memory,
+// HTM, stops & starts).  Migration points only occur at equivalence points, or
+// locations in the program code where there is a direct mapping between
+// architecture-specific execution state, like registers and stack, across
+// different ISAs.  In our implementation, every function call site is an
+// equivalence point; hence, calls inserted to invoke the migration by
+// definition create equivalence points at the migration point.  Thus, all
+// migration points are equivalence points, but not all equivalence points are
+// migration points.
+//
+// By default, the pass only inserts migration points at the beginning and end
+// of a function.  More advanced analyses can be used to instrument function
+// bodies (in particular, loops) with more migration points and HTM execution.
+//
+// More details about equivalence points can be found in the paper "A Unified
+// Model of Pointwise Migration of Procedural Computations" by von Bank et. al
+// (http://dl.acm.org/citation.cfm?id=197402).
+//
+//===----------------------------------------------------------------------===//
+
+#include <cmath>
+#include <map>
+#include <memory>
+#include "llvm/Pass.h"
+#include "llvm/ADT/PostOrderIterator.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/StringSet.h"
+#include "llvm/ADT/SCCIterator.h"
+#include "llvm/Analysis/LoopInfo.h"
+#include "llvm/Analysis/LoopIterator.h"
+#include "llvm/Analysis/LoopPaths.h"
+#include "llvm/Analysis/PopcornUtil.h"
+#include "llvm/Analysis/ScalarEvolution.h"
+#include "llvm/Analysis/ScalarEvolutionExpressions.h"
+#include "llvm/IR/CallSite.h"
+#include "llvm/IR/DiagnosticInfo.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/Intrinsics.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/Module.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_os_ostream.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "migration-points"
+
+/// Insert more migration points into the body of a function.  Analyze
+/// execution behavior & attempt to instrument the code to reduce the time
+/// until the thread reaches a migration point.
+const static cl::opt<bool>
+MoreMigPoints("more-mig-points", cl::Hidden, cl::init(false),
+  cl::desc("Add additional migration points into the body of functions"));
+
+/// By default we assume that loops will execute "enough iterations as to
+/// require instrumentation".  That's not necessarily true, so contrain N in
+/// hitting migration point every N iterations.  If analysis determines that
+/// we need to hit analysis for some number larger than N, don't instrument
+/// the loop.
+const static cl::opt<unsigned>
+MaxItersPerMigPoint("max-iters-per-migpoint", cl::Hidden, cl::init(UINT32_MAX),
+  cl::desc("Max iterations per migration point"));
+
+/// Percent of capacity (determined by analysis type, e.g., HTM buffer size) at
+/// which point weight objects will request a new migration point be inserted.
+const static cl::opt<unsigned>
+CapacityThreshold("cap-threshold", cl::Hidden, cl::init(80),
+  cl::desc("Percent of capacity at which point a new migration point should "
+           "be inserted (only applies to -more-mig-points)"));
+
+/// Per-function capacity threshold.
+const static cl::list<std::string>
+FuncCapThreshold("func-cap", cl::Hidden, cl::ZeroOrMore,
+  cl::desc("Function-specific capacity threshold in function,value pairs"));
+
+/// Normally we instrument function entry points with migration points.  If
+/// we're below some percent of capacity at all exit points & we haven't added
+/// instrumentation into the body (i.e., nothing depends on a clean slate to
+/// start), skip this instrumentation.
+const static cl::opt<unsigned>
+StartThreshold("start-threshold", cl::Hidden, cl::init(5),
+  cl::desc("Don't instrument function entry points under a percent of "
+           "capacity (only applies to -more-mig-points), used for "
+           "small functions"));
+
+/// Per-function starting threshold.
+const static cl::list<std::string>
+FuncStartThreshold("func-start", cl::Hidden, cl::ZeroOrMore,
+  cl::desc("Function-specific start threshold in function,value pairs"));
+
+/// Normally we instrument function exit points with migration points.  If
+/// we're below some percent of capacity, skip this instrumentation (useful for
+/// very small/short-lived functions).
+const static cl::opt<unsigned>
+RetThreshold("ret-threshold", cl::Hidden, cl::init(5),
+  cl::desc("Don't instrument function exit points under a percent of "
+           "capacity (only applies to -more-mig-points)"));
+
+/// Per-function return threshold.
+const static cl::list<std::string>
+FuncRetThreshold("func-ret", cl::Hidden, cl::ZeroOrMore,
+  cl::desc("Function-specific return threshold in function,value pairs"));
+
+/// Don't instrument a specific function with extra migration points.
+const static cl::list<std::string>
+FuncNoInst("func-no-inst", cl::Hidden, cl::ZeroOrMore,
+  cl::desc("Don't instrument a particular function with migration points"));
+
+/// Target cycles between migration points when instrumenting applications with
+/// more migration points (but without HTM).  Allows tuning trade off between
+/// migration point response time and overhead.
+const static cl::opt<unsigned>
+MillionCyclesBetweenMigPoints("migpoint-cycles", cl::Hidden, cl::init(50),
+  cl::desc("Cycles between migration points, in millions of cycles"));
+
+/// Cover the application in transactional execution by inserting HTM
+/// stop/start instructions at migration points.  Tailors the analysis to
+/// reduce capacity aborts by estimating memory access behavior.
+const static cl::opt<bool>
+HTMExec("htm-execution", cl::NotHidden, cl::init(false),
+  cl::desc("Instrument migration points with HTM execution "
+           "(only supported on PowerPC 64-bit & x86-64)"));
+
+/// Disable wrapping mem<set, copy, move> instructions for which we don't know
+/// the size.
+const static cl::opt<bool>
+NoWrapUnknownMem("htm-no-wrap-unknown-mem", cl::Hidden, cl::init(false),
+  cl::desc("Disable wrapping mem<set, copy, move> of unknown size with HTM"));
+
+/// Disable wrapping libc functions which are likely to cause HTM aborts with
+/// HTM stop/start intrinsics.  Wrapping happens by default with HTM execution.
+const static cl::opt<bool>
+NoWrapLibc("htm-no-wrap-libc", cl::Hidden, cl::init(false),
+  cl::desc("Disable wrapping libc functions with HTM stop/start"));
+
+/// HTM memory read buffer size for tuning analysis when inserting additional
+/// migration points.
+const static cl::opt<unsigned>
+HTMReadBufSizeArg("htm-buf-read", cl::Hidden, cl::init(32),
+  cl::desc("HTM analysis tuning - HTM read buffer size, in kilobytes"),
+  cl::value_desc("size"));
+
+/// HTM memory write buffer size for tuning analysis when inserting additional
+/// migration points.
+const static cl::opt<unsigned>
+HTMWriteBufSizeArg("htm-buf-write", cl::Hidden, cl::init(8),
+  cl::desc("HTM analysis tuning - HTM write buffer size, in kilobytes"),
+  cl::value_desc("size"));
+
+#define KB 1024
+#define HTMReadBufSize (HTMReadBufSizeArg * KB)
+#define HTMWriteBufSize (HTMWriteBufSizeArg * KB)
+
+#define MILLION 1000000
+#define CyclesBetweenMigPoints \
+  ((unsigned long)MillionCyclesBetweenMigPoints * MILLION)
+#define MEM_WEIGHT 40
+
+STATISTIC(LoopsTransformed, "Number of loops transformed");
+STATISTIC(NumIVsAdded, "Number of induction variables added");
+
+namespace {
+
+/// Get the integer size of a value, if statically known.
+static int64_t getValueSize(const Value *V) {
+  if(isa<ConstantInt>(V)) return cast<ConstantInt>(V)->getSExtValue();
+  return -1;
+}
+
+/// Return a percentage of a value.
+static inline size_t getValuePercent(size_t V, unsigned P) {
+  assert(P <= 100 && "Invalid percentage");
+  return floor(((double)V) * (((double)P) / 100.0));
+}
+
+/// Return the number of cache lines accessed for a given number of
+/// (assumed contiguous) bytes.
+static inline size_t getNumCacheLines(size_t Bytes, unsigned LineSize) {
+  return ceil((double)Bytes / (double)LineSize);
+}
+
+/// Abstract weight metric.  Child classes implement for analyzing different
+/// resource capacities, e.g., HTM buffer sizes.
+class Weight {
+protected:
+  /// Number of times the weight was reset.
+  size_t Resets;
+
+  Weight() : Resets(0) {}
+  Weight(const Weight &C) : Resets(C.Resets) {}
+
+public:
+  virtual ~Weight() {};
+  virtual Weight *copy() const = 0;
+
+  /// Expose types of child implementations.
+  virtual bool isCycleWeight() const { return false; }
+  virtual bool isHTMWeight() const { return false; }
+
+  /// Analyze an instruction & update accounting.
+  virtual void analyze(const Instruction *I, const DataLayout *DL) = 0;
+
+  /// Return whether or not we should add a migration point.  This is tuned
+  /// based on the resource capacity and percentage threshold options.
+  virtual bool shouldAddMigPoint(unsigned percent) const {
+    return !underPercentOfThreshold(percent);
+  }
+
+  /// Reset the weight.
+  virtual void reset() { Resets++; }
+
+  /// Update this weight with the max of this weight and another.
+  virtual void max(const Weight *RHS) = 0;
+  virtual void max(const std::unique_ptr<Weight> &RHS) { max(RHS.get()); }
+
+  /// Multiply the weight by a factor, e.g., a number of loop iterations.
+  virtual void multiply(size_t factor) = 0;
+
+  /// Add another weight to this weight.
+  virtual void add(const Weight *RHS) = 0;
+  virtual void add(const std::unique_ptr<Weight> &RHS) { add(RHS.get()); }
+
+  /// Number of times this weight "fits" into the resource capacity before we
+  /// need to place a migration point.  This is used for calculating how many
+  /// iterations of a loop can be executed between migration points.
+  virtual size_t numIters(unsigned percent) const = 0;
+
+  /// Return whether or not the weight is within some percent (0-100) of the
+  /// resource capacity for a type of weight.
+  virtual bool underPercentOfThreshold(unsigned percent) const = 0;
+
+  /// Return a human-readable string describing weight information.
+  virtual std::string toString() const = 0;
+};
+
+typedef std::unique_ptr<Weight> WeightPtr;
+
+/// Weight metrics for HTM analysis, which basically depend on the number
+/// of bytes loaded & stored.
+class HTMWeight : public Weight {
+private:
+  // The number of bytes loaded & stored, respectively.
+  size_t LoadBytes, StoreBytes;
+
+  // Statistics about when the weight was reset (i.e., at HTM stop/starts).
+  size_t ResetLoad, ResetStore;
+
+public:
+  HTMWeight(size_t LoadBytes = 0, size_t StoreBytes = 0)
+    : LoadBytes(LoadBytes), StoreBytes(StoreBytes), ResetLoad(0),
+      ResetStore(0) {}
+  HTMWeight(const HTMWeight &C)
+    : Weight(C), LoadBytes(C.LoadBytes), StoreBytes(C.StoreBytes),
+      ResetLoad(C.ResetLoad), ResetStore(C.ResetStore) {}
+  virtual Weight *copy() const { return new HTMWeight(*this); }
+
+  virtual bool isHTMWeight() const { return true; }
+
+  /// Analyze an instruction for memory operations.
+  virtual void analyze(const Instruction *I, const DataLayout *DL) {
+    Type *Ty;
+
+    // TODO do extractelement, insertelement, shufflevector, extractvalue, or
+    // insertvalue read/write memory?
+    // TODO Need to handle the following instructions/instrinsics (also see
+    // Instruction::mayLoad() / Instruction::mayStore()):
+    //   llvm.masked.load
+    //   llvm.masked.store
+    //   llvm.masked.gather
+    //   llvm.masked.store
+    switch(I->getOpcode()) {
+    default: break;
+    case Instruction::Load: {
+      const LoadInst *LI = cast<LoadInst>(I);
+      Ty = LI->getPointerOperand()->getType()->getPointerElementType();
+      LoadBytes += DL->getTypeStoreSize(Ty);
+      break;
+    }
+
+    case Instruction::Store: {
+      const StoreInst *SI = cast<StoreInst>(I);
+      Ty = SI->getValueOperand()->getType();
+      StoreBytes += DL->getTypeStoreSize(Ty);
+      break;
+    }
+
+    case Instruction::AtomicCmpXchg: {
+      const AtomicCmpXchgInst *Cmp = cast<AtomicCmpXchgInst>(I);
+      Ty = Cmp->getPointerOperand()->getType()->getPointerElementType();
+      LoadBytes += DL->getTypeStoreSize(Ty);
+      StoreBytes += DL->getTypeStoreSize(Ty);
+    }
+
+    case Instruction::AtomicRMW: {
+      const AtomicRMWInst *RMW = cast<AtomicRMWInst>(I);
+      Ty = RMW->getPointerOperand()->getType()->getPointerElementType();
+      LoadBytes += DL->getTypeStoreSize(Ty);
+      StoreBytes += DL->getTypeStoreSize(Ty);
+    }
+
+    case Instruction::Call: {
+      const IntrinsicInst *II = dyn_cast<IntrinsicInst>(I);
+      bool Loads = false, Stores = false;
+      int64_t Size = 0;
+
+      if(!II) break;
+      switch(II->getIntrinsicID()) {
+      default: break;
+      case Intrinsic::memcpy:
+      case Intrinsic::memmove:
+        // Arguments: i8* dest, i8* src, i<x> len, i32 align, i1 isvolatile
+        Loads = Stores = true;
+        Size = getValueSize(II->getArgOperand(2));
+        break;
+      case Intrinsic::memset:
+        // Arguments: i8* dest, i8 val, i<x> len, i32 align, i1 isvolatile
+        Stores = true;
+        Size = getValueSize(II->getArgOperand(2));
+        break;
+      }
+
+      // Size > 0: we know the size statically
+      // Size < 0: we can't determine the size statically
+      // Size == 0: some intrinsic we don't care about
+      if(Size > 0) {
+        if(Loads) LoadBytes += Size;
+        if(Stores) StoreBytes += Size;
+      }
+      else if(Size < 0) {
+        // Assume we're doing heavy reading & writing -- may need to revise if
+        // transaction begin/ends are too expensive.
+        if(Loads) LoadBytes += HTMReadBufSize;
+        if(Stores) StoreBytes += HTMWriteBufSize;
+      }
+
+      break;
+    }
+    }
+  }
+
+  virtual void reset() {
+    Weight::reset();
+    ResetLoad += LoadBytes;
+    ResetStore += StoreBytes;
+    LoadBytes = StoreBytes = 0;
+  }
+
+  /// The max value for HTM weights is the max of the two weights' LoadBytes
+  /// and StoreBytes (maintained separately).
+  virtual void max(const Weight *RHS) {
+    assert(RHS->isHTMWeight() && "Cannot mix weight types");
+    const HTMWeight *W = (const HTMWeight *)RHS;
+    if(W->LoadBytes > LoadBytes) LoadBytes = W->LoadBytes;
+    if(W->StoreBytes > StoreBytes) StoreBytes = W->StoreBytes;
+  }
+
+  virtual void multiply(size_t factor) {
+    LoadBytes *= factor;
+    StoreBytes *= factor;
+  }
+
+  virtual void add(const Weight *RHS) {
+    assert(RHS->isHTMWeight() && "Cannot mix weight types");
+    const HTMWeight *W = (const HTMWeight *)RHS;
+    LoadBytes += W->LoadBytes;
+    StoreBytes += W->StoreBytes;
+  }
+
+  /// The number of times this weight's load & stores could be executed without
+  /// overflowing the capacity threshold of the HTM buffers.
+  virtual size_t numIters(unsigned percent) const {
+    size_t NumLoadIters = UINT64_MAX, NumStoreIters = UINT64_MAX,
+      FPHtmReadSize = getValuePercent(HTMReadBufSize, percent),
+      FPHtmWriteSize = getValuePercent(HTMWriteBufSize, percent);
+
+    if(!LoadBytes && !StoreBytes) return 1024; // Return a safe value
+    else {
+      if(LoadBytes) NumLoadIters = FPHtmReadSize / LoadBytes;
+      if(StoreBytes) NumStoreIters = FPHtmWriteSize / StoreBytes;
+
+      if(!NumLoadIters && !NumStoreIters) return 1;
+      else return NumLoadIters < NumStoreIters ? NumLoadIters : NumStoreIters;
+    }
+  }
+
+  virtual bool underPercentOfThreshold(unsigned percent) const {
+    if(LoadBytes <= getValuePercent(HTMReadBufSize, percent) &&
+       StoreBytes <= getValuePercent(HTMWriteBufSize, percent))
+      return true;
+    else return false;
+  }
+
+  virtual std::string toString() const {
+    return std::to_string(LoadBytes) + " byte(s) loaded, " +
+           std::to_string(StoreBytes) + " byte(s) stored";
+  }
+};
+
+/// Weight metric for temporally-spaced migration points.
+class CycleWeight : public Weight {
+private:
+  // An estimate of the number of cycles since the last migration point.
+  size_t Cycles;
+
+  // Statistics about when the weight was reset (i.e., at migration points).
+  size_t ResetCycles;
+
+public:
+  CycleWeight(size_t Cycles = 0) : Cycles(Cycles), ResetCycles(0) {}
+  CycleWeight(const CycleWeight &C)
+    : Weight(C), Cycles(C.Cycles), ResetCycles(C.ResetCycles) {}
+  virtual CycleWeight *copy() const { return new CycleWeight(*this); }
+
+  virtual bool isCycleWeight() const { return true; }
+
+  virtual void analyze(const Instruction *I, const DataLayout *DL) {
+    Type *Ty;
+
+    // Cycles are estimated using Agner Fog's instruction latency guide at
+    // http://www.agner.org/optimize/instruction_tables.pdf for "Broadwell".
+    switch(I->getOpcode()) {
+    default: break;
+    // Terminator instructions
+    // TODO Ret, Invoke, Resume
+    case Instruction::Br: Cycles += 2; break;
+    case Instruction::Switch: Cycles += 2; break;
+    case Instruction::IndirectBr: Cycles += 2; break;
+
+    // Binary instructions
+    case Instruction::Add: Cycles++; break;
+    case Instruction::FAdd: Cycles += 3; break;
+    case Instruction::Sub: Cycles++; break;
+    case Instruction::FSub: Cycles += 3; break;
+    case Instruction::Mul: Cycles += 2; break;
+    case Instruction::FMul: Cycles += 3; break;
+    case Instruction::UDiv: Cycles += 73; break;
+    case Instruction::SDiv: Cycles += 81; break;
+    case Instruction::FDiv: Cycles += 14; break;
+    case Instruction::URem: Cycles += 73; break;
+    case Instruction::SRem: Cycles += 81; break;
+    case Instruction::FRem: Cycles += 14; break;
+
+    // Logical operators
+    case Instruction::Shl: Cycles += 2; break;
+    case Instruction::LShr: Cycles += 2; break;
+    case Instruction::AShr: Cycles += 2; break;
+    case Instruction::And: Cycles += 1; break;
+    case Instruction::Or: Cycles += 1; break;
+    case Instruction::Xor: Cycles += 1; break;
+
+    // Memory instructions
+    case Instruction::Load: {
+      const LoadInst *LI = cast<LoadInst>(I);
+      Ty = LI->getPointerOperand()->getType()->getPointerElementType();
+      Cycles += getNumCacheLines(DL->getTypeStoreSize(Ty), 64) * MEM_WEIGHT;
+      break;
+    }
+    case Instruction::Store: {
+      const StoreInst *SI = cast<StoreInst>(I);
+      Ty = SI->getValueOperand()->getType();
+      Cycles += getNumCacheLines(DL->getTypeStoreSize(Ty), 64) * MEM_WEIGHT;
+      break;
+    }
+    case Instruction::GetElementPtr: Cycles++; break;
+    case Instruction::Fence: Cycles += 33; break;
+    case Instruction::AtomicCmpXchg: Cycles += 21; break;
+    case Instruction::AtomicRMW: Cycles += 21; break;
+
+    // Cast instructions
+    case Instruction::Trunc: Cycles++; break;
+    case Instruction::ZExt: Cycles++; break;
+    case Instruction::SExt: Cycles++; break;
+    case Instruction::FPToUI: Cycles += 4; break;
+    case Instruction::FPToSI: Cycles += 4; break;
+    case Instruction::UIToFP: Cycles += 5; break;
+    case Instruction::SIToFP: Cycles += 5; break;
+    case Instruction::FPTrunc: Cycles += 4; break;
+    case Instruction::FPExt: Cycles += 2; break;
+
+    // Other instructions
+    // TODO VAArg, ExtractElement, InsertElement, ShuffleVector, ExtractValue,
+    // InsertValue, LandingPad
+    case Instruction::ICmp: Cycles++; break;
+    case Instruction::FCmp: Cycles += 3; break;
+    case Instruction::Call: {
+      const IntrinsicInst *II = dyn_cast<IntrinsicInst>(I);
+      int64_t Size = 0;
+
+      if(!II) Cycles += 3;
+      else {
+        switch(II->getIntrinsicID()) {
+        default: break;
+        case Intrinsic::memcpy:
+        case Intrinsic::memmove:
+        case Intrinsic::memset:
+          // Arguments: i8* dest, i8* src, i<x> len, i32 align, i1 isvolatile
+          Size = getValueSize(II->getArgOperand(2));
+          break;
+        }
+
+        if(Size > 0) Cycles += getNumCacheLines(Size, 64) * MEM_WEIGHT;
+      }
+      break;
+    }
+    case Instruction::Select: Cycles += 3; break;
+    }
+  }
+
+  virtual void reset() {
+    Weight::reset();
+    ResetCycles += Cycles;
+    Cycles = 0;
+  }
+
+  virtual void max(const Weight *RHS) {
+    assert(RHS->isCycleWeight() && "Cannot mix weight types");
+    const CycleWeight *W = (const CycleWeight *)RHS;
+    if(W->Cycles > Cycles) Cycles = W->Cycles;
+  }
+
+  virtual void multiply(size_t factor) { Cycles *= factor; }
+  virtual void add(const Weight *RHS) {
+    assert(RHS->isCycleWeight() && "Cannot mix weight types");
+    const CycleWeight *W = (const CycleWeight *)RHS;
+    Cycles += W->Cycles;
+  }
+
+  virtual size_t numIters(unsigned percent) const {
+    if(!Cycles) return 1048576; // Return a safe value
+    else {
+      size_t FPCycleCap = getValuePercent(CyclesBetweenMigPoints, percent);
+      size_t Iters = FPCycleCap / Cycles;
+      return Iters ? Iters : 1;
+    }
+  }
+
+  virtual bool underPercentOfThreshold(unsigned percent) const {
+    if(Cycles <= getValuePercent(CyclesBetweenMigPoints, percent)) return true;
+    else return false;
+  }
+
+  virtual std::string toString() const {
+    return std::to_string(Cycles) + " cycles";
+  }
+};
+
+/// Get a weight object with zero-initialized weight based on the type of
+/// analysis being used to instrument the application.
+///
+/// Note: returns a dynamically allocated object to be managed by the caller
+static Weight *getZeroWeight() {
+  if(HTMExec) return new HTMWeight();
+  else return new CycleWeight();
+}
+
+/// SelectMigrationPoints - select locations at which to insert migration
+/// points into functions.
+class SelectMigrationPoints : public FunctionPass
+{
+public:
+  static char ID;
+
+  SelectMigrationPoints() : FunctionPass(ID) {
+    initializeSelectMigrationPointsPass(*PassRegistry::getPassRegistry());
+  }
+  ~SelectMigrationPoints() {}
+
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const {
+    AU.addRequired<LoopInfoWrapperPass>();
+    AU.addRequired<EnumerateLoopPaths>();
+    AU.addRequired<ScalarEvolution>();
+  }
+
+  virtual const char *getPassName() const
+  { return "Select migration point locations"; }
+
+  static inline unsigned splitFuncValPair(const std::string &Pair,
+                                          std::string &Func) {
+    unsigned Val;
+    size_t Comma = Pair.rfind(',');
+    Func = Pair.substr(0, Comma);
+    Val = stoul(Pair.substr(Comma + 1));
+    assert(Val <= 100 && "Invalid percentage");
+    return Val;
+  }
+
+  /// Parse per-function threshold values from the command line.
+  void parsePerFuncThresholds() {
+    unsigned Val;
+    std::string Name;
+
+    FuncCapList.clear();
+    FuncStartList.clear();
+    FuncRetList.clear();
+    NoInstFuncs.clear();
+
+    for(auto Pair : FuncCapThreshold) {
+      Val = splitFuncValPair(Pair, Name);
+      FuncCapList[Name] = Val;
+    }
+    for(auto Pair : FuncStartThreshold) {
+      Val = splitFuncValPair(Pair, Name);
+      FuncStartList[Name] = Val;
+    }
+    for(auto Pair : FuncRetThreshold) {
+      Val = splitFuncValPair(Pair, Name);
+      FuncRetList[Name] = Val;
+    }
+    for(auto Func : FuncNoInst) NoInstFuncs.insert(Func);
+  }
+
+  virtual bool doInitialization(Module &M) {
+    DL = &M.getDataLayout();
+    addPopcornFnAttributes(M);
+    if(MoreMigPoints) parsePerFuncThresholds();
+    if(HTMExec) Popcorn::setInstrumentationType(M, Popcorn::HTM);
+    else Popcorn::setInstrumentationType(M, Popcorn::Cycles);
+    return false;
+  }
+
+  /// Select where to insert migration points into functions.
+  virtual bool runOnFunction(Function &F)
+  {
+    DEBUG(dbgs() << "\n********** SELECT MIGRATION POINTS **********\n"
+                 << "********** Function: " << F.getName() << "\n\n");
+
+    if(F.hasFnAttribute("popcorn-noinstr") ||
+       NoInstFuncs.find(F.getName()) != NoInstFuncs.end()) return false;
+
+    initializeAnalysis(F);
+
+    // Some operations (e.g., big memory copies, I/O) will cause aborts.
+    // Instrument these operations to stop & resume transactions afterwards.
+    if(HTMExec) {
+      bool AddedMigPoint = wrapWithHTM(F, isBigMemoryOp,
+        "memory operations that overflow HTM buffers");
+      if(!NoWrapLibc)
+        AddedMigPoint |= wrapWithHTM(F, isLibcIO, "I/O functions");
+      if(AddedMigPoint) LP->runOnFunction(F);
+    }
+
+    if(MoreMigPoints && !LP->analysisFailed()) {
+      StringRef FuncName = F.getName();
+      StringMap<unsigned>::const_iterator It;
+      if((It = FuncCapList.find(FuncName)) != FuncCapList.end())
+        CurCapThresh = It->second;
+      else CurCapThresh = CapacityThreshold;
+      if((It = FuncStartList.find(FuncName)) != FuncStartList.end())
+        CurStartThresh = It->second;
+      else CurStartThresh = StartThreshold;
+      if((It = FuncRetList.find(FuncName)) != FuncRetList.end())
+        CurRetThresh = It->second;
+      else CurRetThresh = RetThreshold;
+
+      DEBUG(
+        dbgs() << "\n-> Analyzing function body to add migration points <-\n"
+               << "\nCapacity threshold: " << std::to_string(CurCapThresh)
+               << "\nStart threshold: " << std::to_string(CurStartThresh)
+               << "\nReturn threshold: " << std::to_string(CurRetThresh)
+               << "\nMaximum iterations/migration point: "
+               << std::to_string(MaxItersPerMigPoint);
+
+        if(HTMExec)
+          dbgs() << "\nAnalyzing for HTM Instrumentation"
+                 << "\n  HTM read buffer size: "
+                 << std::to_string(HTMReadBufSizeArg) << "kb"
+                 << "\n  HTM write buffer size: "
+                 << std::to_string(HTMWriteBufSizeArg) << "kb\n";
+        else
+          dbgs() << "\nAnalyzing for migration call out instrumentation"
+                 << "\n  Target millions of cycles between migration points: "
+                 << std::to_string(MillionCyclesBetweenMigPoints) << "\n";
+      );
+
+      // We by default mark the function start as a migration point, but if we
+      // don't add any instrumentation & the function's exit weights are
+      // sufficiently small avoid instrumentation altogether.
+      bool MarkStart = false;
+      if(!analyzeFunctionBody(F)) {
+        for(Function::iterator BB = F.begin(), E = F.end(); BB != E; BB++)
+          if(isa<ReturnInst>(BB->getTerminator()) &&
+             !BBWeights[BB].BlockWeight->underPercentOfThreshold(CurStartThresh))
+            MarkStart = true;
+      }
+      else MarkStart = true;
+
+      if(MarkStart) {
+        DEBUG(dbgs() << "-> Marking function entry as a migration point <-\n");
+        markAsMigPoint(F.getEntryBlock().getFirstInsertionPt(), true, true);
+      }
+      else { DEBUG(dbgs() << "-> Eliding instrumenting function entry <-\n"); }
+    }
+    else {
+      if(MoreMigPoints) {
+        std::string Msg = "too many paths to instrument function with more "
+                          "migration points -- falling back to instrumenting "
+                          "function entry/exit";
+        DiagnosticInfoOptimizationFailure DI(F, nullptr, Msg);
+        F.getContext().diagnose(DI);
+      }
+
+      DEBUG(dbgs() << "-> Marking function entry as a migration point <-\n");
+      markAsMigPoint(F.getEntryBlock().getFirstInsertionPt(), true, true);
+
+      // Instrument function exit point(s)
+      DEBUG(dbgs() << "-> Marking function exit(s) as a migration point <-\n");
+      for(Function::iterator BB = F.begin(), E = F.end(); BB != E; BB++)
+        if(isa<ReturnInst>(BB->getTerminator()))
+          markAsMigPoint(BB->getTerminator(), true, true);
+    }
+
+    // Finally, apply transformations to loops headers according to analysis.
+    transformLoopHeaders(F);
+
+    return true;
+  }
+
+  /// Reset all analysis.
+  void initializeAnalysis(const Function &F) {
+    SE = &getAnalysis<ScalarEvolution>();
+    LI = &getAnalysis<LoopInfoWrapperPass>().getLoopInfo();
+    LP = &getAnalysis<EnumerateLoopPaths>();
+    BBWeights.clear();
+    LoopWeights.clear();
+    TransformLoops.clear();
+    MigPointInsts.clear();
+    HTMBeginInsts.clear();
+    HTMEndInsts.clear();
+  }
+
+private:
+  //===--------------------------------------------------------------------===//
+  // Types & fields
+  //===--------------------------------------------------------------------===//
+
+  /// Configuration for the function currently being analyzed.
+  unsigned CurCapThresh;
+  unsigned CurStartThresh;
+  unsigned CurRetThresh;
+
+  /// The current architecture - used to access architecture-specific HTM calls
+  const DataLayout *DL;
+
+  /// Parsed per-function thresholds.
+  StringMap<unsigned> FuncCapList;
+  StringMap<unsigned> FuncStartList;
+  StringMap<unsigned> FuncRetList;
+  StringSet<> NoInstFuncs;
+
+  /// Analyses on which we depend
+  ScalarEvolution *SE;
+  LoopInfo *LI;
+  EnumerateLoopPaths *LP;
+
+  /// libc functions which are likely to cause an HTM abort through a syscall
+  const static StringSet<> LibcIO;
+
+  /// Weight information for basic blocks.
+  class BasicBlockWeightInfo {
+  public:
+    /// Weight of the basic block at the end of its execution.  If the block has
+    /// a migration point, the weight *only* captures the instructions following
+    /// the migration point (migration points "reset" the weight).
+    WeightPtr BlockWeight;
+
+    BasicBlockWeightInfo() : BlockWeight(nullptr) {}
+    BasicBlockWeightInfo(const Weight *BlockWeight)
+      : BlockWeight(BlockWeight->copy()) {}
+    BasicBlockWeightInfo(const WeightPtr &BlockWeight)
+      : BlockWeight(BlockWeight->copy()) {}
+
+    std::string toString() const {
+      if(BlockWeight) return BlockWeight->toString();
+      else return "<uninitialized basic block weight info>";
+    }
+  };
+
+  /// Weight information for loops.  Maintains weights at loop exit points as
+  /// well as path-specific weight information for the loop & exit blocks.
+  class LoopWeightInfo {
+  private:
+    /// The weight of the loop upon entry.  Zero in the default case, but may
+    /// be set if analysis elides instrumentation in and around the loop.
+    WeightPtr EntryWeight;
+
+    /// The maximum weight when exiting the loop at each of its exit blocks.
+    /// Automatically recalculated when any of its ingredients are changed.
+    DenseMap<const BasicBlock *, WeightPtr> ExitWeights;
+
+    /// Whether the loop has either of the two types of paths, and if so the
+    /// maximum weight of each type.  Note that the spanning path weight is
+    /// *not* scaled by the number of iterations, ItersPerMigPoint.
+    bool LoopHasSpanningPath, LoopHasEqPointPath;
+    WeightPtr LoopSpanningPathWeight, LoopEqPointPathWeight;
+
+    /// Number of iterations between migration points if the loop has one or
+    /// more spanning paths, or zero otherwise.
+    size_t ItersPerMigPoint;
+
+    /// Whether there are either of the two types of paths through each exit
+    /// block, and if so the maximum weight of each type.
+    DenseMap<const BasicBlock *, bool> ExitHasSpanningPath, ExitHasEqPointPath;
+    DenseMap<const BasicBlock *, WeightPtr> ExitSpanningPathWeights,
+                                            ExitEqPointPathWeights;
+
+    /// Calculate the exit block's maximum weight, which is the max of both the
+    /// spanning path exit weight and equivalence point path exit weight.
+    void computeExitWeight(const BasicBlock *BB) {
+      // Note: these operations are in a specific order -- change with care!
+
+      // Calculate the loop weight up until the current iteration
+      WeightPtr BBWeight(getZeroWeight());
+      if(LoopHasSpanningPath) BBWeight->max(getLoopSpanningPathWeight(true));
+      if(LoopHasEqPointPath) BBWeight->max(LoopEqPointPathWeight);
+
+      // Calculate the maximum possible value of the current iteration:
+      //   - Spanning path: loop weight + current path weight
+      //   - Equivalence point path: current weight path
+      if(ExitHasSpanningPath[BB]) BBWeight->add(ExitSpanningPathWeights[BB]);
+      if(ExitHasEqPointPath[BB]) BBWeight->max(ExitEqPointPathWeights[BB]);
+
+      ExitWeights[BB] = std::move(BBWeight);
+    }
+
+    void computeAllExitWeights() {
+      for(auto I = ExitWeights.begin(), E = ExitWeights.end(); I != E; I++)
+        computeExitWeight(I->first);
+    }
+
+  public:
+    LoopWeightInfo() = delete;
+    LoopWeightInfo(const Loop *L)
+      : EntryWeight(getZeroWeight()), LoopHasSpanningPath(false),
+        LoopHasEqPointPath(false), ItersPerMigPoint(0) {
+      SmallVector<BasicBlock *, 4> ExitBlocks;
+      L->getExitingBlocks(ExitBlocks);
+      for(auto Block : ExitBlocks) {
+        ExitHasSpanningPath[Block] = false;
+        ExitHasEqPointPath[Block] = false;
+      }
+    }
+
+    /// Set the weight upon entering the loop & recompute all exit weights.
+    void setEntryWeight(const WeightPtr &W) {
+      EntryWeight.reset(W->copy());
+      computeAllExitWeights();
+    }
+
+    /// Get the number of iterations between migration points, or zero if there
+    /// are no spanning paths through the loop.
+    size_t getItersPerMigPoint() const {
+      return ItersPerMigPoint;
+    }
+
+    /// Get the loop's spanning path weight, scaled based on the number of
+    /// iterations.  Also includes loop entry weight if requested.
+    WeightPtr getLoopSpanningPathWeight(bool AddEntry) const {
+      assert(LoopHasSpanningPath && "No spanning path weight for loop");
+      WeightPtr Ret(LoopSpanningPathWeight->copy());
+      Ret->multiply(ItersPerMigPoint - 1);
+      if(AddEntry) Ret->add(EntryWeight);
+      return Ret;
+    }
+
+    /// Set the loop's spanning path weight & recompute all exit weights.
+    ///  - W: the maximum weight of a single spanning path iteration
+    ///  - I: the number of iterations per migration point
+    void setLoopSpanningPathWeight(const WeightPtr &W, size_t I) {
+      LoopHasSpanningPath = true;
+      LoopSpanningPathWeight.reset(W->copy());
+      ItersPerMigPoint = I;
+      computeAllExitWeights();
+    }
+
+    /// Get the loop's equivalence point path weight.
+    WeightPtr getLoopEqPointPathWeight() const
+    {
+      assert(LoopHasEqPointPath && "No equivalence point path weight for loop");
+      return WeightPtr(LoopEqPointPathWeight->copy());
+    }
+
+    /// Set the loop's equivalence point path weight & recompute all exit
+    /// weights.
+    void setLoopEqPointPathWeight(const WeightPtr &W) {
+      LoopHasEqPointPath = true;
+      LoopEqPointPathWeight.reset(W->copy());
+      computeAllExitWeights();
+    }
+
+    /// Get an exit block's spanning path weight.  This is the raw weight for
+    /// a single iteration of paths through this exiting block, it does *not*
+    /// incorporate loop weights.
+    WeightPtr getExitSpanningPathWeight(const BasicBlock *BB) const
+    {
+      assert(ExitHasSpanningPath.find(BB)->second &&
+             "No spanning path weight for exit block");
+      return WeightPtr(ExitSpanningPathWeights.find(BB)->second->copy());
+    }
+
+    /// Set the exit block's spanning path weight & recompute the exit block's
+    /// overall maximum weight.
+    void setExitSpanningPathWeight(const BasicBlock *BB, const WeightPtr &W)
+    {
+      ExitHasSpanningPath[BB] = true;
+      ExitSpanningPathWeights[BB].reset(W->copy());
+      computeExitWeight(BB);
+    }
+
+    /// Get an exit block's equivalence point path weight.  This is the raw
+    /// weight for a single iteration of paths through this exiting block, it
+    /// does *not* incorporate loop weights.
+    WeightPtr getExitEqPointPathWeight(const BasicBlock *BB) const
+    {
+      assert(ExitHasEqPointPath.find(BB)->second &&
+             "No equivalence point path weight for exit block");
+      return WeightPtr(ExitEqPointPathWeights.find(BB)->second->copy());
+    }
+
+    /// Set the equivalence point path exit block weight & recompute the exit
+    /// block's overall maximum weight.
+    void setExitEqPointPathWeight(const BasicBlock *BB, const WeightPtr &W)
+    {
+      ExitHasEqPointPath[BB] = true;
+      ExitEqPointPathWeights[BB].reset(W->copy());
+      computeExitWeight(BB);
+    }
+
+    /// Return whether the loop/exit block has spanning and equivalence point
+    /// paths through it.
+    bool loopHasSpanningPath() const { return LoopHasSpanningPath; }
+    bool loopHasEqPointPath() const { return LoopHasEqPointPath; }
+    bool exitHasSpanningPath(const BasicBlock *BB) const
+    { return ExitHasSpanningPath.find(BB)->second; }
+    bool exitHasEqPointPath(const BasicBlock *BB) const
+    { return ExitHasEqPointPath.find(BB)->second; }
+
+    /// Return the weight of a given exiting basic block.
+    const WeightPtr &getExitWeight(const BasicBlock *BB) const {
+      assert(ExitWeights.count(BB) && "Invalid exit basic block");
+      return ExitWeights.find(BB)->second;
+    }
+
+    const WeightPtr &operator[](const BasicBlock *BB) const
+    { return getExitWeight(BB); }
+
+    std::string toString() const {
+      if(!ExitWeights.size()) return "<uninitialized loop weight info>";
+      else {
+        std::string buf = "Exit block weights:\n";
+        for(auto It = ExitWeights.begin(), E = ExitWeights.end();
+            It != E; ++It) {
+          buf += "    ";
+          if(It->first->hasName()) {
+            buf += It->first->getName();
+            buf += ": ";
+          }
+          buf += It->second->toString() + "\n";
+        }
+        return buf;
+      }
+    }
+  };
+
+  /// Weight information gathered by analyses for basic blocks & loops
+  typedef std::map<const BasicBlock *, BasicBlockWeightInfo> BlockWeightMap;
+  typedef std::map<const Loop *, LoopWeightInfo> LoopWeightMap;
+  BlockWeightMap BBWeights;
+  LoopWeightMap LoopWeights;
+
+  /// Code locations marked for instrumentation.
+  SmallPtrSet<Loop *, 16> TransformLoops;
+  SmallPtrSet<Instruction *, 32> MigPointInsts;
+  SmallPtrSet<Instruction *, 32> HTMBeginInsts;
+  SmallPtrSet<Instruction *, 32> HTMEndInsts;
+
+  //===--------------------------------------------------------------------===//
+  // Analysis implementation
+  //===--------------------------------------------------------------------===//
+
+  /// Add Popcorn-related function attributes where appropriate.
+  void addPopcornFnAttributes(Module &M) const {
+    auto GlobalAnnos = M.getNamedGlobal("llvm.global.annotations");
+    if(GlobalAnnos) {
+      auto a = cast<ConstantArray>(GlobalAnnos->getOperand(0));
+      for(unsigned int i = 0; i < a->getNumOperands(); i++) {
+        auto e = cast<ConstantStruct>(a->getOperand(i));
+        if(auto fn = dyn_cast<Function>(e->getOperand(0)->getOperand(0))) {
+          auto Anno = cast<ConstantDataArray>(
+                        cast<GlobalVariable>(
+                          e->getOperand(1)->getOperand(0)
+                        )->getOperand(0)
+                      )->getAsCString();
+          fn->addFnAttr(Anno);
+        }
+      }
+    }
+  }
+
+  /// Return whether the instruction requires HTM begin instrumentation.
+  bool shouldAddHTMBegin(Instruction *I) const {
+    if(Popcorn::isHTMBeginPoint(I)) return true;
+    else return HTMBeginInsts.count(I);
+  }
+
+  /// Return whether the instruction requires HTM end instrumentation.
+  bool shouldAddHTMEnd(Instruction *I) const {
+    if(Popcorn::isHTMEndPoint(I)) return true;
+    else return HTMEndInsts.count(I);
+  }
+
+  /// Return whether the instruction is a migration point.  We assume that all
+  /// called functions have migration points internally.
+  bool isMigrationPoint(Instruction *I) const {
+    if(Popcorn::isEquivalencePoint(I)) return true;
+    else return MigPointInsts.count(I);
+  }
+
+  /// Return whether the instruction is marked for any instrumentation.
+  bool isMarkedForInstrumentation(Instruction *I) const {
+    return isMigrationPoint(I) || shouldAddHTMBegin(I) || shouldAddHTMEnd(I);
+  }
+
+  /// Mark an instruction to be instrumented with an HTM begin, directly before
+  /// the instruction
+  bool markAsHTMBegin(Instruction *I) {
+    if(!HTMExec) return false;
+    DEBUG(dbgs() << "  + Marking"; I->print(dbgs());
+          dbgs() << " as HTM begin\n");
+    HTMBeginInsts.insert(I);
+    Popcorn::addHTMBeginMetadata(I);
+    return true;
+  }
+
+  /// Mark an instruction to be instrumented with an HTM end, directly before
+  /// the instruction
+  bool markAsHTMEnd(Instruction *I) {
+    if(!HTMExec) return false;
+    DEBUG(dbgs() << "  + Marking"; I->print(dbgs());
+          dbgs() << " as HTM end\n");
+    HTMEndInsts.insert(I);
+    Popcorn::addHTMEndMetadata(I);
+    return true;
+  }
+
+  /// Mark an instruction to be instrumented with a migration point, directly
+  /// before the instruction.  Optionally mark instruction as needing HTM
+  /// start/stop intrinsics.
+  bool markAsMigPoint(Instruction *I, bool AddHTMBegin, bool AddHTMEnd) {
+    // Don't clobber any existing instrumentation
+    if(isMarkedForInstrumentation(I)) return false;
+    DEBUG(dbgs() << "  + Marking"; I->print(dbgs());
+          dbgs() << " as a migration point\n");
+    MigPointInsts.insert(I);
+    Popcorn::addEquivalencePointMetadata(I);
+    if(AddHTMBegin) markAsHTMBegin(I);
+    if(AddHTMEnd) markAsHTMEnd(I);
+    return true;
+  }
+
+  /// Instruction matching function type.
+  typedef bool (*InstMatch)(const Instruction *, unsigned Thresh);
+
+  /// Return whether the instruction is a memory operation that will overflow
+  /// HTM buffers.
+  static bool isBigMemoryOp(const Instruction *I, unsigned Thresh) {
+    if(!I || !isa<IntrinsicInst>(I)) return false;
+    const IntrinsicInst *II = cast<IntrinsicInst>(I);
+    int64_t Size = 0;
+    switch(II->getIntrinsicID()) {
+    default: return false;
+    case Intrinsic::memcpy: case Intrinsic::memmove: case Intrinsic::memset:
+      // Arguments: i8* dest, i8* src, i<x> len, i32 align, i1 isvolatile
+      Size = getValueSize(II->getArgOperand(2));
+      break;
+    }
+
+    if(Size >= 0) { // We know the size
+      size_t USize = (size_t)Size;
+      return USize >= getValuePercent(HTMReadBufSize, Thresh) ||
+             USize >= getValuePercent(HTMWriteBufSize, Thresh);
+    }
+    else return !NoWrapUnknownMem;
+  }
+
+  /// Return whether the instruction is a libc I/O call.
+  static bool isLibcIO(const Instruction *I, unsigned Thresh) {
+    if(!I || !Popcorn::isCallSite(I)) return false;
+    const ImmutableCallSite CS(I);
+    const Function *CalledFunc = CS.getCalledFunction();
+    if(CalledFunc && CalledFunc->hasName())
+      return LibcIO.find(CalledFunc->getName()) != LibcIO.end();
+    return false;
+  }
+
+  /// Search for & wrap operations that match a certain criteria.
+  bool wrapWithHTM(Function &F, InstMatch Matcher, const char *Desc) {
+    bool AddedMigPoint = false;
+
+    DEBUG(dbgs() << "\n-> Wrapping " << Desc << " with HTM stop/start <-\n");
+    for(Function::iterator BB = F.begin(), BE = F.end(); BB != BE; BB++) {
+      if(LI->getLoopFor(BB)) continue; // Don't do this in loops!
+      for(BasicBlock::iterator I = BB->begin(), E = BB->end(); I != E; I++) {
+        if(Matcher(I, CurCapThresh)) {
+          markAsHTMEnd(I);
+
+          // Search subsequent instructions for other libc calls to prevent
+          // pathological transaction stop/starts.
+          const static size_t searchSpan = 10;
+          BasicBlock::iterator NextI(I->getNextNode());
+          for(size_t rem = searchSpan; rem > 0 && NextI != E; rem--, NextI++) {
+            if(Matcher(NextI, CurCapThresh)) {
+              DEBUG(dbgs() << "  - Found another match:"; NextI->dump());
+              I = NextI;
+              rem = searchSpan;
+            }
+          }
+
+          // TODO analyze successor blocks as well
+
+          AddedMigPoint |= markAsMigPoint(I->getNextNode(), true, false);
+        }
+      }
+    }
+
+    return AddedMigPoint;
+  }
+
+  /// Get the starting weight for a basic block based on the max weights of its
+  /// predecessors.
+  ///
+  /// Note: returns a dynamically allocated object to be managed by the caller
+  Weight *getInitialWeight(const BasicBlock *BB) const {
+    Weight *PredWeight = getZeroWeight();
+    const Loop *L = LI->getLoopFor(BB);
+    bool BBIsHeader = L && (BB == L->getHeader());
+    unsigned LDepth = L ? L->getLoopDepth() : 0;
+
+    for(auto Pred : predecessors(BB)) {
+      const Loop *PredLoop = LI->getLoopFor(Pred);
+
+      // We *only* gather header initial weights when analyzing whether to
+      // instrument loop entry, which doesn't depend on latches.
+      if(BBIsHeader && PredLoop == L) continue;
+
+      // Determine if the predecessor is an exit block from another loop:
+      //
+      //   1. The predecessor is in a loop
+      //   2. The predecessor's loop is not BB's loop
+      //   3. The nesting depth of the predecessor's loop is >= BB's loop*
+      //
+      // If it's an exit block, use the loop weight info to get the exit
+      // weight.  Otherwise, use the basic block weight info.
+      //
+      // *Note: if the predecessor's nesting depth is < BB's, then BB is in a
+      // child loop inside the predecessor's loop, and the predecessor is NOT a
+      // loop exiting block.
+      if(PredLoop && PredLoop != L && PredLoop->getLoopDepth() >= LDepth) {
+        assert(LoopWeights.count(PredLoop) &&
+               "Invalid reverse post-order traversal");
+        PredWeight->max(LoopWeights.at(PredLoop)[Pred]);
+      }
+      else {
+        assert(BBWeights.count(Pred) && "Invalid reverse post-order traversal");
+        PredWeight->max(BBWeights.at(Pred).BlockWeight);
+      }
+    }
+
+    return PredWeight;
+  }
+
+  /// Analyze a single basic block with an initial starting weight and update
+  /// it with the block's ending weight.  Return whether or not a migration
+  /// point was added.
+  bool traverseBlock(BasicBlock *BB, Weight *CurWeight) {
+    bool AddedMigPoint = false;
+
+    DEBUG(
+      dbgs() << "      Analyzing basic block";
+      if(BB->hasName()) dbgs() << " '" << BB->getName() << "'";
+      dbgs() << "\n";
+    );
+
+    // TODO this doesn't respect spans identified by wrapWithHTM()!
+
+    for(BasicBlock::iterator I = BB->begin(), E = BB->end(); I != E; I++) {
+      if(isa<PHINode>(I)) continue;
+
+      // Check if there is or there should be a migration point before the
+      // instruction, and if so, reset the weight.  Note: markAsMigPoint()
+      // internally avoids tampering with existing instrumentation.
+      if(isMigrationPoint(I)) CurWeight->reset();
+      else if(CurWeight->shouldAddMigPoint(CurCapThresh)) {
+        AddedMigPoint |= markAsMigPoint(I, true, true);
+        CurWeight->reset();
+      }
+
+      CurWeight->analyze(I, DL);
+    }
+
+    DEBUG(dbgs() << "       - Weight: " << CurWeight->toString() << "\n");
+
+    return AddedMigPoint;
+  }
+
+  bool traverseBlock(BasicBlock *BB, WeightPtr &Initial)
+  { return traverseBlock(BB, Initial.get()); }
+
+  /// Mark loop predecessors, i.e., all branches into the loop header, as
+  /// migration points.  Return whether or not a migration point was added.
+  bool markLoopPredecessors(const Loop *L) {
+    bool AddedMigPoint = false;
+    BasicBlock *Header = L->getHeader();
+    for(auto Pred : predecessors(Header)) {
+      // Weed out latches
+      if(!L->contains(Pred)) {
+        // Avoid adding migration points in bodies of predecessor loops when
+        // exiting from one loop directly into the header of another, e.g.,
+        //
+        //   for.body:  ;Body of first loop
+        //     ...
+        //     br i1 %cmp, for.body, for.body.2
+        //
+        //   for.body.2: ;Body of second, completely independent loop
+        //     ...
+        const Loop *PredL = LI->getLoopFor(Pred);
+        if(PredL == nullptr || (PredL->getLoopDepth() < L->getLoopDepth()))
+          AddedMigPoint |= markAsMigPoint(Pred->getTerminator(), true, true);
+      }
+    }
+    return AddedMigPoint;
+  }
+
+  /// Analyze & mark loop entry with migration points.  Avoid instrumenting if
+  /// we can execute the entire loop & any entry code without overflowing our
+  /// resource capacity.
+  bool traverseLoopEntry(Loop *L) {
+    // We don't need to instrument around the loop if we're instrumenting the
+    // header, as we'll hit a migration point at the beginning of the loop.
+    if(TransformLoops.count(L)) return false;
+
+    assert(LoopWeights.count(L) && "Invalid reverse post-order traversal");
+    LoopWeightInfo &LWI = LoopWeights.at(L);
+
+    // If the loop only has equivalence point paths, assume that we'll hit an
+    // equivalence point before we abort -- may need to revise if there are too
+    // many capacity aborts.
+    if(!LWI.loopHasSpanningPath()) {
+      DEBUG(dbgs() << "       - Loop only has equivalence point paths, "
+                      "can elide instrumenting loop entry points\n");
+      return false;
+    }
+
+    // TODO what if it's an irreducible loop, i.e., > 1 header?
+    BasicBlock *Header = L->getHeader();
+    WeightPtr HeaderWeight(getInitialWeight(Header));
+
+    DEBUG(dbgs() << "       + Analyzing loop entry points to "
+                 << Header->getName() << ", header weight: "
+                 << HeaderWeight->toString() << "\n");
+
+    // See if any of the exit spanning path weights are too heavy to include
+    // the entry point weight (entry point weights don't affect equivalence
+    // point paths).
+    bool InstrumentLoopEntry = false;
+    SmallVector<BasicBlock *, 4> ExitBlocks;
+    L->getExitingBlocks(ExitBlocks);
+    for(auto Exit : ExitBlocks) {
+      if(LWI.exitHasSpanningPath(Exit)) {
+        WeightPtr SpExitWeight(LWI.getLoopSpanningPathWeight(false));
+        SpExitWeight->add(LWI.getExitSpanningPathWeight(Exit));
+        SpExitWeight->add(HeaderWeight);
+        if(SpExitWeight->shouldAddMigPoint(CurCapThresh))
+          InstrumentLoopEntry = true;
+      }
+    }
+
+    if(InstrumentLoopEntry) {
+      DEBUG(dbgs() << "       - One or more spanning path(s) were too heavy, "
+                      "instrumenting loop entry points\n");
+      return markLoopPredecessors(L);
+    }
+    else {
+      DEBUG(dbgs() << "       + Can elide instrumenting loop entry points\n");
+      LWI.setEntryWeight(HeaderWeight);
+      return false;
+    }
+  }
+
+  /// Traverse a loop and instrument with migration points on paths that are
+  /// too "heavy".  Return whether or not a migration point was added.
+  bool traverseLoop(Loop *L) {
+    bool AddedMigPoint = false;
+    LoopBlocksDFS DFS(L); DFS.perform(LI);
+    LoopBlocksDFS::RPOIterator Block = DFS.beginRPO(), E = DFS.endRPO();
+    SmallPtrSet<const Loop *, 4> MarkedLoops;
+    Loop *BlockLoop;
+
+    assert(Block != E && "Loop with no basic blocks");
+
+    DEBUG(
+      dbgs() << "  + Analyzing "; L->dump();
+      dbgs() << "    - At "; L->getStartLoc().dump();
+    );
+
+    // TODO what if it's an irreducible loop, i.e., > 1 header?
+    BasicBlock *CurBB = *Block;
+    WeightPtr HdrWeight(getZeroWeight());
+    AddedMigPoint |= traverseBlock(CurBB, HdrWeight);
+    BBWeights[CurBB] = std::move(HdrWeight);
+
+    for(++Block; Block != E; ++Block) {
+      CurBB = *Block;
+      BlockLoop = LI->getLoopFor(CurBB);
+      if(BlockLoop == L) { // Block is in same loop & nesting depth
+        WeightPtr PredWeight(getInitialWeight(CurBB));
+        AddedMigPoint |= traverseBlock(CurBB, PredWeight);
+        BBWeights[CurBB] = std::move(PredWeight);
+      }
+      else if(!MarkedLoops.count(BlockLoop)) {
+        // Block is in a sub-loop, analyze & mark sub-loop's entry.  Only
+        // analyze direct sub-loops, as deeper-nested (2+) loops will have
+        // already been analyzed by their parents.
+        if(BlockLoop->getLoopDepth() - L->getLoopDepth() == 1)
+          AddedMigPoint |= traverseLoopEntry(BlockLoop);
+        MarkedLoops.insert(BlockLoop);
+      }
+    }
+
+    DEBUG(dbgs() << "    Finished analyzing loop\n");
+
+    return AddedMigPoint;
+  }
+
+  /// Analyze a path in a loop up until a particular end instruction and return
+  /// its weight.  Doesn't do any marking.
+  ///
+  /// Note: returns a dynamically allocated object to be managed by the caller
+  Weight *traversePathInternal(const LoopPath *LP,
+                               const Instruction *PathEnd,
+                               bool &ActuallyEqPoint) const {
+    assert(LP->cbegin() != LP->cend() && "Trivial loop path, no blocks");
+    assert(LP->contains(PathEnd->getParent()) && "Invalid end instruction");
+    ActuallyEqPoint = false;
+
+    Loop *SubLoop;
+    Weight *PathWeight = getZeroWeight();
+    SetVector<PathNode>::const_iterator Node = LP->cbegin(),
+                                        EndNode = LP->cend();
+    const BasicBlock *NodeBlock = Node->getBlock(),
+                     *EndBlock = PathEnd->getParent();
+    BasicBlock::const_iterator Inst, EndInst, PathEndInst(PathEnd);
+
+    if(Node->isSubLoopExit()) {
+      // Since the sub-loop exit block is the start of the path, it's by
+      // definition exiting from an equivalence point path.
+      SubLoop = LI->getLoopFor(NodeBlock);
+      assert(LoopWeights.count(SubLoop) && "Invalid traversal");
+      const LoopWeightInfo &LWI = LoopWeights.at(SubLoop);
+      PathWeight->add(LWI.getExitEqPointPathWeight(NodeBlock));
+    }
+    else {
+      for(Inst = LP->startInst(), EndInst = NodeBlock->end();
+          Inst != EndInst && Inst != PathEndInst; Inst++)
+        PathWeight->analyze(Inst, DL);
+    }
+
+    if(NodeBlock == EndBlock) {
+      PathWeight->analyze(PathEndInst, DL);
+      return PathWeight;
+    }
+
+    for(Node++; Node != EndNode; Node++) {
+      NodeBlock = Node->getBlock();
+      if(Node->isSubLoopExit()) {
+        // Since the sub-loop exit block is in the middle of the path, it's by
+        // definition exiting from a spanning path.  EnumerateLoopPaths doesn't
+        // know about loops we've marked for transformation, however, so reset
+        // the path weight for loops that'll have a migration point added to
+        // their header.
+        SubLoop = LI->getLoopFor(NodeBlock);
+        assert(LoopWeights.count(SubLoop) && "Invalid traversal");
+        const LoopWeightInfo &LWI = LoopWeights.at(SubLoop);
+        if(TransformLoops.count(SubLoop)) {
+          ActuallyEqPoint = true;
+          PathWeight->reset();
+        }
+
+        // TODO we need to ultimately deal with the following situation more
+        // gracefully:
+        //
+        //   loop 1: all spanning paths, contains loop 2
+        //     loop 2: all spanning paths, contains loop 3
+        //       loop 3: all spanning paths, to be instrumented
+        //
+        // Analysis determines loop 3 needs to be instrumented.  If all paths
+        // in loop 2 go through loop 3, then loop 2 no longer has spanning
+        // paths but only equivalence point paths.  The previous if statement
+        // detects this, and reports it to calculateLoopExitWeights().  However
+        // when analyzing paths through loop 1, we can't detect that loop 2
+        // only has equivalence points paths.
+
+        if(LWI.loopHasSpanningPath()) {
+          PathWeight->add(LWI.getLoopSpanningPathWeight(false));
+          PathWeight->add(LWI.getExitSpanningPathWeight(NodeBlock));
+        }
+        else {
+          ActuallyEqPoint = true;
+          PathWeight->reset();
+          PathWeight->add(LWI[NodeBlock]);
+        }
+      }
+      else {
+        for(Inst = NodeBlock->begin(), EndInst = NodeBlock->end();
+            Inst != EndInst && Inst != PathEndInst; Inst++)
+          PathWeight->analyze(Inst, DL);
+      }
+
+      if(NodeBlock == EndBlock) break;
+    }
+    PathWeight->analyze(PathEndInst, DL);
+
+    return PathWeight;
+  }
+
+  /// Analyze a path in a loop and return its weight.  Doesn't do any marking.
+  ///
+  /// Note: returns a dynamically allocated object to be managed by the caller
+  Weight *traversePath(const LoopPath *LP, bool &ActuallyEqPoint) const {
+    DEBUG(dbgs() << "  + Analyzing loop path: "; LP->dump());
+    return traversePathInternal(LP, LP->endInst(), ActuallyEqPoint);
+  }
+
+  /// Analyze a path until a given exit block & return path's weight up until
+  /// the exit point.
+  ///
+  /// Note: returns a dynamically allocated object to be managed by the caller
+  Weight *traversePathUntilExit(const LoopPath *LP,
+                                BasicBlock *Exit,
+                                bool &ActuallyEqPoint) const
+  { return traversePathInternal(LP, Exit->getTerminator(), ActuallyEqPoint); }
+
+  /// Get the loop trip count if available and less than UINT32_MAX, or 0
+  /// otherwise.
+  ///
+  /// Note: ported from ScalarEvolution::getSmallConstantMaxTripCount() in
+  /// later LLVM releases.
+  unsigned getTripCount(const Loop *L) const {
+    const SCEVConstant *MaxExitCount =
+      dyn_cast<SCEVConstant>(SE->getMaxBackedgeTakenCount(L));
+    if(!MaxExitCount) return 0;
+    ConstantInt *ExitConst = MaxExitCount->getValue();
+    if(ExitConst->getValue().getActiveBits() > 32) return 0;
+    else return ((unsigned)ExitConst->getZExtValue()) + 1;
+  }
+
+  /// Calculate the exit weights of a loop at all exit points.
+  void calculateLoopExitWeights(Loop *L) {
+    assert(!LoopWeights.count(L) && "Previously analyzed loop?");
+
+    bool HasSpPath = false, HasEqPointPath = false, ActuallyEqPoint;
+    std::vector<const LoopPath *> Paths;
+    LoopWeights.emplace(L, LoopWeightInfo(L));
+    LoopWeightInfo &LWI = LoopWeights.at(L);
+    SmallVector<BasicBlock *, 4> ExitBlocks;
+    WeightPtr SpanningWeight(getZeroWeight()),
+              EqPointWeight(getZeroWeight());
+    LP->getBackedgePaths(L, Paths);
+
+    DEBUG(dbgs() << "\n    Calculating loop path weights: "
+                 << std::to_string(Paths.size()) << " backedge path(s)\n");
+
+    // Analyze weights of individual paths through the loop that end at a
+    // backedge, as these will dictate the loop's weight.
+    for(auto Path : Paths) {
+      WeightPtr PathWeight(traversePath(Path, ActuallyEqPoint));
+      DEBUG(dbgs() << "    Path weight: " << PathWeight->toString() << " ");
+      if(Path->isSpanningPath() && !ActuallyEqPoint) {
+        HasSpPath = true;
+        SpanningWeight->max(PathWeight);
+        DEBUG(dbgs() << "(spanning path)\n");
+      }
+      else {
+        HasEqPointPath = true;
+        EqPointWeight->max(PathWeight);
+        DEBUG(dbgs() << "(equivalence point path)\n");
+      }
+    }
+
+    // Calculate/store the loop's spanning and equivalence point path weights.
+    if(HasSpPath) {
+      // Optimization: if the loop trip count is smaller than the number of
+      // iterations between migration points, elide loop instrumentation.
+      size_t NumIters = SpanningWeight->numIters(CurCapThresh);
+      unsigned TripCount = getTripCount(L);
+      assert(NumIters > 0 && "Should have added a migration point");
+      if(TripCount && TripCount < NumIters) {
+        DEBUG(dbgs() << "  Eliding loop instrumentation, loop trip count: "
+                     << std::to_string(TripCount) << "\n");
+        NumIters = TripCount;
+      }
+      else if(L->getLoopDepth() > 1 &&
+              NumIters > (size_t)MaxItersPerMigPoint) {
+        DEBUG(dbgs() << "  Eliding loop instrumentation (exceeded maximum "
+                        " iterations per migration point), loop trip count: "
+                     << std::to_string(MaxItersPerMigPoint) << "\n");
+        NumIters = MaxItersPerMigPoint;
+      }
+      // TODO mark first insertion point in loop header as migration point,
+      // propagate whether we added a migration point as return value
+      else TransformLoops.insert(L);
+      LWI.setLoopSpanningPathWeight(SpanningWeight, NumIters);
+
+      DEBUG(
+        dbgs() << "  Loop spanning path weight: " << SpanningWeight->toString()
+               << ", " << std::to_string(NumIters)
+               << " iteration(s)/migration point\n";
+      );
+    }
+    if(HasEqPointPath) {
+      LWI.setLoopEqPointPathWeight(EqPointWeight);
+
+      DEBUG(dbgs() << "  Loop equivalence point path weight: "
+                   << EqPointWeight->toString() << "\n");
+    }
+
+    DEBUG(dbgs() << "\n    Calculating loop exit weights");
+
+    // Calculate the weight of the loop at every exit point.  Maintain separate
+    // spanning & equivalence point path exit weights so that if we avoid
+    // instrumenting loop boundaries in traverseLoopEntry() we can update the
+    // exit weights.
+    L->getExitingBlocks(ExitBlocks);
+    for(auto Exit : ExitBlocks) {
+      HasSpPath = HasEqPointPath = false;
+      SpanningWeight.reset(getZeroWeight());
+      EqPointWeight.reset(getZeroWeight());
+
+      LP->getPathsThroughBlock(L, Exit, Paths);
+      for(auto Path : Paths) {
+        WeightPtr PathWeight(traversePathUntilExit(Path, Exit, ActuallyEqPoint));
+        if(Path->isSpanningPath() && !ActuallyEqPoint) {
+          HasSpPath = true;
+          SpanningWeight->max(PathWeight);
+        }
+        else {
+          HasEqPointPath = true;
+          EqPointWeight->max(PathWeight);
+        }
+      }
+
+      if(HasSpPath) LWI.setExitSpanningPathWeight(Exit, SpanningWeight);
+      if(HasEqPointPath) LWI.setExitEqPointPathWeight(Exit, EqPointWeight);
+    }
+  }
+
+  /// Analyze loop nests & mark locations for migration points.  Return whether
+  /// or not a migration point was added.
+  bool traverseLoopNest(const std::vector<BasicBlock *> &SCC) {
+    bool AddedMigPoint = false;
+    Loop *L;
+    LoopNest Nest;
+
+    // Get outermost loop in loop nest & enumerate the rest of the nest
+    assert(LI->getLoopFor(SCC.front()) && "No loop in SCC");
+    L = LI->getLoopFor(SCC.front());
+    while(L->getLoopDepth() != 1) L = L->getParentLoop();
+    LoopPathUtilities::populateLoopNest(L, Nest);
+
+    DEBUG(
+      dbgs() << " + Analyzing loop nest at "; L->getStartLoc().print(dbgs());
+      dbgs() << " with " << std::to_string(Nest.size()) << " loop(s)\n\n";
+    );
+
+    for(auto CurLoop : Nest) {
+      // Note: if migration points were added to any sub-loo(s) then we need to
+      // re-run the LoopPaths analysis on the outer loop.
+      // TODO this is a little overzealous, sibling loops (e.g., 2 sub-loops at
+      // the same depth and contained in the same outer loop) can cause
+      // unnecessary re-enumerations.
+      if(traverseLoop(CurLoop) || AddedMigPoint) {
+        AddedMigPoint = true;
+        LP->rerunOnLoop(CurLoop);
+      }
+
+      // TODO if we are instrumenting the loop header, re-enumerate paths
+      calculateLoopExitWeights(CurLoop);
+
+      DEBUG(dbgs() << "\n  Loop analysis: "
+                   << LoopWeights.at(CurLoop).toString() << "\n");
+    }
+
+    DEBUG(dbgs() << " - Finished loop nest\n");
+
+    return AddedMigPoint;
+  }
+
+  /// Analyze the function's body to add migration points.  Return whether or
+  /// not a migration point was added.
+  bool analyzeFunctionBody(Function &F) {
+    std::set<const Loop *> MarkedLoops;
+    bool AddedMigPoint = false;
+    Loop *BlockLoop;
+
+    // Analyze & mark paths through loop nests
+    DEBUG(dbgs() << "\n-> Analyzing loop nests <-\n");
+    for(scc_iterator<Function *> SCC = scc_begin(&F), E = scc_end(&F);
+        SCC != E; ++SCC)
+      if(SCC.hasLoop()) AddedMigPoint |= traverseLoopNest(*SCC);
+
+    // Analyze the rest of the function body
+    DEBUG(dbgs() << "\n-> Analyzing the rest of the function body <-\n");
+    ReversePostOrderTraversal<Function *> RPOT(&F);
+    for(auto BB = RPOT.begin(), BE = RPOT.end(); BB != BE; ++BB) {
+      BlockLoop = LI->getLoopFor(*BB);
+      if(!BlockLoop) {
+        WeightPtr PredWeight(getInitialWeight(*BB));
+        AddedMigPoint |= traverseBlock(*BB, PredWeight);
+        BBWeights[*BB] = std::move(PredWeight);
+      }
+      else if(!MarkedLoops.count(BlockLoop)) {
+        // Block is in a loop, analyze & mark loop's boundaries
+        AddedMigPoint |= traverseLoopEntry(BlockLoop);
+        MarkedLoops.insert(BlockLoop);
+      }
+    }
+
+    // Finally, determine if we should add a migration point at exit block(s).
+    for(Function::iterator BB = F.begin(), E = F.end(); BB != E; BB++) {
+      if(isa<ReturnInst>(BB->getTerminator())) {
+        assert(!LI->getLoopFor(BB) && "Returning inside a loop");
+        assert(BBWeights.count(BB) && "Missing block weight");
+        const BasicBlockWeightInfo &BBWI = BBWeights[BB].BlockWeight;
+        if(!BBWI.BlockWeight->underPercentOfThreshold(CurRetThresh)) {
+          DEBUG(dbgs() << " - Not under weight threshold, marking return\n");
+          markAsMigPoint(BB->getTerminator(), true, true);
+        }
+      }
+    }
+
+    return AddedMigPoint;
+  }
+
+  //===--------------------------------------------------------------------===//
+  // Instrumentation implementation
+  //===--------------------------------------------------------------------===//
+
+  /// Either find an existing induction variable (and its stride), or create
+  /// one for a loop.
+  Instruction *getInductionVariable(Loop *L, size_t &Stride) {
+    BasicBlock *H = L->getHeader();
+    const SCEVAddRecExpr *Induct;
+    const SCEVConstant *StrideExpr;
+    Type *IVTy;
+
+    // Search for the induction variable & it's stride
+    for(BasicBlock::iterator I = H->begin(), E = H->end(); I != E; ++I) {
+      if(!isa<PHINode>(*I)) break;
+      IVTy = I->getType();
+      if(IVTy->isPointerTy() || !SE->isSCEVable(IVTy)) continue;
+      Induct = dyn_cast<SCEVAddRecExpr>(SE->getSCEV(I));
+      if(Induct && isa<SCEVConstant>(Induct->getStepRecurrence(*SE))) {
+        StrideExpr = cast<SCEVConstant>(Induct->getStepRecurrence(*SE));
+        Stride = std::abs(StrideExpr->getValue()->getSExtValue());
+
+        // TODO if stride != 1, it's hard to ensure we're hitting a migration
+        // point every n iterations unless we know the *exact* number at which
+        // it starts.  For example, if stride = 4 but we start at 1, the
+        // migration point checking logic has to add checks for 1, 5, 9, etc.
+        // It's easier to just create our own induction variable.
+        if(Stride != 1) continue;
+
+        DEBUG(dbgs() << "Found induction variable with loop stride of "
+                     << std::to_string(Stride) << ":"; I->print(dbgs());
+              dbgs() << "\n");
+
+        return I;
+      }
+    }
+
+    DEBUG(dbgs() << "No induction variable, adding'migpoint.iv."
+                 << std::to_string(NumIVsAdded) << "' to the loop\n");
+
+    LLVMContext &C = H->getContext();
+    Type *Int32Ty = Type::getInt32Ty(C);
+    IRBuilder<> PhiBuilder(H->getFirstInsertionPt());
+    PHINode *IV = PhiBuilder.CreatePHI(Int32Ty, 0,
+      "migpoint.iv." + std::to_string(NumIVsAdded++));
+    Constant *One = ConstantInt::get(Int32Ty, 1, 0),
+             *Zero = ConstantInt::get(Int32Ty, 0, 0);
+    for(auto Pred : predecessors(H)) {
+      IRBuilder<> AddRecBuilder(Pred->getTerminator());
+      if(L->contains(Pred)) { // Backedge
+        Value *RecVal = AddRecBuilder.CreateAdd(IV, One);
+        IV->addIncoming(RecVal, Pred);
+      }
+      else IV->addIncoming(Zero, Pred);
+    }
+
+    Stride = 1;
+    return IV;
+  }
+
+  /// Round a value down to the nearest power of 2.  Stolen/modified from
+  /// https://graphics.stanford.edu/~seander/bithacks.html#RoundUpPowerOf2
+  unsigned roundDownPowerOf2(unsigned Count) {
+    unsigned Starting = Count;
+    Count--;
+    Count |= Count >> 1;
+    Count |= Count >> 2;
+    Count |= Count >> 4;
+    Count |= Count >> 8;
+    Count |= Count >> 16;
+    Count++;
+
+    // If we're already a power of 2, then the above math returns the same
+    // value.  Otherwise, we've rounded *up* to the nearest power of 2 and need
+    // to divide by 2 to round *down*.
+    if(Count != Starting) Count >>= 1;
+    return Count;
+  }
+
+  /// Transform a loop header so that migration points (and any concomitant
+  /// costs) are only experienced every nth iteration, based on weight metrics
+  void transformLoopHeader(Loop *L) {
+    BasicBlock *Header = L->getHeader();
+    size_t ItersPerMigPoint, Stride = 0, InstrStride;
+
+    // If the first instruction has already been marked due to heuristics that
+    // bookend libc I/O & big memory operations, then there's nothing to do.
+    Instruction *First = Header->getFirstInsertionPt();
+    if(isMarkedForInstrumentation(First)) return;
+
+    DEBUG(dbgs() << "+ Instrumenting "; L->dump());
+
+    assert(LoopWeights.count(L) && "No loop analysis");
+    ItersPerMigPoint = LoopWeights.at(L).getItersPerMigPoint();
+
+    if(ItersPerMigPoint > 1) {
+      BasicBlock *NewSuccBB, *MigPointBB;
+      Instruction *IV = getInductionVariable(L, Stride);
+
+      IntegerType *IVType = cast<IntegerType>(IV->getType());
+      Function *CurF = Header->getParent();
+      LLVMContext &C = Header->getContext();
+
+      // Create new successor for all instructions after migration point
+      NewSuccBB = Header->splitBasicBlock(Header->getFirstInsertionPt(),
+        "l.postmigpoint" + std::to_string(LoopsTransformed));
+
+      // Create new block for migration point
+      MigPointBB = BasicBlock::Create(C,
+        "l.migpoint" + std::to_string(LoopsTransformed), CurF, NewSuccBB);
+      IRBuilder<> MigPointWorker(MigPointBB);
+      Instruction *Br = cast<Instruction>(MigPointWorker.CreateBr(NewSuccBB));
+      markAsMigPoint(Br, true, true);
+
+      // Add check and branch to migration point only every nth iteration.
+      // Round down to nearest power-of-2, which allows us to use a simple
+      // bitmask for migration point check (URem instructions can cause
+      // non-negligible overhead in tight-loops).
+      IRBuilder<> Worker(Header->getTerminator());
+      InstrStride = roundDownPowerOf2(ItersPerMigPoint * Stride) - 1;
+      assert(InstrStride > 0 && "Invalid migration point stride");
+      Constant *N = ConstantInt::get(IVType, InstrStride, IVType->getSignBit()),
+               *Zero = ConstantInt::get(IVType, 0, IVType->getSignBit());
+      Value *Rem = Worker.CreateAnd(IV, N);
+      Value *Cmp = Worker.CreateICmpEQ(Rem, Zero);
+      Worker.CreateCondBr(Cmp, MigPointBB, NewSuccBB);
+      Header->getTerminator()->eraseFromParent();
+
+      DEBUG(dbgs() << "Instrumenting to hit migration point every "
+                   << std::to_string(InstrStride + 1) << " iterations\n");
+    }
+    else {
+      DEBUG(dbgs() << "Instrumenting to hit migration point every iteration\n");
+      markAsMigPoint(Header->getFirstInsertionPt(), true, true);
+    }
+  }
+
+  /// Insert migration points & HTM instrumentation for instructions.
+  void transformLoopHeaders(Function &F) {
+    DEBUG(dbgs() << "\n-> Transforming loop headers <-\n");
+    for(auto Loop : TransformLoops) {
+      transformLoopHeader(Loop);
+      LoopsTransformed++;
+    }
+  }
+};
+
+} /* end anonymous namespace */
+
+char SelectMigrationPoints::ID = 0;
+
+INITIALIZE_PASS_BEGIN(SelectMigrationPoints, "select-migration-points",
+                      "Select migration points locations", true, false)
+INITIALIZE_PASS_DEPENDENCY(LoopInfoWrapperPass)
+INITIALIZE_PASS_DEPENDENCY(EnumerateLoopPaths)
+INITIALIZE_PASS_DEPENDENCY(ScalarEvolution)
+INITIALIZE_PASS_END(SelectMigrationPoints, "select-migration-points",
+                    "Select migration points locations", true, false)
+
+const StringSet<> SelectMigrationPoints::LibcIO = {
+  "fopen", "freopen", "fclose", "fflush", "fwide",
+  "setbuf", "setvbuf", "fread", "fwrite",
+  "fgetc", "getc", "fgets", "fputc", "putc", "fputs",
+  "getchar", "gets", "putchar", "puts", "ungetc",
+  "fgetwc", "getwc", "fgetws", "fputwc", "putwc", "fputws",
+  "getwchar", "putwchar", "ungetwc",
+  "scanf", "fscanf", "vscanf", "vfscanf",
+  "printf", "fprintf", "vprintf", "vfprintf",
+  "wscanf", "fwscanf", "vwscanf", "vfwscanf",
+  "wprintf", "fwprintf", "vwprintf", "vfwprintf",
+  "ftell", "fgetpos", "fseek", "fsetpos", "rewind",
+  "clearerr", "feof", "ferror", "perror",
+  "remove", "rename", "tmpfile", "tmpnam",
+  "__isoc99_fscanf", "exit"
+};
+
+namespace llvm {
+  FunctionPass *createSelectMigrationPointsPass()
+  { return new SelectMigrationPoints(); }
+}
+
diff --git a/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp b/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp
index 125047e7bbb..fe680776557 100644
--- a/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp
+++ b/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp
@@ -1160,6 +1160,41 @@ MCSymbol *AsmPrinter::getCurExceptionSym() {
   return CurExceptionSym;
 }
 
+MachineInstr *AsmPrinter::FindStackMap(MachineBasicBlock &MBB,
+                                       MachineInstr *MI) const {
+  MachineBasicBlock::instr_iterator i, ie;
+  for(i = MI->getNextNode(), ie = MBB.instr_end();
+      i != ie;
+      i = i->getNextNode()) {
+    if(i->getOpcode() == TargetOpcode::STACKMAP)
+      return &*i;
+    else if(i->isCall())
+      break;
+  }
+
+  // Call site without a stackmap implies that either the call was generated by
+  // the backend or the LLVM bitcode was never instrumented by the StackInfo
+  // pass.  This is not necessarily an error!
+  return nullptr;
+}
+
+bool AsmPrinter::TagCallSites(MachineFunction &MF) {
+  bool tagged = false;
+  for(auto MBB = MF.begin(), MBBE = MF.end(); MBB != MBBE; MBB++) {
+    for(auto MI = MBB->instr_begin(), MIE = MBB->instr_end(); MI != MIE; MI++) {
+      if(MI->isCall() && !MI->isPseudo()) {
+        MachineInstr *SMI = FindStackMap(*MBB, &*MI);
+        if(SMI != nullptr) {
+          MBB->remove(SMI);
+          MI = MBB->insert(++MI, SMI);
+          tagged = true;
+        }
+      }
+    }
+  }
+  return tagged;
+}
+
 void AsmPrinter::SetupMachineFunction(MachineFunction &MF) {
   this->MF = &MF;
   // Get the function symbol.
diff --git a/llvm/lib/CodeGen/CMakeLists.txt b/llvm/lib/CodeGen/CMakeLists.txt
index eb7552970d3..44702254b37 100644
--- a/llvm/lib/CodeGen/CMakeLists.txt
+++ b/llvm/lib/CodeGen/CMakeLists.txt
@@ -111,6 +111,8 @@ add_llvm_library(LLVMCodeGen
   StackSlotColoring.cpp
   StackMapLivenessAnalysis.cpp
   StackMaps.cpp
+  StackTransformMetadata.cpp
+  StackTransformTypes.cpp
   StatepointExampleGC.cpp
   TailDuplication.cpp
   TargetFrameLoweringImpl.cpp
@@ -122,6 +124,7 @@ add_llvm_library(LLVMCodeGen
   TargetSchedule.cpp
   TwoAddressInstructionPass.cpp
   UnreachableBlockElim.cpp
+  UnwindInfo.cpp
   VirtRegMap.cpp
   WinEHPrepare.cpp
 
diff --git a/llvm/lib/CodeGen/CodeGen.cpp b/llvm/lib/CodeGen/CodeGen.cpp
index 155c5ecec77..7ce9421dd03 100644
--- a/llvm/lib/CodeGen/CodeGen.cpp
+++ b/llvm/lib/CodeGen/CodeGen.cpp
@@ -68,6 +68,7 @@ void llvm::initializeCodeGen(PassRegistry &Registry) {
   initializeStackMapLivenessPass(Registry);
   initializeStackProtectorPass(Registry);
   initializeStackSlotColoringPass(Registry);
+  initializeStackTransformMetadataPass(Registry);
   initializeTailDuplicatePassPass(Registry);
   initializeTargetPassConfigPass(Registry);
   initializeTwoAddressInstructionPassPass(Registry);
diff --git a/llvm/lib/CodeGen/InlineSpiller.cpp b/llvm/lib/CodeGen/InlineSpiller.cpp
index 9989f233d09..42e706a159d 100644
--- a/llvm/lib/CodeGen/InlineSpiller.cpp
+++ b/llvm/lib/CodeGen/InlineSpiller.cpp
@@ -53,6 +53,10 @@ STATISTIC(NumHoists,          "Number of hoisted spills");
 
 static cl::opt<bool> DisableHoisting("disable-spill-hoist", cl::Hidden,
                                      cl::desc("Disable inline spill hoisting"));
+static cl::opt<bool>
+RestrictStatepointRemat("restrict-statepoint-remat",
+                       cl::init(true), cl::Hidden,
+                       cl::desc("Restrict remat for statepoint operands"));
 
 namespace {
 class InlineSpiller : public Spiller {
@@ -169,6 +173,7 @@ private:
   void eliminateRedundantSpills(LiveInterval &LI, VNInfo *VNI);
 
   void markValueUsed(LiveInterval*, VNInfo*);
+  bool canGuaranteeAssignmentAfterRemat(unsigned VReg, MachineInstr &MI);
   bool reMaterializeFor(LiveInterval&, MachineBasicBlock::iterator MI);
   void reMaterializeAll();
 
@@ -852,6 +857,29 @@ void InlineSpiller::markValueUsed(LiveInterval *LI, VNInfo *VNI) {
   } while (!WorkList.empty());
 }
 
+bool InlineSpiller::canGuaranteeAssignmentAfterRemat(unsigned VReg,
+                                                     MachineInstr &MI) {
+  if (!RestrictStatepointRemat)
+    return true;
+  // Here's a quick explanation of the problem we're trying to handle here:
+  // * There are some pseudo instructions with more vreg uses than there are
+  //   physical registers on the machine.
+  // * This is normally handled by spilling the vreg, and folding the reload
+  //   into the user instruction.  (Thus decreasing the number of used vregs
+  //   until the remainder can be assigned to physregs.)
+  // * However, since we may try to spill vregs in any order, we can end up
+  //   trying to spill each operand to the instruction, and then rematting it
+  //   instead.  When that happens, the new live intervals (for the remats) are
+  //   expected to be trivially assignable (i.e. RS_Done).  However, since we
+  //   may have more remats than physregs, we're guaranteed to fail to assign
+  //   one.
+  // At the moment, we only handle this for STATEPOINTs since they're the only
+  // psuedo op where we've seen this.  If we start seeing other instructions
+  // with the same problem, we need to revisit this.
+  return (MI.getOpcode() != TargetOpcode::STATEPOINT
+	  && MI.getOpcode() != TargetOpcode::STACKMAP);
+}
+
 /// reMaterializeFor - Attempt to rematerialize before MI instead of reloading.
 bool InlineSpiller::reMaterializeFor(LiveInterval &VirtReg,
                                      MachineBasicBlock::iterator MI) {
@@ -909,6 +937,14 @@ bool InlineSpiller::reMaterializeFor(LiveInterval &VirtReg,
     return true;
   }
 
+  // If we can't guarantee that we'll be able to actually assign the new vreg,
+  // we can't remat.
+  if (!canGuaranteeAssignmentAfterRemat(VirtReg.reg, *MI)) {
+    markValueUsed(&VirtReg, ParentVNI);
+    DEBUG(dbgs() << "\tcannot remat for " << UseIdx << '\t' << *MI);
+    return false;
+  }
+
   // Alocate a new register for the remat.
   unsigned NewVReg = Edit->createFrom(Original);
 
diff --git a/llvm/lib/CodeGen/LLVMTargetMachine.cpp b/llvm/lib/CodeGen/LLVMTargetMachine.cpp
index 37299eb664c..58322f11aaa 100644
--- a/llvm/lib/CodeGen/LLVMTargetMachine.cpp
+++ b/llvm/lib/CodeGen/LLVMTargetMachine.cpp
@@ -42,6 +42,28 @@ static cl::opt<cl::boolOrDefault>
 EnableFastISelOption("fast-isel", cl::Hidden,
   cl::desc("Enable the \"fast\" instruction selector"));
 
+// Popcorn-specific IR-level instrumentation
+enum PopcornInstrumentation {
+  none, // No instrumentation
+  metadata, // Only generate migration metadata, don't insert migration points
+  migpoints, // Only add migration points, don't generate rewriting metadata
+  migration, // Add migration points & generate rewriting metadata for migration
+  libc // Generate rewriting metadata for libc thread start functions
+};
+
+static cl::opt<PopcornInstrumentation> PopcornInstrument("popcorn-instrument",
+  cl::desc("Add Popcorn-specific instrumentation to applications"),
+  cl::init(none),
+  cl::values(
+    clEnumVal(none, "No instrumentation (default)"),
+    clEnumVal(metadata, "Only generate migration metadata (no migration points"),
+    clEnumVal(migpoints, "Only add migration points (no migration metadata)"),
+    clEnumVal(migration, "Add migration points & generate migration metadata"),
+    clEnumVal(libc, "Instrument libc thread start functions for migration"),
+    NULL
+  )
+);
+
 void LLVMTargetMachine::initAsmInfo() {
   MRI = TheTarget.createMCRegInfo(getTargetTriple().str());
   MII = TheTarget.createMCInstrInfo();
@@ -105,8 +127,36 @@ addPassesToGenerateCode(LLVMTargetMachine *TM, PassManagerBase &PM,
   // Set PassConfig options provided by TargetMachine.
   PassConfig->setDisableVerify(DisableVerify);
 
+  /// Popcorn compiler - multi-ISA binary configurations.  Requires that IR
+  /// passed to backends is identical, save for certain architecture-specific
+  /// quirks like atomic operations or intrinsics
+  if(PopcornInstrument != PopcornInstrumentation::none) {
+    TM->setArchIROptLevel(CodeGenOpt::None);
+
+    switch(PopcornInstrument) {
+    case PopcornInstrumentation::metadata:
+      PassConfig->setAddStackMaps(true);
+      break;
+    case PopcornInstrumentation::migpoints:
+      PassConfig->setAddMigrationPoints(true);
+      break;
+    case PopcornInstrumentation::migration:
+      PassConfig->setAddMigrationPoints(true);
+      PassConfig->setAddStackMaps(true);
+      break;
+    case PopcornInstrumentation::libc:
+      PassConfig->setAddLibcStackMaps(true);
+      break;
+    default:
+      llvm_unreachable("Invalid instrumentation type");
+      break;
+    }
+  }
+
   PM.add(PassConfig);
 
+  PassConfig->addPopcornPasses();
+
   PassConfig->addIRPasses();
 
   PassConfig->addCodeGenPrepare();
diff --git a/llvm/lib/CodeGen/MachineFunction.cpp b/llvm/lib/CodeGen/MachineFunction.cpp
index 9856e70edae..8f26b45e4cd 100644
--- a/llvm/lib/CodeGen/MachineFunction.cpp
+++ b/llvm/lib/CodeGen/MachineFunction.cpp
@@ -253,6 +253,15 @@ MachineFunction::getMachineMemOperand(const MachineMemOperand *MMO,
                                MMO->getBaseAlignment());
 }
 
+/// Is a register caller-saved?
+bool MachineFunction::isCallerSaved(unsigned Reg) const {
+  assert(TargetRegisterInfo::isPhysicalRegister(Reg) && "Invalid register");
+  CallingConv::ID CC = Fn->getCallingConv();
+  const uint32_t *Mask =
+    RegInfo->getTargetRegisterInfo()->getCallPreservedMask(*this, CC);
+  return !((Mask[Reg / 32] >> Reg % 32) & 1);
+}
+
 MachineInstr::mmo_iterator
 MachineFunction::allocateMemRefsArray(unsigned long Num) {
   return Allocator.Allocate<MachineMemOperand *>(Num);
@@ -482,6 +491,188 @@ MCSymbol *MachineFunction::getPICBaseSymbol() const {
                                Twine(getFunctionNumber()) + "$pb");
 }
 
+/// Get the value for key K in Map M, or create a new one if it doesn't exist.
+template<typename Key, typename Val, typename Map>
+static Val &getOrCreateMapping(const Key *K, Map &M) {
+  typename Map::iterator It;
+  if((It = M.find(K)) == M.end())
+    It = M.insert(std::pair<const Key *, Val>(K, Val())).first;
+  return It->second;
+}
+
+void MachineFunction::addOpLegalizeChange(int64_t SMID,
+                                          unsigned OpNo,
+                                          unsigned Num) {
+  // TODO Popcorn: we currently assume operands are legalized in increasing
+  // operand number order, otherwise OpNo may be invalid or invalidate other
+  // operand numbers
+  OpNumberMap &OpChanges = SMOpLegalizeChanges[SMID];
+  assert(std::max_element(OpChanges.begin(), OpChanges.end())->first < OpNo &&
+         "Invalid legalization sequence");
+  SMOpLegalizeChanges[SMID][OpNo] = Num;
+}
+
+/// Add an IR/architecture-specific location mapping for a stackmap operand
+void MachineFunction::addSMOpLocation(const CallInst *SM,
+                                      const Value *Val,
+                                      const MachineLiveLoc &MLL) {
+  auto containsLoc = [](const MachineLiveLocs &Vals,
+                        const MachineLiveLoc &Cur) -> bool {
+    for(auto &LV : Vals) if(Cur == *LV) return true;
+    return false;
+  };
+
+  assert(SM && "Invalid stackmap");
+  assert(Val && "Invalid stackmap operand");
+
+  IRToMachineLocs &IRMap =
+    getOrCreateMapping<Instruction,
+                       IRToMachineLocs,
+                       InstToOperands>(SM, SMDuplicateLocs);
+  MachineLiveLocs &Vals =
+    getOrCreateMapping<Value,
+                       MachineLiveLocs,
+                       IRToMachineLocs>(Val, IRMap);
+  if(!containsLoc(Vals, MLL))
+    Vals.push_back(MachineLiveLocPtr(MLL.copy()));
+}
+
+/// Add an IR/architecture-specific location mapping for a stackmap operand
+void MachineFunction::addSMOpLocation(const CallInst *SM,
+                                      unsigned Op,
+                                      const MachineLiveLoc &MLL) {
+  assert(SM && "Invalid stackmap");
+  assert(Op < SM->getNumArgOperands() && "Invalid operand number");
+  addSMOpLocation(SM, SM->getArgOperand(Op), MLL);
+}
+
+
+/// Add an architecture-specific live value & location for a stackmap
+void MachineFunction::addSMArchSpecificLocation(const CallInst *SM,
+                                                const MachineLiveLoc &MLL,
+                                                const MachineLiveVal &MLV) {
+  auto containsLoc = [](const ArchLiveValues &Vals,
+                        const MachineLiveLoc &Cur) -> bool {
+    for(auto &LV : Vals) if(Cur == *LV.first) return true;
+    return false;
+  };
+
+  assert(SM && "Invalid stackmap");
+  ArchLiveValues &Vals =
+    getOrCreateMapping<Instruction,
+                       ArchLiveValues,
+                       InstToArchLiveValues>(SM, SMArchSpecificLocs);
+  if(!containsLoc(Vals, MLL))
+    Vals.push_back(ArchLiveValue(MachineLiveLocPtr(MLL.copy()),
+                                 MachineLiveValPtr(MLV.copy())));
+}
+
+/// Update stack slot references to new indexes after stack slot coloring
+void
+MachineFunction::updateSMStackSlotRefs(SmallDenseMap<int, int, 16> &Changes) {
+
+  if(Changes.size()) {
+    DEBUG(dbgs() << "Updating stackmap stack slot references\n";);
+
+    int SS;
+    SmallDenseMap<int, int, 16>::iterator Change;
+
+    // Iterate over all operand duplicate locations
+    for(auto &InstIt : SMDuplicateLocs) {
+      for(auto &IRIt : InstIt.second) {
+        for(auto &MLL : IRIt.second) {
+          if(MLL->isStackSlot()) {
+            MachineLiveStackSlot &LSS = (MachineLiveStackSlot &)*MLL;
+            SS = LSS.getStackSlot();
+            Change = Changes.find(SS);
+            if(Change != Changes.end())
+              LSS.setStackSlot(Change->second);
+          }
+        }
+      }
+    }
+
+    // Iterate over all architecture-specific locations.
+    // Note: both the destination & source location can be stack slots
+    for(auto &InstIt : SMArchSpecificLocs) {
+      for(auto &MLL : InstIt.second) {
+        if(MLL.first->isStackSlot()) {
+          MachineLiveStackSlot &LSS = (MachineLiveStackSlot &)*MLL.first;
+          SS = LSS.getStackSlot();
+          Change = Changes.find(SS);
+          if(Change != Changes.end())
+            LSS.setStackSlot(Change->second);
+        }
+
+        if(MLL.second->isStackObject()) {
+          MachineStackObject &MSO = (MachineStackObject &)*MLL.second;
+          SS = MSO.getIndex();
+          Change = Changes.find(SS);
+          if(Change != Changes.end())
+            MSO.setIndex(Change->second);
+        }
+      }
+    }
+  }
+}
+
+/// Return the number of machine operands corresponding to a given IR operand.
+unsigned
+MachineFunction::getNumLegalizedOps(int64_t SMID, unsigned OpNo) const {
+  SMToOpLegalizeMap::const_iterator SM = SMOpLegalizeChanges.find(SMID);
+  if(SM != SMOpLegalizeChanges.end()) {
+    const OpNumberMap &OpChanges = SM->second;
+    OpNumberMap::const_iterator Op = OpChanges.find(OpNo);
+    if(Op != OpChanges.end()) return Op->second;
+  }
+  return 1;
+}
+
+/// Are there any architecture-specific locations for operand Val in stackmap
+/// SM?
+bool
+MachineFunction::hasSMOpLocations(const CallInst *SM, const Value *Val) const {
+  assert(SM && "Invalid stackmap");
+  assert(Val && "Invalid stackmap operand");
+  InstToOperands::const_iterator InstIt;
+  if((InstIt = SMDuplicateLocs.find(SM)) != SMDuplicateLocs.end())
+    return InstIt->second.find(Val) != InstIt->second.end();
+  return false;
+}
+
+/// Are there any architecture-specific locations for stackmap SM?
+bool
+MachineFunction::hasSMArchSpecificLocations(const llvm::CallInst *SM) const {
+  assert(SM && "Invalid stackmap");
+  return SMArchSpecificLocs.find(SM) != SMArchSpecificLocs.end();
+}
+
+/// Return the architecture-specific locations for a stackmap operand.
+const MachineLiveLocs &
+MachineFunction::getSMOpLocations(const CallInst *SM,
+                                  const Value *Val ) const {
+  assert(SM && "Invalid stackmap");
+  assert(Val && "Invalid stackmap operand");
+  InstToOperands::const_iterator InstIt = SMDuplicateLocs.find(SM);
+  assert(InstIt != SMDuplicateLocs.end() &&
+         "No duplicate locations for stackmap");
+  IRToMachineLocs::const_iterator IRIt = InstIt->second.find(Val);
+  assert(IRIt != InstIt->second.end() &&
+         "No duplicate locations for stackmap operand");
+  return IRIt->second;
+}
+
+/// Return the architecture-specific locations for a stackmap that are not
+/// associated with any operand.
+const ArchLiveValues &
+MachineFunction::getSMArchSpecificLocations(const CallInst *SM) const {
+  assert(SM && "Invalid stackmap");
+  InstToArchLiveValues::const_iterator InstIt = SMArchSpecificLocs.find(SM);
+  assert(InstIt != SMArchSpecificLocs.end() &&
+         "No architecture-specific locations for stackmap");
+  return InstIt->second;
+}
+
 //===----------------------------------------------------------------------===//
 //  MachineFrameInfo implementation
 //===----------------------------------------------------------------------===//
diff --git a/llvm/lib/CodeGen/Passes.cpp b/llvm/lib/CodeGen/Passes.cpp
index 024d166a498..06a28d6a9c3 100644
--- a/llvm/lib/CodeGen/Passes.cpp
+++ b/llvm/lib/CodeGen/Passes.cpp
@@ -217,7 +217,9 @@ TargetPassConfig::TargetPassConfig(TargetMachine *tm, PassManagerBase &pm)
     : ImmutablePass(ID), PM(&pm), StartBefore(nullptr), StartAfter(nullptr),
       StopAfter(nullptr), Started(true), Stopped(false),
       AddingMachinePasses(false), TM(tm), Impl(nullptr), Initialized(false),
-      DisableVerify(false), EnableTailMerge(true), EnableShrinkWrap(false) {
+      DisableVerify(false), EnableTailMerge(true), EnableShrinkWrap(false),
+      AddMigrationPoints(false), AddStackMaps(false),
+      AddLibcStackMaps(false) {
 
   Impl = new PassConfigImpl();
 
@@ -391,7 +393,9 @@ void TargetPassConfig::addIRPasses() {
     addPass(createVerifierPass());
 
   // Run loop strength reduction before anything else.
-  if (getOptLevel() != CodeGenOpt::None && !DisableLSR) {
+  if (getOptLevel() != CodeGenOpt::None &&
+      getArchIROptLevel() != CodeGenOpt::None &&
+      !DisableLSR) {
     addPass(createLoopStrengthReducePass());
     if (PrintLSR)
       addPass(createPrintFunctionPass(dbgs(), "\n\n*** Code after LSR ***\n"));
@@ -406,10 +410,14 @@ void TargetPassConfig::addIRPasses() {
   addPass(createUnreachableBlockEliminationPass());
 
   // Prepare expensive constants for SelectionDAG.
-  if (getOptLevel() != CodeGenOpt::None && !DisableConstantHoisting)
+  if (getOptLevel() != CodeGenOpt::None &&
+      getArchIROptLevel() != CodeGenOpt::None &&
+      !DisableConstantHoisting)
     addPass(createConstantHoistingPass());
 
-  if (getOptLevel() != CodeGenOpt::None && !DisablePartialLibcallInlining)
+  if (getOptLevel() != CodeGenOpt::None &&
+      getArchIROptLevel() != CodeGenOpt::None &&
+      !DisablePartialLibcallInlining)
     addPass(createPartiallyInlineLibCallsPass());
 }
 
@@ -449,11 +457,32 @@ void TargetPassConfig::addPassesToHandleExceptions() {
 /// Add pass to prepare the LLVM IR for code generation. This should be done
 /// before exception handling preparation passes.
 void TargetPassConfig::addCodeGenPrepare() {
-  if (getOptLevel() != CodeGenOpt::None && !DisableCGP)
+  if (getOptLevel() != CodeGenOpt::None &&
+      getArchIROptLevel() != CodeGenOpt::None &&
+      !DisableCGP)
     addPass(createCodeGenPreparePass(TM));
   addPass(createRewriteSymbolsPass());
 }
 
+void TargetPassConfig::addPopcornPasses() {
+  assert(!(AddStackMaps && AddLibcStackMaps) &&
+    "Cannot add both InsertStackMapsPass and LibcStackMapsPass");
+  assert(!(AddMigrationPoints && AddLibcStackMaps) &&
+    "Should not be instrumenting libc with extra migration points");
+
+  // Add pass to instrument IR with equivalence points, which are implemented
+  // various ways depending on other command-line arguments
+  if(AddMigrationPoints) addPass(createMigrationPointsPass());
+
+  // Add pass to instrument IR with stackmap instructions, which get lowered to
+  // metadata needed for Popcorn's stack transformation
+  if(AddStackMaps) addPass(createInsertStackMapsPass());
+
+  // Similar to creatInsertStackMaps pass, but only instruments libc thread
+  // start functions
+  if(AddLibcStackMaps) addPass(createLibcStackMapsPass());
+}
+
 /// Add common passes that perform LLVM IR to IR transforms in preparation for
 /// instruction selection.
 void TargetPassConfig::addISelPrepare() {
@@ -754,6 +783,10 @@ void TargetPassConfig::addOptimizedRegAlloc(FunctionPass *RegAllocPass) {
   // Allow targets to change the register assignments before rewriting.
   addPreRewrite();
 
+  // Gather additional stack transformation metadata before rewriting virtual
+  // registers
+  addPass(&StackTransformMetadataID);
+
   // Finally rewrite virtual registers.
   addPass(&VirtRegRewriterID);
 
@@ -766,7 +799,8 @@ void TargetPassConfig::addOptimizedRegAlloc(FunctionPass *RegAllocPass) {
   // Run post-ra machine LICM to hoist reloads / remats.
   //
   // FIXME: can this move into MachineLateOptimization?
-  addPass(&PostRAMachineLICMID);
+  if(getOptLevel() != CodeGenOpt::None)
+    addPass(&PostRAMachineLICMID);
 }
 
 //===---------------------------------------------------------------------===//
diff --git a/llvm/lib/CodeGen/RegAllocFast.cpp b/llvm/lib/CodeGen/RegAllocFast.cpp
index fd3d4d78968..9947559139d 100644
--- a/llvm/lib/CodeGen/RegAllocFast.cpp
+++ b/llvm/lib/CodeGen/RegAllocFast.cpp
@@ -1078,6 +1078,13 @@ void RAFast::AllocateBasicBlock() {
 /// runOnMachineFunction - Register allocate the whole function
 ///
 bool RAFast::runOnMachineFunction(MachineFunction &Fn) {
+  // TODO the fast register allocator behaves poorly for stackmaps with lots
+  // of operands, and since it doesn't use the VirtRegRewriter pass we can't
+  // capture correct stackmap operand locations
+  if(Fn.getFrameInfo()->hasStackMap())
+    llvm_unreachable("Fast register allocator not supported for stack"
+                     "transformation");
+
   DEBUG(dbgs() << "********** FAST REGISTER ALLOCATION **********\n"
                << "********** Function: " << Fn.getName() << '\n');
   MF = &Fn;
diff --git a/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp b/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp
index 9f060a09a0f..b3ab2d64f60 100644
--- a/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp
+++ b/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp
@@ -886,6 +886,9 @@ bool DAGTypeLegalizer::PromoteIntegerOperand(SDNode *N, unsigned OpNo) {
   case ISD::SRL:
   case ISD::ROTL:
   case ISD::ROTR: Res = PromoteIntOp_Shift(N); break;
+
+  case (uint16_t)~TargetOpcode::STACKMAP:
+    Res = PromoteIntOp_STACKMAP(N, OpNo); break;
   }
 
   // If the result is null, the sub-method took care of registering results etc.
@@ -1131,6 +1134,27 @@ SDValue DAGTypeLegalizer::PromoteIntOp_SINT_TO_FP(SDNode *N) {
                                 SExtPromotedInteger(N->getOperand(0))), 0);
 }
 
+SDValue DAGTypeLegalizer::PromoteIntOp_STACKMAP(SDNode *N, unsigned OpNo) {
+  std::vector<SDValue> Ops(N->getNumOperands());
+  SDLoc dl(N);
+
+  for(unsigned i = 0; i < N->getNumOperands(); i++) {
+    if(i == OpNo) {
+      if(N->getOperand(i).getValueType() == MVT::i1 ||
+         N->getOperand(i).getValueType() == MVT::i8 ||
+         N->getOperand(i).getValueType() == MVT::i16) {
+        if(N->getOperand(i).getOpcode() == ISD::TRUNCATE)
+          Ops[i] = N->getOperand(i)->getOperand(0);
+        else
+          Ops[i] = DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i32, N->getOperand(i));
+      }
+    }
+    else Ops[i] = N->getOperand(i);
+  }
+
+  return SDValue(DAG.UpdateNodeOperands(N, Ops), 0);
+}
+
 SDValue DAGTypeLegalizer::PromoteIntOp_STORE(StoreSDNode *N, unsigned OpNo){
   assert(ISD::isUNINDEXEDStore(N) && "Indexed store during type legalization!");
   SDValue Ch = N->getChain(), Ptr = N->getBasePtr();
@@ -2619,6 +2643,9 @@ bool DAGTypeLegalizer::ExpandIntegerOperand(SDNode *N, unsigned OpNo) {
   case ISD::FRAMEADDR:         Res = ExpandIntOp_RETURNADDR(N); break;
 
   case ISD::ATOMIC_STORE:      Res = ExpandIntOp_ATOMIC_STORE(N); break;
+
+  case (uint16_t)~TargetOpcode::STACKMAP:
+    Res = ExpandIntOp_STACKMAP(N, OpNo); break;
   }
 
   // If the result is null, the sub-method took care of registering results etc.
@@ -3007,6 +3034,36 @@ SDValue DAGTypeLegalizer::ExpandIntOp_ATOMIC_STORE(SDNode *N) {
   return Swap.getValue(1);
 }
 
+SDValue DAGTypeLegalizer::ExpandIntOp_STACKMAP(SDNode *N, unsigned OpNo) {
+  std::vector<SDValue> Ops;
+  SDLoc dl(N);
+  const SDValue &InvOp = N->getOperand(OpNo);
+
+  switch(InvOp.getSimpleValueType().SimpleTy) {
+  default: llvm_unreachable("Unhandled operand type in stackmap");
+  case MVT::i128:
+    if(InvOp->getOpcode() == ISD::BUILD_PAIR) {
+      // If the i128 is the result of a buildpair i64, i64, replace the i128
+      // operand with the two i64 operands in the stackmap
+      DEBUG(dbgs() << "Stackmap: replacing i128 operand with i64 pair\n");
+
+      Ops.reserve(N->getNumOperands() + 1);
+      for(unsigned i = 0; i < N->getNumOperands(); i++) {
+        if(i == OpNo) {
+          Ops.push_back(InvOp->getOperand(0));
+          Ops.push_back(InvOp->getOperand(1));
+        }
+        else Ops.push_back(N->getOperand(i));
+      }
+
+      int64_t SMID = cast<ConstantSDNode>(N->getOperand(0))->getSExtValue();
+      DAG.getMachineFunction().addOpLegalizeChange(SMID, OpNo, 2);
+    }
+    break;
+  }
+
+  return SDValue(DAG.MorphNodeTo(N, N->getOpcode(), N->getVTList(), Ops), 0);
+}
 
 SDValue DAGTypeLegalizer::PromoteIntRes_EXTRACT_SUBVECTOR(SDNode *N) {
   SDValue InOp0 = N->getOperand(0);
diff --git a/llvm/lib/CodeGen/SelectionDAG/LegalizeTypes.h b/llvm/lib/CodeGen/SelectionDAG/LegalizeTypes.h
index d1131a74cf1..23b3c91c1bb 100644
--- a/llvm/lib/CodeGen/SelectionDAG/LegalizeTypes.h
+++ b/llvm/lib/CodeGen/SelectionDAG/LegalizeTypes.h
@@ -288,6 +288,7 @@ private:
   SDValue PromoteIntOp_Shift(SDNode *N);
   SDValue PromoteIntOp_SIGN_EXTEND(SDNode *N);
   SDValue PromoteIntOp_SINT_TO_FP(SDNode *N);
+  SDValue PromoteIntOp_STACKMAP(SDNode *N, unsigned OpNo);
   SDValue PromoteIntOp_STORE(StoreSDNode *N, unsigned OpNo);
   SDValue PromoteIntOp_TRUNCATE(SDNode *N);
   SDValue PromoteIntOp_UINT_TO_FP(SDNode *N);
@@ -367,6 +368,7 @@ private:
   SDValue ExpandIntOp_UINT_TO_FP(SDNode *N);
   SDValue ExpandIntOp_RETURNADDR(SDNode *N);
   SDValue ExpandIntOp_ATOMIC_STORE(SDNode *N);
+  SDValue ExpandIntOp_STACKMAP(SDNode *N, unsigned OpNo);
 
   void IntegerExpandSetCCOperands(SDValue &NewLHS, SDValue &NewRHS,
                                   ISD::CondCode &CCCode, SDLoc dl);
diff --git a/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp b/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp
index 2c3c0eb101a..c44c6a3ed37 100644
--- a/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp
+++ b/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp
@@ -6542,6 +6542,43 @@ void SelectionDAGBuilder::visitStackmap(const CallInst &CI) {
   Callee = getValue(CI.getCalledValue());
   NullPtr = DAG.getIntPtrConstant(0, DL, true);
 
+  // Popcorn: we need to insert the stackmap directly after the call
+  // instruction, so grab the chain from the CALLSEQ_END node.  Note that
+  // although we're moving the stackmap between the call & return value
+  // copy-out, the stackmap doesn't generate code so we're not clobbering
+  // return values.
+  Chain = getRoot();
+  SDNode *CSE = Chain.getNode(), *RetCopyOut = nullptr;
+  while(CSE && CSE->getOpcode() != ISD::CALLSEQ_END) {
+    RetCopyOut = CSE;
+    CSE = CSE->getGluedNode();
+  }
+
+  // It's possible the function call that induced this stackmap to be generated
+  // got lowered directly to a machine instruction, which can lead to weird
+  // cases like trying to glue this stackmap to another.  This causes the
+  // instruction scheduler to choke, so avoid that situation.
+  // TODO why does gluing 2 stackmaps cause it to die?  It complains about
+  // NumLiveRegs already being zero for releasing call dependencies in
+  // ScheduleDAGRRList.cpp:759
+  if(CSE && CSE->getOpcode() == ISD::CALLSEQ_END) {
+    SDNode *Check = CSE;
+    do {
+      if(Check->isMachineOpcode() &&
+         Check->getMachineOpcode() == TargetOpcode::STACKMAP) {
+        CSE = nullptr;
+        break;
+      }
+    } while((Check = Check->getGluedNode()));
+  }
+
+  if(CSE && CSE->getOpcode() == ISD::CALLSEQ_END) Chain = SDValue(CSE, 0);
+  else {
+    DEBUG(dbgs() << "WARNING: no call node for: "; Chain->dump());
+    RetCopyOut = nullptr;
+  }
+  assert(Chain.getSimpleValueType() == MVT::Other && "Invalid chain");
+
   // The stackmap intrinsic only records the live variables (the arguemnts
   // passed to it) and emits NOPS (if requested). Unlike the patchpoint
   // intrinsic, this won't be lowered to a function call. This means we don't
@@ -6552,9 +6589,26 @@ void SelectionDAGBuilder::visitStackmap(const CallInst &CI) {
   // chain, flag = STACKMAP(id, nbytes, ..., chain, flag)
   // chain, flag = CALLSEQ_END(chain, 0, 0, flag)
   //
-  Chain = DAG.getCALLSEQ_START(getRoot(), NullPtr, DL);
+  Chain = DAG.getCALLSEQ_START(Chain, NullPtr, DL);
   InFlag = Chain.getValue(1);
 
+  // Popcorn: add glue operand to CALLSEQ_START to tie the stackmap to the
+  // call sequence (note that it already produces a glue value).
+  if(CSE) {
+    SmallVector<EVT, 2> VTs(Chain->value_begin(), Chain->value_end());
+    SDVTList VTList = DAG.getVTList(VTs);
+    for(auto &Op : Chain->ops()) Ops.push_back(Op);
+    assert(Ops.back().getSimpleValueType() != MVT::Glue &&
+           "Already have glue");
+    Ops.push_back(SDValue(CSE, 1));
+    assert(Ops.back().getSimpleValueType() == MVT::Glue &&
+           "Invalid glue operand");
+    Chain.setNode(DAG.MorphNodeTo(Chain.getNode(), Chain->getOpcode(),
+                                  VTList, Ops));
+    InFlag = Chain.getValue(1);
+    Ops.clear();
+  }
+
   // Add the <id> and <numBytes> constants.
   SDValue IDVal = getValue(CI.getOperand(PatchPointOpers::IDPos));
   Ops.push_back(DAG.getTargetConstant(
@@ -6581,9 +6635,30 @@ void SelectionDAGBuilder::visitStackmap(const CallInst &CI) {
   InFlag = Chain.getValue(1);
 
   Chain = DAG.getCALLSEQ_END(Chain, NullPtr, NullPtr, InFlag, DL);
+  InFlag = Chain.getValue(1);
 
   // Stackmaps don't generate values, so nothing goes into the NodeMap.
 
+  // Popcorn: fix-up the glue so the return-value copy outs happen after
+  // the stackmap's CALLSEQ_END.  Note that this is *required*, otherwise
+  // the backend won't correctly track physical register outputs from call.
+  if(RetCopyOut) {
+    unsigned NOpts = RetCopyOut->getNumOperands();
+    Ops.clear();
+    for(size_t i = 0; i < NOpts; i++) {
+      if(RetCopyOut->getOperand(i).getSimpleValueType() == MVT::Other)
+        Ops.push_back(Chain);
+      else if(RetCopyOut->getOperand(i).getSimpleValueType() == MVT::Glue)
+        Ops.push_back(InFlag);
+      else Ops.push_back(RetCopyOut->getOperand(i));
+    }
+    RetCopyOut = DAG.UpdateNodeOperands(RetCopyOut, Ops);
+
+    while(RetCopyOut->getGluedUser()) RetCopyOut = RetCopyOut->getGluedUser();
+    Chain = SDValue(RetCopyOut, NOpts - 2);
+    assert(Chain.getSimpleValueType() == MVT::Other && "Invalid chain");
+  }
+
   // Set the root to the target-lowered call chain.
   DAG.setRoot(Chain);
 
diff --git a/llvm/lib/CodeGen/StackColoring.cpp b/llvm/lib/CodeGen/StackColoring.cpp
index 3541b33a844..7932b67bb67 100644
--- a/llvm/lib/CodeGen/StackColoring.cpp
+++ b/llvm/lib/CodeGen/StackColoring.cpp
@@ -713,7 +713,7 @@ bool StackColoring::runOnMachineFunction(MachineFunction &Func) {
 
   // This is a simple greedy algorithm for merging allocas. First, sort the
   // slots, placing the largest slots first. Next, perform an n^2 scan and look
-  // for disjoint slots. When you find disjoint slots, merge the samller one
+  // for disjoint slots. When you find disjoint slots, merge the smaller one
   // into the bigger one and update the live interval. Remove the small alloca
   // and continue.
 
diff --git a/llvm/lib/CodeGen/StackMaps.cpp b/llvm/lib/CodeGen/StackMaps.cpp
index 116eef66c58..43b0e34573e 100644
--- a/llvm/lib/CodeGen/StackMaps.cpp
+++ b/llvm/lib/CodeGen/StackMaps.cpp
@@ -12,13 +12,18 @@
 #include "llvm/CodeGen/MachineFrameInfo.h"
 #include "llvm/CodeGen/MachineFunction.h"
 #include "llvm/CodeGen/MachineInstr.h"
+#include "llvm/CodeGen/UnwindInfo.h"
 #include "llvm/IR/DataLayout.h"
+#include "llvm/IR/DiagnosticInfo.h"
+#include "llvm/IR/IntrinsicInst.h"
 #include "llvm/MC/MCContext.h"
 #include "llvm/MC/MCExpr.h"
 #include "llvm/MC/MCObjectFileInfo.h"
 #include "llvm/MC/MCSectionMachO.h"
 #include "llvm/MC/MCStreamer.h"
+#include "llvm/MC/MCSymbol.h"
 #include "llvm/Support/CommandLine.h"
+#include "llvm/Target/TargetFrameLowering.h"
 #include "llvm/Target/TargetMachine.h"
 #include "llvm/Target/TargetOpcodes.h"
 #include "llvm/Target/TargetRegisterInfo.h"
@@ -29,6 +34,16 @@ using namespace llvm;
 
 #define DEBUG_TYPE "stackmaps"
 
+#define TYPE_AND_FLAGS(type, ptr, alloca, dup, temp) \
+  ((uint8_t)type) << 4 | ((uint8_t)ptr) << 3 | \
+  ((uint8_t)alloca) << 2 | ((uint8_t)dup << 1) | \
+  ((uint8_t)temp)
+
+#define ARCH_TYPE_AND_FLAGS(type, ptr) ((uint8_t)type) << 4 | ((uint8_t)ptr)
+
+#define ARCH_OP_TYPE(inst, gen, op) \
+  ((uint8_t)inst) << 4 | ((uint8_t)gen) << 3 | (uint8_t)op
+
 static cl::opt<int> StackMapVersion(
     "stackmap-version", cl::init(1),
     cl::desc("Specify the stackmap encoding version (default = 1)"));
@@ -84,49 +99,183 @@ static unsigned getDwarfRegNum(unsigned Reg, const TargetRegisterInfo *TRI) {
   return (unsigned)RegNum;
 }
 
+/// If the instruction is simply casting a pointer to another type, return
+/// the value used as the source of the cast.  This is required because allocas
+/// may be cast to different pointer types (which appear as non-allocas) but
+/// may still be represented by a direct memory reference in the stackmap.
+static inline const Value *getPointerCastSrc(const Value *Inst) {
+  assert(Inst->getType()->isPointerTy() && "Not a pointer type");
+
+  if(isa<BitCastInst>(Inst)) {
+    // Ensure that we're casting to another pointer type
+    const BitCastInst *BC = cast<BitCastInst>(Inst);
+    if(!BC->getSrcTy()->isPointerTy()) return nullptr;
+    else return BC->getOperand(0);
+  }
+  else if(isa<GetElementPtrInst>(Inst)) {
+    // Ensure that all indexes are 0, meaning we're only referencing the start
+    // of the storage location
+    const GetElementPtrInst *GEP = cast<GetElementPtrInst>(Inst);
+    GetElementPtrInst::const_op_iterator it, e;
+    for(it = GEP->idx_begin(), e = GEP->idx_end(); it != e; it++) {
+      if(!isa<ConstantInt>(it->get())) return nullptr;
+      const ConstantInt *Idx = cast<ConstantInt>(it->get());
+      if(!Idx->isZero()) return nullptr;
+    }
+    return GEP->getOperand(0);
+  }
+  else return nullptr;
+}
+
+/// Get pointer typing information for a stackmap operand
+void StackMaps::getPointerInfo(const Value *Op, const DataLayout &DL,
+                               bool &isPtr, bool &isAlloca,
+                               unsigned &AllocaSize) const {
+  isPtr = false;
+  isAlloca = false;
+  AllocaSize = 0;
+  const PtrToIntInst *PTII;
+
+  if((PTII = dyn_cast<PtrToIntInst>(Op))) Op = PTII->getPointerOperand();
+
+  assert(Op != nullptr && "Invalid stackmap operand");
+  Type *Ty = Op->getType();
+  if(Ty->isPointerTy())
+  {
+    // Walk through cast operations that potentially hide allocas
+    while(!isa<AllocaInst>(Op) && (Op = getPointerCastSrc(Op)));
+    if(Op && isa<AllocaInst>(Op)) {
+      PointerType *PTy = cast<PointerType>(Ty);
+      assert(PTy->getElementType()->isSized() && "Alloca of unknown size?");
+      isPtr = PTy->getElementType()->isPointerTy();
+      isAlloca = true;
+      AllocaSize = DL.getTypeAllocSize(PTy->getElementType());
+    }
+    else isPtr = true;
+  }
+}
+
+/// Get stackmap information for register location
+void StackMaps::getRegLocation(unsigned Phys,
+                               unsigned &Dwarf,
+                               unsigned &Offset) const {
+  const TargetRegisterInfo *TRI = AP.MF->getSubtarget().getRegisterInfo();
+  assert(!TRI->isVirtualRegister(Phys) &&
+         "Virtual registers should have been rewritten by now");
+  Offset = 0;
+  Dwarf = getDwarfRegNum(Phys, TRI);
+  unsigned LLVMRegNum = TRI->getLLVMRegNum(Dwarf, false);
+  unsigned SubRegIdx = TRI->getSubRegIndex(LLVMRegNum, Phys);
+  if(SubRegIdx)
+    Offset = TRI->getSubRegIdxOffset(SubRegIdx);
+}
+
+/// Add duplicate target-specific locations for a stackmap operand
+void StackMaps::addDuplicateLocs(const CallInst *StackMap, const Value *Oper,
+                                 LocationVec &Locs, unsigned Size, bool Ptr,
+                                 bool Alloca, unsigned AllocaSize) const {
+  unsigned DwarfRegNum, Offset;
+  int FrameOff;
+
+  if(AP.MF->hasSMOpLocations(StackMap, Oper)) {
+    const MachineLiveLocs &Dups = AP.MF->getSMOpLocations(StackMap, Oper);
+    const TargetRegisterInfo *TRI = AP.MF->getSubtarget().getRegisterInfo();
+
+    for(const MachineLiveLocPtr &LL : Dups) {
+      if(LL->isReg()) {
+        const MachineLiveReg &MR = (const MachineLiveReg &)*LL;
+        getRegLocation(MR.getReg(), DwarfRegNum, Offset);
+
+        Locs.emplace_back(Location::Register, Size, DwarfRegNum, Offset,
+                          Ptr, Alloca, true, false, AllocaSize);
+      }
+      else if(LL->isStackAddr()) {
+        MachineLiveStackAddr &MLSA = (MachineLiveStackAddr &)*LL;
+        FrameOff = MLSA.calcAndGetRegOffset(AP, DwarfRegNum);
+
+        Locs.emplace_back(Location::Indirect, Size,
+          getDwarfRegNum(DwarfRegNum, TRI),
+          FrameOff, Ptr, Alloca, true, false, AllocaSize);
+      }
+      else llvm_unreachable("Unknown machine live location type");
+    }
+  }
+}
+
 MachineInstr::const_mop_iterator
 StackMaps::parseOperand(MachineInstr::const_mop_iterator MOI,
                         MachineInstr::const_mop_iterator MOE, LocationVec &Locs,
-                        LiveOutVec &LiveOuts) const {
+                        LiveOutVec &LiveOuts, User::const_op_iterator &Op) const {
+  bool isPtr, isAlloca, isTemporary = false;
+  unsigned AllocaSize;
+  auto &DL = AP.MF->getDataLayout();
   const TargetRegisterInfo *TRI = AP.MF->getSubtarget().getRegisterInfo();
+  const CallInst *IRSM = cast<CallInst>(Op->getUser());
+  const Value *IROp = Op->get();
+  int64_t TemporaryOffset = 0;
+  getPointerInfo(IROp, DL, isPtr, isAlloca, AllocaSize);
+
   if (MOI->isImm()) {
+    // Peel off temporary value metadata
+    if (MOI->getImm() == StackMaps::TemporaryOp) {
+      isTemporary = true;
+      AllocaSize = (++MOI)->getImm();
+      TemporaryOffset = (++MOI)->getImm();
+      ++MOI;
+    }
+
     switch (MOI->getImm()) {
     default:
       llvm_unreachable("Unrecognized operand type.");
     case StackMaps::DirectMemRefOp: {
-      unsigned Size = AP.TM.getDataLayout()->getPointerSizeInBits();
+      assert((isAlloca || isTemporary) &&
+             "Did not find alloca value for direct memory reference");
+      unsigned Size = DL.getPointerSizeInBits();
       assert((Size % 8) == 0 && "Need pointer size in bytes.");
       Size /= 8;
       unsigned Reg = (++MOI)->getReg();
-      int64_t Imm = (++MOI)->getImm();
-      Locs.emplace_back(StackMaps::Location::Direct, Size,
-                        getDwarfRegNum(Reg, TRI), Imm);
+      int64_t Imm = (++MOI)->getImm() + TemporaryOffset;
+      Locs.emplace_back(Location::Direct, Size, getDwarfRegNum(Reg, TRI), Imm,
+                        isPtr, true, false, isTemporary, AllocaSize);
       break;
     }
     case StackMaps::IndirectMemRefOp: {
       int64_t Size = (++MOI)->getImm();
       assert(Size > 0 && "Need a valid size for indirect memory locations.");
+      Size = DL.getTypeAllocSize(IROp->getType());
       unsigned Reg = (++MOI)->getReg();
       int64_t Imm = (++MOI)->getImm();
-      Locs.emplace_back(StackMaps::Location::Indirect, Size,
-                        getDwarfRegNum(Reg, TRI), Imm);
+      // Note: getPointerInfo() may have found a suitable alloca for this
+      // operand, but the backend didn't actually turn it into one.
+      Locs.emplace_back(Location::Indirect, (unsigned)Size,
+                        getDwarfRegNum(Reg, TRI), Imm, isPtr, false, false,
+                        isTemporary, 0);
       break;
     }
     case StackMaps::ConstantOp: {
       ++MOI;
       assert(MOI->isImm() && "Expected constant operand.");
       int64_t Imm = MOI->getImm();
-      Locs.emplace_back(Location::Constant, sizeof(int64_t), 0, Imm);
+      // Note: getPointerInfo() may have found a suitable alloca for this
+      // operand, but the backend didn't actually turn it into one.
+      Locs.emplace_back(Location::Constant, sizeof(int64_t), 0, Imm,
+                        isPtr, false, false, isTemporary, 0);
       break;
     }
     }
+    // Note: we shouldn't have alternate locations -- constants aren't stored
+    // anywhere, and stack slots should be either allocas (which shouldn't have
+    // alternate locations) or register spill locations (handled below in the
+    // register path)
+    assert(!AP.MF->hasSMOpLocations(IRSM, IROp) &&
+           "Unhandled duplicate locations");
+    ++Op;
     return ++MOI;
   }
 
   // The physical register number will ultimately be encoded as a DWARF regno.
   // The stack map also records the size of a spill slot that can hold the
-  // register content. (The runtime can track the actual size of the data type
-  // if it needs to.)
+  // register content, accurate to the actual size of the data type.
   if (MOI->isReg()) {
     // Skip implicit registers (this includes our scratch registers)
     if (MOI->isImplicit())
@@ -134,17 +283,18 @@ StackMaps::parseOperand(MachineInstr::const_mop_iterator MOI,
 
     assert(TargetRegisterInfo::isPhysicalRegister(MOI->getReg()) &&
            "Virtreg operands should have been rewritten before now.");
-    const TargetRegisterClass *RC = TRI->getMinimalPhysRegClass(MOI->getReg());
     assert(!MOI->getSubReg() && "Physical subreg still around.");
 
-    unsigned Offset = 0;
-    unsigned DwarfRegNum = getDwarfRegNum(MOI->getReg(), TRI);
-    unsigned LLVMRegNum = TRI->getLLVMRegNum(DwarfRegNum, false);
-    unsigned SubRegIdx = TRI->getSubRegIndex(LLVMRegNum, MOI->getReg());
-    if (SubRegIdx)
-      Offset = TRI->getSubRegIdxOffset(SubRegIdx);
+    size_t ValSize = DL.getTypeAllocSize(IROp->getType());
+    unsigned Offset, DwarfRegNum;
+    getRegLocation(MOI->getReg(), DwarfRegNum, Offset);
 
-    Locs.emplace_back(Location::Register, RC->getSize(), DwarfRegNum, Offset);
+    // Note: getPointerInfo() may have found a suitable alloca for this
+    // operand, but the backend didn't actually turn it into one.
+    Locs.emplace_back(Location::Register, ValSize, DwarfRegNum, Offset,
+                      isPtr, false, false, isTemporary, 0);
+    addDuplicateLocs(IRSM, IROp, Locs, ValSize, isPtr, false, 0);
+    ++Op;
     return ++MOI;
   }
 
@@ -161,6 +311,7 @@ void StackMaps::print(raw_ostream &OS) {
   for (const auto &CSI : CSInfos) {
     const LocationVec &CSLocs = CSI.Locations;
     const LiveOutVec &LiveOuts = CSI.LiveOuts;
+    const ArchValues &Values = CSI.Vals;
 
     OS << WSMP << "callsite " << CSI.ID << "\n";
     OS << WSMP << "  has " << CSLocs.size() << " locations\n";
@@ -194,7 +345,7 @@ void StackMaps::print(raw_ostream &OS) {
           OS << TRI->getName(Loc.Reg);
         else
           OS << Loc.Reg;
-        OS << "+" << Loc.Offset;
+        OS << " + " << Loc.Offset;
         break;
       case Location::Constant:
         OS << "Constant " << Loc.Offset;
@@ -203,8 +354,16 @@ void StackMaps::print(raw_ostream &OS) {
         OS << "Constant Index " << Loc.Offset;
         break;
       }
-      OS << "\t[encoding: .byte " << Loc.Type << ", .byte " << Loc.Size
-         << ", .short " << Loc.Reg << ", .int " << Loc.Offset << "]\n";
+      OS << ", pointer? " << Loc.Ptr << ", alloca? " << Loc.Alloca
+         << ", duplicate? " << Loc.Duplicate
+         << ", temporary? " << Loc.Temporary;
+
+      unsigned TypeAndFlags = TYPE_AND_FLAGS(Loc.Type, Loc.Ptr, Loc.Alloca,
+                                             Loc.Duplicate, Loc.Temporary);
+
+      OS << "\t[encoding: .byte " << TypeAndFlags << ", .byte " << Loc.Size
+         << ", .short " << Loc.Reg << ", .int " << Loc.Offset
+         << ", .uint " << Loc.AllocaSize << "]\n";
       Idx++;
     }
 
@@ -221,6 +380,87 @@ void StackMaps::print(raw_ostream &OS) {
          << LO.Size << "]\n";
       Idx++;
     }
+
+    OS << WSMP << "\thas " << Values.size() << " arch-specific live values\n";
+
+    Idx = 0;
+    for (const auto &V : Values) {
+      const Location &Loc = V.first;
+      const Operation &Op = V.second;
+
+      OS << WSMP << "\t\tArch-Val " << Idx << ": ";
+      switch(Loc.Type) {
+      case Location::Register:
+        OS << "Register ";
+        if (TRI)
+          OS << TRI->getName(Loc.Reg);
+        else
+          OS << Loc.Reg;
+        break;
+      case Location::Indirect:
+        OS << "Indirect ";
+        if (TRI)
+          OS << TRI->getName(Loc.Reg);
+        else
+          OS << Loc.Reg;
+        if (Loc.Offset)
+          OS << " + " << Loc.Offset;
+        break;
+      default:
+        OS << "<Unknown live value type>";
+        break;
+      }
+
+      OS << ", " << ValueGenInst::getInstName(Op.InstType) << " ";
+      switch(Op.OperandType) {
+      case Location::Register:
+        OS << "register ";
+        if (TRI)
+          OS << TRI->getName(Op.DwarfReg);
+        else
+          OS << Op.DwarfReg;
+        break;
+      case Location::Direct:
+        OS << "value stored at register ";
+        if (TRI)
+          OS << TRI->getName(Op.DwarfReg);
+        else
+          OS << Op.DwarfReg;
+        if (Op.Constant)
+          OS << " + " << Op.Constant;
+        break;
+      case Location::Indirect:
+        OS << "register";
+        if (TRI)
+          OS << TRI->getName(Op.DwarfReg);
+        else
+          OS << Op.DwarfReg;
+        if (Op.Constant)
+          OS << " + " << Op.Constant;
+        break;
+      case Location::Constant:
+        if(Op.isSymbol)
+          OS << "address of " << Op.Symbol->getName();
+        else {
+          OS << "immediate ";
+          OS.write_hex(Op.Constant);
+        }
+        break;
+      default:
+        OS << "<Unknown operand type>";
+        break;
+      }
+
+      unsigned TypeAndFlags = ARCH_TYPE_AND_FLAGS(Loc.Type, Loc.Ptr);
+      unsigned OpType = ARCH_OP_TYPE(Op.InstType,
+                                     Op.isGenerated,
+                                     Op.OperandType);
+      OS << "\t[encoding: .byte " << TypeAndFlags << ", .byte " << Loc.Size
+         << ", .short " << Loc.Reg << ", .int " << Loc.Offset
+         << ", .byte " << OpType << ", .byte " << Op.Size << ", .short "
+         << Op.DwarfReg << ", .int64 " << (Op.isSymbol ? 0 : Op.Constant)
+         << "]\n";
+    }
   }
 }
 
@@ -277,6 +517,145 @@ StackMaps::parseRegisterLiveOutMask(const uint32_t *Mask) const {
   return LiveOuts;
 }
 
+/// Convert a list of instructions used to generate an architecture-specific
+/// live value into multiple individual records.
+void StackMaps::genArchValsFromInsts(ArchValues &AV,
+                                     Location &Loc,
+                                     const MachineLiveVal &MLV) {
+  assert(MLV.isGenerated() && "Invalid live value type");
+
+  unsigned PtrSize = AP.MF->getDataLayout().getPointerSizeInBits() / 8;
+  const MachineGeneratedVal &MGV = (const MachineGeneratedVal &)MLV;
+  const ValueGenInstList &I = MGV.getInstructions();
+  const TargetRegisterInfo *TRI = AP.MF->getSubtarget().getRegisterInfo();
+  const TargetRegisterClass *RC;
+  Operation Op;
+  Op.isGenerated = true;
+
+  for(auto &Inst : I) {
+    const RegInstructionBase *RI;
+    const ImmInstructionBase *II;
+    const RefInstruction *RefI;
+
+    Op.DwarfReg = 0;
+    Op.Constant = 0;
+    Op.isSymbol = false;
+    Op.Symbol = nullptr;
+
+    Op.InstType = Inst->type();
+    switch(Inst->opType()) {
+    case ValueGenInst::OpType::Register:
+      RI = (const RegInstructionBase *)Inst.get();
+      assert(TRI->isPhysicalRegister(RI->getReg()) &&
+             "Virtual should have been converted to physical register");
+      RC = TRI->getMinimalPhysRegClass(RI->getReg());
+      Op.OperandType = Location::Register;
+      Op.Size = RC->getSize();
+      Op.DwarfReg = getDwarfRegNum(RI->getReg(), TRI);
+      break;
+    case ValueGenInst::OpType::Immediate:
+      II = (const ImmInstructionBase *)Inst.get();
+      Op.OperandType = Location::Constant;
+      Op.Size = II->getImmSize();
+      Op.Constant = II->getImm();
+      break;
+    case ValueGenInst::OpType::Reference:
+      RefI = (const RefInstruction *)Inst.get();
+      Op.OperandType = Location::Constant;
+      Op.Size = PtrSize;
+      Op.isSymbol = true;
+      Op.Symbol = RefI->getReference(AP);
+      break;
+    default: llvm_unreachable("Invalid operand type"); break;
+    }
+    AV.emplace_back(ArchValue(Loc, Op));
+  }
+}
+
+/// Add architecture-specific locations for the stackmap
+void StackMaps::addArchLiveVals(const CallInst *SM, ArchValues &AV) {
+  unsigned Offset, DwarfReg;
+  unsigned PtrSize = AP.MF->getDataLayout().getPointerSizeInBits() / 8;
+  const MachineFrameInfo *MFI = AP.MF->getFrameInfo();
+  const TargetRegisterInfo *TRI = AP.MF->getSubtarget().getRegisterInfo();
+
+  if(AP.MF->hasSMArchSpecificLocations(SM)) {
+    const ArchLiveValues &Vals = AP.MF->getSMArchSpecificLocations(SM);
+
+    for(auto &Val : Vals) {
+      Location Loc;
+      Operation Op;
+
+      Loc.Ptr = Val.second->isPtr();
+      Loc.Alloca = false;
+      Loc.Duplicate = false;
+      Loc.AllocaSize = 0;
+
+      // Parse the location
+      if(Val.first->isReg()) {
+        const MachineLiveReg &MR = (const MachineLiveReg &)*Val.first;
+        const TargetRegisterClass *RC =
+          TRI->getMinimalPhysRegClass(MR.getReg());
+        getRegLocation(MR.getReg(), DwarfReg, Offset);
+
+        Loc.Type = Location::Register;
+        Loc.Size = RC->getSize();
+        Loc.Reg = DwarfReg;
+        Loc.Offset = Offset;
+      }
+      else if(Val.first->isStackAddr()) {
+        MachineLiveStackAddr &MLSA = (MachineLiveStackAddr &)*Val.first;
+
+        Loc.Type = Location::Indirect;
+        Loc.Size = MLSA.getSize(AP);
+        Loc.Offset = MLSA.calcAndGetRegOffset(AP, DwarfReg);
+        Loc.Reg = getDwarfRegNum(DwarfReg, TRI);
+      }
+      else llvm_unreachable("Invalid architecture-specific live value");
+
+      // Parse the operation
+      Op.InstType = ValueGenInst::Set;
+      Op.isGenerated = false;
+      if(Val.second->isReference()) {
+        const MachineReference &MR = (const MachineReference &)*Val.second;
+        if(MR.isLoad()) Op.InstType = ValueGenInst::Load64;
+        Op.OperandType = Location::Constant;
+        Op.Size = PtrSize;
+        Op.isSymbol = true;
+        Op.Symbol = MR.getReference(AP);
+        AV.emplace_back(ArchValue(Loc, Op));
+      }
+      else if(Val.second->isStackObject()) {
+        const MachineStackObject &MSO = (const MachineStackObject &)*Val.second;
+        if(MSO.isLoad()) { // Loading a value from a stack slot
+          Op.OperandType = Location::Direct;
+          if(MSO.isCommonObject()) Op.Size = PtrSize;
+          else Op.Size = MFI->getObjectSize(MSO.getIndex());
+        }
+        else { // Generating a reference to a stack slot
+          Op.OperandType = Location::Indirect;
+          Op.Size = PtrSize;
+        }
+        Op.Constant = MSO.getOffsetFromReg(AP, DwarfReg);
+        Op.DwarfReg = getDwarfRegNum(DwarfReg, TRI);
+        Op.isSymbol = false;
+        AV.emplace_back(ArchValue(Loc, Op));
+      }
+      else if(Val.second->isImm()) {
+        const MachineImmediate &MI = (const MachineImmediate &)*Val.second;
+        Op.OperandType = Location::Constant;
+        Op.Size = MI.getSize();
+        Op.Constant = MI.getValue();
+        Op.isSymbol = false;
+        AV.emplace_back(ArchValue(Loc, Op));
+      }
+      else if(Val.second->isGenerated())
+        genArchValsFromInsts(AV, Loc, *Val.second);
+      else llvm_unreachable("Invalid architecture-specific live value");
+    }
+  }
+}
+
 void StackMaps::recordStackMapOpers(const MachineInstr &MI, uint64_t ID,
                                     MachineInstr::const_mop_iterator MOI,
                                     MachineInstr::const_mop_iterator MOE,
@@ -285,20 +664,52 @@ void StackMaps::recordStackMapOpers(const MachineInstr &MI, uint64_t ID,
   MCContext &OutContext = AP.OutStreamer->getContext();
   MCSymbol *MILabel = OutContext.createTempSymbol();
   AP.OutStreamer->EmitLabel(MILabel);
+  User::const_op_iterator Op = nullptr;
 
   LocationVec Locations;
   LiveOutVec LiveOuts;
+  ArchValues Constants;
 
   if (recordResult) {
     assert(PatchPointOpers(&MI).hasDef() && "Stackmap has no return value.");
     parseOperand(MI.operands_begin(), std::next(MI.operands_begin()), Locations,
-                 LiveOuts);
+                 LiveOuts, Op);
   }
 
+  // Find the IR stackmap instruction which corresponds to MI so we can emit
+  // type information along with the value's location
+  const BasicBlock *BB = MI.getParent()->getBasicBlock();
+  const IntrinsicInst *IRSM = nullptr;
+  const std::string SMName("llvm.experimental.stackmap");
+  for(auto BBI = BB->begin(), BBE = BB->end(); BBI != BBE; BBI++)
+  {
+    const IntrinsicInst *II;
+    if((II = dyn_cast<IntrinsicInst>(&*BBI)) &&
+       II->getCalledFunction()->getName() == SMName &&
+       cast<ConstantInt>(II->getArgOperand(0))->getZExtValue() == ID)
+    {
+      IRSM = cast<IntrinsicInst>(&*BBI);
+      break;
+    }
+  }
+  assert(IRSM && "Could not find associated stackmap instruction");
+
   // Parse operands.
+  unsigned NumRepeat;
+  Op = std::next(IRSM->op_begin(), 2);
+  MachineInstr::const_mop_iterator MFirst = MI.operands_begin();
   while (MOI != MOE) {
-    MOI = parseOperand(MOI, MOE, Locations, LiveOuts);
+    MOI = parseOperand(MOI, MOE, Locations, LiveOuts, Op);
+    NumRepeat = AP.MF->getNumLegalizedOps(ID, MOI - MFirst) - 1;
+    for(size_t i = 0; i < NumRepeat; i++) {
+      MOI = parseOperand(MOI, MOE, Locations, LiveOuts, Op);
+      --Op;
+    }
   }
+  assert(Op == (IRSM->op_end() - 1) && "did not lower all stackmap operands");
+
+  // Add architecture-specific live values
+  addArchLiveVals(IRSM, Constants);
 
   // Move large constants into the constant pool.
   for (auto &Loc : Locations) {
@@ -323,12 +734,21 @@ void StackMaps::recordStackMapOpers(const MachineInstr &MI, uint64_t ID,
 
   // Create an expression to calculate the offset of the callsite from function
   // entry.
-  const MCExpr *CSOffsetExpr = MCBinaryExpr::createSub(
+  // TODO for Popcorn, we actually want the return address of the call
+  // instruction to which this stackmap is attached.  However some backend
+  // writers, in their infinite wisdom, decided to abstract multiple assembly
+  // instructions into a single machine IR instruction (*ahem* PowerPC *ahem*).
+  // Generate an expression to correct for this "feature".
+  int RAOffset = AP.getCanonicalReturnAddr(MI.getPrevNode());
+  const MCExpr *RAFixup = MCBinaryExpr::createSub(
       MCSymbolRefExpr::create(MILabel, OutContext),
+      MCConstantExpr::create(RAOffset, OutContext), OutContext);
+  const MCExpr *CSOffsetExpr = MCBinaryExpr::createSub(RAFixup,
       MCSymbolRefExpr::create(AP.CurrentFnSymForSize, OutContext), OutContext);
 
-  CSInfos.emplace_back(CSOffsetExpr, ID, std::move(Locations),
-                       std::move(LiveOuts));
+  CSInfos.emplace_back(AP.CurrentFnSym, CSOffsetExpr, ID,
+                       std::move(Locations), std::move(LiveOuts),
+                       std::move(Constants));
 
   // Record the stack size of the current function.
   const MachineFrameInfo *MFI = AP.MF->getFrameInfo();
@@ -411,8 +831,11 @@ void StackMaps::emitStackmapHeader(MCStreamer &OS) {
 /// StkSizeRecord[NumFunctions] {
 ///   uint64 : Function Address
 ///   uint64 : Stack Size
+///   uint32 : Number of Unwinding Entries
+///   uint32 : Offset into Unwinding Section
 /// }
-void StackMaps::emitFunctionFrameRecords(MCStreamer &OS) {
+void StackMaps::emitFunctionFrameRecords(MCStreamer &OS,
+                                         const UnwindInfo *UI) {
   // Function Frame records.
   DEBUG(dbgs() << WSMP << "functions:\n");
   for (auto const &FR : FnStackSize) {
@@ -420,6 +843,15 @@ void StackMaps::emitFunctionFrameRecords(MCStreamer &OS) {
                  << " frame size: " << FR.second);
     OS.EmitSymbolValue(FR.first, 8);
     OS.EmitIntValue(FR.second, 8);
+
+    if(UI) {
+      const UnwindInfo::FuncUnwindInfo &FUI = UI->getUnwindInfo(FR.first);
+      DEBUG(dbgs() << " unwind info start: " << FUI.SecOffset
+                   << " (" << FUI.NumUnwindRecord << " entries)\n");
+      OS.EmitIntValue(FUI.NumUnwindRecord, 4);
+      OS.EmitIntValue(FUI.SecOffset, 4);
+    }
+    else OS.EmitIntValue(0, 8);
   }
 }
 
@@ -439,14 +871,20 @@ void StackMaps::emitConstantPoolEntries(MCStreamer &OS) {
 ///
 /// StkMapRecord[NumRecords] {
 ///   uint64 : PatchPoint ID
+///   uint32 : Index of Function Record
 ///   uint32 : Instruction Offset
 ///   uint16 : Reserved (record flags)
 ///   uint16 : NumLocations
 ///   Location[NumLocations] {
-///     uint8  : Register | Direct | Indirect | Constant | ConstantIndex
-///     uint8  : Size in Bytes
-///     uint16 : Dwarf RegNum
-///     int32  : Offset
+///     uint8 (4 bits) : Register | Direct | Indirect | Constant | ConstantIndex
+///     uint8 (1 bit)  : Is it a pointer?
+///     uint8 (1 bit)  : Is it an alloca?
+///     uint8 (1 bit)  : Is it a duplicate record for the same live value?
+///     uint8 (1 bit)  : Is it a temporary value created for the stackmap?
+///     uint8          : Size in Bytes
+///     uint16         : Dwarf RegNum
+///     int32          : Offset
+///     uint32         : Size of pointed-to alloca data
 ///   }
 ///   uint16 : Padding
 ///   uint16 : NumLiveOuts
@@ -455,6 +893,25 @@ void StackMaps::emitConstantPoolEntries(MCStreamer &OS) {
 ///     uint8  : Reserved
 ///     uint8  : Size in Bytes
 ///   }
+///   uint16 : Padding
+///   uint16 : NumArchValues
+///   ArchValues[NumArchValues] {
+///     Location {
+///       uint8 (4 bits) : Register | Indirect
+///       uint8 (3 bits) : Padding
+///       uint8 (1 bit)  : Is it a pointer?
+///       uint8          : Size in Bytes
+///       uint16         : Dwarf RegNum
+///       int32          : Offset
+///     }
+///     Value {
+///       uint8_t (4 bits) : Instruction
+///       uint8_t (4 bits) : Register | Direct | Constant
+///       uint8_t          : Size
+///       uint16_t         : Dwarf RegNum
+///       int64_t          : Offset or Constant
+///     }
+///   }
 ///   uint32 : Padding (only if required to align to 8 byte)
 /// }
 ///
@@ -470,23 +927,29 @@ void StackMaps::emitCallsiteEntries(MCStreamer &OS) {
   for (const auto &CSI : CSInfos) {
     const LocationVec &CSLocs = CSI.Locations;
     const LiveOutVec &LiveOuts = CSI.LiveOuts;
+    const ArchValues &Values = CSI.Vals;
 
     // Verify stack map entry. It's better to communicate a problem to the
     // runtime than crash in case of in-process compilation. Currently, we do
     // simple overflow checks, but we may eventually communicate other
     // compilation errors this way.
-    if (CSLocs.size() > UINT16_MAX || LiveOuts.size() > UINT16_MAX) {
+    if (CSLocs.size() > UINT16_MAX || LiveOuts.size() > UINT16_MAX ||
+        Values.size() > UINT16_MAX) {
       OS.EmitIntValue(UINT64_MAX, 8); // Invalid ID.
+      OS.EmitIntValue(UINT32_MAX, 4); // Invalid index.
       OS.EmitValue(CSI.CSOffsetExpr, 4);
       OS.EmitIntValue(0, 2); // Reserved.
       OS.EmitIntValue(0, 2); // 0 locations.
       OS.EmitIntValue(0, 2); // padding.
       OS.EmitIntValue(0, 2); // 0 live-out registers.
+      OS.EmitIntValue(0, 2); // padding.
+      OS.EmitIntValue(0, 2); // 0 arch-specific values.
       OS.EmitIntValue(0, 4); // padding.
       continue;
     }
 
     OS.EmitIntValue(CSI.ID, 8);
+    OS.EmitIntValue(FnStackSize.find(CSI.Func) - FnStackSize.begin(), 4);
     OS.EmitValue(CSI.CSOffsetExpr, 4);
 
     // Reserved for flags.
@@ -494,10 +957,14 @@ void StackMaps::emitCallsiteEntries(MCStreamer &OS) {
     OS.EmitIntValue(CSLocs.size(), 2);
 
     for (const auto &Loc : CSLocs) {
-      OS.EmitIntValue(Loc.Type, 1);
+      uint8_t TypeAndFlags =
+        TYPE_AND_FLAGS(Loc.Type, Loc.Ptr, Loc.Alloca,
+                       Loc.Duplicate, Loc.Temporary);
+      OS.EmitIntValue(TypeAndFlags, 1);
       OS.EmitIntValue(Loc.Size, 1);
       OS.EmitIntValue(Loc.Reg, 2);
       OS.EmitIntValue(Loc.Offset, 4);
+      OS.EmitIntValue(Loc.AllocaSize, 4);
     }
 
     // Num live-out registers and padding to align to 4 byte.
@@ -509,13 +976,38 @@ void StackMaps::emitCallsiteEntries(MCStreamer &OS) {
       OS.EmitIntValue(0, 1);
       OS.EmitIntValue(LO.Size, 1);
     }
+
+    // Num arch-specific constants and padding to align to 4 bytes.
+    OS.EmitIntValue(0, 2);
+    OS.EmitIntValue(Values.size(), 2);
+
+    for (const auto &C : Values) {
+      const Location &Loc = C.first;
+      const Operation &Op = C.second;
+
+      uint8_t TypeAndFlags = ARCH_TYPE_AND_FLAGS(Loc.Type, Loc.Ptr);
+      OS.EmitIntValue(TypeAndFlags, 1);
+      OS.EmitIntValue(Loc.Size, 1);
+      OS.EmitIntValue(Loc.Reg, 2);
+      OS.EmitIntValue(Loc.Offset, 4);
+
+      uint8_t OpType = ARCH_OP_TYPE(Op.InstType,
+                                    Op.isGenerated,
+                                    Op.OperandType);
+      OS.EmitIntValue(OpType, 1);
+      OS.EmitIntValue(Op.Size, 1);
+      OS.EmitIntValue(Op.DwarfReg, 2);
+      if(Op.isSymbol) OS.EmitSymbolValue(Op.Symbol, 8);
+      else OS.EmitIntValue(Op.Constant, 8);
+    }
+
     // Emit alignment to 8 byte.
     OS.EmitValueToAlignment(8);
   }
 }
 
 /// Serialize the stackmap data.
-void StackMaps::serializeToStackMapSection() {
+void StackMaps::serializeToStackMapSection(const UnwindInfo *UI) {
   (void)WSMP;
   // Bail out if there's no stack map data.
   assert((!CSInfos.empty() || (CSInfos.empty() && ConstPool.empty())) &&
@@ -539,7 +1031,7 @@ void StackMaps::serializeToStackMapSection() {
   // Serialize data.
   DEBUG(dbgs() << "********** Stack Map Output **********\n");
   emitStackmapHeader(OS);
-  emitFunctionFrameRecords(OS);
+  emitFunctionFrameRecords(OS, UI);
   emitConstantPoolEntries(OS);
   emitCallsiteEntries(OS);
   OS.AddBlankLine();
diff --git a/llvm/lib/CodeGen/StackSlotColoring.cpp b/llvm/lib/CodeGen/StackSlotColoring.cpp
index a5a175f2c8f..264e0f20d04 100644
--- a/llvm/lib/CodeGen/StackSlotColoring.cpp
+++ b/llvm/lib/CodeGen/StackSlotColoring.cpp
@@ -278,6 +278,7 @@ bool StackSlotColoring::ColorSlots(MachineFunction &MF) {
   SmallVector<int, 16> SlotMapping(NumObjs, -1);
   SmallVector<float, 16> SlotWeights(NumObjs, 0.0);
   SmallVector<SmallVector<int, 4>, 16> RevMap(NumObjs);
+  SmallDenseMap<int, int, 16> SlotChanges;
   BitVector UsedColors(NumObjs);
 
   DEBUG(dbgs() << "Color spill slot intervals:\n");
@@ -292,7 +293,9 @@ bool StackSlotColoring::ColorSlots(MachineFunction &MF) {
     SlotWeights[NewSS] += li->weight;
     UsedColors.set(NewSS);
     Changed |= (SS != NewSS);
+    if(SS != NewSS) SlotChanges[SS] = NewSS;
   }
+  MF.updateSMStackSlotRefs(SlotChanges);
 
   DEBUG(dbgs() << "\nSpill slots after coloring:\n");
   for (unsigned i = 0, e = SSIntervals.size(); i != e; ++i) {
diff --git a/llvm/lib/CodeGen/StackTransformMetadata.cpp b/llvm/lib/CodeGen/StackTransformMetadata.cpp
new file mode 100644
index 00000000000..279ebc221b1
--- /dev/null
+++ b/llvm/lib/CodeGen/StackTransformMetadata.cpp
@@ -0,0 +1,1391 @@
+//=== llvm/CodeGen/StackTransformMetadata.cpp - Stack Transformation Metadata ===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file accumulates additional data from machine functions needed to do
+// correct and complete stack transformation.
+//
+// Note: the dataflow analysis in this implementation assumes the ISA does not
+// allow memory-to-memory copies.
+//
+//===----------------------------------------------------------------------===//
+
+#include <queue>
+#include "llvm/CodeGen/LiveIntervalAnalysis.h"
+#include "llvm/CodeGen/LiveStackAnalysis.h"
+#include "llvm/CodeGen/MachineFrameInfo.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/MachineInstrBuilder.h"
+#include "llvm/CodeGen/MachineMemOperand.h"
+#include "llvm/CodeGen/MachineRegisterInfo.h"
+#include "llvm/CodeGen/Passes.h"
+#include "llvm/CodeGen/PseudoSourceValue.h"
+#include "llvm/CodeGen/StackMaps.h"
+#include "llvm/CodeGen/StackTransformTypes.h"
+#include "llvm/CodeGen/VirtRegMap.h"
+#include "llvm/IR/DiagnosticInfo.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/LLVMContext.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/Target/TargetInstrInfo.h"
+#include "llvm/Target/TargetValues.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "stacktransform"
+
+static cl::opt<bool>
+NoWarnings("no-sm-warn", cl::desc("Don't issue warnings about stackmaps"),
+           cl::init(false), cl::Hidden);
+
+//===----------------------------------------------------------------------===//
+//                          StackTransformMetadata
+//===----------------------------------------------------------------------===//
+//
+// Run analyses over machine functions (before virtual register rewriting) to
+// glean additional information about live values.  This analysis finds
+// duplicate locations for live values (including backing stack slots and other
+// registers) and architecture-specific live values that must be materialized.
+//
+//===----------------------------------------------------------------------===//
+namespace {
+class StackTransformMetadata : public MachineFunctionPass {
+
+  /* Types */
+
+  /// A bundle tying together a stackmap IR instruction, the generated stackmap
+  /// machine instruction and the call machine instruction that caused the
+  /// stackmap to be emitted in the IR, respectively
+  typedef std::tuple<const CallInst *,
+                     MachineInstr *,
+                     const MachineInstr *> SMInstBundle;
+
+  /// Getters for individual elements of instruction bundles
+  static inline const
+  CallInst *getIRSM(const SMInstBundle &B) { return std::get<0>(B); }
+  static inline
+  MachineInstr *getMISM(const SMInstBundle &B) { return std::get<1>(B); }
+  static inline const
+  MachineInstr *getMICall(const SMInstBundle &B) { return std::get<2>(B); }
+
+  /// A vector of IR values.  Used when mapping from registers/stack slots to
+  /// IR values.
+  typedef SmallVector<const Value *, 4> ValueVec;
+  typedef std::shared_ptr<ValueVec> ValueVecPtr;
+
+  /// Mapping between virtual registers and IR operands
+  typedef std::pair<unsigned, ValueVecPtr> RegValsPair;
+  typedef std::map<unsigned, ValueVecPtr> RegValsMap;
+
+  /// Mapping between stackmaps and virtual registers referenced by the stackmap
+  typedef std::pair<const MachineInstr *, RegValsMap> SMRegPair;
+  typedef std::map<const MachineInstr *, RegValsMap> SMRegMap;
+
+  /// Mapping between stack slots and IR operands
+  typedef std::pair<int, ValueVecPtr> StackValsPair;
+  typedef std::map<int, ValueVecPtr> StackValsMap;
+
+  /// Mapping between stackmaps and stack slots referenced by the stackmap
+  typedef std::pair<const MachineInstr *, StackValsMap> SMStackSlotPair;
+  typedef std::map<const MachineInstr *, StackValsMap> SMStackSlotMap;
+
+  /// A value's spill location
+  class CopyLoc {
+  public:
+    enum Type { NONE, VREG, STACK_LOAD, STACK_STORE };
+    unsigned Vreg;
+    const MachineInstr *Instr;
+    CopyLoc() : Vreg(VirtRegMap::NO_PHYS_REG), Instr(nullptr) {}
+    CopyLoc(unsigned Vreg, const MachineInstr *Instr) :
+      Vreg(Vreg), Instr(Instr) {}
+    virtual CopyLoc *copy() const = 0;
+    virtual ~CopyLoc() {}
+    virtual Type getType() const = 0;
+  };
+  typedef std::shared_ptr<CopyLoc> CopyLocPtr;
+
+  /// A spill to a stack slot
+  class StackCopyLoc : public CopyLoc {
+  public:
+    int StackSlot;
+    StackCopyLoc() : StackSlot(VirtRegMap::NO_STACK_SLOT) {}
+    StackCopyLoc(unsigned Vreg, int StackSlot, const MachineInstr *Instr) :
+      CopyLoc(Vreg, Instr), StackSlot(StackSlot) {}
+    virtual CopyLoc *copy() const = 0;
+    virtual Type getType() const = 0;
+  };
+
+  /// A load from a stack slot
+  class StackLoadLoc : public StackCopyLoc {
+  public:
+    StackLoadLoc() {}
+    StackLoadLoc(unsigned Vreg, int StackSlot, const MachineInstr *Instr) :
+      StackCopyLoc(Vreg, StackSlot, Instr) {}
+    virtual CopyLoc *copy() const
+    { return new StackLoadLoc(Vreg, StackSlot, Instr); }
+    virtual Type getType() const { return CopyLoc::STACK_LOAD; }
+  };
+
+  /// A store to a stack slot
+  class StackStoreLoc : public StackCopyLoc {
+  public:
+    StackStoreLoc() {}
+    StackStoreLoc(unsigned Vreg, int StackSlot, const MachineInstr *Instr) :
+      StackCopyLoc(Vreg, StackSlot, Instr) {}
+    virtual CopyLoc *copy() const
+    { return new StackStoreLoc(Vreg, StackSlot, Instr); }
+    virtual Type getType() const { return CopyLoc::STACK_STORE; }
+  };
+
+  /// A spill to another register
+  class RegCopyLoc : public CopyLoc {
+  public:
+    unsigned SrcVreg;
+    RegCopyLoc() : SrcVreg(VirtRegMap::NO_PHYS_REG) {}
+    RegCopyLoc(unsigned DefVreg, unsigned SrcVreg, const MachineInstr *Instr) :
+      CopyLoc(DefVreg, Instr), SrcVreg(SrcVreg) {}
+    virtual CopyLoc *copy() const
+    { return new RegCopyLoc(Vreg, SrcVreg, Instr); }
+    virtual Type getType() const { return CopyLoc::VREG; }
+  };
+
+  /// Mapping between stack slots and copy locations (e.g., load from or store
+  /// to the stack slot)
+  typedef SmallVector<CopyLocPtr, 8> CopyLocVec;
+  typedef std::shared_ptr<CopyLocVec> CopyLocVecPtr;
+  typedef std::pair<int, CopyLocVecPtr> StackSlotCopyPair;
+  typedef std::map<int, CopyLocVecPtr> StackSlotCopies;
+
+  /// A work item to analyze in dataflow analysis.  Can selectively enable
+  /// traversing definitions.
+  struct WorkItem {
+    WorkItem() : Vreg(0), TraverseDefs(false) {}
+    WorkItem(unsigned Vreg, bool TraverseDefs)
+      : Vreg(Vreg), TraverseDefs(TraverseDefs) {}
+
+    unsigned Vreg;
+    bool TraverseDefs;
+  };
+
+  /* Data */
+
+  /// LLVM-provided analysis & metadata
+  MachineFunction *MF;
+  const MachineFrameInfo *MFI;
+  const MachineRegisterInfo *MRI;
+  const TargetInstrInfo *TII;
+  const TargetRegisterInfo *TRI;
+  const TargetValues *TVG;
+  LiveIntervals *LI;
+  const LiveStacks *LS;
+  const SlotIndexes *Indexes;
+  const VirtRegMap *VRM;
+
+  /// Stackmap/call instructions, mapping of virtual registers & stack slots to
+  /// IR values, stack slots used in the function, list of instructions that
+  /// copy to/from the stack
+  SmallVector<SMInstBundle, 32> SM;
+  SMRegMap SMRegs;
+  SMStackSlotMap SMStackSlots;
+  SmallSet<int, 32> UsedSS;
+  StackSlotCopies SSCopies;
+
+  /* Functions */
+
+  // Reset the analysis for a new function
+  void reset() {
+    SM.clear();
+    SMRegs.clear();
+    SMStackSlots.clear();
+    UsedSS.clear();
+    SSCopies.clear();
+  }
+
+  /// Print information about a virtual register and it's associated IR value
+  void dumpReg(unsigned Reg, const Value *IRVal) const;
+
+  /// Print information about a stack slot and it's associated IR value
+  void dumpStackSlot(int SS, const Value *IRVal) const;
+
+  /// Analyze a machine instruction to see if a value is getting copied from
+  /// another location such as a stack slot or register.
+  CopyLocPtr getCopyLocation(const MachineInstr *MI) const;
+
+  /// Gather stackmap machine instructions, the IR instructions which generated
+  /// the stackmaps, and their associated call machine instructions.  Also,
+  /// find copies to/from stack slots (since there's no other mechanism to
+  /// find/traverse them).
+  void findStackmapsAndStackSlotCopies();
+
+  /// Find all virtual register/stack slot operands in a stackmap and collect
+  /// virtual register/stack slot <-> IR value mappings
+  void mapOpsToIR(const CallInst *IRSM, const MachineInstr *MISM);
+
+  /// Extend the live range for a register to include an instruction.
+  void updateRegisterLiveInterval(MachineOperand &Src,
+                                  const MachineInstr *Inst);
+
+  /// Rather than modifying the backend machinery to prevent hoisting code
+  /// between the stackmap and call site, unwind instructions in order to get
+  /// real live value locations at the function call.
+  bool unwindToCallSite(MachineInstr *SM, const MachineInstr *Call);
+
+  /// Is a virtual register live across the machine instruction?
+  /// Note: returns false if the MI is the last instruction for which the
+  /// virtual register is alive
+  bool isVregLiveAcrossInstr(unsigned Vreg, const MachineInstr *MI) const;
+
+  /// Is a stack slot live across the machine instruction?
+  /// Note: returns false if the MI is the last instruction for which the stack
+  /// slot is alive
+  bool isSSLiveAcrossInstr(int SS, const MachineInstr *MI) const;
+
+  /// Add duplicate location information for a virtual register.  Return true
+  /// if metadata was added, or false if the virtual register is not live
+  /// across the call instruction/stackmap.
+  bool addVregMetadata(unsigned Vreg,
+                       ValueVecPtr IRVals,
+                       const SMInstBundle &SM);
+
+  /// Add duplicate location information for a stack slot.  Return true if
+  /// metadata was added, or false if the stack slot is not live across the
+  /// call instruction/stackmap.
+  bool addSSMetadata(int SS, ValueVecPtr IRVals, const SMInstBundle &SM);
+
+  /// Search stack slot copies for additional virtual registers which are live
+  /// across the stackmap.  Will check to see if the copy instructions have
+  /// already been visited, and if appropriate, will add virtual registers to
+  /// work queue.
+  void inline
+  searchStackSlotCopies(int SS,
+                        ValueVecPtr IRVals,
+                        const SMInstBundle &SM,
+                        SmallPtrSet<const MachineInstr *, 32> &Visited,
+                        std::queue<WorkItem> &work,
+                        bool TraverseDefs);
+
+  /// Find all alternate locations for virtual registers in a stackmap, and add
+  /// them to the metadata to be generated.
+  void findAlternateVregLocs(const SMInstBundle &SM);
+
+  /// Find stackmap operands that have been spilled to alternate locations
+  bool findAlternateOpLocs();
+
+  /// Ensure virtual registers used to generate architecture-specific values
+  /// are handled by the stackmap & convert to physical registers
+  void sanitizeVregs(MachineLiveValPtr &LV, const MachineInstr *SM) const;
+
+  /// Find architecture-specific live values added by the backend
+  void findArchSpecificLiveVals();
+
+  /// Warn about unhandled registers & stack slots
+  void warnUnhandled() const;
+
+public:
+  static char ID;
+  static const std::string SMName;
+
+  StackTransformMetadata() : MachineFunctionPass(ID) {}
+
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const override;
+
+  virtual bool runOnMachineFunction(MachineFunction&) override;
+
+};
+
+} // end anonymous namespace
+
+char &llvm::StackTransformMetadataID = StackTransformMetadata::ID;
+const std::string StackTransformMetadata::SMName("llvm.experimental.stackmap");
+
+INITIALIZE_PASS_BEGIN(StackTransformMetadata, "stacktransformmetadata",
+  "Gather stack transformation metadata", false, false)
+INITIALIZE_PASS_DEPENDENCY(SlotIndexes)
+INITIALIZE_PASS_DEPENDENCY(LiveIntervals)
+INITIALIZE_PASS_DEPENDENCY(LiveStacks)
+INITIALIZE_PASS_DEPENDENCY(VirtRegMap)
+INITIALIZE_PASS_END(StackTransformMetadata, "stacktransformmetadata",
+  "Gather stack transformation metadata", false, false)
+
+char StackTransformMetadata::ID = 0;
+
+void StackTransformMetadata::getAnalysisUsage(AnalysisUsage &AU) const {
+  AU.setPreservesAll();
+  AU.addRequired<LiveIntervals>();
+  AU.addRequired<LiveStacks>();
+  AU.addRequired<SlotIndexes>();
+  AU.addRequired<VirtRegMap>();
+  MachineFunctionPass::getAnalysisUsage(AU);
+}
+
+bool StackTransformMetadata::runOnMachineFunction(MachineFunction &fn) {
+  bool Changed = false;
+
+  if(fn.getFrameInfo()->hasStackMap()) {
+    MF = &fn;
+    MFI = MF->getFrameInfo();
+    MRI = &MF->getRegInfo();
+    TII = MF->getSubtarget().getInstrInfo();
+    TRI = MF->getSubtarget().getRegisterInfo();
+    TVG = MF->getSubtarget().getValues();
+    Indexes = &getAnalysis<SlotIndexes>();
+    LI = &getAnalysis<LiveIntervals>();
+    LS = &getAnalysis<LiveStacks>();
+    VRM = &getAnalysis<VirtRegMap>();
+    reset();
+
+    DEBUG(
+      dbgs() << "\n********** STACK TRANSFORMATION METADATA **********\n"
+             << "********** Function: " << MF->getName() << "\n";
+      VRM->dump();
+    );
+
+    findStackmapsAndStackSlotCopies();
+    Changed = findAlternateOpLocs();
+    findArchSpecificLiveVals();
+    if(!NoWarnings) warnUnhandled();
+  }
+
+  return Changed;
+}
+
+/// Print information about a virtual register and it's associated IR value
+void StackTransformMetadata::dumpReg(unsigned Reg, const Value *IRVal) const {
+  if(IRVal) IRVal->printAsOperand(dbgs());
+  if(TargetRegisterInfo::isPhysicalRegister(Reg))
+    dbgs() << ": in register " << PrintReg(Reg, TRI);
+  else {
+    assert(VRM->hasPhys(Reg) && "Invalid virtual register");
+    unsigned Phys = VRM->getPhys(Reg);
+    dbgs() << ": in register " << PrintReg(Phys, TRI)
+           << " (vreg " << TargetRegisterInfo::virtReg2Index(Reg) << ")";
+  }
+  dbgs() << "\n";
+}
+
+/// Print information about a stack slot and it's associated IR value
+void StackTransformMetadata::dumpStackSlot(int SS, const Value *IRVal) const {
+  assert(!MFI->isDeadObjectIndex(SS) && "Invalid stack slot");
+  if(IRVal) IRVal->printAsOperand(dbgs());
+  dbgs() << ": in stack slot " << SS << " (size: " << MFI->getObjectSize(SS)
+         << ")\n";
+}
+
+/// Analyze a machine instruction to see if a value is getting copied from
+/// another location such as a stack slot or register.
+StackTransformMetadata::CopyLocPtr
+StackTransformMetadata::getCopyLocation(const MachineInstr *MI) const {
+  unsigned SrcVreg = 0;
+  unsigned DefVreg = 0;
+  int SS;
+
+  assert(MI && "Invalid machine instruction");
+
+  // Is it a copy from another register?
+  if(MI->isCopyLike()) {
+    for(unsigned i = 0, e = MI->getNumOperands(); i != e; i++) {
+      const MachineOperand &MO = MI->getOperand(i);
+      if(MO.isReg()) {
+        if(MO.isDef()) DefVreg = MO.getReg();
+        else SrcVreg = MO.getReg();
+      }
+    }
+
+    // TODO does it have to be a virtual register or can it be a physical one?
+    // Liveness analysis seems to apply only to virtual registers.
+    if(TargetRegisterInfo::isVirtualRegister(SrcVreg) &&
+       TargetRegisterInfo::isVirtualRegister(DefVreg))
+      return CopyLocPtr(new RegCopyLoc(DefVreg, SrcVreg, MI));
+  }
+
+  // Is it a load from the stack?
+  if((DefVreg = TII->isLoadFromStackSlot(MI, SS)) &&
+     TargetRegisterInfo::isVirtualRegister(DefVreg))
+    return CopyLocPtr(new StackLoadLoc(DefVreg, SS, MI));
+
+  // Is it a store to the stack?
+  if((SrcVreg = TII->isStoreToStackSlot(MI, SS)) &&
+     TargetRegisterInfo::isVirtualRegister(SrcVreg))
+    return CopyLocPtr(new StackStoreLoc(SrcVreg, SS, MI));
+
+  // A non-copylike instruction
+  return CopyLocPtr(nullptr);
+}
+
+/// Gather stackmap machine instructions, the IR instructions which generated
+/// the stackmaps, and their associated call machine instructions.  Also,
+/// find copies to/from stack slots (since there's no other mechanism to
+/// find/traverse them).
+void StackTransformMetadata::findStackmapsAndStackSlotCopies() {
+  for(auto MBB = MF->begin(), MBBE = MF->end(); MBB != MBBE; MBB++) {
+    for(auto MI = MBB->instr_begin(), ME = MBB->instr_end(); MI != ME; MI++) {
+      if(MI->getOpcode() == TargetOpcode::STACKMAP) {
+        // Find the stackmap IR instruction
+        assert(MI->getOperand(0).isImm() && "Invalid stackmap ID");
+        int64_t ID = MI->getOperand(0).getImm();
+        const BasicBlock *BB = MI->getParent()->getBasicBlock();
+        const CallInst *IRSM = nullptr;
+        for(auto I = BB->begin(), IE = BB->end(); I != IE; I++)
+        {
+          const IntrinsicInst *II;
+          if((II = dyn_cast<IntrinsicInst>(&*I)) &&
+             II->getCalledFunction()->getName() == SMName &&
+             cast<ConstantInt>(II->getArgOperand(0))->getSExtValue() == ID) {
+            IRSM = cast<CallInst>(II);
+            break;
+          }
+        }
+        assert(IRSM && "Could not find stackmap IR instruction");
+
+        // Find the call instruction
+        const MachineInstr *MCI = MI->getPrevNode();
+        while(MCI != nullptr) {
+          if(MCI->isCall()) {
+            if(MCI->getOpcode() == TargetOpcode::STACKMAP)
+              MCI = nullptr;
+            break;
+          }
+          MCI = MCI->getPrevNode();
+        }
+
+        if(!MCI) {
+          DEBUG(dbgs() << "NOTE: stackmap " << ID << " ";
+                IRSM->print(dbgs());
+                dbgs() << ": could not find associated call instruction "
+                          "(lowered to a native instruction?)\n");
+          continue;
+        }
+
+        SM.push_back(SMInstBundle(IRSM, &*MI, MCI));
+      }
+      else {
+        // Record all stack slots that are actually used.  Note that this is
+        // necessary because analysis maintained in MachineFrameInfo/LiveStacks
+        // may denote stack slots as live even though register allocation
+        // actually all references to them.
+        const PseudoSourceValue *PSV;
+        const FixedStackPseudoSourceValue *FI;
+        for(auto MemOp : MI->memoperands()) {
+          PSV = MemOp->getPseudoValue();
+          if(PSV && PSV->isFixed) {
+            FI = cast<FixedStackPseudoSourceValue>(PSV);
+            UsedSS.insert(FI->getFrameIndex());
+          }
+        }
+
+        // See if instruction copies to/from stack slot
+        StackSlotCopies::iterator it;
+        CopyLocPtr loc;
+        if(!(loc = getCopyLocation(&*MI))) continue;
+        enum CopyLoc::Type type = loc->getType();
+        if(type == CopyLoc::STACK_LOAD || type == CopyLoc::STACK_STORE) {
+          StackCopyLoc *SCL = (StackCopyLoc *)loc.get();
+          if((it = SSCopies.find(SCL->StackSlot)) == SSCopies.end())
+            it = SSCopies.emplace(SCL->StackSlot,
+                                  CopyLocVecPtr(new CopyLocVec)).first;
+          it->second->push_back(loc);
+        }
+      }
+    }
+  }
+
+  DEBUG(
+    dbgs() << "\n*** Stack slot copies ***\n\n";
+    for(auto SC = SSCopies.begin(), SCe = SSCopies.end(); SC != SCe; SC++) {
+      dbgs() << "Stack slot " << SC->first << ":\n";
+      for(size_t i = 0, e = SC->second->size(); i < e; i++) {
+        (*SC->second)[i]->Instr->dump();
+      }
+    }
+  );
+}
+
+/// Find all virtual register/stack slot operands in a stackmap and collect
+/// virtual register/stack slot <-> IR value mappings
+void StackTransformMetadata::mapOpsToIR(const CallInst *IRSM,
+                                        const MachineInstr *MISM) {
+  RegValsMap::iterator RegIt;
+  StackValsMap::iterator SSIt;
+  MachineInstr::const_mop_iterator MOit;
+  int64_t SMID = cast<ConstantInt>(IRSM->getArgOperand(0))->getSExtValue();
+  unsigned NumMO;
+
+  // Initialize new storage location/IR map objects (i.e., for virtual
+  // registers & stack slots) for the stackmap
+  SMRegs.emplace(MISM, RegValsMap());
+  SMStackSlots.emplace(MISM, StackValsMap());
+
+  // Loop over all operands
+  MOit = std::next(MISM->operands_begin(), 2);
+  for(size_t i = 2; i < IRSM->getNumArgOperands(); i++) {
+    const Value *IRVal = IRSM->getArgOperand(i);
+    assert(IRVal && "Invalid stackmap IR operand");
+
+    // Legalization may have changed how many machine operands map to the IR
+    // value.  Loop over all relevant machine operands.
+    NumMO = MF->getNumLegalizedOps(SMID, MOit - MISM->operands_begin());
+    for(size_t j = 0; j < NumMO; j++) {
+      if(MOit->isImm()) { // Map IR values to stack slots
+        int FrameIdx = INT32_MAX;
+        switch(MOit->getImm()) {
+        case StackMaps::DirectMemRefOp:
+          MOit++;
+          assert(MOit->isFI() && "Invalid operand type");
+          FrameIdx = MOit->getIndex();
+          MOit = std::next(MOit, 2);
+          break;
+        case StackMaps::IndirectMemRefOp:
+          MOit = std::next(MOit, 2);
+          assert(MOit->isFI() && "Invalid operand type");
+          FrameIdx = MOit->getIndex();
+          MOit = std::next(MOit, 2);
+          break;
+        case StackMaps::ConstantOp: MOit = std::next(MOit, 2); continue;
+        default: llvm_unreachable("Unrecognized stackmap operand type"); break;
+        }
+
+        assert(MFI->getObjectIndexBegin() <= FrameIdx &&
+               FrameIdx <= MFI->getObjectIndexEnd() && "Invalid frame index");
+        assert(!MFI->isDeadObjectIndex(FrameIdx) && "Dead frame index");
+        DEBUG(dumpStackSlot(FrameIdx, IRVal););
+
+        // Update the list of IR values mapped to the stack slot (multiple IR
+        // values may be mapped to a single stack slot).
+        SSIt = SMStackSlots[MISM].find(FrameIdx);
+        if(SSIt == SMStackSlots[MISM].end())
+          SSIt = SMStackSlots[MISM].emplace(FrameIdx,
+                                            ValueVecPtr(new ValueVec)).first;
+        SSIt->second->push_back(IRVal);
+      }
+      else if(MOit->isReg()) { // Map IR values to virtual registers
+        unsigned Reg = MOit->getReg();
+        MOit++;
+
+        DEBUG(dumpReg(Reg, IRVal););
+
+        // Update the list of IR values mapped to the virtual register
+        // (multiple IR values may be mapped to a single virtual register).
+        RegIt = SMRegs[MISM].find(Reg);
+        if(RegIt == SMRegs[MISM].end())
+          RegIt = SMRegs[MISM].emplace(Reg, ValueVecPtr(new ValueVec)).first;
+        RegIt->second->push_back(IRVal);
+      } else {
+        llvm_unreachable("Unrecognized stackmap operand type.");
+      }
+    }
+  }
+}
+
+/// Extend the live range for a register to include an instruction.
+void
+StackTransformMetadata::updateRegisterLiveInterval(MachineOperand &Src,
+                                                   const MachineInstr *SM) {
+  typedef LiveInterval::Segment Segment;
+
+  assert(Src.isReg() && "Cannot update live range for non-register operand");
+
+  unsigned Vreg = Src.getReg();
+  bool hasRegUnit = false;
+  SlotIndex Slots[2] = {
+    Indexes->getInstructionIndex(Src.getParent()).getRegSlot(),
+    Indexes->getInstructionIndex(SM).getRegSlot()
+  };
+
+  // Find the segment ending at or containing the call instruction.  Note that
+  // we search using the insruction's base index, as the interval may end at
+  // the register index (and the end of the range is non-inclusive).
+  LiveInterval &Reg = LI->getInterval(Vreg);
+  LiveInterval::iterator Seg = Reg.find(Slots[0].getBaseIndex());
+  assert(Seg != Reg.end() && Seg->contains(Slots[0].getBaseIndex()) &&
+         "Invalid live interval");
+
+  if(Seg->end < Slots[1]) {
+    // Update the segment to include the stackmap
+    Seg = Reg.addSegment(Segment(Seg->start, Slots[1], Seg->valno));
+    DEBUG(dbgs() << "    -> Updated register live interval: "; Seg->dump());
+
+    // We also need to update the physical register's register unit (RU) live
+    // range because LiveIntervals::addKillFlags() will use the RU's live range
+    // to avoid marking a physical register dead if two virtual registers
+    // (mapped to that physical register) have overlapping live ranges.
+    MCRegUnitIterator Outer(VRM->getPhys(Vreg), TRI);
+    for(MCRegUnitIterator Unit(Outer); Unit.isValid(); ++Unit) {
+      LiveRange &RURange = LI->getRegUnit(*Unit);
+      LiveRange::iterator RUS;
+
+      for(size_t i = 0; i < 2; i++, RUS = RURange.end()) {
+        RUS = RURange.find(Slots[i]);
+        if(RUS != RURange.end() && RUS->contains(Slots[i])) break;
+      }
+
+      if(RUS != RURange.end()) {
+        hasRegUnit = true;
+        Seg = RURange.addSegment(
+          Segment(RUS->start, Slots[1].getNextIndex(), RUS->valno));
+        DEBUG(
+          dbgs() << "    -> Updated segment for register unit "
+                 << *Unit << ": ";
+          Seg->dump();
+        );
+        break;
+      }
+    }
+
+    // If we can't extend one of the current RU ranges, add a new range.
+    if(!hasRegUnit) {
+      LiveRange &RURange = LI->getRegUnit(*Outer);
+      VNInfo *Valno = RURange.getNextValue(Slots[0], LI->getVNInfoAllocator());
+      Seg = RURange.addSegment(
+        Segment(Slots[0], Slots[1].getNextIndex(), Valno));
+      DEBUG(
+        dbgs() << "    -> Added segment for register unit "
+               << *Outer << ": ";
+        Seg->dump();
+      );
+    }
+  }
+}
+
+/// Rather than modifying the backend machinery to prevent hoisting code
+/// between the stackmap and call site, unwind instructions in order to get
+/// real live value locations at the function call.
+bool StackTransformMetadata::unwindToCallSite(MachineInstr *SM,
+                                              const MachineInstr *Call) {
+  bool Changed = false, Found;
+  MachineOperand *SrcOp;
+  MachineInstr *InB = SM;
+  RegValsMap::iterator VregIt, SrcIt;
+  StackValsMap::iterator SSIt;
+  CopyLocPtr C;
+  RegCopyLoc *RCL;
+  StackCopyLoc *SCL;
+  TemporaryValuePtr Tmp;
+
+  // Note: anything named or related to "Src" refers to the source of the
+  // copy operation, i.e., the originating location for the value
+
+  DEBUG(dbgs() << "\nUnwinding stackmap back to call site:\n\n");
+  while((InB = InB->getPrevNode()) && InB != Call) {
+    if((C = getCopyLocation(InB))) {
+      DEBUG(dbgs() << "  + Copy instruction: "; InB->dump());
+
+      switch(C->getType()) {
+      default: DEBUG(dbgs() << "    Unhandled copy type\n"); break;
+      case CopyLoc::VREG:
+        RCL = (RegCopyLoc *)C.get();
+        SrcOp = &InB->getOperand(InB->findRegisterUseOperandIdx(RCL->SrcVreg));
+
+        // Replace current vreg with source
+        Found = false;
+        for(size_t i = 2; i < SM->getNumOperands(); i++) {
+          MachineOperand &MO = SM->getOperand(i);
+          if(MO.isReg() && MO.getReg() == RCL->Vreg) {
+            MO.ChangeToRegister(RCL->SrcVreg, false, false, SrcOp->isKill(),
+                                SrcOp->isDead(), false, false);
+            InB->clearRegisterKills(RCL->SrcVreg, TRI);
+            InB->clearRegisterDeads(RCL->SrcVreg);
+            updateRegisterLiveInterval(*SrcOp, SM);
+            Found = true;
+          }
+        }
+
+        // Update operand -> IR mapping to source vreg
+        if(Found) {
+          assert(SMRegs[SM].count(RCL->Vreg) &&
+                 "Unhandled register operand in stackmap!");
+          VregIt = SMRegs[SM].find(RCL->Vreg);
+          SrcIt = SMRegs[SM].find(RCL->SrcVreg);
+          if(SrcIt != SMRegs[SM].end()) {
+            for(auto IRVal : *VregIt->second)
+              SrcIt->second->push_back(IRVal);
+          }
+          else SMRegs[SM].emplace(RCL->SrcVreg, VregIt->second);
+          SMRegs[SM].erase(RCL->Vreg);
+          Changed = true;
+        }
+
+        break;
+      case CopyLoc::STACK_LOAD:
+        SCL = (StackCopyLoc *)C.get();
+
+        // Replace current vreg with stack slot.
+        // Note: stack slots don't have liveness information to fix up
+        Found = false;
+        for(size_t i = 2; i < SM->getNumOperands(); i++) {
+          MachineOperand &MO = SM->getOperand(i);
+          if(MO.isReg() && MO.getReg() == SCL->Vreg) {
+            // There's not a great way to add new operands, so trash all
+            // trailing operands up to and including the Vreg, add the spill
+            // slot, and finally add the trailing operands back.
+            SmallVector<MachineOperand, 4> TrailOps(std::next(&MO),
+                                                    SM->operands_end());
+            while(SM->getNumOperands() > (i + 1)) SM->RemoveOperand(i);
+            MachineInstrBuilder Worker(*MF, SM);
+            Worker.addImm(StackMaps::IndirectMemRefOp);
+            Worker.addImm(MFI->getObjectSize(SCL->StackSlot));
+            Worker.addFrameIndex(SCL->StackSlot);
+            Worker.addImm(0);
+            for(auto Trailing : TrailOps) Worker.addOperand(Trailing);
+            Found = true;
+          }
+        }
+
+        // Update operand -> IR mapping to source stack slot
+        if(Found) {
+          assert(SMRegs[SM].count(SCL->Vreg) &&
+                 "Unhandled register operand in stackmap!");
+          SSIt = SMStackSlots[SM].find(SCL->StackSlot);
+          VregIt = SMRegs[SM].find(SCL->Vreg);
+          if(SSIt != SMStackSlots[SM].end()) {
+            for(auto IRVal : *VregIt->second)
+              SSIt->second->push_back(IRVal);
+          }
+          else SMStackSlots[SM].emplace(SCL->StackSlot, VregIt->second);
+          SMRegs[SM].erase(SCL->Vreg);
+          Changed = true;
+        }
+
+        break;
+      case CopyLoc::STACK_STORE:
+        SCL = (StackCopyLoc *)C.get();
+        SrcOp = &InB->getOperand(InB->findRegisterUseOperandIdx(SCL->Vreg));
+
+        // Replace current stack slot with vreg
+        // Note: this *must* be an indirect memory reference (spill slot)
+        // since we're copying to a register!
+        Found = false;
+        for(size_t i = 2; i < SM->getNumOperands(); i++) {
+          MachineOperand &MO = SM->getOperand(i);
+          if(MO.isFI() && MO.getIndex() == SCL->StackSlot) {
+            // TODO if the sibling register is killed/dead in the intervening
+            // instruction we probably need to propagate that to the stackmap
+            // and remove it from the other instruction.
+            unsigned StartIdx = i - 2;
+            SM->getOperand(StartIdx).ChangeToRegister(SCL->Vreg, false);
+            SM->RemoveOperand(StartIdx + 1); // Size
+            SM->RemoveOperand(StartIdx + 1); // Frame index
+            SM->RemoveOperand(StartIdx + 1); // Frame pointer offset
+            Found = true;
+          }
+        }
+
+        // Update operand -> IR mapping to source vreg
+        if(Found) {
+          assert(SMStackSlots[SM].count(SCL->StackSlot) &&
+                 "Unhandled stack slot operand in stackmap!");
+
+          // Update liveness information to include the stackmap
+          InB->clearRegisterKills(SCL->Vreg, TRI);
+          InB->clearRegisterDeads(SCL->Vreg);
+          updateRegisterLiveInterval(*SrcOp, SM);
+
+          VregIt = SMRegs[SM].find(SCL->Vreg);
+          SSIt = SMStackSlots[SM].find(SCL->StackSlot);
+          if(VregIt != SMRegs[SM].end()) {
+            for(auto IRVal : *SSIt->second)
+              VregIt->second->push_back(IRVal);
+          }
+          else SMRegs[SM].emplace(SCL->Vreg, SSIt->second);
+          SMStackSlots[SM].erase(SCL->StackSlot);
+          Changed = true;
+        }
+
+        break;
+      }
+    }
+    else if((Tmp = TVG->getTemporaryValue(InB, VRM))) {
+      DEBUG(dbgs() << "  - Temporary for stackmap: "; InB->dump());
+      assert(Tmp->Type == TemporaryValue::StackSlotRef &&
+             "Unhandled temporary value");
+
+      // Replace current vreg with stack slot reference.
+      // Note: stack slots don't have liveness information to fix up
+      Found = false;
+      for(size_t i = 2; i < SM->getNumOperands(); i++) {
+        MachineOperand &MO = SM->getOperand(i);
+        if(MO.isReg() && MO.getReg() == Tmp->Vreg) {
+          // There's not a great way to add new operands, so trash all trailing
+          // operands up to and including the Vreg, add the metadata, and
+          // finally add the trailing operands back.
+          SmallVector<MachineOperand, 4> TrailOps(std::next(&MO),
+                                                  SM->operands_end());
+          while(SM->getNumOperands() > (i + 1)) SM->RemoveOperand(i);
+          MachineInstrBuilder Worker(*MF, SM);
+          Worker.addImm(StackMaps::TemporaryOp);
+          Worker.addImm(Tmp->Size);
+          Worker.addImm(Tmp->Offset);
+          Worker.addImm(StackMaps::DirectMemRefOp);
+          Worker.addFrameIndex(Tmp->StackSlot);
+          Worker.addImm(0);
+          for(auto Trailing : TrailOps) Worker.addOperand(Trailing);
+          Found = true;
+        }
+      }
+
+      // Update operand -> IR mapping to source stack slot
+      if(Found) {
+        assert(SMRegs[SM].count(Tmp->Vreg) &&
+               "Unhandled register operand in stackmap!");
+        SSIt = SMStackSlots[SM].find(Tmp->StackSlot);
+        VregIt = SMRegs[SM].find(Tmp->Vreg);
+        if(SSIt != SMStackSlots[SM].end()) {
+          for(auto IRVal : *VregIt->second)
+            SSIt->second->push_back(IRVal);
+        }
+        else SMStackSlots[SM].emplace(Tmp->StackSlot, VregIt->second);
+        SMRegs[SM].erase(Tmp->Vreg);
+        Changed = true;
+      }
+    }
+    else DEBUG(dbgs() << "  - Skipping "; InB->dump());
+  }
+
+  if(Changed) DEBUG(dbgs() << "\n  Transformed stackmap: "; SM->dump());
+  return Changed;
+}
+
+/// Is a virtual register live across the machine instruction?
+/// Note: returns false if the MI is the last instruction for which the virtual
+/// register is alive
+bool
+StackTransformMetadata::isVregLiveAcrossInstr(unsigned Vreg,
+                                              const MachineInstr *MI) const {
+  assert(TRI->isVirtualRegister(Vreg) && "Invalid virtual register");
+
+  if(LI->hasInterval(Vreg)) {
+    const LiveInterval &TheLI = LI->getInterval(Vreg);
+    SlotIndex InstrIdx = Indexes->getInstructionIndex(MI);
+    LiveInterval::const_iterator Seg = TheLI.find(InstrIdx);
+    if(Seg != TheLI.end() && Seg->contains(InstrIdx) &&
+       InstrIdx.getInstrDistance(Seg->end) != 0)
+      return true;
+  }
+  return false;
+}
+
+/// Is a stack slot live across the machine instruction?
+/// Note: returns false if the MI is the last instruction for which the stack
+/// slot is alive
+bool
+StackTransformMetadata::isSSLiveAcrossInstr(int SS,
+                                            const MachineInstr *MI) const {
+  if(LS->hasInterval(SS)) {
+    const LiveInterval &TheLI = LS->getInterval(SS);
+    SlotIndex InstrIdx = Indexes->getInstructionIndex(MI);
+    LiveInterval::const_iterator Seg = TheLI.find(InstrIdx);
+    if(Seg != TheLI.end() && Seg->contains(InstrIdx) &&
+       InstrIdx.getInstrDistance(Seg->end) != 0)
+      return true;
+  }
+  return false;
+}
+
+/// Add duplicate location information for a virtual register.
+bool StackTransformMetadata::addVregMetadata(unsigned Vreg,
+                                             ValueVecPtr IRVals,
+                                             const SMInstBundle &SM) {
+  const CallInst *IRSM = getIRSM(SM);
+  const MachineInstr *MICall = getMICall(SM);
+  RegValsMap &Vregs = SMRegs[getMISM(SM)];
+
+  assert(TargetRegisterInfo::isVirtualRegister(Vreg) && VRM->hasPhys(Vreg) &&
+         "Cannot add virtual register metadata -- invalid virtual register");
+
+  if(Vregs.find(Vreg) == Vregs.end() && isVregLiveAcrossInstr(Vreg, MICall))
+  {
+    unsigned Phys = VRM->getPhys(Vreg);
+    for(size_t sz = 0; sz < IRVals->size(); sz++) {
+      DEBUG(dumpReg(Vreg, (*IRVals)[sz]););
+      MF->addSMOpLocation(IRSM, (*IRVals)[sz], MachineLiveReg(Phys));
+    }
+    Vregs[Vreg] = IRVals;
+    return true;
+  }
+  else return false;
+}
+
+/// Add duplicate location information for a stack slot.
+bool StackTransformMetadata::addSSMetadata(int SS,
+                                           ValueVecPtr IRVals,
+                                           const SMInstBundle &SM) {
+  const CallInst *IRSM = getIRSM(SM);
+  const MachineInstr *MICall = getMICall(SM);
+  StackValsMap &SSlots = SMStackSlots[getMISM(SM)];
+
+  assert(!MFI->isDeadObjectIndex(SS) &&
+         "Cannot add stack slot metadata -- invalid stack slot");
+
+  if(SSlots.find(SS) == SSlots.end() && isSSLiveAcrossInstr(SS, MICall))
+  {
+    for(size_t sz = 0; sz < IRVals->size(); sz++) {
+      DEBUG(dumpStackSlot(SS, (*IRVals)[sz]););
+      MF->addSMOpLocation(IRSM, (*IRVals)[sz], MachineLiveStackSlot(SS));
+    }
+    SSlots[SS] = IRVals;
+    return true;
+  }
+  else return false;
+}
+
+/// Search stack slot copies for additional virtual registers which are live
+/// across the stackmap.  Will check to see if the copy instructions have
+/// already been visited, and if appropriate, will add virtual registers to
+/// work queue.
+void inline
+StackTransformMetadata::searchStackSlotCopies(int SS,
+                                 ValueVecPtr IRVals,
+                                 const SMInstBundle &SM,
+                                 SmallPtrSet<const MachineInstr *, 32> &Visited,
+                                 std::queue<WorkItem> &work,
+                                 bool TraverseDefs) {
+  StackSlotCopies::const_iterator Copies;
+  CopyLocVecPtr CL;
+  CopyLocVec::const_iterator Copy, CE;
+
+  if((Copies = SSCopies.find(SS)) != SSCopies.end()) {
+    CL = Copies->second;
+    for(Copy = CL->begin(), CE = CL->end(); Copy != CE; Copy++) {
+      unsigned Vreg = (*Copy)->Vreg;
+      const MachineInstr *Instr = (*Copy)->Instr;
+
+      if(!Visited.count(Instr)) {
+        addVregMetadata(Vreg, IRVals, SM);
+        Visited.insert(Instr);
+        work.emplace(Vreg, TraverseDefs);
+      }
+    }
+  }
+}
+
+/// Find all alternate locations for virtual registers in a stackmap, and add
+/// them to the metadata to be generated.
+void
+StackTransformMetadata::findAlternateVregLocs(const SMInstBundle &SM) {
+  RegValsMap &Regs = SMRegs[getMISM(SM)];
+  std::queue<WorkItem> work;
+  SmallPtrSet<const MachineInstr *, 32> Visited;
+  StackCopyLoc *SCL;
+  RegCopyLoc *RCL;
+
+  DEBUG(dbgs() << "\nDuplicate operand locations:\n\n";);
+
+  // Iterate over all vregs in the stackmap
+  for(RegValsMap::iterator it = Regs.begin(), end = Regs.end();
+      it != end; it++) {
+    unsigned origVreg = it->first;
+    ValueVecPtr IRVals = it->second;
+    Visited.clear();
+
+    // Follow data flow to search for all duplicate locations, including stack
+    // slots and other registers.  It's a duplicate if the following are true:
+    //
+    //   1. It's a copy-like instruction, e.g., a register move or a load
+    //      from/store to stack slot
+    //   2. The alternate location (virtual register/stack slot) is live across
+    //      the machine call instruction
+    //
+    // Note: we *must* search exhaustively (i.e., across copies from registers
+    // that are *not* live across the call) because the following can happen:
+    //
+    //   STORE vreg0, <fi#0>
+    //   ...
+    //   COPY vreg0, vreg1
+    //   ...
+    //   STACKMAP 0, 0, vreg1
+    //
+    // Here, vreg0 is *not* live across the stackmap, but <fi#0> *is*
+    work.emplace(origVreg, true);
+    while(!work.empty()) {
+      WorkItem cur;
+      unsigned vreg;
+      int ss;
+
+      // Walk over definitions
+      cur = work.front();
+      work.pop();
+      if(cur.TraverseDefs) {
+        for(auto instr = MRI->def_instr_begin(cur.Vreg),
+                 ei = MRI->def_instr_end();
+            instr != ei; instr++) {
+
+          if(Visited.count(&*instr)) continue;
+          CopyLocPtr loc = getCopyLocation(&*instr);
+          if(!loc) continue;
+
+          switch(loc->getType()) {
+          case CopyLoc::VREG:
+            RCL = (RegCopyLoc *)loc.get();
+            vreg = RCL->SrcVreg;
+            addVregMetadata(vreg, IRVals, SM);
+            Visited.insert(&*instr);
+            work.emplace(vreg, true);
+            break;
+          case CopyLoc::STACK_LOAD:
+            SCL = (StackCopyLoc *)loc.get();
+            ss = SCL->StackSlot;
+            if(addSSMetadata(ss, IRVals, SM)) {
+              Visited.insert(&*instr);
+              searchStackSlotCopies(ss, IRVals, SM, Visited, work, true);
+            }
+            break;
+          default: llvm_unreachable("Unknown/invalid location type"); break;
+          }
+        }
+      }
+
+      // Walk over uses
+      for(auto instr = MRI->use_instr_begin(cur.Vreg),
+               ei = MRI->use_instr_end();
+          instr != ei; instr++) {
+
+        if(Visited.count(&*instr)) continue;
+        CopyLocPtr loc = getCopyLocation(&*instr);
+        if(!loc) continue;
+
+        // Note: in traversing uses of the given vreg, we *don't* want to
+        // traverse definitions of sibling vregs.  Because we're in pseudo-SSA,
+        // it's possible we could be defining a register in separate dataflow
+        // paths, e.g.:
+        //
+        // BB A:
+        //   %vreg3<def> = COPY %vreg1
+        //   JMP <BB C>
+        //
+        // BB B:
+        //   %vreg3<def> = COPY %vreg2
+        //   JMP <BB C>
+        //
+        // ...
+        //
+        // If we discovered block A through vreg 1, we don't want to explore
+        // through block B in which vreg 3 is defined with a different value.
+        switch(loc->getType()) {
+        case CopyLoc::VREG:
+          RCL = (RegCopyLoc *)loc.get();
+          vreg = RCL->Vreg;
+          addVregMetadata(vreg, IRVals, SM);
+          Visited.insert(&*instr);
+          work.emplace(vreg, false);
+          break;
+        case CopyLoc::STACK_STORE:
+          SCL = (StackCopyLoc *)loc.get();
+          ss = SCL->StackSlot;
+          if(addSSMetadata(ss, IRVals, SM)) {
+            Visited.insert(&*instr);
+            searchStackSlotCopies(ss, IRVals, SM, Visited, work, false);
+          }
+          break;
+        default: llvm_unreachable("Unknown/invalid location type"); break;
+        }
+      }
+    }
+  }
+}
+
+/// Find alternate storage locations for stackmap operands
+bool StackTransformMetadata::findAlternateOpLocs() {
+  bool Changed = false;
+  RegValsMap::iterator vregIt, vregEnd;
+
+  for(auto S = SM.begin(), SE = SM.end(); S != SE; S++) {
+    const CallInst *IRSM = getIRSM(*S);
+    const MachineInstr *MICall = getMICall(*S);
+    MachineInstr *MISM = getMISM(*S);
+
+    DEBUG(
+      dbgs() << "\nStackmap " << MISM->getOperand(0).getImm() << ":\n";
+      MISM->dump();
+      dbgs() << "\n";
+    );
+
+    // Get all virtual register/stack slot operands & their associated IR
+    // values
+    mapOpsToIR(IRSM, MISM);
+
+    // Because the CodeGen machinery is wily (and may hoist instructions above
+    // the stackmap), unwind copies until the call site.
+    Changed |= unwindToCallSite(MISM, MICall);
+
+    // Find alternate locations for vregs in stack map.  Note we don't need to
+    // find alternate stack slot locations, as allocas *should* already be in
+    // the stackmap, so the remaining stack slots are spilled registers (which
+    // are covered here).
+    findAlternateVregLocs(*S);
+  }
+
+  return Changed;
+}
+
+/// Ensure virtual registers used to generate architecture-specific values are
+/// handled by the stackmap & convert to physical registers
+void StackTransformMetadata::sanitizeVregs(MachineLiveValPtr &LV,
+                                           const MachineInstr *SM) const {
+  if(!LV) return;
+  if(LV->isGenerated()) {
+    MachineGeneratedVal *MGV = (MachineGeneratedVal *)LV.get();
+    const ValueGenInstList &Inst = MGV->getInstructions();
+    for(size_t i = 0, num = Inst.size(); i < num; i++) {
+      if(Inst[i]->opType() == ValueGenInst::OpType::Register) {
+        RegInstructionBase *RI = (RegInstructionBase *)Inst[i].get();
+        if(!TRI->isVirtualRegister(RI->getReg())) {
+          if(RI->getReg() == TRI->getFrameRegister(*MF)) continue;
+          // TODO walk through stackmap and see if physical register in
+          // instruction is contained in stackmap
+          LV.reset(nullptr);
+          return;
+        }
+        else if(!SMRegs.at(SM).count(RI->getReg())) {
+          DEBUG(dbgs() << "WARNING: vreg "
+                       << TargetRegisterInfo::virtReg2Index(RI->getReg())
+                       << " used to generate value not handled in stackmap\n");
+          LV.reset(nullptr);
+          return;
+        }
+        else {
+          assert(VRM->hasPhys(RI->getReg()) && "Invalid virtual register");
+          RI->setReg(VRM->getPhys(RI->getReg()));
+        }
+      }
+    }
+  }
+}
+
+/// Filter out register definitions we've previously seen.
+static void
+getUnseenDefinitions(MachineRegisterInfo::def_instr_iterator DefIt,
+                     const SmallPtrSet<const MachineInstr *, 4> &Seen,
+                     SmallPtrSet<const MachineInstr *, 4> &NewDefs) {
+  NewDefs.clear();
+  do { if(!Seen.count(&*DefIt)) NewDefs.insert(&*DefIt);
+  } while((++DefIt) != MachineRegisterInfo::def_instr_end());
+}
+
+/// Try to find the best defining instruction.
+static const MachineInstr *
+tryToBreakDefMITie(const MachineInstr *MICall,
+                   const SmallPtrSet<const MachineInstr *, 4> &Definitions) {
+  // First heuristic -- find closest preceding defining instruction in the same
+  // machine basic block.
+  const MachineInstr *Cur, *BestDef = nullptr;
+  unsigned Distance, Best = UINT32_MAX;
+  SmallVector<std::pair<const MachineInstr *, unsigned>, 4> SearchDefs;
+  for(auto Def : Definitions) {
+    Cur = MICall;
+    Distance = 1;
+    while((Cur = Cur->getPrevNode())) {
+      if(Cur == Def) {
+        SearchDefs.emplace_back(Def, Distance);
+        break;
+      }
+      Distance++;
+    }
+  }
+
+  for(auto Pair : SearchDefs) {
+    if(Pair.second < Best) {
+      BestDef = Pair.first;
+      Best = Pair.second;
+    }
+  }
+
+  if(BestDef)
+    DEBUG(dbgs() << "Choosing defining instruction"; BestDef->dump());
+  return BestDef;
+}
+
+/// Find architecture-specific live values added by the backend
+void StackTransformMetadata::findArchSpecificLiveVals() {
+  DEBUG(dbgs() << "\n*** Finding architecture-specific live values ***\n\n";);
+
+  for(auto S = SM.begin(), SE = SM.end(); S != SE; S++)
+  {
+    const MachineInstr *MISM = getMISM(*S);
+    const MachineInstr *MICall = getMICall(*S);
+    const CallInst *IRSM = getIRSM(*S);
+    RegValsMap &CurVregs = SMRegs[MISM];
+    StackValsMap &CurSS = SMStackSlots[MISM];
+
+    DEBUG(
+      MISM->dump();
+      dbgs() << "  -> Call instruction SlotIndex ";
+      Indexes->getInstructionIndex(MICall).print(dbgs());
+      dbgs() << ", searching vregs 0 -> " << MRI->getNumVirtRegs()
+             << " and stack slots " << MFI->getObjectIndexBegin() << " -> "
+             << MFI->getObjectIndexEnd() << "\n";
+    );
+
+    // Include any mandatory architecture-specific live values
+    TVG->addRequiredArchLiveValues(MF, MISM, IRSM);
+
+    // Search for virtual registers not handled by the stackmap.  Registers
+    // spilled to the stack should have been converted to frame index
+    // references by now.
+    for(unsigned i = 0, numVregs = MRI->getNumVirtRegs(); i < numVregs; i++) {
+      unsigned Vreg = TargetRegisterInfo::index2VirtReg(i);
+      MachineLiveValPtr MLV;
+      MachineLiveReg MLR(0);
+
+      if(VRM->hasPhys(Vreg) && isVregLiveAcrossInstr(Vreg, MICall) &&
+         CurVregs.find(Vreg) == CurVregs.end()) {
+        DEBUG(dbgs() << "    + vreg" << i
+                     << " is live in register but not in stackmap\n";);
+
+        // Walk the use-def chain to see if we can find a valid value.  Note we
+        // keep track of seen definitions because even though we're supposed to
+        // be in SSA form it's possible to find definition cycles.
+        const MachineInstr *DefMI;
+        unsigned ChainVreg = Vreg;
+        SmallPtrSet<const MachineInstr *, 4> SeenDefs, NewDefs;
+        do {
+          getUnseenDefinitions(MRI->def_instr_begin(ChainVreg),
+                               SeenDefs, NewDefs);
+
+          // Try to find a suitable defining instruction
+          if(NewDefs.size() == 0) {
+            DEBUG(dbgs() << "WARNING: no unseen definition\n");
+            break;
+          }
+          else if(NewDefs.size() == 1) DefMI = *NewDefs.begin();
+          else if(!(DefMI = tryToBreakDefMITie(MICall, NewDefs))) {
+            // No suitable defining instruction, not much we can do...
+            DEBUG(
+              dbgs() << "WARNING: multiple definitions for virtual "
+                        "register, missed in live-value analysis?\n";
+              for(auto d = MRI->def_instr_begin(ChainVreg),
+                  e = MRI->def_instr_end(); d != e; d++)
+                d->dump();
+            );
+            break;
+          }
+
+          SeenDefs.insert(DefMI);
+          MLV = TVG->getMachineValue(DefMI);
+          sanitizeVregs(MLV, MISM);
+
+          if(MLV) break; // We got a value!
+          else {
+            // Couldn't get a value, follow the use-def chain
+            CopyLocPtr Copy = getCopyLocation(DefMI);
+            if(Copy) {
+              switch(Copy->getType()) {
+              default: ChainVreg = 0; break;
+              case CopyLoc::VREG:
+                ChainVreg = ((RegCopyLoc *)Copy.get())->SrcVreg;
+                break;
+              }
+            }
+            else ChainVreg = 0;
+          }
+        } while(TargetRegisterInfo::isVirtualRegister(ChainVreg));
+
+        if(MLV) {
+          DEBUG(dbgs() << "      Defining instruction: ";
+                MLV->getDefiningInst()->print(dbgs());
+                dbgs() << "      Value: " << MLV->toString() << "\n");
+
+          MLR.setReg(VRM->getPhys(Vreg));
+          MF->addSMArchSpecificLocation(IRSM, MLR, *MLV);
+          CurVregs.emplace(Vreg, ValueVecPtr(nullptr));
+        }
+        else {
+          DEBUG(
+            DefMI = &*MRI->def_instr_begin(Vreg);
+            StringRef BBName = DefMI->getParent()->getName();
+            dbgs() << "      Unhandled defining instruction in basic block "
+                   << BBName << ":";
+            DefMI->print(dbgs());
+          );
+        }
+      }
+    }
+
+    // Search for stack slots not handled by the stackmap
+    for(int SS = MFI->getObjectIndexBegin(), e = MFI->getObjectIndexEnd();
+        SS < e; SS++) {
+      if(UsedSS.count(SS) && !MFI->isDeadObjectIndex(SS) &&
+         isSSLiveAcrossInstr(SS, MICall) && CurSS.find(SS) == CurSS.end()) {
+        DEBUG(dbgs() << "    + stack slot " << SS
+                     << " is live but not in stackmap\n";);
+        // TODO add arch-specific stack slot information to machine function
+      }
+    }
+
+    DEBUG(dbgs() << "\n";);
+  }
+}
+
+/// Find IR call instruction which generated the stackmap
+static inline const CallInst *findCalledFunc(const llvm::CallInst *IRSM) {
+  const Instruction *Func = IRSM->getPrevNode();
+  while(Func && !isa<CallInst>(Func)) Func = Func->getPrevNode();
+  return dyn_cast<CallInst>(Func);
+}
+
+/// Display a warning about unhandled values
+static inline void displayWarning(std::string &Msg,
+                                  const CallInst *CI,
+                                  const Function *F) {
+  assert(CI && "Invalid arguments");
+
+  // Note: it may be possible for us to not have a called function, for example
+  // if we call a function using a function pointer
+  const Function *CurF = CI->getParent()->getParent();
+  const std::string &Triple = CurF->getParent()->getTargetTriple();
+  Msg = "(" + Triple + ") " + Msg;
+  if(F && F->hasName()) {
+    Msg += " across call to ";
+    Msg += F->getName();
+  }
+  DiagnosticInfoOptimizationFailure DI(*CurF, CI->getDebugLoc(), Msg);
+  CurF->getContext().diagnose(DI);
+}
+
+/// Warn about unhandled registers & stack slots
+void StackTransformMetadata::warnUnhandled() const {
+  std::string Msg;
+  const CallInst *IRCall;
+  const Function *CalledFunc;
+
+  for(auto S = SM.begin(), SE = SM.end(); S != SE; S++)
+  {
+    const MachineInstr *MISM = getMISM(*S);
+    const MachineInstr *MICall = getMICall(*S);
+    const RegValsMap &CurVregs = SMRegs.at(MISM);
+    const StackValsMap &CurSS = SMStackSlots.at(MISM);
+    IRCall = findCalledFunc(getIRSM(*S));
+    CalledFunc = IRCall->getCalledFunction();
+    assert(IRCall && "No call instruction for stackmap");
+
+    // Search for virtual registers not handled by the stackmap
+    for(unsigned i = 0; i < MRI->getNumVirtRegs(); i++) {
+      unsigned Vreg = TargetRegisterInfo::index2VirtReg(i);
+
+      // Virtual register allocated to physical register
+      if(VRM->hasPhys(Vreg) && isVregLiveAcrossInstr(Vreg, MICall) &&
+         CurVregs.find(Vreg) == CurVregs.end()) {
+        Msg = "Stack transformation: unhandled register ";
+        Msg += TRI->getName(VRM->getPhys(Vreg));
+        displayWarning(Msg, IRCall, CalledFunc);
+      }
+    }
+
+    // Search for all stack slots not handled by the stackmap
+    for(int SS = MFI->getObjectIndexBegin(), e = MFI->getObjectIndexEnd();
+        SS < e; SS++) {
+      if(UsedSS.count(SS) && !MFI->isDeadObjectIndex(SS) &&
+         isSSLiveAcrossInstr(SS, MICall) && CurSS.find(SS) == CurSS.end()) {
+        Msg = "Stack transformation: unhandled stack slot ";
+        Msg += std::to_string(SS);
+        displayWarning(Msg, IRCall, CalledFunc);
+      }
+    }
+  }
+}
+
diff --git a/llvm/lib/CodeGen/StackTransformTypes.cpp b/llvm/lib/CodeGen/StackTransformTypes.cpp
new file mode 100644
index 00000000000..619e9b2c59f
--- /dev/null
+++ b/llvm/lib/CodeGen/StackTransformTypes.cpp
@@ -0,0 +1,303 @@
+//===-- llvm/Target/TargetValueGenerator.cpp - Value Generator --*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/CodeGen/AsmPrinter.h"
+#include "llvm/CodeGen/MachineFrameInfo.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/StackTransformTypes.h"
+#include "llvm/IR/Mangler.h"
+#include "llvm/MC/MCContext.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Target/TargetFrameLowering.h"
+#include "llvm/Target/TargetMachine.h"
+#include "llvm/Target/TargetRegisterInfo.h"
+#include "llvm/Target/TargetSubtargetInfo.h"
+
+#define DEBUG_TYPE "stacktransform"
+
+using namespace llvm;
+
+//===----------------------------------------------------------------------===//
+// Common functions
+//
+
+static MCSymbol *GetExternalSymbol(AsmPrinter &AP, StringRef Sym) {
+  SmallString<60> Name;
+  Mangler::getNameWithPrefix(Name, Sym, *AP.TM.getDataLayout());
+  return AP.OutContext.lookupSymbol(Name);
+}
+
+//===----------------------------------------------------------------------===//
+// Types for generating more complex architecture-specific live values
+//
+
+const char *ValueGenInst::InstTypeStr[] = {
+#define X(type) #type ,
+  VALUE_GEN_INST
+#undef X
+};
+
+const char *ValueGenInst::getInstName(enum InstType Type) {
+  switch(Type) {
+#define X(type) case type: 
+    VALUE_GEN_INST
+#undef X
+    return InstTypeStr[Type];
+    break;
+  default:
+    return "unknown";
+  };
+}
+
+std::string ValueGenInst::getInstNameStr(enum InstType Type) {
+  return std::string(getInstName(Type));
+}
+
+std::string RefInstruction::str() const {
+  std::string buf = "reference to '";
+  switch(Symbol.getType()) {
+  case MachineOperand::MO_GlobalAddress:
+    buf += Symbol.getGlobal()->getName();
+    buf += "' (global)";
+    break;
+  case MachineOperand::MO_ExternalSymbol:
+    buf += Symbol.getSymbolName();
+    buf += "' (external)";
+    break;
+  case MachineOperand::MO_MCSymbol:
+    buf += Symbol.getMCSymbol()->getName();
+    buf += "' (MC symbol)";
+    break;
+  default:
+    DEBUG(dbgs() << "Unhandled reference type: ";
+          Symbol.print(dbgs());
+          dbgs() << "\n";);
+    buf += "n/a' (unhandled type)";
+    break;
+  }
+  return buf;
+}
+
+MCSymbol *RefInstruction::getReference(AsmPrinter &AP) const {
+  switch(Symbol.getType()) {
+  case MachineOperand::MO_ExternalSymbol:
+    return GetExternalSymbol(AP, Symbol.getSymbolName());
+  case MachineOperand::MO_GlobalAddress:
+    return AP.TM.getSymbol(Symbol.getGlobal(), *AP.Mang);
+  case MachineOperand::MO_MCSymbol:
+    return Symbol.getMCSymbol();
+  default:
+    DEBUG(dbgs() << "Unhandled reference type: ";
+          Symbol.print(dbgs());
+          dbgs() << "\n";);
+    return nullptr;
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// MachineSymbolRef implementation
+//
+
+bool MachineSymbolRef::operator==(const MachineLiveVal &RHS) const {
+  if(RHS.isSymbolRef()) {
+    const MachineSymbolRef &MSR = (const MachineSymbolRef &)RHS;
+    if(&MSR.Symbol == &Symbol && MSR.Load == Load) return true;
+  }
+  return false;
+}
+
+std::string MachineSymbolRef::toString() const {
+  std::string buf;
+  if(Load) buf = "dereference symbol '";
+  else buf = "reference to symbol '";
+  switch(Symbol.getType()) {
+  case MachineOperand::MO_GlobalAddress:
+    buf += Symbol.getGlobal()->getName();
+    buf += "' (global)";
+    break;
+  case MachineOperand::MO_ExternalSymbol:
+    buf += Symbol.getSymbolName();
+    buf += "' (external)";
+    break;
+  case MachineOperand::MO_MCSymbol:
+    buf += Symbol.getMCSymbol()->getName();
+    buf += "' (MC symbol)";
+    break;
+  default:
+    DEBUG(dbgs() << "Unhandled reference type: ";
+          Symbol.print(dbgs());
+          dbgs() << "\n";);
+    buf += "n/a' (unhandled type)";
+    break;
+  }
+  return buf;
+}
+
+MCSymbol *MachineSymbolRef::getReference(AsmPrinter &AP) const {
+  switch(Symbol.getType()) {
+  case MachineOperand::MO_ExternalSymbol:
+    return GetExternalSymbol(AP, Symbol.getSymbolName());
+  case MachineOperand::MO_GlobalAddress:
+    return AP.TM.getSymbol(Symbol.getGlobal(), *AP.Mang);
+  case MachineOperand::MO_MCSymbol:
+    return Symbol.getMCSymbol();
+  default:
+    DEBUG(dbgs() << "Unhandled reference type: ";
+          Symbol.print(dbgs());
+          dbgs() << "\n";);
+    return nullptr;
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// MachineConstPoolRef implementation
+//
+
+bool MachineConstPoolRef::operator==(const MachineLiveVal &RHS) const {
+  if(RHS.isConstPoolRef()) {
+    const MachineConstPoolRef &MCPR = (const MachineConstPoolRef &)RHS;
+    if(MCPR.Index == Index) return true;
+  }
+  return false;
+}
+
+MCSymbol *MachineConstPoolRef::getReference(AsmPrinter &AP) const {
+  MCSymbol *Sym = AP.GetCPISymbol(Index);
+  assert(Sym && "Could not get constant pool reference");
+  return Sym;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineStackObject implementation
+//
+
+bool MachineStackObject::operator==(const MachineLiveVal &RHS) const {
+  if(RHS.isStackObject()) {
+    const MachineStackObject &MSO = (const MachineStackObject &)RHS;
+    if(MSO.Index == Index) return true;
+  }
+  return false;
+}
+
+std::string MachineStackObject::toString() const {
+  std::string buf;
+  if(Load) buf = "load from ";
+  else buf = "reference to ";
+  return buf + "stack slot " + std::to_string(Index);
+}
+
+int
+MachineStackObject::getOffsetFromReg(AsmPrinter &AP, unsigned &BR) const {
+  const TargetFrameLowering *TFL = AP.MF->getSubtarget().getFrameLowering();
+  return TFL->getFrameIndexReference(*AP.MF, Index, BR);
+}
+
+//===----------------------------------------------------------------------===//
+// ReturnAddress implementation
+//
+
+int ReturnAddress::getOffsetFromReg(AsmPrinter &AP, unsigned &BR) const {
+  int Off = AP.MF->getSubtarget().getRegisterInfo()->getReturnAddrLoc(*AP.MF,
+                                                                      BR);
+  if(BR == 0) llvm_unreachable("No saved return address!");
+  return Off;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineImmediate implementation
+//
+
+MachineImmediate::MachineImmediate(unsigned Size,
+                                   uint64_t Value,
+                                   const MachineInstr *DefMI,
+                                   bool Ptr)
+  : MachineLiveVal(DefMI, Ptr), Size(Size), Value(Value)
+{
+  if(Size > 8)
+    llvm_unreachable("Unsupported immediate value size of > 8 bytes");
+}
+
+bool MachineImmediate::operator==(const MachineLiveVal &RHS) const {
+  if(RHS.isImm()) {
+    const MachineImmediate &MI = (const MachineImmediate &)RHS;
+    if(MI.Size == Size && MI.Value == Value) return true;
+  }
+  return false;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineGeneratedVal implementation
+//
+
+bool MachineGeneratedVal::operator==(const MachineLiveVal &RHS) const {
+  if(!RHS.isGenerated()) return false;
+  const MachineGeneratedVal &MGV = (const MachineGeneratedVal &)RHS;
+
+  if(VG.size() != MGV.VG.size()) return false;
+  for(size_t i = 0, num = VG.size(); i < num; i++)
+    if(VG[i] != MGV.VG[i]) return false;
+  return true;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineLiveReg implementation
+//
+
+bool MachineLiveReg::operator==(const MachineLiveLoc &RHS) const {
+  if(RHS.isReg()) {
+    const MachineLiveReg &MLR = (const MachineLiveReg &)RHS;
+    if(MLR.Reg == Reg) return true;
+  }
+  return false;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineLiveStackAddr implementation
+//
+
+bool MachineLiveStackAddr::operator==(const MachineLiveLoc &RHS) const {
+  if(RHS.isStackAddr() && !RHS.isStackSlot()) {
+    const MachineLiveStackAddr &MLSA = (const MachineLiveStackAddr &)RHS;
+    if(Offset != INT32_MAX && MLSA.Offset != INT32_MAX &&
+       Offset == MLSA.Offset && Reg == MLSA.Reg && Size == MLSA.Size)
+      return true;
+  }
+  return false;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineLiveStackSlot implementation
+//
+
+bool MachineLiveStackSlot::operator==(const MachineLiveLoc &RHS) const {
+  if(RHS.isStackSlot()) {
+    const MachineLiveStackSlot &MLSS = (const MachineLiveStackSlot &)RHS;
+    if(MLSS.Index == Index) return true;
+  }
+  return false;
+}
+
+int MachineLiveStackSlot::calcAndGetRegOffset(const AsmPrinter &AP, unsigned &BP) {
+  if(Offset == INT32_MAX) {
+    const TargetFrameLowering *TFL = AP.MF->getSubtarget().getFrameLowering();
+    Offset = TFL->getFrameIndexReference(*AP.MF, Index, Reg);
+  }
+  BP = Reg;
+  return Offset;
+}
+
+unsigned MachineLiveStackSlot::getSize(const AsmPrinter &AP) {
+  if(Size == 0) {
+    const MachineFrameInfo *MFI = AP.MF->getFrameInfo();
+    Size = MFI->getObjectSize(Index);
+  }
+  return Size;
+}
+
diff --git a/llvm/lib/CodeGen/UnwindInfo.cpp b/llvm/lib/CodeGen/UnwindInfo.cpp
new file mode 100644
index 00000000000..59fd5f41835
--- /dev/null
+++ b/llvm/lib/CodeGen/UnwindInfo.cpp
@@ -0,0 +1,186 @@
+//===--------------------------- UnwindInfo.cpp ---------------------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/CodeGen/UnwindInfo.h"
+#include "llvm/MC/MCSectionELF.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/MC/MCObjectFileInfo.h"
+#include "llvm/Target/TargetFrameLowering.h"
+#include "llvm/Target/TargetRegisterInfo.h"
+#include "llvm/Target/TargetSubtargetInfo.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "unwindinfo"
+
+static const char *UIDbg = "Unwind Info: ";
+
+void UnwindInfo::recordUnwindInfo(const MachineFunction &MF) {
+  // We *only* need this information for functions which have a stackmap, as
+  // only those function activations can be unwound during stack
+  // transformation.  This may also be a correctness criterion since we record
+  // offsets from the FBP, and not all functions may have one (stackmaps are
+  // implemented using FBPs, and thus prevent the FP-elimination optimization).
+  if(!MF.getFrameInfo()->hasStackMap()) return;
+
+  DEBUG(dbgs() << "**** UnwindInfo: Analyzing " << MF.getName() << "****\n");
+
+  const MachineFrameInfo *MFI = MF.getFrameInfo();
+  assert(MFI->isCalleeSavedInfoValid() && "No callee-saved information!");
+
+  // Get this function's saved registers
+  unsigned FrameReg;
+  const TargetFrameLowering *TFL = MF.getSubtarget().getFrameLowering();
+  const TargetRegisterInfo *TRI = MF.getSubtarget().getRegisterInfo();
+  const std::vector<CalleeSavedInfo> &CSI = MFI->getCalleeSavedInfo();
+
+  DEBUG(dbgs() << CSI.size() << " saved registers\n");
+
+  // Get DWARF register number and FBP offset using callee saved information
+  CalleeSavedRegisters SavedRegs(CSI.size());
+  for(unsigned i = 0; i < CSI.size(); i++) {
+    SavedRegs[i].DwarfReg = TRI->getDwarfRegNum(CSI[i].getReg(), false);
+    SavedRegs[i].Offset =
+      TFL->getFrameIndexReferenceFromFP(MF, CSI[i].getFrameIdx(), FrameReg);
+
+    DEBUG(dbgs() << "Register " << SavedRegs[i].DwarfReg << " at register "
+                 << PrintReg(FrameReg, TRI) << " + " << SavedRegs[i].Offset
+                 << "\n");
+    assert(FrameReg == TRI->getFrameRegister(MF) &&
+           "Invalid register used as offset for unwinding information");
+  }
+
+  // Save the information for when we emit the section
+  const MCSymbol *FuncSym = OutContext.lookupSymbol(MF.getName());
+  assert(FuncSym && "Could not find function symbol");
+  FuncCalleeSaved.insert(FuncCalleePair(FuncSym, std::move(SavedRegs)));
+}
+
+void UnwindInfo::addRegisterUnwindInfo(const MachineFunction &MF,
+                                       uint32_t MachineReg,
+                                       int32_t Offset) {
+  if(!MF.getFrameInfo()->hasStackMap()) return;
+
+  const MCSymbol *FuncSym = OutContext.lookupSymbol(MF.getName());
+  assert(FuncSym && "Could not find function symbol");
+  assert(FuncCalleeSaved.find(FuncSym) != FuncCalleeSaved.end() &&
+         "Cannot add register restore information -- function not found");
+  const TargetRegisterInfo *TRI = MF.getSubtarget().getRegisterInfo();
+  FuncCalleeSaved[FuncSym].push_back(
+    RegOffset(TRI->getDwarfRegNum(MachineReg, false), Offset));
+}
+
+void UnwindInfo::emitUnwindInfo(MCStreamer &OS) {
+  unsigned curIdx = 0;
+  unsigned startIdx;
+  FuncCalleeMap::const_iterator f, e;
+  for(f = FuncCalleeSaved.begin(), e = FuncCalleeSaved.end(); f != e; f++) {
+    const MCSymbol *FuncSym = f->first;
+    const CalleeSavedRegisters &CSR = f->second;
+
+    assert(FuncSym && "Invalid machine function");
+    if(CSR.size() < 2)
+      DEBUG(dbgs() << "WARNING: should have at least 2 registers to restore "
+                               "(return address & saved FBP");
+
+    DEBUG(dbgs() << UIDbg << "Function " << FuncSym->getName()
+                 << " (offset " << curIdx << ", "
+                 << CSR.size() << " entries):\n");
+
+    startIdx = curIdx;
+    CalleeSavedRegisters::const_iterator cs, cse;
+    for(cs = CSR.begin(), cse = CSR.end(); cs != cse; cs++) {
+      assert(cs->DwarfReg < UINT16_MAX &&
+             "Register number too large for resolution");
+      assert(INT16_MIN < cs->Offset && cs->Offset < INT16_MAX &&
+             "Register save offset too large for resolution");
+
+      DEBUG(dbgs() << UIDbg << "  Register " << cs->DwarfReg
+                   << " saved at " << cs->Offset << "\n";);
+
+      OS.EmitIntValue(cs->DwarfReg, 2);
+      OS.EmitIntValue(cs->Offset, 2);
+      curIdx++;
+    }
+    FuncUnwindInfo FUI(startIdx, curIdx - startIdx);
+    FuncUnwindMetadata.insert(FuncUnwindPair(FuncSym, std::move(FUI)));
+  }
+}
+
+void UnwindInfo::emitAddrRangeInfo(MCStreamer &OS) {
+  FuncUnwindMap::const_iterator f, e;
+  for(f = FuncUnwindMetadata.begin(), e = FuncUnwindMetadata.end();
+      f != e;
+      f++) {
+    const MCSymbol *Func = f->first;
+    const FuncUnwindInfo &FUI = f->second;
+    OS.EmitSymbolValue(Func, 8);
+    OS.EmitIntValue(FUI.NumUnwindRecord, 4);
+    OS.EmitIntValue(FUI.SecOffset, 4);
+  }
+}
+
+/// Serialize the unwinding information.
+void UnwindInfo::serializeToUnwindInfoSection() {
+  // Bail out if there's no unwind info.
+  if(FuncCalleeSaved.empty()) return;
+
+  // Emit unwinding record information.
+  // FIXME: we only support ELF object files for now
+
+  // Switch to the unwind info section
+  MCStreamer &OS = *AP.OutStreamer;
+  MCSection *UnwindInfoSection =
+      OutContext.getObjectFileInfo()->getUnwindInfoSection();
+  OS.SwitchSection(UnwindInfoSection);
+
+  // Emit a dummy symbol to force section inclusion.
+  OS.EmitLabel(OutContext.getOrCreateSymbol(Twine("__StackTransform_UnwindInfo")));
+
+  // Serialize data.
+  DEBUG(dbgs() << "********** Unwind Info Output **********\n");
+  emitUnwindInfo(OS);
+  OS.AddBlankLine();
+
+  // Switch to the unwind address range section & emit section
+  MCSection *UnwindAddrRangeSection =
+      OutContext.getObjectFileInfo()->getUnwindAddrRangeSection();
+  OS.SwitchSection(UnwindAddrRangeSection);
+  OS.EmitLabel(OutContext.getOrCreateSymbol(Twine("__StackTransform_UnwindAddrRange")));
+  emitAddrRangeInfo(OS);
+  OS.AddBlankLine();
+
+  Emitted = true;
+}
+
+const UnwindInfo::FuncUnwindInfo &
+UnwindInfo::getUnwindInfo(const MCSymbol *Func) const {
+  assert(Emitted && "Have not yet calculated per-function unwinding metadata");
+
+  FuncUnwindMap::const_iterator it = FuncUnwindMetadata.find(Func);
+  assert(it != FuncUnwindMetadata.end() && "Invalid function");
+  return it->second;
+}
+
+void UnwindInfo::print(raw_ostream &OS) {
+  OS << UIDbg << "Function unwinding information\n";
+  FuncCalleeMap::const_iterator b, e;
+  for(b = FuncCalleeSaved.begin(), e = FuncCalleeSaved.end();
+      b != e;
+      b++) {
+    OS << UIDbg << "Function - " << b->first->getName() << "\n";
+    const CalleeSavedRegisters &CSR = b->second;
+    CalleeSavedRegisters::const_iterator br, be;
+    for(br = CSR.begin(), be = CSR.end(); br != be; br++) {
+      OS << UIDbg << "Register " << br->DwarfReg
+                  << " at offset " << br->Offset << "\n";
+    }
+  }
+}
+
diff --git a/llvm/lib/IR/AsmWriter.cpp b/llvm/lib/IR/AsmWriter.cpp
index b553f11018c..e22b2744e6f 100644
--- a/llvm/lib/IR/AsmWriter.cpp
+++ b/llvm/lib/IR/AsmWriter.cpp
@@ -691,6 +691,11 @@ void ModuleSlotTracker::incorporateFunction(const Function &F) {
   this->F = &F;
 }
 
+int ModuleSlotTracker::getLocalSlot(const Value *V) {
+  assert(F && "No function incorporated");
+  return Machine->getLocalSlot(V);
+}
+
 static SlotTracker *createSlotTracker(const Module *M) {
   return new SlotTracker(M);
 }
diff --git a/llvm/lib/IR/DiagnosticInfo.cpp b/llvm/lib/IR/DiagnosticInfo.cpp
index b8f77eda15a..f8eec60902a 100644
--- a/llvm/lib/IR/DiagnosticInfo.cpp
+++ b/llvm/lib/IR/DiagnosticInfo.cpp
@@ -201,6 +201,11 @@ bool DiagnosticInfoOptimizationFailure::isEnabled() const {
   return getSeverity() == DS_Warning;
 }
 
+bool DiagnosticInfoOptimizationError::isEnabled() const {
+  // Only print errors.
+  return getSeverity() == DS_Error;
+}
+
 void llvm::emitLoopVectorizeWarning(LLVMContext &Ctx, const Function &Fn,
                                     const DebugLoc &DLoc, const Twine &Msg) {
   Ctx.diagnose(DiagnosticInfoOptimizationFailure(
diff --git a/llvm/lib/MC/MCCodeGenInfo.cpp b/llvm/lib/MC/MCCodeGenInfo.cpp
index 347ec2cd01e..eab3cea13df 100644
--- a/llvm/lib/MC/MCCodeGenInfo.cpp
+++ b/llvm/lib/MC/MCCodeGenInfo.cpp
@@ -20,4 +20,5 @@ void MCCodeGenInfo::initMCCodeGenInfo(Reloc::Model RM, CodeModel::Model CM,
   RelocationModel = RM;
   CMModel = CM;
   OptLevel = OL;
+  ArchIROptLevel = OL;
 }
diff --git a/llvm/lib/MC/MCExpr.cpp b/llvm/lib/MC/MCExpr.cpp
index a30ceecc952..d5a037ae5c4 100644
--- a/llvm/lib/MC/MCExpr.cpp
+++ b/llvm/lib/MC/MCExpr.cpp
@@ -43,7 +43,7 @@ void MCExpr::print(raw_ostream &OS, const MCAsmInfo *MAI) const {
     const MCSymbol &Sym = SRE.getSymbol();
     // Parenthesize names that start with $ so that they don't look like
     // absolute names.
-    bool UseParens = Sym.getName()[0] == '$';
+    bool UseParens = Sym.getName().size() > 0 && Sym.getName()[0] == '$';
     if (UseParens) {
       OS << '(';
       Sym.print(OS, MAI);
diff --git a/llvm/lib/MC/MCObjectFileInfo.cpp b/llvm/lib/MC/MCObjectFileInfo.cpp
index 576827a72d5..09f1ff60969 100644
--- a/llvm/lib/MC/MCObjectFileInfo.cpp
+++ b/llvm/lib/MC/MCObjectFileInfo.cpp
@@ -519,6 +519,15 @@ void MCObjectFileInfo::initELFMCObjectFileInfo(Triple T) {
   DwarfAddrSection =
       Ctx->getELFSection(".debug_addr", ELF::SHT_PROGBITS, 0, "addr_sec");
 
+  UnwindAddrRangeSection =
+      Ctx->getELFSection(".stack_transform.unwind_arange", ELF::SHT_PROGBITS,
+                         0, sizeof(uint64_t) + sizeof(uint64_t), "");
+  UnwindInfoSection =
+      Ctx->getELFSection(".stack_transform.unwind", ELF::SHT_PROGBITS, 0,
+                         sizeof(uint16_t) + sizeof(int16_t), "");
+  UnwindAddrRangeSection->setAlignment(sizeof(uint64_t));
+  UnwindInfoSection->setAlignment(sizeof(uint16_t) + sizeof(int16_t));
+
   StackMapSection =
       Ctx->getELFSection(".llvm_stackmaps", ELF::SHT_PROGBITS, ELF::SHF_ALLOC);
 
diff --git a/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp b/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
index ada995bad37..030c0063abb 100644
--- a/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
+++ b/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
@@ -28,6 +28,7 @@
 #include "llvm/CodeGen/MachineModuleInfoImpls.h"
 #include "llvm/CodeGen/StackMaps.h"
 #include "llvm/CodeGen/TargetLoweringObjectFileImpl.h"
+#include "llvm/CodeGen/UnwindInfo.h"
 #include "llvm/IR/DataLayout.h"
 #include "llvm/IR/DebugInfo.h"
 #include "llvm/MC/MCAsmInfo.h"
@@ -49,11 +50,12 @@ namespace {
 class AArch64AsmPrinter : public AsmPrinter {
   AArch64MCInstLower MCInstLowering;
   StackMaps SM;
+  UnwindInfo UI;
 
 public:
   AArch64AsmPrinter(TargetMachine &TM, std::unique_ptr<MCStreamer> Streamer)
       : AsmPrinter(TM, std::move(Streamer)), MCInstLowering(OutContext, *this),
-        SM(*this), AArch64FI(nullptr) {}
+        SM(*this), UI(*this), AArch64FI(nullptr) {}
 
   const char *getPassName() const override {
     return "AArch64 Assembly Printer";
@@ -83,7 +85,9 @@ public:
 
   bool runOnMachineFunction(MachineFunction &F) override {
     AArch64FI = F.getInfo<AArch64FunctionInfo>();
-    return AsmPrinter::runOnMachineFunction(F);
+    bool retval = AsmPrinter::runOnMachineFunction(F);
+    UI.recordUnwindInfo(F);
+    return retval;
   }
 
 private:
@@ -129,8 +133,10 @@ void AArch64AsmPrinter::EmitEndOfAsmFile(Module &M) {
     // linker can safely perform dead code stripping.  Since LLVM never
     // generates code that does this, it is always safe to set.
     OutStreamer->EmitAssemblerFlag(MCAF_SubsectionsViaSymbols);
-    SM.serializeToStackMapSection();
   }
+  UI.serializeToUnwindInfoSection();
+  SM.serializeToStackMapSection(&UI);
+  UI.reset(); // Must reset after SM serialization to clear metadata
 }
 
 MachineLocation
diff --git a/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp b/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
index a76473f7e53..51a50a5da5e 100644
--- a/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
+++ b/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
@@ -655,6 +655,15 @@ int AArch64FrameLowering::getFrameIndexReference(const MachineFunction &MF,
   return resolveFrameIndexReference(MF, FI, FrameReg);
 }
 
+/// getFrameIndexReferenceFromFP - Provide a base+offset reference to an FI
+/// slot for debug info, but force base to be the frame pointer (x29).
+int
+AArch64FrameLowering::getFrameIndexReferenceFromFP(const MachineFunction &MF,
+                                                   int FI,
+                                                   unsigned &FrameReg) const {
+  return resolveFrameIndexReference(MF, FI, FrameReg, true);
+}
+
 int AArch64FrameLowering::resolveFrameIndexReference(const MachineFunction &MF,
                                                      int FI, unsigned &FrameReg,
                                                      bool PreferFP) const {
diff --git a/llvm/lib/Target/AArch64/AArch64FrameLowering.h b/llvm/lib/Target/AArch64/AArch64FrameLowering.h
index 731f031ff85..edf7f5ad888 100644
--- a/llvm/lib/Target/AArch64/AArch64FrameLowering.h
+++ b/llvm/lib/Target/AArch64/AArch64FrameLowering.h
@@ -40,6 +40,8 @@ public:
   int getFrameIndexOffset(const MachineFunction &MF, int FI) const override;
   int getFrameIndexReference(const MachineFunction &MF, int FI,
                              unsigned &FrameReg) const override;
+  int getFrameIndexReferenceFromFP(const MachineFunction &MF, int FI,
+                                   unsigned &FrameReg) const override;
   int resolveFrameIndexReference(const MachineFunction &MF, int FI,
                                  unsigned &FrameReg,
                                  bool PreferFP = false) const;
diff --git a/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp b/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
index 3e8f46cf1ec..9e4e8243521 100644
--- a/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
+++ b/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
@@ -2032,6 +2032,8 @@ SDValue AArch64TargetLowering::LowerOperation(SDValue Op,
     return LowerFSINCOS(Op, DAG);
   case ISD::MUL:
     return LowerMUL(Op, DAG);
+  case (uint16_t)~TargetOpcode::STACKMAP:
+    return SDValue(); // Use generic stackmap type legalizer
   }
 }
 
diff --git a/llvm/lib/Target/AArch64/AArch64InstrInfo.td b/llvm/lib/Target/AArch64/AArch64InstrInfo.td
index fa1a46acba8..d6b4681327d 100644
--- a/llvm/lib/Target/AArch64/AArch64InstrInfo.td
+++ b/llvm/lib/Target/AArch64/AArch64InstrInfo.td
@@ -123,7 +123,7 @@ def AArch64addlow        : SDNode<"AArch64ISD::ADDlow", SDTIntBinOp, []>;
 def AArch64LOADgot       : SDNode<"AArch64ISD::LOADgot", SDTIntUnaryOp>;
 def AArch64callseq_start : SDNode<"ISD::CALLSEQ_START",
                                 SDCallSeqStart<[ SDTCisVT<0, i32> ]>,
-                                [SDNPHasChain, SDNPOutGlue]>;
+                                [SDNPHasChain, SDNPOptInGlue, SDNPOutGlue]>;
 def AArch64callseq_end   : SDNode<"ISD::CALLSEQ_END",
                                 SDCallSeqEnd<[ SDTCisVT<0, i32>,
                                                SDTCisVT<1, i32> ]>,
diff --git a/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp b/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
index 841af55f7a6..f3fd03b4cbc 100644
--- a/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
+++ b/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
@@ -215,6 +215,23 @@ AArch64RegisterInfo::getFrameRegister(const MachineFunction &MF) const {
   return TFI->hasFP(MF) ? AArch64::FP : AArch64::SP;
 }
 
+int AArch64RegisterInfo::getReturnAddrLoc(const MachineFunction &MF,
+                                          unsigned &BaseReg) const {
+  const TargetFrameLowering *TFL = MF.getSubtarget().getFrameLowering();
+  const MachineFrameInfo *MFI = MF.getFrameInfo();
+  assert(MFI->isCalleeSavedInfoValid() && "No callee-saved information");
+  const std::vector<CalleeSavedInfo> &CSI = MFI->getCalleeSavedInfo();
+
+  // The return address' location is the the link register's spill slot
+  for(unsigned i = 0, e = CSI.size(); i < e; i++)
+    if(CSI[i].getReg() == AArch64::LR)
+      return TFL->getFrameIndexReference(MF, CSI[i].getFrameIdx(), BaseReg);
+
+  // We didn't find it, is it actually saved?
+  BaseReg = 0;
+  return INT32_MAX;
+}
+
 bool AArch64RegisterInfo::requiresRegisterScavenging(
     const MachineFunction &MF) const {
   return true;
diff --git a/llvm/lib/Target/AArch64/AArch64RegisterInfo.h b/llvm/lib/Target/AArch64/AArch64RegisterInfo.h
index 8c379d92610..f040e59e5d5 100644
--- a/llvm/lib/Target/AArch64/AArch64RegisterInfo.h
+++ b/llvm/lib/Target/AArch64/AArch64RegisterInfo.h
@@ -91,6 +91,9 @@ public:
   // Debug information queries.
   unsigned getFrameRegister(const MachineFunction &MF) const override;
 
+  int getReturnAddrLoc(const MachineFunction &MF,
+                       unsigned &BaseReg) const override;
+
   unsigned getRegPressureLimit(const TargetRegisterClass *RC,
                                MachineFunction &MF) const override;
   // Base pointer (stack realignment) support.
diff --git a/llvm/lib/Target/AArch64/AArch64Subtarget.h b/llvm/lib/Target/AArch64/AArch64Subtarget.h
index 6bb06942306..580bf891797 100644
--- a/llvm/lib/Target/AArch64/AArch64Subtarget.h
+++ b/llvm/lib/Target/AArch64/AArch64Subtarget.h
@@ -19,6 +19,7 @@
 #include "AArch64InstrInfo.h"
 #include "AArch64RegisterInfo.h"
 #include "AArch64SelectionDAGInfo.h"
+#include "AArch64Values.h"
 #include "llvm/IR/DataLayout.h"
 #include "llvm/Target/TargetSubtargetInfo.h"
 #include <string>
@@ -63,6 +64,7 @@ protected:
   AArch64InstrInfo InstrInfo;
   AArch64SelectionDAGInfo TSInfo;
   AArch64TargetLowering TLInfo;
+  AArch64Values VGen;
 private:
   /// initializeSubtargetDependencies - Initializes using CPUString and the
   /// passed in feature string so that we can use initializer lists for
@@ -89,6 +91,9 @@ public:
   const AArch64RegisterInfo *getRegisterInfo() const override {
     return &getInstrInfo()->getRegisterInfo();
   }
+  const AArch64Values *getValues() const override {
+    return &VGen;
+  }
   const Triple &getTargetTriple() const { return TargetTriple; }
   bool enableMachineScheduler() const override { return true; }
   bool enablePostRAScheduler() const override {
diff --git a/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp b/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
index db6e244337a..4083aa78728 100644
--- a/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
+++ b/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
@@ -220,16 +220,20 @@ void AArch64PassConfig::addIRPasses() {
   // Cmpxchg instructions are often used with a subsequent comparison to
   // determine whether it succeeded. We can exploit existing control-flow in
   // ldrex/strex loops to simplify this, but it needs tidying up.
-  if (TM->getOptLevel() != CodeGenOpt::None && EnableAtomicTidy)
+  if (TM->getOptLevel() != CodeGenOpt::None &&
+      TM->getArchIROptLevel() != CodeGenOpt::None &&
+      EnableAtomicTidy)
     addPass(createCFGSimplificationPass());
 
   TargetPassConfig::addIRPasses();
 
   // Match interleaved memory accesses to ldN/stN intrinsics.
-  if (TM->getOptLevel() != CodeGenOpt::None)
+  if (TM->getOptLevel() != CodeGenOpt::None &&
+      TM->getArchIROptLevel() != CodeGenOpt::None)
     addPass(createInterleavedAccessPass(TM));
 
-  if (TM->getOptLevel() == CodeGenOpt::Aggressive && EnableGEPOpt) {
+  if (TM->getOptLevel() == CodeGenOpt::Aggressive &&
+      TM->getArchIROptLevel() != CodeGenOpt::None && EnableGEPOpt) {
     // Call SeparateConstOffsetFromGEP pass to extract constants within indices
     // and lower a GEP with multiple indices to either arithmetic operations or
     // multiple GEPs with single index.
@@ -247,12 +251,14 @@ void AArch64PassConfig::addIRPasses() {
 bool AArch64PassConfig::addPreISel() {
   // Run promote constant before global merge, so that the promoted constants
   // get a chance to be merged
-  if (TM->getOptLevel() != CodeGenOpt::None && EnablePromoteConstant)
+  if (TM->getOptLevel() != CodeGenOpt::None &&
+      TM->getArchIROptLevel() != CodeGenOpt::None && EnablePromoteConstant)
     addPass(createAArch64PromoteConstantPass());
   // FIXME: On AArch64, this depends on the type.
   // Basically, the addressable offsets are up to 4095 * Ty.getSizeInBytes().
   // and the offset has to be a multiple of the related size in bytes.
   if ((TM->getOptLevel() != CodeGenOpt::None &&
+       TM->getArchIROptLevel() != CodeGenOpt::None &&
        EnableGlobalMerge == cl::BOU_UNSET) ||
       EnableGlobalMerge == cl::BOU_TRUE) {
     bool OnlyOptimizeForSize = (TM->getOptLevel() < CodeGenOpt::Aggressive) &&
@@ -260,7 +266,8 @@ bool AArch64PassConfig::addPreISel() {
     addPass(createGlobalMergePass(TM, 4095, OnlyOptimizeForSize));
   }
 
-  if (TM->getOptLevel() != CodeGenOpt::None)
+  if (TM->getOptLevel() != CodeGenOpt::None &&
+      TM->getArchIROptLevel() != CodeGenOpt::None)
     addPass(createAArch64AddressTypePromotionPass());
 
   return false;
diff --git a/llvm/lib/Target/AArch64/AArch64Values.cpp b/llvm/lib/Target/AArch64/AArch64Values.cpp
new file mode 100644
index 00000000000..906e3cf4bdc
--- /dev/null
+++ b/llvm/lib/Target/AArch64/AArch64Values.cpp
@@ -0,0 +1,253 @@
+//===- AArch64TargetValues.cpp - AArch64 specific value generator -===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "AArch64Values.h"
+#include "AArch64.h"
+#include "MCTargetDesc/AArch64AddressingModes.h"
+#include "llvm/CodeGen/MachineConstantPool.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Target/TargetInstrInfo.h"
+#include "llvm/Target/TargetSubtargetInfo.h"
+
+#define DEBUG_TYPE "stacktransform"
+
+using namespace llvm;
+
+static TemporaryValue *getTemporaryReference(const MachineInstr *MI,
+                                             const VirtRegMap *VRM,
+                                             unsigned Size) {
+  TemporaryValue *Val = nullptr;
+  if(MI->getOperand(0).isReg()) {
+    // Instruction format:    ADDXri  xd    xn    imm#  lsl#
+    // Stack slot reference:                <fi>  0     0
+    if(MI->getOperand(1).isFI() &&
+       MI->getOperand(2).isImm() && MI->getOperand(2).getImm() == 0 &&
+       MI->getOperand(3).isImm() && MI->getOperand(3).getImm() == 0) {
+      Val = new TemporaryValue;
+      Val->Type = TemporaryValue::StackSlotRef;
+      Val->Size = Size;
+      Val->Vreg = MI->getOperand(0).getReg();
+      Val->StackSlot = MI->getOperand(1).getIndex();
+      Val->Offset = 0;
+    }
+  }
+
+  return Val;
+}
+
+TemporaryValuePtr
+AArch64Values::getTemporaryValue(const MachineInstr *MI,
+                                 const VirtRegMap *VRM) const {
+  TemporaryValue *Val = nullptr;
+  switch(MI->getOpcode()) {
+  case AArch64::ADDXri: Val = getTemporaryReference(MI, VRM, 8); break;
+  default: break;
+  }
+  return TemporaryValuePtr(Val);
+}
+
+typedef ValueGenInst::InstType InstType;
+template <InstType T> using RegInstruction = RegInstruction<T>;
+template <InstType T> using ImmInstruction = ImmInstruction<T>;
+
+// Bitwise-conversions between floats & ints
+union IntFloat64 { double d; uint64_t i; };
+union IntFloat32 { float f; uint64_t i; };
+
+MachineLiveVal *
+AArch64Values::genADDInstructions(const MachineInstr *MI) const {
+  int Index;
+
+  switch(MI->getOpcode()) {
+  case AArch64::ADDXri:
+    if(MI->getOperand(1).isFI()) {
+      Index = MI->getOperand(1).getIndex();
+      assert(MI->getOperand(2).isImm() && MI->getOperand(2).getImm() == 0);
+      assert(MI->getOperand(3).isImm() && MI->getOperand(3).getImm() == 0);
+      return new MachineStackObject(Index, false, MI, true);
+    }
+    break;
+  default:
+    DEBUG(dbgs() << "Unhandled ADD machine instruction");
+    break;
+  }
+  return nullptr;
+}
+
+MachineLiveVal *
+AArch64Values::genADRPInstructions(const MachineInstr *MI) const {
+  ValueGenInstList IL;
+  if(isSymbolValue(MI->getOperand(1))) {
+    IL.emplace_back(new RefInstruction(MI->getOperand(1)));
+    IL.emplace_back(new ImmInstruction<InstType::Mask>(8, ~0xfff));
+    return new MachineGeneratedVal(IL, MI, false);
+  }
+  return nullptr;
+}
+
+MachineLiveVal *
+AArch64Values::genBitfieldInstructions(const MachineInstr *MI) const {
+  int64_t R, S;
+  unsigned Size, Bits;
+  uint64_t Mask;
+  ValueGenInstList IL;
+
+  switch(MI->getOpcode()) {
+  case AArch64::UBFMXri:
+    Size = 8;
+    Bits = 64;
+    Mask = UINT64_MAX;
+
+    assert(MI->getOperand(1).isReg() &&
+           MI->getOperand(2).isImm() &&
+           MI->getOperand(3).isImm());
+
+    // TODO ensure this is correct
+    IL.emplace_back(
+      new RegInstruction<InstType::Set>(MI->getOperand(1).getReg()));
+    R = MI->getOperand(2).getImm();
+    S = MI->getOperand(3).getImm();
+    if(S >= R) {
+      IL.emplace_back(new ImmInstruction<InstType::RightShiftLog>(Size, R));
+      IL.emplace_back(
+        new ImmInstruction<InstType::Mask>(Size, ~(Mask << (S - R + 1))));
+    }
+    else {
+      IL.emplace_back(
+        new ImmInstruction<InstType::Mask>(Size, ~(Mask << (S + 1))));
+      IL.emplace_back(new ImmInstruction<InstType::LeftShift>(Size, Bits - R));
+    }
+    return new MachineGeneratedVal(IL, MI, false);
+    break;
+  default:
+    DEBUG(dbgs() << "Unhandled bitfield instruction");
+    break;
+  }
+  return nullptr;
+}
+
+MachineLiveVal *
+AArch64Values::genLoadRegValue(const MachineInstr *MI) const {
+  switch(MI->getOpcode()) {
+  case AArch64::LDRDui:
+    if(MI->getOperand(2).isCPI()) {
+      int Idx = MI->getOperand(2).getIndex();
+      const MachineFunction *MF = MI->getParent()->getParent();
+      const MachineConstantPool *MCP = MF->getConstantPool();
+      const std::vector<MachineConstantPoolEntry> &CP = MCP->getConstants();
+      if(CP[Idx].isMachineConstantPoolEntry()) {
+        // TODO unhandled for now
+      }
+      else {
+        const Constant *Val = CP[Idx].Val.ConstVal;
+        if(isa<ConstantFP>(Val)) {
+          const ConstantFP *FPVal = cast<ConstantFP>(Val);
+          const APFloat &Flt = FPVal->getValueAPF();
+          switch(APFloat::getSizeInBits(Flt.getSemantics())) {
+          case 32: {
+            IntFloat32 I2F = { Flt.convertToFloat() };
+            return new MachineImmediate(4, I2F.i, MI, false);
+          }
+          case 64: {
+            IntFloat64 I2D = { Flt.convertToDouble() };
+            return new MachineImmediate(8, I2D.i, MI, false);
+          }
+          default: break;
+          }
+        }
+      }
+    }
+    break;
+  case AArch64::LDRXui:
+    // Note: if this is of the form %vreg, <ga:...>, then the compiler has
+    // emitted multiple instructions in order to form the full address.  We,
+    // however, don't have the instruction encoding limitations.
+    // TODO verify this note above is true, maybe using MO::getTargetFlags?
+    // Note 2: we *must* ensure the symbol is const-qualified, otherwise we
+    // risk creating a new value if the symbol's value changes between when the
+    // initial load would have occurred and the transformation, e.g.,
+    //
+    //   ldr x20, <ga:mysym>
+    //   ... (somebody changes mysym's value) ...
+    //   bl <ga:myfunc>
+    //
+    // In this situation, the transformation occurs at the call site and
+    // retrieves the updated value rather than the value that would have been
+    // loaded at the ldr instruction.
+    if(TargetValues::isSymbolValue(MI->getOperand(2)) &&
+       TargetValues::isSymbolValueConstant(MI->getOperand(2)))
+      return new MachineSymbolRef(MI->getOperand(2), true, MI);
+    break;
+  default: break;
+  }
+  return nullptr;
+}
+
+MachineLiveValPtr AArch64Values::getMachineValue(const MachineInstr *MI) const {
+  IntFloat64 Conv64;
+  MachineLiveVal* Val = nullptr;
+  const MachineOperand *MO;
+  const TargetInstrInfo *TII;
+
+  switch(MI->getOpcode()) {
+  case AArch64::ADDXri:
+    Val = genADDInstructions(MI);
+    break;
+  case AArch64::ADRP:
+    Val = genADRPInstructions(MI);
+    break;
+  case AArch64::MOVaddr:
+    MO = &MI->getOperand(1);
+    if(MO->isCPI())
+      Val = new MachineConstPoolRef(MO->getIndex(), MI);
+    else if(TargetValues::isSymbolValue(MO))
+      Val = new MachineSymbolRef(*MO, false, MI);
+    break;
+  case AArch64::COPY:
+    MO = &MI->getOperand(1);
+    if(MO->isReg() && MO->getReg() == AArch64::LR) Val = new ReturnAddress(MI);
+    break;
+  case AArch64::FMOVD0:
+    Conv64.d = 0.0;
+    Val = new MachineImmediate(8, Conv64.i, MI, false);
+    break;
+  case AArch64::FMOVDi:
+    Conv64.d = (double)AArch64_AM::getFPImmFloat(MI->getOperand(1).getImm());
+    Val = new MachineImmediate(8, Conv64.i, MI, false);
+    break;
+  case AArch64::LDRXui:
+  case AArch64::LDRDui:
+    Val = genLoadRegValue(MI);
+    break;
+  case AArch64::MOVi32imm:
+    MO = &MI->getOperand(1);
+    assert(MO->isImm() && "Invalid immediate for MOVi32imm");
+    Val = new MachineImmediate(4, MO->getImm(), MI, false);
+    break;
+  case AArch64::MOVi64imm:
+    MO = &MI->getOperand(1);
+    assert(MO->isImm() && "Invalid immediate for MOVi64imm");
+    Val = new MachineImmediate(8, MO->getImm(), MI, false);
+    break;
+  case AArch64::UBFMXri:
+    Val = genBitfieldInstructions(MI);
+    break;
+  default:
+    TII =  MI->getParent()->getParent()->getSubtarget().getInstrInfo();
+    DEBUG(dbgs() << "Unhandled opcode: "
+                 << TII->getName(MI->getOpcode()) << "\n");
+    break;
+  }
+
+  return MachineLiveValPtr(Val);
+}
+
diff --git a/llvm/lib/Target/AArch64/AArch64Values.h b/llvm/lib/Target/AArch64/AArch64Values.h
new file mode 100644
index 00000000000..14b4a0e8d08
--- /dev/null
+++ b/llvm/lib/Target/AArch64/AArch64Values.h
@@ -0,0 +1,29 @@
+//===----- AArch64TargetValues.cpp - AArch64 specific value generator -----===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Target/TargetValues.h"
+
+namespace llvm {
+
+class AArch64Values final : public TargetValues {
+public:
+  AArch64Values() {}
+  virtual TemporaryValuePtr getTemporaryValue(const MachineInstr *MI,
+                                              const VirtRegMap *VRM) const;
+  virtual MachineLiveValPtr getMachineValue(const MachineInstr *MI) const;
+
+private:
+  MachineLiveVal *genADDInstructions(const MachineInstr *MI) const;
+  MachineLiveVal *genADRPInstructions(const MachineInstr *MI) const;
+  MachineLiveVal *genBitfieldInstructions(const MachineInstr *MI) const;
+  MachineLiveVal *genLoadRegValue(const MachineInstr *MI) const;
+};
+
+}
+
diff --git a/llvm/lib/Target/AArch64/CMakeLists.txt b/llvm/lib/Target/AArch64/CMakeLists.txt
index f26327ff84a..95496e2f75c 100644
--- a/llvm/lib/Target/AArch64/CMakeLists.txt
+++ b/llvm/lib/Target/AArch64/CMakeLists.txt
@@ -43,6 +43,7 @@ add_llvm_target(AArch64CodeGen
   AArch64TargetMachine.cpp
   AArch64TargetObjectFile.cpp
   AArch64TargetTransformInfo.cpp
+  AArch64Values.cpp
 )
 
 add_dependencies(LLVMAArch64CodeGen intrinsics_gen)
diff --git a/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp b/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp
index 7d8e79bc63c..c2a6efe5d0e 100644
--- a/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp
+++ b/llvm/lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp
@@ -281,7 +281,13 @@ AArch64MCCodeEmitter::getAddSubImmOpValue(const MCInst &MI, unsigned OpIdx,
 
   ++MCNumFixups;
 
-  return 0;
+  // Set the shift bit of the add instruction for relocation types
+  // R_AARCH64_TLSLE_ADD_TPREL_HI12 and R_AARCH64_TLSLD_ADD_DTPREL_HI12.
+  AArch64MCExpr::VariantKind RefKind = cast<AArch64MCExpr>(Expr)->getKind();
+  if (RefKind == AArch64MCExpr::VK_TPREL_HI12 ||
+      RefKind == AArch64MCExpr::VK_DTPREL_HI12)
+    ShiftVal = 12;
+  return ShiftVal == 0 ? 0 : (1 << ShiftVal);
 }
 
 /// getCondBranchTargetOpValue - Return the encoded value for a conditional
diff --git a/llvm/lib/Target/CMakeLists.txt b/llvm/lib/Target/CMakeLists.txt
index e6d0199952f..c8adde63fee 100644
--- a/llvm/lib/Target/CMakeLists.txt
+++ b/llvm/lib/Target/CMakeLists.txt
@@ -8,6 +8,7 @@ add_llvm_library(LLVMTarget
   TargetMachineC.cpp
   TargetRecip.cpp
   TargetSubtargetInfo.cpp
+  TargetValues.cpp
 
   ADDITIONAL_HEADER_DIRS
   ${LLVM_MAIN_INCLUDE_DIR}/llvm/Target
diff --git a/llvm/lib/Target/PowerPC/CMakeLists.txt b/llvm/lib/Target/PowerPC/CMakeLists.txt
index c0c83cc258b..f600cea6195 100644
--- a/llvm/lib/Target/PowerPC/CMakeLists.txt
+++ b/llvm/lib/Target/PowerPC/CMakeLists.txt
@@ -34,6 +34,7 @@ add_llvm_target(PowerPCCodeGen
   PPCTargetTransformInfo.cpp
   PPCTOCRegDeps.cpp
   PPCTLSDynamicCall.cpp
+  PPCValues.cpp
   PPCVSXCopy.cpp
   PPCVSXFMAMutate.cpp
   PPCVSXSwapRemoval.cpp
diff --git a/llvm/lib/Target/PowerPC/PPCAsmPrinter.cpp b/llvm/lib/Target/PowerPC/PPCAsmPrinter.cpp
index 8e118ec27e6..63a186212d6 100644
--- a/llvm/lib/Target/PowerPC/PPCAsmPrinter.cpp
+++ b/llvm/lib/Target/PowerPC/PPCAsmPrinter.cpp
@@ -36,6 +36,7 @@
 #include "llvm/CodeGen/MachineRegisterInfo.h"
 #include "llvm/CodeGen/StackMaps.h"
 #include "llvm/CodeGen/TargetLoweringObjectFileImpl.h"
+#include "llvm/CodeGen/UnwindInfo.h"
 #include "llvm/IR/Constants.h"
 #include "llvm/IR/DebugInfo.h"
 #include "llvm/IR/DerivedTypes.h"
@@ -70,10 +71,11 @@ namespace {
     MapVector<MCSymbol*, MCSymbol*> TOC;
     const PPCSubtarget *Subtarget;
     StackMaps SM;
+    UnwindInfo UI;
   public:
     explicit PPCAsmPrinter(TargetMachine &TM,
                            std::unique_ptr<MCStreamer> Streamer)
-        : AsmPrinter(TM, std::move(Streamer)), SM(*this) {}
+        : AsmPrinter(TM, std::move(Streamer)), SM(*this), UI(*this) {}
 
     const char *getPassName() const override {
       return "PowerPC Assembly Printer";
@@ -99,9 +101,33 @@ namespace {
     void LowerPATCHPOINT(MCStreamer &OutStreamer, StackMaps &SM,
                          const MachineInstr &MI);
     void EmitTlsCall(const MachineInstr *MI, MCSymbolRefExpr::VariantKind VK);
+
+    virtual int getCanonicalReturnAddr(const MachineInstr *Call) const override;
+
     bool runOnMachineFunction(MachineFunction &MF) override {
       Subtarget = &MF.getSubtarget<PPCSubtarget>();
-      return AsmPrinter::runOnMachineFunction(MF);
+      bool retval = AsmPrinter::runOnMachineFunction(MF);
+
+      // Add this function's register unwind info.  The PowerPC backend doesn't
+      // maintain the saved FBP (old r31) and link register as callee-saved
+      // registers, so manually add where they're saved.
+      if(MF.getFrameInfo()->hasStackMap()) {
+        UI.recordUnwindInfo(MF);
+
+        // Add the LR & FP save slots
+        const TargetFrameLowering *TFL = Subtarget->getFrameLowering();
+        const PPCFunctionInfo *FI = MF.getInfo<PPCFunctionInfo>();
+        int Index, Offset;
+        unsigned BaseReg;
+
+        Offset = MF.getFrameInfo()->getStackSize() + 16;
+        UI.addRegisterUnwindInfo(MF, PPC::LR8, Offset);
+
+        Index = FI->getFramePointerSaveIndex();
+        Offset = TFL->getFrameIndexReferenceFromFP(MF, Index, BaseReg);
+        UI.addRegisterUnwindInfo(MF, PPC::X31, Offset);
+      }
+      return retval;
     }
   };
 
@@ -327,7 +353,9 @@ MCSymbol *PPCAsmPrinter::lookUpOrCreateTOCEntry(MCSymbol *Sym) {
 }
 
 void PPCAsmPrinter::EmitEndOfAsmFile(Module &M) {
-  SM.serializeToStackMapSection();
+  UI.serializeToUnwindInfoSection();
+  SM.serializeToStackMapSection(&UI);
+  UI.reset(); // Must reset after SM serialization to clear metadata
 }
 
 void PPCAsmPrinter::LowerSTACKMAP(MCStreamer &OutStreamer, StackMaps &SM,
@@ -490,6 +518,19 @@ void PPCAsmPrinter::EmitTlsCall(const MachineInstr *MI,
                  .addExpr(SymVar));
 }
 
+/// getCanonicalReturnAddr -- for machine instructions which actually codegen
+/// a call + other instructions, return an offset which would correct a label
+/// to point to the call's actual return address.
+int PPCAsmPrinter::getCanonicalReturnAddr(const MachineInstr *Call) const {
+  switch(Call->getOpcode()) {
+  case PPC::BL8_NOP:
+  case PPC::BLA8_NOP:
+  case PPC::BL8_NOP_TLS:
+  case PPC::BCTRL8_LDinto_toc: return 4;
+  default: return 0;
+  }
+}
+
 /// EmitInstruction -- Print out a single PowerPC MI in Darwin syntax to
 /// the current output stream.
 ///
diff --git a/llvm/lib/Target/PowerPC/PPCInstrInfo.td b/llvm/lib/Target/PowerPC/PPCInstrInfo.td
index 24fd9bd5c1f..d9da6b5674b 100644
--- a/llvm/lib/Target/PowerPC/PPCInstrInfo.td
+++ b/llvm/lib/Target/PowerPC/PPCInstrInfo.td
@@ -162,7 +162,7 @@ def PPCshl        : SDNode<"PPCISD::SHL"       , SDTIntShiftOp>;
 
 // These are target-independent nodes, but have target-specific formats.
 def callseq_start : SDNode<"ISD::CALLSEQ_START", SDT_PPCCallSeqStart,
-                           [SDNPHasChain, SDNPOutGlue]>;
+                           [SDNPHasChain, SDNPOptInGlue, SDNPOutGlue]>;
 def callseq_end   : SDNode<"ISD::CALLSEQ_END",   SDT_PPCCallSeqEnd,
                            [SDNPHasChain, SDNPOptInGlue, SDNPOutGlue]>;
 
diff --git a/llvm/lib/Target/PowerPC/PPCSubtarget.h b/llvm/lib/Target/PowerPC/PPCSubtarget.h
index 0616c1f6560..ba2120e8c85 100644
--- a/llvm/lib/Target/PowerPC/PPCSubtarget.h
+++ b/llvm/lib/Target/PowerPC/PPCSubtarget.h
@@ -17,6 +17,7 @@
 #include "PPCFrameLowering.h"
 #include "PPCISelLowering.h"
 #include "PPCInstrInfo.h"
+#include "PPCValues.h"
 #include "llvm/ADT/Triple.h"
 #include "llvm/IR/DataLayout.h"
 #include "llvm/MC/MCInstrItineraries.h"
@@ -129,6 +130,7 @@ protected:
   PPCFrameLowering FrameLowering;
   PPCInstrInfo InstrInfo;
   PPCTargetLowering TLInfo;
+  PPCValues VGen;
   TargetSelectionDAGInfo TSInfo;
 
 public:
@@ -171,6 +173,7 @@ public:
     return &getInstrInfo()->getRegisterInfo();
   }
   const PPCTargetMachine &getTargetMachine() const { return TM; }
+  const PPCValues *getValues() const override { return &VGen; }
 
   /// initializeSubtargetDependencies - Initializes using a CPU and feature string
   /// so that we can use initializer lists for subtarget initialization.
diff --git a/llvm/lib/Target/PowerPC/PPCValues.cpp b/llvm/lib/Target/PowerPC/PPCValues.cpp
new file mode 100644
index 00000000000..f4d3c3ec9e4
--- /dev/null
+++ b/llvm/lib/Target/PowerPC/PPCValues.cpp
@@ -0,0 +1,46 @@
+//===--------- PPCTargetValues.cpp - PPC specific value generator ---------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "PPCFrameLowering.h"
+#include "PPCValues.h"
+#include "PPC.h"
+#include "llvm/CodeGen/MachineRegisterInfo.h"
+#include "llvm/CodeGen/MachineOperand.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+#define DEBUG_TYPE "stacktransform"
+
+using namespace llvm;
+
+MachineLiveValPtr PPCValues::getMachineValue(const MachineInstr *MI) const {
+  // TODO
+  return nullptr;
+}
+
+void PPCValues::addRequiredArchLiveValues(MachineFunction *MF,
+                                          const MachineInstr *MIStackMap,
+                                          const CallInst *IRStackMap) const {
+  if(!MF->getRegInfo().use_empty(PPC::X2)) {
+    MachineOperand TOCRef = MachineOperand::CreateES(".TOC.");
+    MachineSymbolRef TOCSym(TOCRef, false, MIStackMap);
+
+    DEBUG(dbgs() << "   + Setting R2 to be TOC pointer\n");
+    MachineLiveReg TOCPtr(PPC::X2);
+    MF->addSMArchSpecificLocation(IRStackMap, TOCPtr, TOCSym);
+
+    // Per the ELFv2 ABI, the TOC Pointer Doubleword save area is at SP + 24
+    DEBUG(dbgs() << "   + Setting TOC pointer save slot to be TOC pointer\n");
+    const PPCFrameLowering *PFL =
+      (const PPCFrameLowering *)MF->getSubtarget().getFrameLowering();
+    MachineLiveStackAddr TOCSS(PFL->getTOCSaveOffset(), PPC::X1, 8);
+    MF->addSMArchSpecificLocation(IRStackMap, TOCSS, TOCSym);
+  }
+}
+
diff --git a/llvm/lib/Target/PowerPC/PPCValues.h b/llvm/lib/Target/PowerPC/PPCValues.h
new file mode 100644
index 00000000000..4d34bfeebef
--- /dev/null
+++ b/llvm/lib/Target/PowerPC/PPCValues.h
@@ -0,0 +1,24 @@
+//===--------- PPCTargetValues.cpp - PPC specific value generator ---------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Target/TargetValues.h"
+
+namespace llvm {
+
+class PPCValues final : public TargetValues {
+public:
+  PPCValues() {}
+  virtual MachineLiveValPtr getMachineValue(const MachineInstr *MI) const;
+  virtual void addRequiredArchLiveValues(MachineFunction *MF,
+                                         const MachineInstr *MIStackMap,
+                                         const CallInst *IRStackMap) const;
+};
+
+}
+
diff --git a/llvm/lib/Target/TargetMachine.cpp b/llvm/lib/Target/TargetMachine.cpp
index 83174c20c8e..ddca2fe2839 100644
--- a/llvm/lib/Target/TargetMachine.cpp
+++ b/llvm/lib/Target/TargetMachine.cpp
@@ -149,6 +149,17 @@ void TargetMachine::setOptLevel(CodeGenOpt::Level Level) const {
     CodeGenInfo->setOptLevel(Level);
 }
 
+CodeGenOpt::Level TargetMachine::getArchIROptLevel() const {
+  if (!CodeGenInfo)
+    return CodeGenOpt::Default;
+  return CodeGenInfo->getArchIROptLevel();
+}
+
+void TargetMachine::setArchIROptLevel(CodeGenOpt::Level Level) const {
+  if (CodeGenInfo)
+    CodeGenInfo->setArchIROptLevel(Level);
+}
+
 TargetIRAnalysis TargetMachine::getTargetIRAnalysis() {
   return TargetIRAnalysis([this](Function &F) {
     return TargetTransformInfo(F.getParent()->getDataLayout());
diff --git a/llvm/lib/Target/TargetValues.cpp b/llvm/lib/Target/TargetValues.cpp
new file mode 100644
index 00000000000..64c9563ee42
--- /dev/null
+++ b/llvm/lib/Target/TargetValues.cpp
@@ -0,0 +1,41 @@
+//===--------- TargetValues.cpp - Target value generator helpers ----------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/IR/GlobalVariable.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Target/TargetValues.h"
+
+#define DEBUG_TYPE "stacktransform"
+
+using namespace llvm;
+
+bool TargetValues::isSymbolValueConstant(const MachineOperand &MO) {
+  const GlobalValue *GV;
+  const GlobalVariable *GVar;
+
+  switch(MO.getType()) {
+  case MachineOperand::MO_GlobalAddress:
+    GV = MO.getGlobal();
+    if(isa<Function>(GV)) return true;
+    else if((GVar = dyn_cast<GlobalVariable>(GV)) && GVar->isConstant())
+      return true;
+    break;
+  case MachineOperand::MO_ExternalSymbol:
+    // TODO
+    break;
+  case MachineOperand::MO_MCSymbol:
+    // TODO
+    break;
+  default:
+    DEBUG(dbgs() << "Unhandled reference type\n");
+    break;
+  }
+  return false;
+}
+
diff --git a/llvm/lib/Target/X86/CMakeLists.txt b/llvm/lib/Target/X86/CMakeLists.txt
index fba2c280d20..3754b35c242 100644
--- a/llvm/lib/Target/X86/CMakeLists.txt
+++ b/llvm/lib/Target/X86/CMakeLists.txt
@@ -34,6 +34,7 @@ set(sources
   X86VZeroUpper.cpp
   X86FixupLEAs.cpp
   X86WinEHState.cpp
+  X86Values.cpp
   )
 
 if( CMAKE_CL_64 )
diff --git a/llvm/lib/Target/X86/X86AsmPrinter.cpp b/llvm/lib/Target/X86/X86AsmPrinter.cpp
index ba33248d203..ae306e593fc 100644
--- a/llvm/lib/Target/X86/X86AsmPrinter.cpp
+++ b/llvm/lib/Target/X86/X86AsmPrinter.cpp
@@ -24,6 +24,7 @@
 #include "llvm/CodeGen/TargetLoweringObjectFileImpl.h"
 #include "llvm/IR/DebugInfo.h"
 #include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/DiagnosticInfo.h"
 #include "llvm/IR/Mangler.h"
 #include "llvm/IR/Module.h"
 #include "llvm/IR/Type.h"
@@ -49,6 +50,8 @@ using namespace llvm;
 bool X86AsmPrinter::runOnMachineFunction(MachineFunction &MF) {
   Subtarget = &MF.getSubtarget<X86Subtarget>();
 
+  bool modified = TagCallSites(MF);
+
   SMShadowTracker.startFunction(MF);
 
   SetupMachineFunction(MF);
@@ -66,8 +69,17 @@ bool X86AsmPrinter::runOnMachineFunction(MachineFunction &MF) {
   // Emit the rest of the function body.
   EmitFunctionBody();
 
-  // We didn't modify anything.
-  return false;
+  // Add this function's register unwind info.  The x86 backend doesn't
+  // maintain the saved FBP (old RBP) and return address (RIP) as callee-saved
+  // registers, so manually add where they're saved.
+  if(MF.getFrameInfo()->hasStackMap()) {
+    UI.recordUnwindInfo(MF);
+    UI.addRegisterUnwindInfo(MF, X86::RIP, 8);
+    UI.addRegisterUnwindInfo(MF, X86::RBP, 0);
+  }
+
+  // We may have modified where stack map intrinsics are located.
+  return modified;
 }
 
 /// printSymbolOperand - Print a raw symbol reference operand.  This handles
@@ -689,8 +701,10 @@ void X86AsmPrinter::EmitEndOfAsmFile(Module &M) {
   }
 
   if (TT.isOSBinFormatELF()) {
-    SM.serializeToStackMapSection();
+    UI.serializeToUnwindInfoSection();
+    SM.serializeToStackMapSection(&UI);
     FM.serializeToFaultMapSection();
+    UI.reset(); // Must reset after SM serialization to clear metadata
   }
 }
 
diff --git a/llvm/lib/Target/X86/X86AsmPrinter.h b/llvm/lib/Target/X86/X86AsmPrinter.h
index 7f5d127c68d..eb12dc5e192 100644
--- a/llvm/lib/Target/X86/X86AsmPrinter.h
+++ b/llvm/lib/Target/X86/X86AsmPrinter.h
@@ -14,6 +14,7 @@
 #include "llvm/CodeGen/AsmPrinter.h"
 #include "llvm/CodeGen/FaultMaps.h"
 #include "llvm/CodeGen/StackMaps.h"
+#include "llvm/CodeGen/UnwindInfo.h"
 #include "llvm/Target/TargetMachine.h"
 
 // Implemented in X86MCInstLower.cpp
@@ -28,6 +29,7 @@ class MCSymbol;
 class LLVM_LIBRARY_VISIBILITY X86AsmPrinter : public AsmPrinter {
   const X86Subtarget *Subtarget;
   StackMaps SM;
+  UnwindInfo UI;
   FaultMaps FM;
 
   // This utility class tracks the length of a stackmap instruction's 'shadow'.
@@ -90,8 +92,8 @@ class LLVM_LIBRARY_VISIBILITY X86AsmPrinter : public AsmPrinter {
  public:
    explicit X86AsmPrinter(TargetMachine &TM,
                           std::unique_ptr<MCStreamer> Streamer)
-       : AsmPrinter(TM, std::move(Streamer)), SM(*this), FM(*this),
-         SMShadowTracker(TM) {}
+       : AsmPrinter(TM, std::move(Streamer)), SM(*this), UI(*this),
+         FM(*this), SMShadowTracker(TM) {}
 
   const char *getPassName() const override {
     return "X86 Assembly / Object Emitter";
diff --git a/llvm/lib/Target/X86/X86ISelLowering.cpp b/llvm/lib/Target/X86/X86ISelLowering.cpp
index 0f29b514146..45da9bd05df 100644
--- a/llvm/lib/Target/X86/X86ISelLowering.cpp
+++ b/llvm/lib/Target/X86/X86ISelLowering.cpp
@@ -18621,6 +18621,8 @@ SDValue X86TargetLowering::LowerOperation(SDValue Op, SelectionDAG &DAG) const {
   case ISD::GC_TRANSITION_START:
                                 return LowerGC_TRANSITION_START(Op, DAG);
   case ISD::GC_TRANSITION_END:  return LowerGC_TRANSITION_END(Op, DAG);
+  case (uint16_t)~TargetOpcode::STACKMAP:
+    return SDValue(); // Use generic stackmap type legalizer
   }
 }
 
diff --git a/llvm/lib/Target/X86/X86InstrInfo.td b/llvm/lib/Target/X86/X86InstrInfo.td
index 52bab9c79b4..8ab4790aa6f 100644
--- a/llvm/lib/Target/X86/X86InstrInfo.td
+++ b/llvm/lib/Target/X86/X86InstrInfo.td
@@ -169,7 +169,7 @@ def X86vaarg64 :
                          SDNPMemOperand]>;
 def X86callseq_start :
                  SDNode<"ISD::CALLSEQ_START", SDT_X86CallSeqStart,
-                        [SDNPHasChain, SDNPOutGlue]>;
+                        [SDNPHasChain, SDNPOptInGlue, SDNPOutGlue]>;
 def X86callseq_end :
                  SDNode<"ISD::CALLSEQ_END",   SDT_X86CallSeqEnd,
                         [SDNPHasChain, SDNPOptInGlue, SDNPOutGlue]>;
diff --git a/llvm/lib/Target/X86/X86RegisterInfo.cpp b/llvm/lib/Target/X86/X86RegisterInfo.cpp
index d8495e53e0e..f57fdc729a5 100644
--- a/llvm/lib/Target/X86/X86RegisterInfo.cpp
+++ b/llvm/lib/Target/X86/X86RegisterInfo.cpp
@@ -596,6 +596,13 @@ X86RegisterInfo::getPtrSizedFrameRegister(const MachineFunction &MF) const {
   return FrameReg;
 }
 
+int X86RegisterInfo::getReturnAddrLoc(const MachineFunction &MF,
+                                      unsigned &BaseReg) const {
+  const X86MachineFunctionInfo *X86FI = MF.getInfo<X86MachineFunctionInfo>();
+  const TargetFrameLowering *TFL = MF.getSubtarget().getFrameLowering();
+  return TFL->getFrameIndexReference(MF, X86FI->getRAIndex(), BaseReg);
+}
+
 namespace llvm {
 unsigned getX86SubSuperRegisterOrZero(unsigned Reg, MVT::SimpleValueType VT,
                                       bool High) {
diff --git a/llvm/lib/Target/X86/X86RegisterInfo.h b/llvm/lib/Target/X86/X86RegisterInfo.h
index 8de1d0bf8ec..be31ffc3847 100644
--- a/llvm/lib/Target/X86/X86RegisterInfo.h
+++ b/llvm/lib/Target/X86/X86RegisterInfo.h
@@ -126,6 +126,9 @@ public:
   unsigned getBaseRegister() const { return BasePtr; }
   // FIXME: Move to FrameInfok
   unsigned getSlotSize() const { return SlotSize; }
+
+  int getReturnAddrLoc(const MachineFunction &MF,
+                       unsigned &BaseReg) const override;
 };
 
 /// Returns the sub or super register of a specific X86 register.
diff --git a/llvm/lib/Target/X86/X86Subtarget.h b/llvm/lib/Target/X86/X86Subtarget.h
index f026d4295f7..ab87f14c0f4 100644
--- a/llvm/lib/Target/X86/X86Subtarget.h
+++ b/llvm/lib/Target/X86/X86Subtarget.h
@@ -18,6 +18,7 @@
 #include "X86ISelLowering.h"
 #include "X86InstrInfo.h"
 #include "X86SelectionDAGInfo.h"
+#include "X86Values.h"
 #include "llvm/ADT/Triple.h"
 #include "llvm/IR/CallingConv.h"
 #include "llvm/Target/TargetSubtargetInfo.h"
@@ -248,7 +249,7 @@ private:
   X86InstrInfo InstrInfo;
   X86TargetLowering TLInfo;
   X86FrameLowering FrameLowering;
-
+  X86Values VGen;
 public:
   /// This constructor initializes the data members to match that
   /// of the specified triple.
@@ -269,6 +270,7 @@ public:
   const X86RegisterInfo *getRegisterInfo() const override {
     return &getInstrInfo()->getRegisterInfo();
   }
+  const X86Values *getValues() const override { return &VGen; }
 
   /// Returns the minimum alignment known to hold of the
   /// stack frame on entry to the function and which must be maintained by every
diff --git a/llvm/lib/Target/X86/X86Values.cpp b/llvm/lib/Target/X86/X86Values.cpp
new file mode 100644
index 00000000000..f6985e8d71a
--- /dev/null
+++ b/llvm/lib/Target/X86/X86Values.cpp
@@ -0,0 +1,212 @@
+//===--------- X86TargetValues.cpp - X86 specific value generator ---------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "X86Values.h"
+#include "X86InstrInfo.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Target/TargetInstrInfo.h"
+#include "llvm/Target/TargetSubtargetInfo.h"
+
+#define DEBUG_TYPE "stacktransform"
+
+using namespace llvm;
+
+static TemporaryValue *getTemporaryReference(const MachineInstr *MI,
+                                             const VirtRegMap *VRM,
+                                             unsigned Size) {
+  TemporaryValue *Val = nullptr;
+  if(MI->getOperand(0).isReg()) {
+    // Instruction format:  LEA64  rd     rbase  scale# ridx   disp#  rseg
+    // Stack slot reference:              <fi>   1      noreg  off    noreg
+    // TODO check for noreg in ridx & rseg?
+    if(MI->getOperand(1 + X86::AddrBaseReg).isFI() &&
+       MI->getOperand(1 + X86::AddrScaleAmt).isImm() &&
+       MI->getOperand(1 + X86::AddrScaleAmt).getImm() == 1) {
+      assert(MI->getOperand(1 + X86::AddrDisp).isImm() && "Invalid encoding");
+      Val = new TemporaryValue;
+      Val->Type = TemporaryValue::StackSlotRef;
+      Val->Size = Size;
+      Val->Vreg = MI->getOperand(0).getReg();
+      Val->StackSlot = MI->getOperand(1 + X86::AddrBaseReg).getIndex();
+      Val->Offset = MI->getOperand(1 + X86::AddrDisp).getImm();
+    }
+  }
+  return Val;
+}
+
+TemporaryValuePtr
+X86Values::getTemporaryValue(const MachineInstr *MI,
+                             const VirtRegMap *VRM) const {
+  TemporaryValue *Val = nullptr;
+  switch(MI->getOpcode()) {
+  case X86::LEA64r: Val = getTemporaryReference(MI, VRM, 8); break;
+  default: break;
+  }
+  return TemporaryValuePtr(Val);
+}
+
+typedef ValueGenInst::InstType InstType;
+template <InstType T> using RegInstruction = RegInstruction<T>;
+template <InstType T> using ImmInstruction = ImmInstruction<T>;
+
+/// Return whether the machine operand is a specific immediate value.
+static bool isImmOp(const MachineOperand &MO, int64_t Imm) {
+  if(!MO.isImm()) return false;
+  else if(MO.getImm() != Imm) return false;
+  else return true;
+}
+
+/// Return whether the machine operand is a specific register.
+static bool isRegOp(const MachineOperand &MO, unsigned Reg) {
+  if(!MO.isReg()) return false;
+  else if(MO.getReg() != Reg) return false;
+  else return true;
+}
+
+/// Return whether the machine operand is the sentinal %noreg register.
+static bool isNoRegOp(const MachineOperand &MO) { return isRegOp(MO, 0); }
+
+MachineLiveVal *X86Values::genLEAInstructions(const MachineInstr *MI) const {
+  unsigned Reg, Size;
+  int64_t Imm;
+  ValueGenInstList IL;
+
+  // TODO do we need to handle the segment register operand?
+  switch(MI->getOpcode()) {
+  case X86::LEA64r:
+    Size = 8;
+
+    if(MI->getOperand(1 + X86::AddrBaseReg).isFI()) {
+      // Stack slot address
+      if(!isImmOp(MI->getOperand(1 + X86::AddrScaleAmt), 1)) {
+        DEBUG(dbgs() << "Unhandled scale amount for frame index\n");
+        break;
+      }
+
+      if(!isNoRegOp(MI->getOperand(1 + X86::AddrIndexReg))) {
+        DEBUG(dbgs() <<  "Unhandled index register for frame index\n");
+        break;
+      }
+
+      if(!isImmOp(MI->getOperand(1 + X86::AddrDisp), 0)) {
+        DEBUG(dbgs() << "Unhandled index register for frame index\n");
+        break;
+      }
+
+      return new
+        MachineStackObject(MI->getOperand(1 + X86::AddrBaseReg).getIndex(),
+                           false, MI, true);
+    }
+    else if(isRegOp(MI->getOperand(1 + X86::AddrBaseReg), X86::RIP)) {
+      // PC-relative symbol address
+      if(!isImmOp(MI->getOperand(1 + X86::AddrScaleAmt), 1)) {
+        DEBUG(dbgs() << "Unhandled scale amount for PC-relative address\n");
+        break;
+      }
+
+      if(!isNoRegOp(MI->getOperand(1 + X86::AddrIndexReg))) {
+        DEBUG(dbgs() << "Unhandled index register for PC-relative address\n");
+        break;
+      }
+
+      return new
+        MachineSymbolRef(MI->getOperand(1 + X86::AddrDisp), false, MI);
+    }
+    else {
+      // Raw form of LEA
+      if(!MI->getOperand(1 + X86::AddrBaseReg).isReg() ||
+         !MI->getOperand(1 + X86::AddrDisp).isImm()) {
+        DEBUG(dbgs() << "Unhandled base register/displacement operands\n");
+        break;
+      }
+
+      // Initialize to index register * scale if indexing, or zero otherwise
+      Reg = MI->getOperand(1 + X86::AddrIndexReg).getReg();
+      if(Reg) {
+        Imm = MI->getOperand(1 + X86::AddrScaleAmt).getImm();
+        IL.emplace_back(new RegInstruction<InstType::Set>(Reg));
+        IL.emplace_back(new ImmInstruction<InstType::Multiply>(Size, Imm));
+      }
+      else IL.emplace_back(new ImmInstruction<InstType::Set>(Size, 0));
+
+      // Add the base register & displacement
+      Reg = MI->getOperand(1 + X86::AddrBaseReg).getReg();
+      Imm = MI->getOperand(1 + X86::AddrDisp).getImm();
+      IL.emplace_back(new RegInstruction<InstType::Add>(Reg));
+      IL.emplace_back(new ImmInstruction<InstType::Add>(Size, Imm));
+      return new MachineGeneratedVal(IL, MI, true);
+    }
+
+    break;
+  default:
+    DEBUG(dbgs() << "Unhandled LEA machine instruction");
+    break;
+  }
+  return nullptr;
+}
+
+MachineLiveValPtr X86Values::getMachineValue(const MachineInstr *MI) const {
+  MachineLiveVal* Val = nullptr;
+  const MachineOperand *MO, *MO2;
+  const TargetInstrInfo *TII;
+
+  switch(MI->getOpcode()) {
+  case X86::LEA64r:
+    Val = genLEAInstructions(MI);
+    break;
+  case X86::MOV32r0:
+    Val = new MachineImmediate(4, 0, MI, false);
+    break;
+  case X86::MOV32ri:
+    MO = &MI->getOperand(1);
+    if(MO->isImm()) Val = new MachineImmediate(4, MO->getImm(), MI, false);
+    break;
+  case X86::MOV32ri64:
+    // TODO the upper 32 bits of this reference are supposed to be masked
+    MO = &MI->getOperand(1);
+    if(TargetValues::isSymbolValue(MO))
+      Val = new MachineSymbolRef(*MO, false, MI);
+    break;
+  case X86::MOV64ri:
+    MO = &MI->getOperand(1);
+    if(MO->isImm()) Val = new MachineImmediate(8, MO->getImm(), MI, false);
+    else if(TargetValues::isSymbolValue(MO))
+      Val = new MachineSymbolRef(*MO, false, MI);
+    break;
+  case X86::MOV64rm:
+    MO = &MI->getOperand(1 + X86::AddrBaseReg);
+    MO2 = &MI->getOperand(1 + X86::AddrDisp);
+    // Note: codegen'd a PC relative symbol reference
+    // Note 2: we *must* ensure the symbol is const-qualified, otherwise we
+    // risk creating a new value if the symbol's value changes between when the
+    // initial load would have occurred and the transformation, e.g.,
+    //
+    //   movq <ga:mysym>, %rax
+    //   ... (somebody changes mysym's value) ...
+    //   callq <ga:myfunc>
+    //
+    // In this situation, the transformation occurs at the call site and
+    // retrieves the updated value rather than the value that would have been
+    // loaded at the ldr instruction.
+    if(MO->isReg() && MO->getReg() == X86::RIP &&
+       TargetValues::isSymbolValue(MO2) &&
+       TargetValues::isSymbolValueConstant(MO2))
+        Val = new MachineSymbolRef(*MO2, true, MI);
+    break;
+  default:
+    TII =  MI->getParent()->getParent()->getSubtarget().getInstrInfo();
+    DEBUG(dbgs() << "Unhandled opcode: "
+                 << TII->getName(MI->getOpcode()) << "\n");
+    break;
+  }
+
+  return MachineLiveValPtr(Val);
+}
+
diff --git a/llvm/lib/Target/X86/X86Values.h b/llvm/lib/Target/X86/X86Values.h
new file mode 100644
index 00000000000..e468646cbf4
--- /dev/null
+++ b/llvm/lib/Target/X86/X86Values.h
@@ -0,0 +1,26 @@
+//===--------- X86TargetValues.cpp - X86 specific value generator ---------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Target/TargetValues.h"
+
+namespace llvm {
+
+class X86Values final : public TargetValues {
+public:
+  X86Values() {}
+  virtual TemporaryValuePtr getTemporaryValue(const MachineInstr *MI,
+                                              const VirtRegMap *VRM) const;
+  virtual MachineLiveValPtr getMachineValue(const MachineInstr *MI) const;
+
+private:
+  MachineLiveVal *genLEAInstructions(const MachineInstr *LEA) const;
+};
+
+}
+
diff --git a/llvm/lib/Transforms/Instrumentation/CMakeLists.txt b/llvm/lib/Transforms/Instrumentation/CMakeLists.txt
index 9b81f4bb161..7c54e729ebf 100644
--- a/llvm/lib/Transforms/Instrumentation/CMakeLists.txt
+++ b/llvm/lib/Transforms/Instrumentation/CMakeLists.txt
@@ -2,10 +2,13 @@ add_llvm_library(LLVMInstrumentation
   AddressSanitizer.cpp
   BoundsChecking.cpp
   DataFlowSanitizer.cpp
+  MigrationPoints.cpp
   GCOVProfiling.cpp
   MemorySanitizer.cpp
+  InsertStackMaps.cpp
   Instrumentation.cpp
   InstrProfiling.cpp
+  LibcStackMaps.cpp
   SafeStack.cpp
   SanitizerCoverage.cpp
   ThreadSanitizer.cpp
diff --git a/llvm/lib/Transforms/Instrumentation/InsertStackMaps.cpp b/llvm/lib/Transforms/Instrumentation/InsertStackMaps.cpp
new file mode 100644
index 00000000000..fa49e1db123
--- /dev/null
+++ b/llvm/lib/Transforms/Instrumentation/InsertStackMaps.cpp
@@ -0,0 +1,357 @@
+#include <map>
+#include <set>
+#include <vector>
+#include "llvm/Pass.h"
+#include "llvm/Analysis/LiveValues.h"
+#include "llvm/Analysis/PopcornUtil.h"
+#include "llvm/IR/Dominators.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/InstIterator.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/ModuleSlotTracker.h"
+#include "llvm/IR/Type.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+#define DEBUG_TYPE "insert-stackmaps"
+
+using namespace llvm;
+
+static cl::opt<bool>
+NoLiveVals("no-live-vals",
+           cl::desc("Don't add live values to inserted stackmaps"),
+           cl::init(false),
+           cl::Hidden);
+
+namespace {
+
+/* Track slots for unnamed values */
+static ModuleSlotTracker *SlotTracker = nullptr;
+
+/* Sort values based on name */
+struct ValueComp
+{
+  bool operator ()(const Value *a, const Value *b)
+  {
+    if(a->hasName() && b->hasName())
+      return a->getName().compare(b->getName()) < 0;
+    else if(a->hasName()) return true;
+    else if(b->hasName()) return false;
+    else {
+      int slot_a = SlotTracker->getLocalSlot(a),
+          slot_b = SlotTracker->getLocalSlot(b);
+      return slot_a < slot_b;
+    }
+  }
+};
+
+/**
+ * This class instruments equivalence points in the IR with LLVM's stackmap
+ * intrinsic.  This tells the backend to record the locations of IR values
+ * after register allocation in a separate ELF section.
+ */
+class InsertStackMaps : public ModulePass
+{
+private:
+  /* Some useful typedefs */
+  typedef SmallVector<const Instruction *, 4> InstVec;
+  typedef DenseMap<const Instruction *, InstVec> InstHidingMap;
+  typedef SmallVector<const Argument *, 4> ArgVec;
+  typedef DenseMap<const Instruction *, ArgVec> ArgHidingMap;
+
+public:
+  static char ID;
+  size_t callSiteID;
+  size_t numInstrumented;
+
+  InsertStackMaps() : ModulePass(ID), callSiteID(0), numInstrumented(0) {
+    initializeInsertStackMapsPass(*PassRegistry::getPassRegistry());
+  }
+  ~InsertStackMaps() {}
+
+  /* ModulePass virtual methods */
+  virtual const char *getPassName() const { return "Insert stackmaps"; }
+
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const
+  {
+    AU.addRequired<LiveValues>();
+    AU.addRequired<DominatorTreeWrapperPass>();
+    AU.setPreservesCFG();
+  }
+
+  /**
+   * Use liveness analysis to insert stackmap intrinsics into the IR to record
+   * live values at equivalence points.
+   *
+   * Note: currently we only insert stackmaps at function call sites.
+   */
+  virtual bool runOnModule(Module &M)
+  {
+    bool modified = false;
+
+    std::set<const Value *> *live;
+    std::set<const Value *, ValueComp> sortedLive;
+    InstHidingMap hiddenInst;
+    ArgHidingMap hiddenArgs;
+
+    DEBUG(errs() << "\n********** Begin InsertStackMaps **********\n"
+                 << "********** Module: " << M.getName() << " **********\n\n");
+
+    this->createSMType(M);
+    if(this->addSMDeclaration(M)) modified = true;
+    SlotTracker = new ModuleSlotTracker(&M);
+
+    modified |= this->removeOldStackmaps(M);
+
+    /* Iterate over all functions/basic blocks/instructions. */
+    for(Module::iterator f = M.begin(), fe = M.end(); f != fe; f++)
+    {
+      if(f->isDeclaration()) continue;
+
+      DEBUG(errs() << "InsertStackMaps: entering function "
+                   << f->getName() << "\n");
+
+      LiveValues &liveVals = getAnalysis<LiveValues>(*f);
+      DominatorTree &DT = getAnalysis<DominatorTreeWrapperPass>(*f).getDomTree();
+      SlotTracker->incorporateFunction(*f);
+      std::set<const Value *>::const_iterator v, ve;
+      getHiddenVals(*f, hiddenInst, hiddenArgs);
+
+      /* Find call sites in the function. */
+      for(Function::iterator b = f->begin(), be = f->end(); b != be; b++)
+      {
+        DEBUG(
+          errs() << "InsertStackMaps: entering basic block ";
+          b->printAsOperand(errs(), false);
+          errs() << "\n"
+        );
+
+        for(BasicBlock::iterator i = b->begin(), ie = b->end(); i != ie; i++)
+        {
+          if(Popcorn::isCallSite(&*i))
+          {
+            CallSite CS(i);
+            if(CS.isInvoke())
+            {
+              DEBUG(dbgs() << "WARNING: unhandled invoke:"; CS->dump());
+              continue;
+            }
+
+            IRBuilder<> builder(CS->getNextNode());
+            std::vector<Value *> args(2);
+            args[0] = ConstantInt::getSigned(Type::getInt64Ty(M.getContext()),
+                                             this->callSiteID++);
+            args[1] = ConstantInt::getSigned(Type::getInt32Ty(M.getContext()),
+                                             0);
+
+            if(NoLiveVals) {
+              builder.CreateCall(this->SMFunc, ArrayRef<Value*>(args));
+              this->numInstrumented++;
+              continue;
+            }
+
+            live = liveVals.getLiveValues(&*i);
+            for(const Value *val : *live) sortedLive.insert(val);
+            for(const auto &pair : hiddenInst) {
+              /*
+               * The two criteria for inclusion of a hidden value are:
+               *   1. The value's definition dominates the call
+               *   2. A use which hides the definition is in the stackmap
+               */
+              if(DT.dominates(pair.first, i) && live->count(pair.first))
+                for(auto &inst : pair.second) sortedLive.insert(inst);
+            }
+            for(const auto &pair : hiddenArgs) {
+              /*
+               * Similar criteria apply as above, except we know arguments
+               * dominate the entire function.
+               */
+              if(live->count(pair.first))
+                for(auto &inst : pair.second) sortedLive.insert(inst);
+            }
+            delete live;
+
+            DEBUG(
+              const Function *calledFunc;
+
+              errs() << "  ";
+              if(!CS->getType()->isVoidTy()) {
+                CS->printAsOperand(errs(), false);
+                errs() << " ";
+              }
+              else errs() << "(void) ";
+
+              calledFunc = CS.getCalledFunction();
+              if(calledFunc && calledFunc->hasName())
+              {
+                StringRef name = CS.getCalledFunction()->getName();
+                errs() << name << " ";
+              }
+              errs() << "ID: " << this->callSiteID;
+
+              errs() << ", " << sortedLive.size() << " live value(s)\n   ";
+              for(const Value *val : sortedLive) {
+                errs() << " ";
+                val->printAsOperand(errs(), false);
+              }
+              errs() << "\n";
+            );
+
+            for(v = sortedLive.begin(), ve = sortedLive.end(); v != ve; v++)
+              args.push_back((Value*)*v);
+            builder.CreateCall(this->SMFunc, ArrayRef<Value*>(args));
+            sortedLive.clear();
+            this->numInstrumented++;
+          }
+        }
+      }
+
+      hiddenInst.clear();
+      hiddenArgs.clear();
+      this->callSiteID = 0;
+    }
+
+    DEBUG(
+      errs() << "InsertStackMaps: finished module " << M.getName() << ", added "
+             << this->numInstrumented << " stackmaps\n\n";
+    );
+
+    if(numInstrumented > 0) modified = true;
+    delete SlotTracker;
+
+    return modified;
+  }
+
+private:
+  /* Name of stack map intrinsic */
+  static const StringRef SMName;
+
+  /* Stack map instruction creation */
+  Function *SMFunc;
+  FunctionType *SMTy; // Used for creating function declaration
+
+  /**
+   * Create the function type for the stack map intrinsic.
+   */
+  void createSMType(const Module &M)
+  {
+    std::vector<Type*> params(2);
+    params[0] = Type::getInt64Ty(M.getContext());
+    params[1] = Type::getInt32Ty(M.getContext());
+    this->SMTy = FunctionType::get(Type::getVoidTy(M.getContext()),
+                                                   ArrayRef<Type*>(params),
+                                                   true);
+  }
+
+  /**
+   * Add the stackmap intrinisic's function declaration if not already present.
+   * Return true if the declaration was added, or false if it's already there.
+   */
+  bool addSMDeclaration(Module &M)
+  {
+    if(!(this->SMFunc = M.getFunction(this->SMName)))
+    {
+      DEBUG(errs() << "Adding stackmap function declaration to " << M.getName() << "\n");
+      this->SMFunc = cast<Function>(M.getOrInsertFunction(this->SMName, this->SMTy));
+      this->SMFunc->setCallingConv(CallingConv::C);
+      return true;
+    }
+    else return false;
+  }
+
+  /**
+   * Iterate over all instructions, removing previously found stackmaps.
+   */
+  bool removeOldStackmaps(Module &M)
+  {
+    bool modified = false;
+    CallInst* CI;
+    const Function *F;
+
+    DEBUG(dbgs() << "Searching for/removing old stackmaps\n";);
+
+    for(Module::iterator f = M.begin(), fe = M.end(); f != fe; f++) {
+      for(Function::iterator bb = f->begin(), bbe = f->end(); bb != bbe; bb++) {
+        for(BasicBlock::iterator i = bb->begin(), ie = bb->end(); i != ie; i++) {
+          if((CI = dyn_cast<CallInst>(&*i))) {
+            F = CI->getCalledFunction();
+            if(F && F->hasName() && F->getName() == SMName) {
+              i = i->eraseFromParent()->getPrevNode();
+              modified = true;
+            }
+          }
+        }
+      }
+    }
+
+    DEBUG(if(modified)
+            dbgs() << "WARNING: found previous run of Popcorn passes!\n";);
+
+    return modified;
+  }
+
+  /**
+   * Gather a list of values which may be "hidden" from live value analysis.
+   * This function collects the values used in these instructions, which are
+   * later added to the appropriate stackmaps.
+   *
+   *  - Instructions which access fields of structs or entries of arrays, like
+   *    getelementptr, can interfere with the live value analysis to hide the
+   *    backing values used in the instruction.  For example, the following IR
+   *    obscures %arr from the live value analysis:
+   *
+   *  %arr = alloca [4 x double], align 8
+   *  %arrayidx = getelementptr inbounds [4 x double], [4 x double]* %arr, i64 0, i64 0
+   *
+   *  -> Access to %arr might only happen through %arrayidx, and %arr may not
+   *     be used any more
+   *
+   */
+  void getHiddenVals(Function &F, InstHidingMap &inst, ArgHidingMap &args)
+  {
+    /* Does the instruction potentially hide values from liveness analysis? */
+    auto hidesValues = [](const Instruction *I) {
+      if(isa<ExtractElementInst>(I) || isa<InsertElementInst>(I) ||
+         isa<ExtractValueInst>(I) || isa<InsertValueInst>(I) ||
+         isa<GetElementPtrInst>(I) || isa<BitCastInst>(I))
+        return true;
+      else return false;
+    };
+
+    /* Search for instructions that obscure live values & record operands */
+    for(inst_iterator i = inst_begin(F), e = inst_end(F); i != e; ++i) {
+      InstVec &InstsHidden = inst[&*i];
+      ArgVec &ArgsHidden = args[&*i];
+
+      if(hidesValues(&*i)) {
+        for(unsigned op = 0; op < i->getNumOperands(); op++) {
+          if(isa<Instruction>(i->getOperand(op)))
+            InstsHidden.push_back(cast<Instruction>(i->getOperand(op)));
+          else if(isa<Argument>(i->getOperand(op)))
+            ArgsHidden.push_back(cast<Argument>(i->getOperand(op)));
+        }
+      }
+    }
+  }
+};
+
+} /* end anonymous namespace */
+
+char InsertStackMaps::ID = 0;
+const StringRef InsertStackMaps::SMName = "llvm.experimental.stackmap";
+
+INITIALIZE_PASS_BEGIN(InsertStackMaps, "insert-stackmaps",
+                      "Instrument equivalence points with stack maps",
+                      false, false)
+INITIALIZE_PASS_DEPENDENCY(LiveValues)
+INITIALIZE_PASS_DEPENDENCY(DominatorTreeWrapperPass)
+INITIALIZE_PASS_END(InsertStackMaps, "insert-stackmaps",
+                    "Instrument equivalence points with stack maps",
+                    false, false)
+
+namespace llvm {
+  ModulePass *createInsertStackMapsPass() { return new InsertStackMaps(); }
+}
+
diff --git a/llvm/lib/Transforms/Instrumentation/Instrumentation.cpp b/llvm/lib/Transforms/Instrumentation/Instrumentation.cpp
index 27505859100..937a3be43fc 100644
--- a/llvm/lib/Transforms/Instrumentation/Instrumentation.cpp
+++ b/llvm/lib/Transforms/Instrumentation/Instrumentation.cpp
@@ -24,8 +24,11 @@ void llvm::initializeInstrumentation(PassRegistry &Registry) {
   initializeAddressSanitizerPass(Registry);
   initializeAddressSanitizerModulePass(Registry);
   initializeBoundsCheckingPass(Registry);
+  initializeMigrationPointsPass(Registry);
   initializeGCOVProfilerPass(Registry);
+  initializeInsertStackMapsPass(Registry);
   initializeInstrProfilingPass(Registry);
+  initializeLibcStackMapsPass(Registry);
   initializeMemorySanitizerPass(Registry);
   initializeThreadSanitizerPass(Registry);
   initializeSanitizerCoverageModulePass(Registry);
diff --git a/llvm/lib/Transforms/Instrumentation/LibcStackMaps.cpp b/llvm/lib/Transforms/Instrumentation/LibcStackMaps.cpp
new file mode 100644
index 00000000000..40d71b02dca
--- /dev/null
+++ b/llvm/lib/Transforms/Instrumentation/LibcStackMaps.cpp
@@ -0,0 +1,229 @@
+#include <map>
+#include <vector>
+#include "llvm/Pass.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/Type.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/Path.h"
+#include "llvm/Support/raw_ostream.h"
+
+#define DEBUG_TYPE "libc-stackmaps"
+
+using namespace llvm;
+
+namespace {
+
+/**
+ * Instrument thread starting points with stackmaps.  These are the only
+ * functions inside of libc for which we want to generate metadata, since we
+ * disallow migration inside the public libc API.
+ */
+// TODO: only implemented for musl-libc!
+class LibcStackMaps : public ModulePass
+{
+public:
+  static char ID;
+  size_t numInstrumented;
+
+  LibcStackMaps() : ModulePass(ID), numInstrumented(0) {
+    initializeLibcStackMapsPass(*PassRegistry::getPassRegistry());
+  }
+  ~LibcStackMaps() {}
+
+  /* ModulePass virtual methods */
+  virtual const char *getPassName() const
+  { return "Insert stackmaps in libc thread start functions"; }
+
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const
+  { AU.setPreservesCFG(); }
+
+  virtual bool runOnModule(Module &M)
+  {
+    int64_t smid;
+    bool modified = false;
+    Function *F;
+    std::map<std::string, std::vector<std::string> >::const_iterator file;
+
+    /* Is this a module (i.e., source file) we're interested in? */
+    if((file = funcs.find(sys::path::stem(M.getName()))) != funcs.end())
+    {
+      DEBUG(dbgs() << "\n********** Begin LibcStackMaps **********\n"
+                   << "********** Module: " << file->first << " **********\n\n");
+
+      this->createSMType(M);
+      modified |= this->addSMDeclaration(M);
+
+      /* Iterate over thread starting functions in the module */
+      for(size_t f = 0, fe = file->second.size(); f < fe; f++)
+      {
+        DEBUG(dbgs() << "LibcStackMaps: entering thread starting function "
+                     << file->second[f] << "\n");
+
+        F = M.getFunction(file->second[f]);
+        assert(F && !F->isDeclaration() && "No thread function definition");
+        modified |= this->removeOldStackmaps(F);
+        assert(smids.find(file->second[f]) != smids.end() && "No ID for function");
+        smid = smids.find(file->second[f])->second;
+
+        /*
+         * Look for & instrument a generic call instruction followed by a call
+         * to an exit function, e.g.,
+         *
+         *   %call = call i32 %main(...)
+         *   call void @exit(i32 %call)
+         */
+        for(Function::iterator bb = F->begin(), be = F->end(); bb != be; bb++)
+        {
+          bool track = false;
+          for(BasicBlock::reverse_iterator i = bb->rbegin(), ie = bb->rend();
+              i != ie; i++)
+          {
+            if(isExitCall(*i)) track = true;
+            else if(track && isa<CallInst>(*i))
+            {
+              IRBuilder<> builder(i->getNextNode());
+              std::vector<Value *> args(2);
+              args[0] = ConstantInt::getSigned(Type::getInt64Ty(M.getContext()), smid);
+              args[1] = ConstantInt::getSigned(Type::getInt32Ty(M.getContext()), 0);
+              builder.CreateCall(this->SMFunc, ArrayRef<Value*>(args));
+              this->numInstrumented++;
+              break;
+            }
+          }
+        }
+      }
+
+      DEBUG(dbgs() << "LibcStackMaps: finished module " << M.getName()
+                   << ", added " << this->numInstrumented << " stackmaps\n\n";);
+    }
+
+    if(numInstrumented > 0) modified = true;
+    return modified;
+  }
+
+private:
+  /* Name of stack map intrinsic */
+  static const StringRef SMName;
+
+  /* Stack map instruction creation */
+  Function *SMFunc;
+  FunctionType *SMTy; // Used for creating function declaration
+
+  /* Files, functions & IDs */
+  static const std::map<std::string, std::vector<std::string> > funcs;
+  static const std::map<std::string, int64_t> smids;
+  static const std::vector<std::string> exitFuncs;
+
+  /**
+   * Create the function type for the stack map intrinsic.
+   */
+  void createSMType(const Module &M)
+  {
+    std::vector<Type*> params(2);
+    params[0] = Type::getInt64Ty(M.getContext());
+    params[1] = Type::getInt32Ty(M.getContext());
+    this->SMTy = FunctionType::get(Type::getVoidTy(M.getContext()),
+                                                   ArrayRef<Type*>(params),
+                                                   true);
+  }
+
+  /**
+   * Add the stackmap intrinisic's function declaration if not already present.
+   * Return true if the declaration was added, or false if it's already there.
+   */
+  bool addSMDeclaration(Module &M)
+  {
+    if(!(this->SMFunc = M.getFunction(this->SMName)))
+    {
+      DEBUG(dbgs() << "Adding stackmap function declaration to " << M.getName() << "\n");
+      this->SMFunc = cast<Function>(M.getOrInsertFunction(this->SMName, this->SMTy));
+      this->SMFunc->setCallingConv(CallingConv::C);
+      return true;
+    }
+    else return false;
+  }
+
+  /**
+   * Iterate over all instructions, removing previously found stackmaps.
+   */
+  bool removeOldStackmaps(Function *F)
+  {
+    bool modified = false;
+    CallInst* CI;
+    const Function *CurF;
+
+    DEBUG(dbgs() << "Searching for/removing old stackmaps\n";);
+
+    for(Function::iterator bb = F->begin(), bbe = F->end(); bb != bbe; bb++) {
+      for(BasicBlock::iterator i = bb->begin(), ie = bb->end(); i != ie; i++) {
+        if((CI = dyn_cast<CallInst>(&*i))) {
+          CurF = CI->getCalledFunction();
+          if(CurF && CurF->hasName() && CurF->getName() == SMName) {
+            i = i->eraseFromParent()->getPrevNode();
+            modified = true;
+          }
+        }
+      }
+    }
+
+    DEBUG(if(modified) dbgs() << "WARNING: found previous stackmaps!\n";);
+    return modified;
+  }
+
+  /**
+   * Return whether or not the instruction is a call to an exit function.
+   */
+  bool isExitCall(Instruction &I)
+  {
+    CallInst *CI;
+    Function *F;
+
+    if((CI = dyn_cast<CallInst>(&I)))
+    {
+      F = CI->getCalledFunction();
+      if(F && F->hasName())
+        for(size_t i = 0, e = exitFuncs.size(); i < e; i++)
+          if(F->getName() == exitFuncs[i]) return true;
+    }
+
+    return false;
+  }
+};
+
+} /* end anonymous namespace */
+
+char LibcStackMaps::ID = 0;
+const StringRef LibcStackMaps::SMName = "llvm.experimental.stackmap";
+
+/**
+ * Map a source code filename (minus the extension) to the names of functions
+ * inside which are to be instrumented.
+ */
+const std::map<std::string, std::vector<std::string> > LibcStackMaps::funcs = {
+  {"__libc_start_main", {"__libc_start_main"}},
+  {"pthread_create", {"start", "start_c11"}}
+};
+
+/* Map a function name to the stackmap ID representing that function. */
+const std::map<std::string, int64_t> LibcStackMaps::smids = {
+  {"__libc_start_main", UINT64_MAX},
+  {"start", UINT64_MAX - 1},
+  {"start_c11", UINT64_MAX - 2}
+};
+
+/**
+ * Thread exit function names, used to search for starting function call site
+ * to be instrumented with stackmap.
+ */
+const std::vector<std::string> LibcStackMaps::exitFuncs = {
+  "exit", "pthread_exit", "__pthread_exit"
+};
+
+INITIALIZE_PASS(LibcStackMaps, "libc-stackmaps",
+  "Instrument libc thread start functions with stack maps", false, false)
+
+namespace llvm {
+  ModulePass *createLibcStackMapsPass() { return new LibcStackMaps(); }
+}
+
diff --git a/llvm/lib/Transforms/Instrumentation/MigrationPoints.cpp b/llvm/lib/Transforms/Instrumentation/MigrationPoints.cpp
new file mode 100644
index 00000000000..884f8999c81
--- /dev/null
+++ b/llvm/lib/Transforms/Instrumentation/MigrationPoints.cpp
@@ -0,0 +1,490 @@
+//===- MigrationPoints.cpp ------------------------------------------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Instruments the migration points selected by SelectMigrationPoints.  May
+// additionally add HTM instrumentation if selected by user & supported by the
+// architecture.
+//
+//===----------------------------------------------------------------------===//
+
+#include <fstream>
+#include <map>
+#include "llvm/Pass.h"
+#include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/Triple.h"
+#include "llvm/Analysis/PopcornUtil.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/Module.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_os_ostream.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "migration-points"
+#define MIGRATE_FLAG_NAME "__migrate_flag"
+
+/// Disable rollback-only transactions for PowerPC.
+const static cl::opt<bool>
+NoROTPPC("htm-ppc-no-rot", cl::Hidden, cl::init(false),
+  cl::desc("Disable rollback-only transactions in HTM instrumentation "
+           "(PowerPC only)"));
+
+/// Add counters to abort handlers for the specified function.  Allows in-depth
+/// profiling of which HTM sections added to the function are causing aborts.
+const static cl::opt<std::string>
+AbortCount("abort-count", cl::Hidden, cl::init(""),
+  cl::desc("Add counters for each abort handler in the specified function"),
+  cl::value_desc("function"));
+
+STATISTIC(NumMigPoints, "Number of migration points added");
+STATISTIC(NumHTMBegins, "Number of HTM begin intrinsics added");
+STATISTIC(NumHTMEnds, "Number of HTM end intrinsics added");
+
+namespace {
+
+/// MigrationPoints - insert migration points into functions, optionally adding
+/// HTM execution.
+class MigrationPoints : public FunctionPass
+{
+public:
+  static char ID;
+
+  MigrationPoints() : FunctionPass(ID) {}
+  ~MigrationPoints() {}
+
+  virtual const char *getPassName() const
+  { return "Insert migration points"; }
+
+  /// Generate the migration library API function declaration.
+  void addMigrationIntrinsic(Module &M, bool DoHTM) {
+    LLVMContext &C = M.getContext();
+    Type *VoidTy = Type::getVoidTy(C);
+    PointerType *VoidPtrTy = Type::getInt8PtrTy(C, 0);
+    std::vector<Type *> FuncPtrArgTy = { VoidPtrTy };
+    FunctionType *FuncPtrTy = FunctionType::get(VoidTy, FuncPtrArgTy, false);
+    CallbackType = PointerType::get(FuncPtrTy, 0);
+    std::vector<Type *> ArgTy = { CallbackType, VoidPtrTy };
+    FunctionType *FuncTy = FunctionType::get(VoidTy, ArgTy, false);
+    if(DoHTM) {
+      MigrateAPI = M.getOrInsertFunction("migrate", FuncTy);
+      MigrateFlag = cast<GlobalValue>(
+        M.getOrInsertGlobal(MIGRATE_FLAG_NAME, Type::getInt32Ty(C)));
+      // TODO this needs to be thread-local storage
+      //MigrateFlag->setThreadLocal(true);
+    }
+    else {
+      MigrateAPI = M.getOrInsertFunction("check_migrate", FuncTy);
+      MigrateFlag = nullptr;
+    }
+  }
+
+  virtual bool doInitialization(Module &M) {
+    Triple TheTriple(M.getTargetTriple());
+    Arch = TheTriple.getArch();
+    Ty = Popcorn::getInstrumentationType(M);
+
+    switch(Ty) {
+    case Popcorn::HTM:
+      if(HTMBegin.find(Arch) != HTMBegin.end()) {
+        DEBUG(dbgs() << "\n-> MigrationPoints: Adding HTM intrinsics for '"
+                     << TheTriple.getArchName() << "' <-\n");
+        HTMBeginDecl =
+          Intrinsic::getDeclaration(&M, HTMBegin.find(Arch)->second);
+        HTMEndDecl = Intrinsic::getDeclaration(&M, HTMEnd.find(Arch)->second);
+        HTMTestDecl =
+          Intrinsic::getDeclaration(&M, HTMTest.find(Arch)->second);
+        addMigrationIntrinsic(M, true);
+      }
+      else {
+        DEBUG(
+          dbgs() << "\n-> MigrationPoints: Selected HTM instrumentation but '"
+                 << TheTriple.getArchName()
+                 << "' is not supported, falling back to call-outs <-\n"
+        );
+        Ty = Popcorn::Cycles;
+        addMigrationIntrinsic(M, false);
+      }
+      break;
+    case Popcorn::Cycles:
+      addMigrationIntrinsic(M, false);
+      break;
+    case Popcorn::None:
+      return false;
+    default: llvm_unreachable("Unknown instrumentation type"); break;
+    }
+
+    // Add abort counters if somebody requested abort profiling.
+    const Function *CounterFunc;
+    if(AbortCount != "" && (CounterFunc = M.getFunction(AbortCount)) &&
+       !CounterFunc->isDeclaration()) {
+      LLVMContext &C = M.getContext();
+      IntegerType *Unsigned = Type::getInt32Ty(C);
+      GlobalVariable *NumCtrs = cast<GlobalVariable>(
+        M.getOrInsertGlobal("__num_abort_counters", Unsigned));
+      NumCtrs->setInitializer(ConstantInt::get(Unsigned, 1024, false));
+      Type *ArrType = ArrayType::get(Type::getInt64Ty(C), 1024);
+      AbortCounters = cast<GlobalVariable>(
+        M.getOrInsertGlobal("__abort_counters", ArrType));
+      AbortCounters->setInitializer(ConstantAggregateZero::get(ArrType));
+    }
+
+    return true;
+  }
+
+  /// Insert migration points into functions
+  virtual bool runOnFunction(Function &F)
+  {
+    if(Ty == Popcorn::None) return false;
+
+    DEBUG(dbgs() << "\n********** ADD MIGRATION POINTS **********\n"
+                 << "********** Function: " << F.getName() << "\n\n");
+
+    initializeAnalysis(F);
+
+    // Find all instrumentation points marked by previous analysis passes.
+    findInstrumentationPoints(F);
+
+    // Apply code transformations to marked instructions, including adding
+    // migration points & HTM instrumentation.
+    addMigrationPoints(F);
+
+    // Write the modified IR & close the abort handler map file if we
+    // instrumented the code to profile abort handlers.
+    if(MapFile.is_open()) {
+      MapFile.close();
+      std::fstream TheIR("htm-abort-ir.ll", std::ios::out | std::ios::trunc);
+      raw_os_ostream IRStream(TheIR);
+      F.print(IRStream);
+      TheIR.close();
+    }
+
+    return true;
+  }
+
+  /// Reset all analysis.
+  void initializeAnalysis(const Function &F) {
+    DoHTMInst = false;
+    DoAbortInstrument = false;
+    MigPointInsts.clear();
+    HTMBeginInsts.clear();
+    HTMEndInsts.clear();
+
+    if(Ty == Popcorn::HTM) {
+      // We've checked at the global scope whether HTM is enabled for the
+      // module.  Check whether the target-specific feature for HTM is enabled
+      // for the current function.
+      if(!F.hasFnAttribute("target-features")) {
+        DEBUG(dbgs() << "-> Disabled HTM instrumentation, "
+                        "no 'target-features' attribute\n");
+        return;
+      }
+
+      Attribute TargetAttr = F.getFnAttribute("target-features");
+      assert(TargetAttr.isStringAttribute() && "Invalid target features");
+      StringRef AttrVal = TargetAttr.getValueAsString();
+      size_t pos = StringRef::npos;
+
+      switch(Arch) {
+      case Triple::ppc64le: pos = AttrVal.find("+htm"); break;
+      case Triple::x86_64: pos = AttrVal.find("+rtm"); break;
+      default: break;
+      }
+
+      DoHTMInst = (pos != StringRef::npos);
+
+      DEBUG(if(!DoHTMInst) dbgs() << "-> Disabled HTM instrumentation, HTM "
+                                     "not listed in target-features\n");
+
+      // Enable HTM abort handler profiling if specified
+      if(DoHTMInst && AbortCount == F.getName()) {
+        DoAbortInstrument = true;
+        AbortHandlerCount = 0;
+        MapFile.open("htm-abort.map", std::ios::out | std::ios::trunc);
+        assert(MapFile.is_open() && MapFile.good() &&
+               "Could not open abort handler map file");
+      }
+    }
+
+    DEBUG(
+      if(DoHTMInst) {
+        dbgs() << "-> Adding HTM instrumentation\n";
+        if(DoAbortInstrument) dbgs() << "  - Adding abort counters\n";
+      }
+      else dbgs() << "-> Adding call-out instrumentation\n";
+    );
+  }
+
+private:
+  //===--------------------------------------------------------------------===//
+  // Types & fields
+  //===--------------------------------------------------------------------===//
+
+  /// Type of the instrumentation to be applied to functions.
+  enum Popcorn::InstrumentType Ty;
+
+  /// The current architecture - used to access architecture-specific HTM calls
+  Triple::ArchType Arch;
+
+  /// Should we instrument code with HTM execution?  Set if HTM is enabled on
+  /// the command line and if the target is supported
+  bool DoHTMInst;
+
+  /// Should we instrument HTM abort handlers with counters for precise
+  /// profiling of which code locations cause aborts & all associated state.
+  bool DoAbortInstrument;
+  GlobalVariable *AbortCounters;
+  unsigned AbortHandlerCount;
+  std::ofstream MapFile;
+
+  /// Function declaration & migration node ID for migration library API
+  Constant *MigrateAPI;
+  GlobalValue *MigrateFlag;
+  PointerType *CallbackType;
+
+  /// Function declarations for HTM intrinsics
+  Value *HTMBeginDecl;
+  Value *HTMEndDecl;
+  Value *HTMTestDecl;
+
+  /// Per-architecture LLVM intrinsic IDs for HTM begin, HTM end, and testing
+  /// if executing transactionally
+  typedef std::map<Triple::ArchType, Intrinsic::ID> IntrinsicMap;
+  const static IntrinsicMap HTMBegin;
+  const static IntrinsicMap HTMEnd;
+  const static IntrinsicMap HTMTest;
+
+  /// Code locations marked for instrumentation.
+  SmallPtrSet<Instruction *, 32> MigPointInsts;
+  SmallPtrSet<Instruction *, 32> HTMBeginInsts;
+  SmallPtrSet<Instruction *, 32> HTMEndInsts;
+
+  //===--------------------------------------------------------------------===//
+  // Instrumentation implementation
+  //===--------------------------------------------------------------------===//
+
+  /// Find instructions tagged by SelectMigrationPoints with instrumentation
+  /// metadata.
+  void findInstrumentationPoints(Function &F) {
+    for(Function::iterator BB = F.begin(), BBE = F.end(); BB != BBE; BB++) {
+      for(BasicBlock::iterator I = BB->begin(), IE = BB->end(); I != IE; I++) {
+        if(Popcorn::hasEquivalencePointMetadata(I)) MigPointInsts.insert(I);
+        if(Popcorn::isHTMBeginPoint(I)) HTMBeginInsts.insert(I);
+        if(Popcorn::isHTMEndPoint(I)) HTMEndInsts.insert(I);
+      }
+    }
+  }
+
+  /// Add a migration point directly before an instruction.
+  void addMigrationPoint(Instruction *I) {
+    LLVMContext &C = I->getContext();
+    IRBuilder<> Worker(I);
+    std::vector<Value *> Args = {
+      ConstantPointerNull::get(CallbackType),
+      ConstantPointerNull::get(Type::getInt8PtrTy(C, 0))
+    };
+    Worker.CreateCall(MigrateAPI, Args);
+  }
+
+  // Note: because we're only supporting 2 architectures for now, we're not
+  // going to abstract this out into the appropriate Target/* folders
+
+  /// Add HTM begin which avoids doing any work unless there's an abort.  In
+  /// the event of an abort, the instrumentation checks if it should migrate,
+  /// and if so, invokes the migration API.
+  void addHTMBeginInternal(Instruction *I, Value *Begin, Value *Comparison) {
+    LLVMContext &C = I->getContext();
+    BasicBlock *CurBB = I->getParent(), *NewSuccBB, *FlagCheckBB, *MigPointBB;
+
+    // Set up each of the new basic blocks
+    NewSuccBB =
+      CurBB->splitBasicBlock(I, "migpointsucc" + std::to_string(NumMigPoints));
+    MigPointBB =
+      BasicBlock::Create(C, "migpoint" + std::to_string(NumMigPoints),
+                         CurBB->getParent(), NewSuccBB);
+    FlagCheckBB =
+      BasicBlock::Create(C, "migflagcheck" + std::to_string(NumMigPoints),
+                         CurBB->getParent(), MigPointBB);
+
+    // Add check & branch based on HTM begin result Comparison.  The true
+    // target of the branch is when we've started the transaction.
+    IRBuilder<> HTMWorker(CurBB->getTerminator());
+    HTMWorker.CreateCondBr(Comparison, NewSuccBB, FlagCheckBB);
+    CurBB->getTerminator()->eraseFromParent();
+
+    // Check flag to see if we should invoke migration library API.
+    IRBuilder<> FlagCheckWorker(FlagCheckBB);
+    if(DoAbortInstrument) {
+      assert(AbortHandlerCount < 1024 && "Too abort handler many counters!");
+
+      // Write the name of the basic block to the map file so we can map abort
+      // counters to their basic blocks.
+      if(!AbortHandlerCount) MapFile << FlagCheckBB->getName().str();
+      else MapFile << " " << FlagCheckBB->getName().str();
+
+      // Add instrumentation to increment the counter's value.
+      std::string CtrNum(std::to_string(AbortHandlerCount));
+      std::vector<Value *> Idx = {
+        ConstantInt::get(Type::getInt64Ty(C), 0),
+        ConstantInt::get(Type::getInt64Ty(C), AbortHandlerCount),
+      };
+      Value *One = ConstantInt::get(Type::getInt64Ty(C), 1, false);
+      Value *GEP = FlagCheckWorker.CreateInBoundsGEP(AbortCounters, Idx,
+                                                     "ctrptr" + CtrNum);
+      Value *CtrVal = FlagCheckWorker.CreateLoad(GEP, "ctr" + CtrNum);
+      Value *Inc = FlagCheckWorker.CreateAdd(CtrVal, One);
+      FlagCheckWorker.CreateStore(Inc, GEP);
+
+      AbortHandlerCount++;
+    }
+    Value *Flag = FlagCheckWorker.CreateLoad(MigrateFlag);
+    Value *NegOne = ConstantInt::get(Type::getInt32Ty(C), -1, true);
+    Value *Cmp = FlagCheckWorker.CreateICmpEQ(Flag, NegOne);
+    FlagCheckWorker.CreateCondBr(Cmp, NewSuccBB, MigPointBB);
+
+    // Add call to migration library API.
+    IRBuilder<> MigPointWorker(MigPointBB);
+    std::vector<Value *> Args = {
+      ConstantPointerNull::get(CallbackType),
+      ConstantPointerNull::get(Type::getInt8PtrTy(C, 0))
+    };
+    MigPointWorker.CreateCall(MigrateAPI, Args);
+    MigPointWorker.CreateBr(NewSuccBB);
+  }
+
+  /// Add a transactional execution begin intrinsic for PowerPC, optionally
+  /// with rollback-only transactions.
+  void addPowerPCHTMBegin(Instruction *I) {
+    LLVMContext &C = I->getContext();
+    IRBuilder<> Worker(I);
+    std::vector<Value *> Args = { ConstantInt::get(Type::getInt32Ty(C),
+                                                   NoROTPPC ? 0 : 1,
+                                                   false) };
+    Value *HTMBeginVal = Worker.CreateCall(HTMBeginDecl, Args);
+    Value *Zero = ConstantInt::get(Type::getInt32Ty(C), 0, false);
+    Value *Cmp = Worker.CreateICmpNE(HTMBeginVal, Zero);
+    addHTMBeginInternal(I, HTMBeginVal, Cmp);
+  }
+
+  /// Add a transactional execution begin intrinsice for x86.
+  void addX86HTMBegin(Instruction *I) {
+    LLVMContext &C = I->getContext();
+    IRBuilder<> Worker(I);
+    Value *HTMBeginVal = Worker.CreateCall(HTMBeginDecl);
+    Value *Success = ConstantInt::get(Type::getInt32Ty(C), 0xffffffff, false);
+    Value *Cmp = Worker.CreateICmpEQ(HTMBeginVal, Success);
+    addHTMBeginInternal(I, HTMBeginVal, Cmp);
+  }
+
+  /// Add transactional execution end intrinsic for PowerPC.
+  void addPowerPCHTMEnd(Instruction *I) {
+    LLVMContext &C = I->getContext();
+    IRBuilder<> EndWorker(I);
+    ConstantInt *One = ConstantInt::get(IntegerType::getInt32Ty(C),
+                                        1, false);
+    EndWorker.CreateCall(HTMEndDecl, ArrayRef<Value *>(One));
+  }
+
+  /// Add transactional execution check & end intrinsics for x86.
+  void addX86HTMCheckAndEnd(Instruction *I) {
+    // Note: x86's HTM facility will cause a segfault if an xend instruction is
+    // called outside of a transaction, hence we need to check if we're in a
+    // transaction before actually trying to end it.
+    LLVMContext &C = I->getContext();
+    BasicBlock *CurBB = I->getParent(), *NewSuccBB, *HTMEndBB;
+    Function *CurF = CurBB->getParent();
+
+    // Create a new successor which contains all instructions after the HTM
+    // check & end
+    NewSuccBB = CurBB->splitBasicBlock(I,
+      ".htmendsucc" + std::to_string(NumHTMEnds));
+
+    // Create an HTM end block, which ends the transaction and jumps to the
+    // new successor
+    HTMEndBB = BasicBlock::Create(C,
+      ".htmend" + std::to_string(NumHTMEnds), CurF, NewSuccBB);
+    IRBuilder<> EndWorker(HTMEndBB);
+    EndWorker.CreateCall(HTMEndDecl);
+    EndWorker.CreateBr(NewSuccBB);
+
+    // Finally, add the HTM test & replace the unconditional branch created by
+    // splitBasicBlock() with a conditional branch to either end the
+    // transaction or continue on to the new successor
+    IRBuilder<> PredWorker(CurBB->getTerminator());
+    CallInst *HTMTestVal = PredWorker.CreateCall(HTMTestDecl);
+    ConstantInt *Zero = ConstantInt::get(IntegerType::getInt32Ty(C), 0, true);
+    Value *Cmp = PredWorker.CreateICmpNE(HTMTestVal, Zero,
+      "htmcmp" + std::to_string(NumHTMEnds));
+    PredWorker.CreateCondBr(Cmp, HTMEndBB, NewSuccBB);
+    CurBB->getTerminator()->eraseFromParent();
+  }
+
+  /// Insert migration points & HTM instrumentation for instructions.
+  void addMigrationPoints(Function &F) {
+    if(DoHTMInst) {
+      // Note: need to add the HTM ends before begins
+      for(auto I = HTMEndInsts.begin(), E = HTMEndInsts.end(); I != E; ++I) {
+        switch(Arch) {
+        case Triple::ppc64le: addPowerPCHTMEnd(*I); break;
+        case Triple::x86_64: addX86HTMCheckAndEnd(*I); break;
+        default: llvm_unreachable("HTM -- unsupported architecture");
+        }
+        NumHTMEnds++;
+      }
+
+      // The following APIs both insert HTM begins & migration points because
+      // the control flow with/without abort handlers is intertwined
+      for(auto I = HTMBeginInsts.begin(), E = HTMBeginInsts.end();
+          I != E; ++I) {
+        switch(Arch) {
+        case Triple::ppc64le: addPowerPCHTMBegin(*I); break;
+        case Triple::x86_64: addX86HTMBegin(*I); break;
+        default: llvm_unreachable("HTM -- unsupported architecture");
+        }
+        NumHTMBegins++;
+        NumMigPoints++;
+      }
+    }
+    else {
+      for(auto I = MigPointInsts.begin(), E = MigPointInsts.end();
+          I != E; ++I) {
+        addMigrationPoint(*I);
+        NumMigPoints++;
+      }
+    }
+  }
+};
+
+} /* end anonymous namespace */
+
+char MigrationPoints::ID = 0;
+
+const MigrationPoints::IntrinsicMap MigrationPoints::HTMBegin = {
+  {Triple::x86_64, Intrinsic::x86_xbegin},
+  {Triple::ppc64le, Intrinsic::ppc_tbegin}
+};
+
+const MigrationPoints::IntrinsicMap MigrationPoints::HTMEnd = {
+  {Triple::x86_64, Intrinsic::x86_xend},
+  {Triple::ppc64le, Intrinsic::ppc_tend}
+};
+
+const MigrationPoints::IntrinsicMap MigrationPoints::HTMTest = {
+  {Triple::x86_64, Intrinsic::x86_xtest},
+  {Triple::ppc64le, Intrinsic::ppc_ttest}
+};
+
+INITIALIZE_PASS(MigrationPoints, "migration-points",
+                "Insert migration points into functions", true, false)
+
+namespace llvm {
+  FunctionPass *createMigrationPointsPass()
+  { return new MigrationPoints(); }
+}
+
diff --git a/llvm/lib/Transforms/Utils/CMakeLists.txt b/llvm/lib/Transforms/Utils/CMakeLists.txt
index 716e655affb..6e80755d7a6 100644
--- a/llvm/lib/Transforms/Utils/CMakeLists.txt
+++ b/llvm/lib/Transforms/Utils/CMakeLists.txt
@@ -28,12 +28,14 @@ add_llvm_library(LLVMTransformUtils
   Mem2Reg.cpp
   MetaRenamer.cpp
   ModuleUtils.cpp
+  NameStringLiterals.cpp
   PromoteMemoryToRegister.cpp
   SSAUpdater.cpp
   SimplifyCFG.cpp
   SimplifyIndVar.cpp
   SimplifyInstructions.cpp
   SimplifyLibCalls.cpp
+  StaticVarSections.cpp
   SymbolRewriter.cpp
   UnifyFunctionExitNodes.cpp
   Utils.cpp
diff --git a/llvm/lib/Transforms/Utils/NameStringLiterals.cpp b/llvm/lib/Transforms/Utils/NameStringLiterals.cpp
new file mode 100644
index 00000000000..c00f36b9efb
--- /dev/null
+++ b/llvm/lib/Transforms/Utils/NameStringLiterals.cpp
@@ -0,0 +1,119 @@
+#include <algorithm>
+#include <cctype>
+#include "llvm/Pass.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/GlobalVariable.h"
+#include "llvm/IR/GlobalValue.h"
+#include "llvm/IR/Module.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+#define DEBUG_TYPE "name-string-literals"
+#define CHARS_FOR_NAME 10
+
+using namespace llvm;
+
+namespace
+{
+
+/**
+ * Generate unique name for private anonymous string literals.  Uses the
+ * filename, LLVM's temporary name and (up to) the first 10 characters of the
+ * string.  Converts non-alphanumeric characters to underscores.
+ */
+std::string UniquifySymbol(const Module &M, GlobalVariable &Sym)
+{
+  std::string newName;
+  std::string::size_type loc;
+  auto filter = [](char c){ return !isalnum(c); };
+
+  newName = M.getName();
+  loc = newName.find_last_of('.');
+  newName = newName.substr(0, loc) + "_" + Sym.getName().str() + "_";
+  std::replace_if(newName.begin(), newName.end(), filter, '_');
+
+  // Check if it's a string, and if so use string content to uniquify
+  if(Sym.hasInitializer()) {
+    Constant *Initializer = Sym.getInitializer();
+    if(isa<ConstantDataSequential>(Initializer)) {
+      ConstantDataSequential *CDS = cast<ConstantDataSequential>(Initializer);
+      if(CDS->isString()) {
+        std::string data = CDS->getAsString().substr(0, CHARS_FOR_NAME);
+        std::replace_if(data.begin(), data.end(), filter, '_');
+        newName += data;
+      }
+    }
+  }
+
+  return newName;
+}
+
+/**
+ * This pass searches for anonymous read-only data for which there is no symbol
+ * and generates a symbol for the data.  This is required by the Popcorn
+ * compiler in order to align the data at link-time.
+ */
+class NameStringLiterals : public ModulePass
+{
+public:
+	static char ID;
+
+  NameStringLiterals() : ModulePass(ID) {}
+  ~NameStringLiterals() {}
+
+	/* ModulePass virtual methods */
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const { AU.setPreservesCFG(); }
+	virtual bool runOnModule(Module &M)
+  {
+    bool modified = false;
+    std::string newName;
+    Module::global_iterator gl, gle; // for global variables
+
+    DEBUG(errs() << "\n********** Begin NameStringLiterals **********\n"
+                 << "********** Module: " << M.getName() << " **********\n\n");
+
+    // Iterate over all globals and generate symbol for anonymous string
+    // literals in each module
+    for(gl = M.global_begin(), gle = M.global_end(); gl != gle; gl++) {
+      // DONT NEED TO CHANGE NAME PER-SE just change type
+      // PrivateLinkage does NOT show up in any symbol table in the object file!
+      if(gl->getLinkage() == GlobalValue::PrivateLinkage) {
+        //change Linkage
+        //FROM private unnamed_addr constant [num x i8]
+        //TO global [num x i8]
+        gl->setLinkage(GlobalValue::ExternalLinkage);
+
+        // Make the global's name unique so we don't clash when linking with
+        // other files
+        newName = UniquifySymbol(M, *gl);
+        gl->setName(newName);
+
+        // Also REMOVE unnamed_addr value
+        if(gl->hasUnnamedAddr()) {
+          gl->setUnnamedAddr(false);
+        }
+
+        modified = true;
+
+        DEBUG(errs() << "New anonymous string name: " << newName << "\n";);
+      } else {
+        DEBUG(errs() << "> " <<  *gl << ", linkage: "
+                     << gl->getLinkage() << "\n");
+      }
+    }
+  
+    return modified;
+  }
+  virtual const char *getPassName() const { return "Name string literals"; }
+};
+
+} /* end anonymous namespace */
+
+char NameStringLiterals::ID = 0;
+INITIALIZE_PASS(NameStringLiterals, "name-string-literals",
+  "Generate symbols for anonymous string literals", false, false)
+
+namespace llvm {
+  ModulePass *createNameStringLiteralsPass() { return new NameStringLiterals(); }
+}
+
diff --git a/llvm/lib/Transforms/Utils/StaticVarSections.cpp b/llvm/lib/Transforms/Utils/StaticVarSections.cpp
new file mode 100644
index 00000000000..52cf52e24ec
--- /dev/null
+++ b/llvm/lib/Transforms/Utils/StaticVarSections.cpp
@@ -0,0 +1,106 @@
+#include <algorithm>
+#include "llvm/Pass.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/GlobalVariable.h"
+#include "llvm/IR/GlobalValue.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+#define DEBUG_TYPE "static-var-sections"
+
+using namespace llvm;
+
+namespace
+{
+
+std::string UniquifySymbol(const Module &M,
+                           std::string &section,
+                           GlobalVariable &Sym)
+{
+  std::string newName;
+  auto filter = [](char c){ return !isalnum(c); };
+
+  newName = M.getName().str() + "_" + Sym.getName().str();
+  std::replace_if(newName.begin(), newName.end(), filter, '_');
+
+  return section + newName;
+}
+
+/**
+ * This pass searches for static, i.e., module-private, global variables and
+ * modifies their linkage to be in their own sections similarly to other
+ * global variables with the -fdata-sections switch.  By default, LLVM doesn't
+ * apply -fdata-sections to static global variables.
+ */
+class StaticVarSections : public ModulePass
+{
+public:
+	static char ID;
+
+	StaticVarSections() : ModulePass(ID) {}
+	~StaticVarSections() {}
+
+	/* ModulePass virtual methods */
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const { AU.setPreservesCFG(); }
+	virtual bool runOnModule(Module &M)
+  {
+    bool modified = false;
+    Module::iterator it, ite;
+    Module::global_iterator gl, gle; // for global variables
+  
+    DEBUG(errs() << "\n********** Beginning StaticVarSections **********\n"
+                 << "********** Module: " << M.getName() << " **********\n\n");
+  
+    // Iterate over all static globals and place them in their own section
+    for(gl = M.global_begin(), gle = M.global_end(); gl != gle; gl++) {
+      std::string secName = ".";
+      if(gl->isThreadLocal()) secName += "t";
+  
+      if(gl->hasCommonLinkage() &&
+         gl->getName().find(".cache.") != std::string::npos) {
+        gl->setLinkage(GlobalValue::InternalLinkage);
+      }
+  
+      // InternalLinkage is specifically for STATIC variables
+      if(gl->hasInternalLinkage() && !gl->hasSection()) {
+        if(gl->isConstant()) {
+          //Belongs in RODATA
+          assert(!gl->isThreadLocal() && "TLS data should not be in .rodata");
+          secName += "rodata.";
+        }
+        else if(gl->getInitializer()->isZeroValue()) {
+          //Belongs in BSS
+          secName += "bss.";
+        }
+        else {
+          //Belongs in DATA
+          secName += "data.";
+        }
+
+        secName = UniquifySymbol(M, secName, *gl);
+        gl->setSection(secName);
+        modified = true;
+
+        DEBUG(errs() << *gl << " - new section: " << secName << "\n");
+      } else {
+        DEBUG(errs() << "> " <<  *gl << ", linkage: "
+                     << gl->getLinkage() << "\n");
+        continue;
+      }
+    }
+    
+    return modified;
+  }
+  virtual const char *getPassName() const { return "Static variables in separate sections"; }
+};
+
+} /* end anonymous namespace */
+
+char StaticVarSections::ID = 0;
+INITIALIZE_PASS(StaticVarSections, "static-var-sections",
+  "Put static variables into separate sections", false, false)
+
+namespace llvm {
+  ModulePass *createStaticVarSectionsPass() { return new StaticVarSections(); }
+}
+
diff --git a/llvm/lib/Transforms/Utils/Utils.cpp b/llvm/lib/Transforms/Utils/Utils.cpp
index ed4f45c6a61..c094c72c1f4 100644
--- a/llvm/lib/Transforms/Utils/Utils.cpp
+++ b/llvm/lib/Transforms/Utils/Utils.cpp
@@ -28,7 +28,9 @@ void llvm::initializeTransformUtils(PassRegistry &Registry) {
   initializeLoopSimplifyPass(Registry);
   initializeLowerInvokePass(Registry);
   initializeLowerSwitchPass(Registry);
+  initializeNameStringLiteralsPass(Registry);
   initializePromotePassPass(Registry);
+  initializeStaticVarSectionsPass(Registry);
   initializeUnifyFunctionExitNodesPass(Registry);
   initializeInstSimplifierPass(Registry);
   initializeMetaRenamerPass(Registry);
