diff --git a/clang/include/clang/AST/ASTContext.h b/clang/include/clang/AST/ASTContext.h
index 1d1aaf4fb11..9bd3a1add69 100644
--- a/clang/include/clang/AST/ASTContext.h
+++ b/clang/include/clang/AST/ASTContext.h
@@ -43,6 +43,7 @@
 #include "clang/Basic/Specifiers.h"
 #include "clang/Basic/TargetInfo.h"
 #include "clang/Basic/XRayLists.h"
+#include "clang/Sema/PrefetchAnalysis.h"
 #include "llvm/ADT/APSInt.h"
 #include "llvm/ADT/ArrayRef.h"
 #include "llvm/ADT/DenseMap.h"
@@ -419,6 +420,9 @@ private:
 
   ASTContext &this_() { return *this; }
 
+  /// \brief Analysis on statements for which prefetching has been enabled.
+  mutable llvm::DenseMap<const Stmt *, PrefetchAnalysis> PrefetchAnalyses;
+
 public:
   /// A type synonym for the TemplateOrInstantiation mapping.
   using TemplateOrSpecializationInfo =
@@ -1115,6 +1119,20 @@ public:
   /// Retrieve the declaration for the 128-bit unsigned integer type.
   TypedefDecl *getUInt128Decl() const;
 
+  /// \brief Add a new prefetching analysis.  Note that analysis should be done
+  /// before adding it to the context.
+  void addPrefetchAnalysis(const Stmt *S, PrefetchAnalysis &PA)
+  { PrefetchAnalyses[S] = PA; }
+
+  /// \brief Retrieve prefetching analysis for a statement if it exists, or
+  /// return a nullptr otherwise.
+  const PrefetchAnalysis *getPrefetchAnalysis(const Stmt *S) const {
+    llvm::DenseMap<const Stmt *, PrefetchAnalysis>::iterator it;
+    it = PrefetchAnalyses.find(S);
+    if(it != PrefetchAnalyses.end()) return &it->second;
+    else return nullptr;
+  }
+
   //===--------------------------------------------------------------------===//
   //                           Type Constructors
   //===--------------------------------------------------------------------===//
diff --git a/clang/include/clang/AST/OpenMPClause.h b/clang/include/clang/AST/OpenMPClause.h
index eadcc62a345..a4ef31985a6 100644
--- a/clang/include/clang/AST/OpenMPClause.h
+++ b/clang/include/clang/AST/OpenMPClause.h
@@ -6181,6 +6181,64 @@ public:
   }
 };
 
+/// This represents a memory prefetch request for Popcorn Linux.  This
+/// should only be used for prefetching contiguous blocks of memory, e.g.,
+/// arrays or pointers to chunks of memory.
+  class OMPPrefetchClause final
+    : public OMPVarListClause<OMPPrefetchClause>,
+      private llvm::TrailingObjects<OMPPrefetchClause, Expr *> {
+  friend OMPVarListClause;
+  friend TrailingObjects;
+
+private:
+  /// What type of prefetching to perform.
+  OpenMPPrefetchClauseKind Kind;
+
+  /// Expressions describing the memory range to be prefetched.
+  ///
+  /// 1. If both are nullptr, then the entire array should be prefetched
+  /// 2. If Start is valid and End is nullptr, then use the expression (which
+  ///    should be affine to a for-loop iteration variable) to prefetch the
+  ///    region of memory based on the loop iterations assigned to the thread
+  /// 3. If both Start and End are valid, then prefetch the absolute range
+  ///    denoted by the starting & ending expressions
+  Expr *Start, *End;
+
+  /// Locations of the kind specifier, colons used to separate the
+  /// variable list and range expression(s).  These are valid in conjunction
+  /// with Start & End, respectively.
+  SourceLocation KindLoc, FirstColonLoc, SecondColonLoc;
+
+  OMPPrefetchClause(SourceLocation StartLoc, SourceLocation LParenLoc,
+                    SourceLocation FirstColonLoc,
+                    SourceLocation SecondColonLoc, SourceLocation EndLoc,
+                    unsigned N)
+      : OMPVarListClause<OMPPrefetchClause>(OMPC_prefetch, StartLoc, LParenLoc,
+                                            EndLoc, N),
+        FirstColonLoc(FirstColonLoc), SecondColonLoc(SecondColonLoc) {}
+public:
+  static OMPPrefetchClause *
+  Create(const ASTContext &C, OpenMPPrefetchClauseKind Kind,
+         SourceLocation KindLoc, ArrayRef<Expr *> VL, Expr *Start, Expr *End,
+         SourceLocation StartLoc, SourceLocation LParenLoc,
+         SourceLocation FirstColonLoc, SourceLocation SecondColonLoc,
+         SourceLocation EndLoc);
+
+  OpenMPPrefetchClauseKind getPrefetchKind() const { return Kind; }
+  void setPrefetchKind(OpenMPPrefetchClauseKind Kind) { this->Kind = Kind; }
+
+  Expr *getStartOfRange() const { return Start; }
+  void setStartOfRange(Expr *Start) { this->Start = Start; }
+  Expr *getEndOfRange() const { return End; }
+  void setEndOfRange(Expr *End) { this->End = End; }
+
+  SourceLocation getPrefetchKindLoc() const { return KindLoc; }
+  void setPrefetchKindLoc(SourceLocation Loc) { KindLoc = Loc; }
+  SourceLocation getFirstColonLoc() const { return FirstColonLoc; }
+  SourceLocation getSecondColonLoc() const { return SecondColonLoc; }
+
+};
+
 /// This class implements a simple visitor for OMPClause
 /// subclasses.
 template<class ImplClass, template <typename> class Ptr, typename RetTy>
diff --git a/clang/include/clang/AST/Prefetch.h b/clang/include/clang/AST/Prefetch.h
new file mode 100644
index 00000000000..1fe4127c15c
--- /dev/null
+++ b/clang/include/clang/AST/Prefetch.h
@@ -0,0 +1,38 @@
+//===--- Prefetch.h - Prefetching classes for Popcorn Linux -----*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file defines classes for enabling prefetching analysis & instrumentation
+// on Popcorn Linux.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_CLANG_AST_PREFETCH_H
+#define LLVM_CLANG_AST_PREFETCH_H
+
+namespace clang {
+
+/// Statements for which prefetching analysis & instrumentation can be performed
+/// should inherit this class.
+class Prefetchable {
+protected:
+  /// \brief Whether prefetching has been enabled for the statement.
+  bool PrefetchEnabled;
+
+public:
+  Prefetchable(bool PrefetchEnabled = false)
+    : PrefetchEnabled(PrefetchEnabled) {}
+
+  void setPrefetchEnabled(bool Enable) { PrefetchEnabled = Enable; }
+  bool prefetchEnabled() const { return PrefetchEnabled; }
+};
+
+} // end namespace clang
+
+#endif
+
diff --git a/clang/include/clang/AST/RecursiveASTVisitor.h b/clang/include/clang/AST/RecursiveASTVisitor.h
index 698fba2f4ed..ebd73374b69 100644
--- a/clang/include/clang/AST/RecursiveASTVisitor.h
+++ b/clang/include/clang/AST/RecursiveASTVisitor.h
@@ -3304,6 +3304,14 @@ bool RecursiveASTVisitor<Derived>::VisitOMPIsDevicePtrClause(
   return true;
 }
 
+template <typename Derived>
+bool RecursiveASTVisitor<Derived>::VisitOMPPrefetchClause(OMPPrefetchClause *C) {
+  TRY_TO(VisitOMPClauseList(C));
+  TRY_TO(TraverseStmt(C->getStartOfRange()));
+  TRY_TO(TraverseStmt(C->getEndOfRange()));
+  return true;
+}
+
 // FIXME: look at the following tricky-seeming exprs to see if we
 // need to recurse on anything.  These are ones that have methods
 // returning decls or qualtypes or nestednamespecifier -- though I'm
diff --git a/clang/include/clang/AST/Stmt.h b/clang/include/clang/AST/Stmt.h
index 403b88ac3a3..3019c656068 100644
--- a/clang/include/clang/AST/Stmt.h
+++ b/clang/include/clang/AST/Stmt.h
@@ -15,6 +15,7 @@
 
 #include "clang/AST/DeclGroup.h"
 #include "clang/AST/StmtIterator.h"
+#include "clang/AST/Prefetch.h"
 #include "clang/Basic/CapturedStmt.h"
 #include "clang/Basic/IdentifierTable.h"
 #include "clang/Basic/LLVM.h"
@@ -2381,7 +2382,7 @@ public:
 /// ForStmt - This represents a 'for (init;cond;inc)' stmt.  Note that any of
 /// the init/cond/inc parts of the ForStmt will be null if they were not
 /// specified in the source.
-class ForStmt : public Stmt {
+  class ForStmt : public Stmt, public Prefetchable {
   enum { INIT, CONDVAR, COND, INC, BODY, END_EXPR };
   Stmt* SubExprs[END_EXPR]; // SubExprs[INIT] is an expression or declstmt.
   SourceLocation LParenLoc, RParenLoc;
@@ -3418,6 +3419,12 @@ private:
   /// The record for captured variables, a RecordDecl or CXXRecordDecl.
   RecordDecl *TheRecordDecl = nullptr;
 
+  /// For captured OpenMP parallel regions, variables declared in the
+  /// shared clause may be stored on the main thread's stack.  This
+  /// causes false sharing in Popcorn's distributed execution.  If
+  /// set, offload shared variables to global memory for the capture.
+  bool OffloadShared;
+
   /// Construct a captured statement.
   CapturedStmt(Stmt *S, CapturedRegionKind Kind, ArrayRef<Capture> Captures,
                ArrayRef<Expr *> CaptureInits, CapturedDecl *CD, RecordDecl *RD);
@@ -3476,6 +3483,10 @@ public:
   /// True if this variable has been captured.
   bool capturesVariable(const VarDecl *Var) const;
 
+  /// Getters/setters for the offloading shared variables flag
+  bool offloadShared() const { return OffloadShared; }
+  void setOffloadShared(bool OS) { OffloadShared = OS; }
+
   /// An iterator that walks over the captures.
   using capture_iterator = Capture *;
   using const_capture_iterator = const Capture *;
diff --git a/clang/include/clang/AST/StmtOpenMP.h b/clang/include/clang/AST/StmtOpenMP.h
index e37f5b1e000..535f5a84835 100644
--- a/clang/include/clang/AST/StmtOpenMP.h
+++ b/clang/include/clang/AST/StmtOpenMP.h
@@ -41,6 +41,8 @@ class OMPExecutableDirective : public Stmt {
   const unsigned NumClauses;
   /// Number of child expressions/stmts.
   const unsigned NumChildren;
+  /// Enable prefetching code generation for Popcorn Linux
+  bool Prefetch;
   /// Offset from this to the start of clauses.
   /// There are NumClauses pointers to clauses, they are followed by
   /// NumChildren pointers to child stmts/exprs (if the directive type
@@ -68,7 +70,7 @@ protected:
                          unsigned NumClauses, unsigned NumChildren)
       : Stmt(SC), Kind(K), StartLoc(std::move(StartLoc)),
         EndLoc(std::move(EndLoc)), NumClauses(NumClauses),
-        NumChildren(NumChildren),
+        NumChildren(NumChildren), Prefetch(false),
         ClausesOffset(llvm::alignTo(sizeof(T), alignof(OMPClause *))) {}
 
   /// Sets the list of variables for this clause.
@@ -342,6 +344,9 @@ public:
     return const_cast<Stmt *>(
         const_cast<const OMPExecutableDirective *>(this)->getStructuredBlock());
   }
+
+  bool prefetchingEnabled() const { return Prefetch; }
+  void setPrefetching(bool Prefetch) { this->Prefetch = Prefetch; }
 };
 
 /// This represents '#pragma omp parallel' directive.
diff --git a/clang/include/clang/Basic/CodeGenOptions.def b/clang/include/clang/Basic/CodeGenOptions.def
index cd7a8454876..b870fb707d7 100644
--- a/clang/include/clang/Basic/CodeGenOptions.def
+++ b/clang/include/clang/Basic/CodeGenOptions.def
@@ -271,6 +271,12 @@ CODEGENOPT(DebugFwdTemplateParams, 1, 0) ///< Whether to emit complete
 
 CODEGENOPT(EmitLLVMUseLists, 1, 0) ///< Control whether to serialize use-lists.
 
+/// Select places inside functions to instrument with migration points
+CODEGENOPT(PopcornMigratable, 1, 0)
+
+/// Adjust linkage of global values for symbol alignment
+CODEGENOPT(PopcornAlignment, 1, 0)
+
 CODEGENOPT(WholeProgramVTables, 1, 0) ///< Whether to apply whole-program
                                       ///  vtable optimization.
 
diff --git a/clang/include/clang/Basic/CodeGenOptions.h b/clang/include/clang/Basic/CodeGenOptions.h
index 4e9025d2fea..5f0f5353f48 100644
--- a/clang/include/clang/Basic/CodeGenOptions.h
+++ b/clang/include/clang/Basic/CodeGenOptions.h
@@ -283,6 +283,9 @@ public:
   /// Set of sanitizer checks that trap rather than diagnose.
   SanitizerSet SanitizeTrap;
 
+  /// Targets for which to emit object code
+  std::vector<std::string> PopcornTargets;
+
   /// List of backend command-line options for -fembed-bitcode.
   std::vector<uint8_t> CmdArgs;
 
diff --git a/clang/include/clang/Basic/DiagnosticParseKinds.td b/clang/include/clang/Basic/DiagnosticParseKinds.td
index 8e6ced0dea5..cc209b12d5c 100644
--- a/clang/include/clang/Basic/DiagnosticParseKinds.td
+++ b/clang/include/clang/Basic/DiagnosticParseKinds.td
@@ -1202,6 +1202,14 @@ def err_omp_mapper_illegal_identifier : Error<
   "illegal OpenMP user-defined mapper identifier">;
 def err_omp_mapper_expected_declarator : Error<
   "expected declarator on 'omp declare mapper' directive">;
+def err_omp_invalid_prefetch_kind : Error<
+  "invalid argument; expected 'read', 'write' or 'smart'">;
+// TODO Technically these are semantics issues, but we're doing checking in the
+// parser for this particular clause
+def err_omp_invalid_prefetch_capture : Error<
+  "can only prefetch variables used in the loop body">;
+def err_omp_invalid_prefetch_loop_var : Error<
+  "can only prefetch loop iteration range using a loop iteration variable">;
 
 // Pragma loop support.
 def err_pragma_loop_missing_argument : Error<
@@ -1240,6 +1248,22 @@ def warn_pragma_force_cuda_host_device_bad_arg : Warning<
 def err_pragma_cannot_end_force_cuda_host_device : Error<
   "force_cuda_host_device end pragma without matching "
   "force_cuda_host_device begin">;
+
+// Pragma popcorn support.
+def warn_pragma_popcorn_ignored : Warning<
+  "Popcorn: unexpected '#pragma popcorn...' in program">, DefaultIgnore;
+def warn_pragma_popcorn_no_arg : Warning<
+  "Popcorn: missing argument; expected 'prefetch'">;
+def warn_pragma_popcorn_invalid_option : Warning<
+  "Popcorn: invalid pragma argument '%0'; expected 'prefetch'">;
+def err_pragma_popcorn_invalid_clause : Error<
+  "Popcorn: invalid clause '%0'; expected 'ignore'">;
+def err_pragma_popcorn_expected_var_name : Error<
+  "Popcorn: expected variable name">;
+
+// Pragma popcorn prefetch support.
+def warn_pragma_popcorn_prefetch_invalid_stmt : Warning<
+  "Popcorn: cannot prefetch for this statement">;
 } // end of Parse Issue category.
 
 let CategoryName = "Modules Issue" in {
diff --git a/clang/include/clang/Basic/DiagnosticSemaKinds.td b/clang/include/clang/Basic/DiagnosticSemaKinds.td
index 275c4e4365d..0be14ff1d6d 100644
--- a/clang/include/clang/Basic/DiagnosticSemaKinds.td
+++ b/clang/include/clang/Basic/DiagnosticSemaKinds.td
@@ -9290,6 +9290,10 @@ def err_omp_wrong_dependency_iterator_type : Error<
   "expected an integer or a pointer type of the outer loop counter '%0' for non-rectangular nests">;
 def err_omp_unsupported_type : Error <
   "host requires %0 bit size %1 type support, but device '%2' does not support it">;
+def err_omp_invalid_prefetch_var_type : Error<
+  "invalid variable type; must be %0 type">;
+def err_omp_invalid_prefetch_range_type : Error<
+  "invalid range specifier type; must be of integer (signed or unsigned) type">;
 } // end of OpenMP category
 
 let CategoryName = "Related Result Type Issue" in {
diff --git a/clang/include/clang/Basic/LangOptions.def b/clang/include/clang/Basic/LangOptions.def
index 31aca2b0d69..21e4b96e2be 100644
--- a/clang/include/clang/Basic/LangOptions.def
+++ b/clang/include/clang/Basic/LangOptions.def
@@ -330,6 +330,9 @@ LANGOPT(PaddingOnUnsignedFixedPoint, 1, 0,
 
 LANGOPT(RegisterStaticDestructors, 1, 1, "Register C++ static destructors")
 
+// Optimize OpenMP code generation for distributed execution on Popcorn Linux
+BENIGN_LANGOPT(DistributedOmp, 1, 0, "Optimize OpenMP for distributed execution")
+
 #undef LANGOPT
 #undef COMPATIBLE_LANGOPT
 #undef BENIGN_LANGOPT
diff --git a/clang/include/clang/Basic/OpenMPKinds.def b/clang/include/clang/Basic/OpenMPKinds.def
index 9685af4cade..1d4915407be 100644
--- a/clang/include/clang/Basic/OpenMPKinds.def
+++ b/clang/include/clang/Basic/OpenMPKinds.def
@@ -191,6 +191,9 @@
 #ifndef OPENMP_ALLOCATE_CLAUSE
 # define OPENMP_ALLOCATE_CLAUSE(Name)
 #endif
+#ifndef OPENMP_PREFETCH_KIND
+#define OPENMP_PREFETCH_KIND(Name)
+#endif
 
 // OpenMP directives.
 OPENMP_DIRECTIVE(threadprivate)
@@ -305,6 +308,7 @@ OPENMP_CLAUSE(reverse_offload, OMPReverseOffloadClause)
 OPENMP_CLAUSE(dynamic_allocators, OMPDynamicAllocatorsClause)
 OPENMP_CLAUSE(atomic_default_mem_order, OMPAtomicDefaultMemOrderClause)
 OPENMP_CLAUSE(allocate, OMPAllocateClause)
+OPENMP_CLAUSE(prefetch, OMPPrefetchClause)
 
 // Clauses allowed for OpenMP directive 'parallel'.
 OPENMP_PARALLEL_CLAUSE(if)
@@ -340,6 +344,7 @@ OPENMP_FOR_CLAUSE(ordered)
 OPENMP_FOR_CLAUSE(nowait)
 OPENMP_FOR_CLAUSE(linear)
 OPENMP_FOR_CLAUSE(allocate)
+OPENMP_FOR_CLAUSE(prefetch)
 
 // Clauses allowed for directive 'omp for simd'.
 OPENMP_FOR_SIMD_CLAUSE(private)
@@ -355,6 +360,7 @@ OPENMP_FOR_SIMD_CLAUSE(linear)
 OPENMP_FOR_SIMD_CLAUSE(aligned)
 OPENMP_FOR_SIMD_CLAUSE(ordered)
 OPENMP_FOR_SIMD_CLAUSE(allocate)
+OPENMP_FOR_SIMD_CLAUSE(prefetch)
 
 // Clauses allowed for OpenMP directive 'omp sections'.
 OPENMP_SECTIONS_CLAUSE(private)
@@ -389,6 +395,7 @@ OPENMP_SCHEDULE_KIND(dynamic)
 OPENMP_SCHEDULE_KIND(guided)
 OPENMP_SCHEDULE_KIND(auto)
 OPENMP_SCHEDULE_KIND(runtime)
+OPENMP_SCHEDULE_KIND(hetprobe)
 
 // Modifiers for 'schedule' clause.
 OPENMP_SCHEDULE_MODIFIER(monotonic)
@@ -414,6 +421,11 @@ OPENMP_LINEAR_KIND(val)
 OPENMP_LINEAR_KIND(ref)
 OPENMP_LINEAR_KIND(uval)
 
+// Static attributes for 'prefetch' clause.
+OPENMP_PREFETCH_KIND(read)
+OPENMP_PREFETCH_KIND(write)
+OPENMP_PREFETCH_KIND(smart)
+
 // Clauses allowed for OpenMP directive 'parallel for'.
 OPENMP_PARALLEL_FOR_CLAUSE(if)
 OPENMP_PARALLEL_FOR_CLAUSE(num_threads)
@@ -430,6 +442,7 @@ OPENMP_PARALLEL_FOR_CLAUSE(schedule)
 OPENMP_PARALLEL_FOR_CLAUSE(ordered)
 OPENMP_PARALLEL_FOR_CLAUSE(linear)
 OPENMP_PARALLEL_FOR_CLAUSE(allocate)
+OPENMP_PARALLEL_FOR_CLAUSE(prefetch)
 
 // Clauses allowed for OpenMP directive 'parallel for simd'.
 OPENMP_PARALLEL_FOR_SIMD_CLAUSE(if)
@@ -450,6 +463,7 @@ OPENMP_PARALLEL_FOR_SIMD_CLAUSE(linear)
 OPENMP_PARALLEL_FOR_SIMD_CLAUSE(aligned)
 OPENMP_PARALLEL_FOR_SIMD_CLAUSE(ordered)
 OPENMP_PARALLEL_FOR_SIMD_CLAUSE(allocate)
+OPENMP_PARALLEL_FOR_SIMD_CLAUSE(prefetch)
 
 // Clauses allowed for OpenMP directive 'parallel sections'.
 OPENMP_PARALLEL_SECTIONS_CLAUSE(if)
@@ -950,6 +964,7 @@ OPENMP_TASKGROUP_CLAUSE(allocate)
 // Clauses allowed for OpenMP directive 'declare mapper'.
 OPENMP_DECLARE_MAPPER_CLAUSE(map)
 
+#undef OPENMP_PREFETCH_KIND
 #undef OPENMP_ALLOCATE_CLAUSE
 #undef OPENMP_DECLARE_MAPPER_CLAUSE
 #undef OPENMP_TASKGROUP_CLAUSE
diff --git a/clang/include/clang/Basic/OpenMPKinds.h b/clang/include/clang/Basic/OpenMPKinds.h
index d8dee2310ec..f1f83a63be8 100644
--- a/clang/include/clang/Basic/OpenMPKinds.h
+++ b/clang/include/clang/Basic/OpenMPKinds.h
@@ -159,6 +159,14 @@ struct OpenMPScheduleTy final {
   OpenMPScheduleClauseModifier M2 = OMPC_SCHEDULE_MODIFIER_unknown;
 };
 
+/// \brief OpenMP attributes for 'prefetch' clause.
+enum OpenMPPrefetchClauseKind {
+#define OPENMP_PREFETCH_KIND(Name) \
+  OMPC_PREFETCH_##Name,
+#include "clang/Basic/OpenMPKinds.def"
+  OMPC_PREFETCH_unknown
+};
+
 OpenMPDirectiveKind getOpenMPDirectiveKind(llvm::StringRef Str);
 const char *getOpenMPDirectiveName(OpenMPDirectiveKind Kind);
 
diff --git a/clang/include/clang/Basic/TokenKinds.def b/clang/include/clang/Basic/TokenKinds.def
index 55e94d387c9..1f7004d29e0 100644
--- a/clang/include/clang/Basic/TokenKinds.def
+++ b/clang/include/clang/Basic/TokenKinds.def
@@ -836,6 +836,12 @@ ANNOTATION(module_end)
 // into the name of a header unit.
 ANNOTATION(header_unit)
 
+// Annotations for Popcorn Linux - #pragma popcorn ...
+// The lexer produces these so that they only take effect when the parser
+// handles #pragma popcorn ... directives.
+ANNOTATION(pragma_popcorn_prefetch)
+ANNOTATION(pragma_popcorn_prefetch_end)
+
 #undef ANNOTATION
 #undef TESTING_KEYWORD
 #undef OBJC_AT_KEYWORD
diff --git a/clang/include/clang/CodeGen/BackendUtil.h b/clang/include/clang/CodeGen/BackendUtil.h
index 01b1f5bbd6e..2401d8ad5ff 100644
--- a/clang/include/clang/CodeGen/BackendUtil.h
+++ b/clang/include/clang/CodeGen/BackendUtil.h
@@ -33,7 +33,8 @@ namespace clang {
     Backend_EmitLL,        ///< Emit human-readable LLVM assembly
     Backend_EmitNothing,   ///< Don't emit anything (benchmarking mode)
     Backend_EmitMCNull,    ///< Run CodeGen, but don't emit anything
-    Backend_EmitObj        ///< Emit native object files
+    Backend_EmitObj,       ///< Emit native object files
+    Backend_EmitMultiObj   ///< Emit native object files for multiple ISAs
   };
 
   void EmitBackendOutput(DiagnosticsEngine &Diags, const HeaderSearchOptions &,
@@ -41,7 +42,7 @@ namespace clang {
                          const TargetOptions &TOpts, const LangOptions &LOpts,
                          const llvm::DataLayout &TDesc, llvm::Module *M,
                          BackendAction Action,
-                         std::unique_ptr<raw_pwrite_stream> OS);
+                         raw_pwrite_stream *OS);
 
   void EmbedBitcode(llvm::Module *M, const CodeGenOptions &CGOpts,
                     llvm::MemoryBufferRef Buf);
@@ -50,6 +51,25 @@ namespace clang {
   FindThinLTOModule(llvm::MemoryBufferRef MBRef);
   llvm::BitcodeModule *
   FindThinLTOModule(llvm::MutableArrayRef<llvm::BitcodeModule> BMs);
+
+  /// Run IR optimization passes
+  void ApplyIROptimizations(DiagnosticsEngine &Diags,
+			    const HeaderSearchOptions &HeaderOpts,
+                            const CodeGenOptions &CGOpts,
+                            const TargetOptions &TOpts,
+                            const LangOptions &LOpts,
+                            llvm::Module *M, BackendAction Action,
+                            raw_pwrite_stream *OS);
+
+  /// Run backend code-generation passes
+  void CodegenBackendOutput(DiagnosticsEngine &Diags,
+			    const HeaderSearchOptions &HeaderOpts,
+                            const CodeGenOptions &CGOpts,
+                            const TargetOptions &TOpts,
+                            const LangOptions &LOpts,
+                            StringRef TDesc, llvm::Module *M,
+                            BackendAction Action,
+                            raw_pwrite_stream *OS);
 }
 
 #endif
diff --git a/clang/include/clang/CodeGen/CodeGenAction.h b/clang/include/clang/CodeGen/CodeGenAction.h
index 1db904ea974..cecb2a809d2 100644
--- a/clang/include/clang/CodeGen/CodeGenAction.h
+++ b/clang/include/clang/CodeGen/CodeGenAction.h
@@ -10,6 +10,7 @@
 #define LLVM_CLANG_CODEGEN_CODEGENACTION_H
 
 #include "clang/Frontend/FrontendAction.h"
+#include "clang/CodeGen/BackendUtil.h"
 #include <memory>
 
 namespace llvm {
@@ -19,9 +20,10 @@ namespace llvm {
 
 namespace clang {
 class BackendConsumer;
+class CoverageSourceInfo;
 
 class CodeGenAction : public ASTFrontendAction {
-private:
+protected:
   // Let BackendConsumer access LinkModule.
   friend class BackendConsumer;
 
@@ -52,7 +54,6 @@ private:
 
   std::unique_ptr<llvm::Module> loadModule(llvm::MemoryBufferRef MBRef);
 
-protected:
   /// Create a new code generation action.  If the optional \p _VMContext
   /// parameter is supplied, the action uses it without taking ownership,
   /// otherwise it creates a fresh LLVM context and takes ownership.
@@ -60,6 +61,14 @@ protected:
 
   bool hasIRSupport() const override;
 
+  /// Helpers called in CreateASTConsumer
+  SmallVector<clang::CodeGenAction::LinkModule, 4> getLinkModuleToUse(CompilerInstance &CI);
+  CoverageSourceInfo *getCoverageInfo(CompilerInstance &CI);
+
+  /// Helper called in ExecuteAction.  Returns true if the compilation is
+  /// invalid and should therefore be aborted.
+  bool ExecuteActionIRCommon(BackendAction &BA, CompilerInstance &CI);
+
   std::unique_ptr<ASTConsumer> CreateASTConsumer(CompilerInstance &CI,
                                                  StringRef InFile) override;
 
@@ -116,6 +125,23 @@ public:
   EmitObjAction(llvm::LLVMContext *_VMContext = nullptr);
 };
 
+/// Emit multiple object files using a single set of IR.  Used by the Popcorn
+/// Linux compiler toolchain.
+class EmitMultiObjAction : public CodeGenAction {
+  virtual void anchor();
+  SmallVector<std::string, 2> Targets;
+  SmallVector<raw_pwrite_stream *, 2> OutFiles;
+  SmallVector<std::shared_ptr<TargetOptions>, 2> TargetOpts;
+  SmallVector<TargetInfo *, 2> TargetInfos;
+protected:
+  bool InitializeTargets(CompilerInstance &CI, StringRef InFile);
+  std::unique_ptr<ASTConsumer> CreateASTConsumer(CompilerInstance &CI,
+                                                 StringRef InFile) override;
+  void ExecuteAction() override;
+public:
+  EmitMultiObjAction(llvm::LLVMContext *_VMContext = nullptr);
+};
+
 }
 
 #endif
diff --git a/clang/include/clang/CodeGen/PopcornUtil.h b/clang/include/clang/CodeGen/PopcornUtil.h
new file mode 100644
index 00000000000..b1a64b3e58b
--- /dev/null
+++ b/clang/include/clang/CodeGen/PopcornUtil.h
@@ -0,0 +1,47 @@
+//===--- PopcornUtil.h - Popcorn Linux Utilities ----------------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_CLANG_CODEGEN_POPCORNUTIL_H
+#define LLVM_CLANG_CODEGEN_POPCORNUTIL_H
+
+#include <llvm/ADT/StringRef.h>
+#include <llvm/IR/Module.h>
+#include <clang/Basic/TargetOptions.h>
+#include <memory>
+
+namespace clang {
+namespace Popcorn {
+
+/// Return whether a given target is supported by the compiler.
+bool SupportedTarget(const llvm::StringRef Target);
+
+/// Populate an array with all targets currently supported by the Popcorn
+/// compiler.
+void GetAllTargets(llvm::SmallVector<std::string, 2> &Targets);
+
+/// Return a TargetOptions with features appropriate for Popcorn Linux
+std::shared_ptr<TargetOptions>
+GetPopcornTargetOpts(const llvm::StringRef TripleStr);
+
+/// Strip target-specific CPUs & features from function attributes in all
+/// functions in the module.  This silences warnings from the compiler about
+/// unsupported target features when compiling the IR for multiple
+/// architectures.
+void StripTargetAttributes(llvm::Module &M);
+
+
+/// Add the target-features attribute specified in TargetOpts to every function
+/// in module M.
+void AddArchSpecificTargetFeatures(llvm::Module &M,
+                                   std::shared_ptr<TargetOptions> TargetOpts);
+
+} /* end Popcorn namespace */
+} /* end clang namespace */
+
+#endif
diff --git a/clang/include/clang/CodeGen/PrefetchBuilder.h b/clang/include/clang/CodeGen/PrefetchBuilder.h
new file mode 100644
index 00000000000..9716abe37e2
--- /dev/null
+++ b/clang/include/clang/CodeGen/PrefetchBuilder.h
@@ -0,0 +1,56 @@
+//===- Prefetch.h - Prefetching Analysis for Statements -----------*- C++ --*-//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file defines the interface for building prefetching calls based on the
+// prefetching analysis.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_CLANG_CODEGEN_PREFETCHBUILDER_H
+#define LLVM_CLANG_CODEGEN_PREFETCHBUILDER_H
+
+#include "CodeGenFunction.h"
+#include "clang/Sema/PrefetchAnalysis.h"
+#include "llvm/Support/raw_ostream.h"
+
+namespace clang {
+
+/// Generate calls to the prefetching API for analyzed regions.
+class PrefetchBuilder {
+public:
+  PrefetchBuilder(clang::CodeGen::CodeGenFunction *CGF)
+    : CGM(CGF->CGM), CGF(*CGF), Ctx(CGF->getContext()) {}
+
+  /// Emit prefetching API declarations.
+  void EmitPrefetchCallDeclarations();
+
+  /// Emit a prefetch call for a particular range of memory.
+  void EmitPrefetchCall(const PrefetchRange &P);
+
+  /// Emit a call to send the prefetch requests to the OS.
+  void EmitPrefetchExecuteCall();
+
+  // TODO print & dump
+
+private:
+  clang::CodeGen::CodeGenModule &CGM;
+  clang::CodeGen::CodeGenFunction &CGF;
+  ASTContext &Ctx;
+
+  // Prefetch API declarations
+  llvm::FunctionCallee Prefetch, Execute;
+
+  Expr *buildAddrOf(Expr *ArrSub);
+  Expr *buildArrayIndex(VarDecl *Base, Expr *Subscript);
+};
+
+} // end namespace clang
+
+#endif
+
diff --git a/clang/include/clang/Driver/Driver.h b/clang/include/clang/Driver/Driver.h
index f9528641073..2a3b5f02599 100644
--- a/clang/include/clang/Driver/Driver.h
+++ b/clang/include/clang/Driver/Driver.h
@@ -203,6 +203,9 @@ private:
   /// Raw target triple.
   std::string TargetTriple;
 
+  /// Default popcorn target triple.
+  std::string PopcornTargetTriple;
+
   /// Name to use when invoking gcc/g++.
   std::string CCCGenericGCCName;
 
diff --git a/clang/include/clang/Driver/Options.td b/clang/include/clang/Driver/Options.td
index 4ea8bfff097..b8bcd0889e9 100644
--- a/clang/include/clang/Driver/Options.td
+++ b/clang/include/clang/Driver/Options.td
@@ -191,6 +191,8 @@ def Link_Group : OptionGroup<"<T/e/s/t/u group>">, DocName<"Linker flags">,
 def T_Group : OptionGroup<"<T group>">, Group<Link_Group>, DocFlatten;
 def u_Group : OptionGroup<"<u group>">, Group<Link_Group>, DocFlatten;
 
+def Popcorn_Target_Group  : OptionGroup<"<Popcorn target group>">, Group<CompileOnly_Group>;
+
 def reserved_lib_Group : OptionGroup<"<reserved libs group>">,
                          Flags<[Unsupported]>;
 
@@ -2533,6 +2535,12 @@ def pedantic : Flag<["-", "--"], "pedantic">, Group<pedantic_Group>, Flags<[CC1O
 def pg : Flag<["-"], "pg">, HelpText<"Enable mcount instrumentation">, Flags<[CC1Option]>;
 def pipe : Flag<["-", "--"], "pipe">,
   HelpText<"Use pipes between commands, when possible">;
+def popcorn_migratable : Flag<["-"], "popcorn-migratable">, HelpText<"Instrument code to be migratable on Popcorn Linux (implies -popcorn-alignment)">, Flags<[CC1Option]>;
+def popcorn_metadata : Flag<["-"], "popcorn-metadata">, HelpText<"Generate stack transformation metadata without inserting migration points (implies -popcorn-alignment)">, Flags<[CC1Option]>;
+def popcorn_libc : Flag<["-"], "popcorn-libc">, HelpText<"Compile libc code with appropriate instrumentation for migration (implies -popcorn-alignment)">, Flags<[CC1Option]>;
+def popcorn_alignment : Flag<["-"], "popcorn-alignment">, HelpText<"Run Popcorn passes to prepare for link-time symbol alignment">, Flags<[CC1Option]>;
+def popcorn_target : Joined<["-"], "popcorn-target=">, HelpText<"Targets for which to generate object files (requires -popcorn-migratable)">, Group<Popcorn_Target_Group>, Flags<[CC1Option]>, MetaVarName<"<target>">;
+def distributed_omp : Flag<["-"], "distributed-omp">, HelpText<"Optimize OpenMP code generation for distributed execution on Popcorn Linux">, Flags<[CC1Option]>;
 def prebind__all__twolevel__modules : Flag<["-"], "prebind_all_twolevel_modules">;
 def prebind : Flag<["-"], "prebind">;
 def preload : Flag<["-"], "preload">;
diff --git a/clang/include/clang/Frontend/CompilerInstance.h b/clang/include/clang/Frontend/CompilerInstance.h
index eb49c53ff40..9a951fad430 100644
--- a/clang/include/clang/Frontend/CompilerInstance.h
+++ b/clang/include/clang/Frontend/CompilerInstance.h
@@ -262,6 +262,13 @@ public:
     return Invocation->getCodeGenOpts();
   }
 
+  CodeGenOptions &getCodeGenNoOpts() {
+    return Invocation->getCodeGenNoOpts();
+  }
+  const CodeGenOptions &getCodeGenNoOpts() const {
+    return Invocation->getCodeGenNoOpts();
+  }
+
   DependencyOutputOptions &getDependencyOutputOpts() {
     return Invocation->getDependencyOutputOpts();
   }
diff --git a/clang/include/clang/Frontend/CompilerInvocation.h b/clang/include/clang/Frontend/CompilerInvocation.h
index 413134be4ce..d37d84cdd55 100644
--- a/clang/include/clang/Frontend/CompilerInvocation.h
+++ b/clang/include/clang/Frontend/CompilerInvocation.h
@@ -126,6 +126,9 @@ class CompilerInvocation : public CompilerInvocationBase {
   /// Options controlling IRgen and the backend.
   CodeGenOptions CodeGenOpts;
 
+  /// Options controlling IRgen and the backend (with optimization disabled).
+  CodeGenOptions CodeGenOptsNoOpt;
+
   /// Options controlling dependency output.
   DependencyOutputOptions DependencyOutputOpts;
 
@@ -194,6 +197,11 @@ public:
   CodeGenOptions &getCodeGenOpts() { return CodeGenOpts; }
   const CodeGenOptions &getCodeGenOpts() const { return CodeGenOpts; }
 
+  CodeGenOptions &getCodeGenNoOpts() { return CodeGenOptsNoOpt; }
+  const CodeGenOptions &getCodeGenNoOpts() const {
+    return CodeGenOptsNoOpt;
+  }
+
   DependencyOutputOptions &getDependencyOutputOpts() {
     return DependencyOutputOpts;
   }
diff --git a/clang/include/clang/Frontend/FrontendOptions.h b/clang/include/clang/Frontend/FrontendOptions.h
index a0acb1f066f..8995e5a7c03 100644
--- a/clang/include/clang/Frontend/FrontendOptions.h
+++ b/clang/include/clang/Frontend/FrontendOptions.h
@@ -73,6 +73,9 @@ enum ActionKind {
   /// Emit a .o file.
   EmitObj,
 
+  /// Emit a .o file for multiple ISAs.
+  EmitMultiObj,
+
   /// Parse and apply any fixits to the source.
   FixIt,
 
diff --git a/clang/include/clang/Parse/Parser.h b/clang/include/clang/Parse/Parser.h
index 7c67c35f615..324f21bc7ae 100644
--- a/clang/include/clang/Parse/Parser.h
+++ b/clang/include/clang/Parse/Parser.h
@@ -200,6 +200,7 @@ class Parser : public CodeCompletionHandler {
   std::unique_ptr<PragmaHandler> STDCCXLIMITHandler;
   std::unique_ptr<PragmaHandler> STDCUnknownHandler;
   std::unique_ptr<PragmaHandler> AttributePragmaHandler;
+  std::unique_ptr<PragmaHandler> PopcornHandler;
 
   std::unique_ptr<CommentHandler> CommentSemaHandler;
 
@@ -737,6 +738,13 @@ private:
 
   void HandlePragmaAttribute();
 
+  /// \brief Handle the annotation token produced for
+  /// #pragma popcorn...
+  StmtResult HandlePragmaPopcorn();
+
+  /// \brief Parse a comma-separated variable list
+  void ParseVarList(llvm::SmallPtrSet<VarDecl *, 4> &Vars);
+
   /// GetLookAheadToken - This peeks ahead N tokens and returns that token
   /// without consuming any tokens.  LookAhead(0) returns 'Tok', LookAhead(1)
   /// returns the token after Tok, etc.
@@ -2829,6 +2837,11 @@ private:
 
   //===--------------------------------------------------------------------===//
   // OpenMP: Directives and clauses.
+  /// Checks 'prefetch' clauses for correctness.  Note that we can only
+  /// perform some semantic checks *after* the entire compound statement
+  /// representing the directive's body has been parsed.
+  void CheckOpenMPPrefetchClauses(StmtResult Directive);
+
   /// Parse clauses for '#pragma omp declare simd'.
   DeclGroupPtrTy ParseOMPDeclareSimdClauses(DeclGroupPtrTy Ptr,
                                             CachedTokens &Toks,
@@ -2932,9 +2945,11 @@ public:
   struct OpenMPVarListDataTy {
     Expr *TailExpr = nullptr;
     SourceLocation ColonLoc;
+    SourceLocation EndColonLoc;
     SourceLocation RLoc;
     CXXScopeSpec ReductionOrMapperIdScopeSpec;
     DeclarationNameInfo ReductionOrMapperId;
+    OpenMPPrefetchClauseKind PrefKind = OMPC_PREFETCH_unknown;
     OpenMPDependClauseKind DepKind = OMPC_DEPEND_unknown;
     OpenMPLinearClauseKind LinKind = OMPC_LINEAR_val;
     SmallVector<OpenMPMapModifierKind, OMPMapClause::NumberOfModifiers>
@@ -2944,6 +2959,8 @@ public:
     OpenMPMapClauseKind MapType = OMPC_MAP_unknown;
     bool IsMapTypeImplicit = false;
     SourceLocation DepLinMapLoc;
+    SourceLocation PrefLoc;
+    Expr *EndExpr;
   };
 
   /// Parses clauses with list.
diff --git a/clang/include/clang/Sema/PrefetchAnalysis.h b/clang/include/clang/Sema/PrefetchAnalysis.h
new file mode 100644
index 00000000000..bbfe5b5fda6
--- /dev/null
+++ b/clang/include/clang/Sema/PrefetchAnalysis.h
@@ -0,0 +1,146 @@
+//===- PrefetchAnalysis.h - Prefetching Analysis for Statements ---*- C++ --*-//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file defines the interface for prefetching analysis over structured
+// blocks.  The analysis traverses the AST to determine how arrays are accessed
+// in structured blocks and generates expressions defining ranges of elements
+// accessed inside arrays.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_CLANG_AST_PREFETCHANALYSIS_H
+#define LLVM_CLANG_AST_PREFETCHANALYSIS_H
+
+#include "clang/AST/Decl.h"
+#include "clang/AST/Expr.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/Support/raw_ostream.h"
+#include <memory>
+
+class LoopNestTraversal;
+class ArrayAccessPattern;
+
+namespace clang {
+
+class ASTContext;
+
+/// A range of memory to be prefetched.
+class PrefetchRange {
+public:
+  /// Access type for array.  Sorted in increasing importance.
+  enum Type { Read, Write };
+
+  PrefetchRange(enum Type Ty, VarDecl *Array, Expr *Start, Expr *End)
+    : Ty(Ty), Array(Array), Start(Start), End(End) {}
+
+  enum Type getType() const { return Ty; }
+  VarDecl *getArray() const { return Array; }
+  Expr *getStart() const { return Start; }
+  Expr *getEnd() const { return End; }
+  void setType(enum Type Ty) { this->Ty = Ty; }
+  void setArray(VarDecl *Array) { this->Array = Array; }
+  void setStart(Expr *Start) { this->Start = Start; }
+  void setEnd(Expr *End) { this->End = End; }
+
+  /// Return true if the other prefetch range is equal to this one (ignoring
+  /// prefetch type differences), or false otherwise.
+  bool equalExceptType(const PrefetchRange &RHS);
+
+  /// Return true if the other prefetch range is equal to this one, or false
+  /// otherwise.
+  bool operator==(const PrefetchRange &RHS);
+
+  // TODO print & dump
+  const char *getTypeName() const {
+    if (Ty != Read && Ty != Write)
+      return "unknown";
+    switch(Ty) {
+    case Read: return "read";
+    case Write: return "write";
+    }
+  }
+
+private:
+  enum Type Ty;
+  VarDecl *Array;
+  Expr *Start, *End;
+};
+
+class PrefetchAnalysis {
+public:
+  /// Default constructor, really only defined to enable storage in a DenseMap.
+  PrefetchAnalysis() : Ctx(nullptr), S(nullptr) {}
+
+  /// Construct a new prefetch analysis object to analyze a statement.  Doesn't
+  /// run the analysis.
+  PrefetchAnalysis(ASTContext *Ctx, Stmt *S) : Ctx(Ctx), S(S) {}
+
+  /// Ignore a set of variables during access analysis.  In other words, ignore
+  /// memory accesses which use these variables as their base.
+  void ignoreVars(const llvm::SmallPtrSet<VarDecl *, 4> &Ignore)
+  { this->Ignore = Ignore; }
+
+  /// Analyze the statement to capture loop information & array accesses.
+  void analyzeStmt();
+
+  /// Construct prefetch ranges from array accesses & induction variables.
+  void calculatePrefetchRanges();
+
+  /// Get prefetch ranges discovered by analysis.
+  const SmallVector<PrefetchRange, 8> &getArraysToPrefetch() const
+  { return ToPrefetch; }
+
+  /// Return true if the QualType is both scalar and of integer type, or false
+  /// otherwise.
+  static bool isScalarIntType(const QualType &Ty);
+
+  /// Return the size in bits of a builtin integer type, or UINT32_MAX if not a
+  /// builtin integer type.
+  static unsigned getTypeSize(BuiltinType::Kind K);
+
+  /// Cast the value declaration to a variable declaration if it is a varaible
+  /// of scalar integer type.
+  static VarDecl *getVarIfScalarInt(ValueDecl *VD);
+
+  void print(llvm::raw_ostream &O) const;
+  void dump() const { print(llvm::errs()); }
+
+private:
+  ASTContext *Ctx;
+  Stmt *S;
+
+  /// Analysis information.
+  std::shared_ptr<LoopNestTraversal> Loops;
+  std::shared_ptr<ArrayAccessPattern> ArrAccesses;
+
+  /// Variables (i.e., arrays) to ignore during analysis
+  llvm::SmallPtrSet<VarDecl *, 4> Ignore;
+
+  /// The good stuff -- ranges of memory to prefetch
+  llvm::SmallVector<PrefetchRange, 8> ToPrefetch;
+
+  /// Analyze individual types of statements.
+  void analyzeForStmt();
+
+  /// Merge overlapping or contiguous prefetch ranges.
+  void mergePrefetchRanges();
+
+  /// Remove trivial or redundant array accesses.  This is split into two as
+  /// some array accesses may only become redundant after expansion into a
+  /// prefetch range.
+  void pruneArrayAccesses();
+  void prunePrefetchRanges();
+};
+
+} // end namespace clang
+
+#endif
+
diff --git a/clang/include/clang/Sema/PrefetchDataflow.h b/clang/include/clang/Sema/PrefetchDataflow.h
new file mode 100644
index 00000000000..07aa621cd31
--- /dev/null
+++ b/clang/include/clang/Sema/PrefetchDataflow.h
@@ -0,0 +1,84 @@
+//=- PrefetchDataflow.cpp - Dataflow analysis for prefetching ------------*-==//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements the dataflow of expressions as required for prefetching
+// analysis.  This is required to correctly discover how variables are used in
+// memory accesses in order to construct memory access ranges.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef _AST_PREFETCHDATAFLOW_H
+#define _AST_PREFETCHDATAFLOW_H
+
+#include "clang/Analysis/CFG.h"
+#include "clang/Analysis/CFGStmtMap.h"
+#include "clang/AST/ParentMap.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include <memory>
+
+namespace clang {
+
+/// Data structures for containing symbolic execution values.
+typedef llvm::SmallPtrSet<Expr *, 1> ExprList;
+typedef llvm::DenseMap<const VarDecl *, ExprList> SymbolicValueMap;
+typedef std::pair<const VarDecl *, ExprList> SymbolicValuePair;
+typedef llvm::SmallPtrSet<const CFGBlock *, 32> CFGBlockSet;
+
+/// Class which runs dataflow analysis over the specified statement.  Tracks
+/// the value of a given set of variables as they change throughout the
+/// statement.
+class PrefetchDataflow {
+public:
+  typedef llvm::SmallPtrSet<const VarDecl *, 8> VarSet;
+
+  /// A lot of boilerplate so we can embed analysis into data structures like
+  /// llvm::DenseMap.  Required because objects use std::unique_ptrs.
+  PrefetchDataflow();
+  PrefetchDataflow(ASTContext *Ctx);
+
+  /// Copy constructor -- only copies the AST.
+  PrefetchDataflow(const PrefetchDataflow &RHS);
+
+  /// Assignment operator -- only copies the AST.
+  PrefetchDataflow &operator=(const PrefetchDataflow &RHS);
+
+  /// Run dataflow analysis over the statement specified at build time.
+  void runDataflow(Stmt *S, VarSet &VarsToTrack);
+
+  /// Get possible values of a variable at a specific use in a statement, if
+  /// any.  The list argument will be populated with expressions.
+  void getVariableValues(VarDecl *Var, const Stmt *Use, ExprList &Exprs) const;
+
+  /// Reset any previous analysis.
+  void reset();
+
+  void print(llvm::raw_ostream &O) const;
+  void dump() const;
+
+private:
+  ASTContext *Ctx;
+  Stmt *S;
+
+  /// Data used in analysis/retrieving results
+  std::unique_ptr<CFG> TheCFG;
+  std::unique_ptr<ParentMap> PMap;
+  std::unique_ptr<CFGStmtMap> StmtToBlock;
+
+  /// Analysis results -- keep an expression used to calculate a variable's
+  /// value for each control flow block.
+  typedef std::pair<const CFGBlock *, SymbolicValueMap> BlockValuesPair;
+  typedef llvm::DenseMap<const CFGBlock *, SymbolicValueMap> BlockValuesMap;
+  BlockValuesMap VarValues;
+};
+
+}
+
+#endif
+
diff --git a/clang/include/clang/Sema/PrefetchExprBuilder.h b/clang/include/clang/Sema/PrefetchExprBuilder.h
new file mode 100644
index 00000000000..755d9bfff5c
--- /dev/null
+++ b/clang/include/clang/Sema/PrefetchExprBuilder.h
@@ -0,0 +1,102 @@
+//===- PrefetchExprBuilder.h - Prefetching expression builder -----*- C++ --*-//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file defines a set of utilities for building expressions for
+// prefetching.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_CLANG_AST_PREFETCHEXPRBUILDER_H
+#define LLVM_CLANG_AST_PREFETCHEXPRBUILDER_H
+
+#include "clang/AST/Decl.h"
+#include "clang/AST/Expr.h"
+#include "llvm/ADT/DenseMap.h"
+
+namespace clang {
+
+class ASTContext;
+
+typedef std::pair<VarDecl *, Expr *> ReplacePair;
+typedef llvm::DenseMap<VarDecl *, Expr *> ReplaceMap;
+
+/// Utilities for compairing expressions by value.
+namespace PrefetchExprEquality {
+
+/// Compare two expressions by value to see if they're equal.
+bool exprEqual(const Expr *A, const Expr *B);
+
+}
+
+/// Utilities for building expressions.
+namespace PrefetchExprBuilder {
+
+/// Information describing how a statement should be modified.
+struct Modifier {
+  enum Type { Add, Sub, Mul, Div, None, Unknown };
+  void ClassifyModifier(const Expr *E, const ASTContext *Ctx);
+  enum Type getType() const { return Ty; }
+  const llvm::APInt &getVal() const { return Val; }
+private:
+  enum Type Ty;
+  llvm::APInt Val;
+};
+
+/// Information needed for building expressions.
+struct BuildInfo {
+public:
+  BuildInfo(ASTContext *Ctx, ReplaceMap &VarReplace, bool dumpInColor)
+    : Ctx(Ctx), VarReplace(VarReplace), dumpInColor(dumpInColor) {}
+
+  ASTContext *Ctx;
+  ReplaceMap &VarReplace;
+  llvm::SmallPtrSet<VarDecl *, 8> SeenVars;
+  bool dumpInColor;
+
+  void reset() {
+    VarReplace.clear();
+    SeenVars.clear();
+  }
+};
+
+/// Reconstruct expressions with variables replaced by user-supplied
+/// expressions (in Info.VarReplacements).
+Expr *cloneWithReplacement(Expr *E, BuildInfo &Info);
+
+/// Clone an expression, but don't replace any variables.
+Expr *clone(Expr *E, ASTContext *Ctx);
+
+/// Clone a binary operation.
+Expr *cloneBinaryOperator(BinaryOperator *B, BuildInfo &Info);
+
+/// Clone a unary operation.
+Expr *cloneUnaryOperator(UnaryOperator *U, BuildInfo &Info);
+
+/// Clone an array subscript.
+Expr *cloneArraySubscriptExpr(ArraySubscriptExpr *A, BuildInfo &Info);
+
+/// Clone a declaration reference.  If it's an induction variable, replace
+/// with the bound specified by the Upper flag.
+Expr *cloneDeclRefExpr(DeclRefExpr *D, BuildInfo &Info);
+
+/// Clone an implicit cast.
+Expr *cloneImplicitCastExpr(ImplicitCastExpr *E, BuildInfo &Info);
+
+/// Clone an integer literal.
+Expr *cloneIntegerLiteral(IntegerLiteral *L, BuildInfo &Info);
+
+/// Modify an expression according to a configuration.
+Expr *cloneAndModifyExpr(Expr *E, const Modifier &Mod, ASTContext *Ctx);
+
+} // end namespace PrefetchExprBuilder
+
+} // end namespace clang
+
+#endif
+
diff --git a/clang/include/clang/Sema/Sema.h b/clang/include/clang/Sema/Sema.h
index e6c63fd9c01..a9aa18a1886 100644
--- a/clang/include/clang/Sema/Sema.h
+++ b/clang/include/clang/Sema/Sema.h
@@ -9595,13 +9595,14 @@ public:
 
   OMPClause *ActOnOpenMPVarListClause(
       OpenMPClauseKind Kind, ArrayRef<Expr *> Vars, Expr *TailExpr,
-      const OMPVarListLocTy &Locs, SourceLocation ColonLoc,
-      CXXScopeSpec &ReductionOrMapperIdScopeSpec,
+      Expr *EndExpr, const OMPVarListLocTy &Locs, SourceLocation ColonLoc,
+      SourceLocation EndColonLoc, CXXScopeSpec &ReductionOrMapperIdScopeSpec,
       DeclarationNameInfo &ReductionOrMapperId, OpenMPDependClauseKind DepKind,
       OpenMPLinearClauseKind LinKind,
       ArrayRef<OpenMPMapModifierKind> MapTypeModifiers,
       ArrayRef<SourceLocation> MapTypeModifiersLoc, OpenMPMapClauseKind MapType,
-      bool IsMapTypeImplicit, SourceLocation DepLinMapLoc);
+      bool IsMapTypeImplicit, SourceLocation DepLinMapLoc,
+      OpenMPPrefetchClauseKind PrefKind, SourceLocation PrefLoc);
   /// Called on well-formed 'allocate' clause.
   OMPClause *
   ActOnOpenMPAllocateClause(Expr *Allocator, ArrayRef<Expr *> VarList,
@@ -9737,6 +9738,18 @@ public:
   OMPClause *ActOnOpenMPIsDevicePtrClause(ArrayRef<Expr *> VarList,
                                           const OMPVarListLocTy &Locs);
 
+  /// Called on well-formed 'prefetch' clause.
+  OMPClause *
+  ActOnOpenMPPrefetchClause(OpenMPPrefetchClauseKind PrefKind,
+                            SourceLocation PrefLoc,
+                            ArrayRef<Expr *> VarList,
+                            Expr *Start, Expr *End,
+                            SourceLocation StartLoc,
+                            SourceLocation LParenLoc,
+                            SourceLocation FirstColonLoc,
+                            SourceLocation SecondColonLoc,
+                            SourceLocation EndLoc);
+
   /// The kind of conversion being performed.
   enum CheckedConversionKind {
     /// An implicit conversion.
diff --git a/clang/lib/AST/OpenMPClause.cpp b/clang/lib/AST/OpenMPClause.cpp
index 9d8a7ebc302..e687eb3bfd3 100644
--- a/clang/lib/AST/OpenMPClause.cpp
+++ b/clang/lib/AST/OpenMPClause.cpp
@@ -127,6 +127,7 @@ const OMPClauseWithPreInit *OMPClauseWithPreInit::get(const OMPClause *C) {
   case OMPC_reverse_offload:
   case OMPC_dynamic_allocators:
   case OMPC_atomic_default_mem_order:
+  case OMPC_prefetch:
     break;
   }
 
@@ -203,6 +204,7 @@ const OMPClauseWithPostUpdate *OMPClauseWithPostUpdate::get(const OMPClause *C)
   case OMPC_reverse_offload:
   case OMPC_dynamic_allocators:
   case OMPC_atomic_default_mem_order:
+  case OMPC_prefetch:
     break;
   }
 
@@ -279,6 +281,26 @@ const Expr *OMPOrderedClause::getLoopCounter(unsigned NumLoop) const {
   return getTrailingObjects<Expr *>()[NumberOfLoops + NumLoop];
 }
 
+OMPPrefetchClause *
+OMPPrefetchClause::Create(const ASTContext &C, OpenMPPrefetchClauseKind Kind,
+                          SourceLocation KindLoc, ArrayRef<Expr *> VL,
+                          Expr *Start, Expr *End, SourceLocation StartLoc,
+                          SourceLocation LParenLoc,
+                          SourceLocation FirstColonLoc,
+                          SourceLocation SecondColonLoc,
+                          SourceLocation EndLoc) {
+  void *Mem = C.Allocate(totalSizeToAlloc<Expr *>(VL.size()));
+  OMPPrefetchClause *Clause =
+    new (Mem) OMPPrefetchClause(StartLoc, LParenLoc, FirstColonLoc, SecondColonLoc,
+                                EndLoc, VL.size());
+  Clause->setPrefetchKind(Kind);
+  Clause->setPrefetchKindLoc(KindLoc);
+  Clause->setStartOfRange(Start);
+  Clause->setEndOfRange(End);
+  Clause->setVarRefs(VL);
+  return Clause;
+}
+
 void OMPPrivateClause::setPrivateCopies(ArrayRef<Expr *> VL) {
   assert(VL.size() == varlist_size() &&
          "Number of private copies is not the same as the preallocated buffer");
@@ -1597,3 +1619,17 @@ void OMPClausePrinter::VisitOMPIsDevicePtrClause(OMPIsDevicePtrClause *Node) {
   }
 }
 
+void OMPClausePrinter::VisitOMPPrefetchClause(OMPPrefetchClause *Node) {
+  if (!Node->varlist_empty()) {
+    OS << "prefetch(";
+    OS << getOpenMPSimpleClauseTypeName(Node->getClauseKind(),
+                                        Node->getPrefetchKind())
+       << " :";
+    VisitOMPClauseList(Node, ' ');
+    OS << ",";
+    Node->getStartOfRange()->printPretty(OS, nullptr, Policy, 0);
+    OS << ",";
+    Node->getEndOfRange()->printPretty(OS, nullptr, Policy, 0);
+    OS << ")";
+  }
+}
diff --git a/clang/lib/AST/StmtProfile.cpp b/clang/lib/AST/StmtProfile.cpp
index f92c3dc60ba..8966b4d52eb 100644
--- a/clang/lib/AST/StmtProfile.cpp
+++ b/clang/lib/AST/StmtProfile.cpp
@@ -765,6 +765,11 @@ void OMPClauseProfiler::VisitOMPIsDevicePtrClause(
     const OMPIsDevicePtrClause *C) {
   VisitOMPClauseList(C);
 }
+void OMPClauseProfiler::VisitOMPPrefetchClause(const OMPPrefetchClause *C) {
+  VisitOMPClauseList(C);
+  Profiler->VisitStmt(C->getStartOfRange());
+  Profiler->VisitStmt(C->getEndOfRange());
+}
 }
 
 void
diff --git a/clang/lib/Basic/OpenMPKinds.cpp b/clang/lib/Basic/OpenMPKinds.cpp
index 82e193efef3..67bf7748a22 100644
--- a/clang/lib/Basic/OpenMPKinds.cpp
+++ b/clang/lib/Basic/OpenMPKinds.cpp
@@ -145,6 +145,11 @@ unsigned clang::getOpenMPSimpleClauseType(OpenMPClauseKind Kind,
   .Case(#Name, OMPC_ATOMIC_DEFAULT_MEM_ORDER_##Name)
 #include "clang/Basic/OpenMPKinds.def"
         .Default(OMPC_ATOMIC_DEFAULT_MEM_ORDER_unknown);
+  case OMPC_prefetch:
+    return llvm::StringSwitch<OpenMPPrefetchClauseKind>(Str)
+#define OPENMP_PREFETCH_KIND(Name) .Case(#Name, OMPC_PREFETCH_##Name)
+#include "clang/Basic/OpenMPKinds.def"
+        .Default(OMPC_PREFETCH_unknown);
   case OMPC_unknown:
   case OMPC_threadprivate:
   case OMPC_if:
@@ -375,6 +380,7 @@ const char *clang::getOpenMPSimpleClauseTypeName(OpenMPClauseKind Kind,
   case OMPC_unified_shared_memory:
   case OMPC_reverse_offload:
   case OMPC_dynamic_allocators:
+  case OMPC_prefetch:
     break;
   }
   llvm_unreachable("Invalid OpenMP simple clause kind");
diff --git a/clang/lib/Basic/Targets/RISCV.cpp b/clang/lib/Basic/Targets/RISCV.cpp
index 930b825e94d..e9def4fcbfd 100644
--- a/clang/lib/Basic/Targets/RISCV.cpp
+++ b/clang/lib/Basic/Targets/RISCV.cpp
@@ -72,6 +72,8 @@ void RISCVTargetInfo::getTargetDefines(const LangOptions &Opts,
   Builder.defineMacro("__ELF__");
   Builder.defineMacro("__riscv");
   bool Is64Bit = getTriple().getArch() == llvm::Triple::riscv64;
+  if (Is64Bit)
+    Builder.defineMacro("__riscv64__");
   Builder.defineMacro("__riscv_xlen", Is64Bit ? "64" : "32");
   // TODO: modify when more code models are supported.
   Builder.defineMacro("__riscv_cmodel_medlow");
diff --git a/clang/lib/CodeGen/BackendUtil.cpp b/clang/lib/CodeGen/BackendUtil.cpp
old mode 100644
new mode 100755
index 497652e85b4..768e8946f89
--- a/clang/lib/CodeGen/BackendUtil.cpp
+++ b/clang/lib/CodeGen/BackendUtil.cpp
@@ -18,6 +18,7 @@
 #include "llvm/ADT/StringExtras.h"
 #include "llvm/ADT/StringSwitch.h"
 #include "llvm/ADT/Triple.h"
+#include "llvm/Analysis/Passes.h"
 #include "llvm/Analysis/TargetLibraryInfo.h"
 #include "llvm/Analysis/TargetTransformInfo.h"
 #include "llvm/Bitcode/BitcodeReader.h"
@@ -88,7 +89,12 @@ class EmitAssemblyHelper {
 
   Timer CodeGenerationTime;
 
-  std::unique_ptr<raw_pwrite_stream> OS;
+  legacy::PassManager *CodeGenPasses;
+  legacy::PassManager *PerModulePasses;
+  legacy::FunctionPassManager *PerFunctionPasses;
+  std::unique_ptr<llvm::ToolOutputFile> ThinLinkOS, DwoOS;
+
+  raw_pwrite_stream *OS;
 
   TargetIRAnalysis getTargetIRAnalysis() const {
     if (TM)
@@ -97,6 +103,33 @@ class EmitAssemblyHelper {
     return TargetIRAnalysis();
   }
 
+  legacy::PassManager *getCodeGenPasses() {
+    if (!CodeGenPasses) {
+      CodeGenPasses = new legacy::PassManager();
+      CodeGenPasses->add(
+	  createTargetTransformInfoWrapperPass(getTargetIRAnalysis()));
+    }
+    return CodeGenPasses;
+  }
+
+  legacy::PassManager *getPerModulePasses() {
+    if (!PerModulePasses) {
+      PerModulePasses = new legacy::PassManager();
+      PerModulePasses->add(
+	  createTargetTransformInfoWrapperPass(getTargetIRAnalysis()));
+    }
+    return PerModulePasses;
+  }
+
+  legacy::FunctionPassManager *getPerFunctionPasses() {
+    if (!PerFunctionPasses) {
+      PerFunctionPasses = new legacy::FunctionPassManager(TheModule);
+      PerFunctionPasses->add(
+	  createTargetTransformInfoWrapperPass(getTargetIRAnalysis()));
+    }
+    return PerFunctionPasses;
+  }
+
   void CreatePasses(legacy::PassManager &MPM, legacy::FunctionPassManager &FPM);
 
   /// Generates the TargetMachine.
@@ -134,20 +167,37 @@ public:
                      const LangOptions &LOpts, Module *M)
       : Diags(_Diags), HSOpts(HeaderSearchOpts), CodeGenOpts(CGOpts),
         TargetOpts(TOpts), LangOpts(LOpts), TheModule(M),
-        CodeGenerationTime("codegen", "Code Generation Time") {}
+        CodeGenerationTime("codegen", "Code Generation Time") {
+    CodeGenPasses = nullptr;
+    PerModulePasses = nullptr;
+    PerFunctionPasses = nullptr;
+  }
 
   ~EmitAssemblyHelper() {
+    delete CodeGenPasses;
+    delete PerModulePasses;
+    delete PerFunctionPasses;
     if (CodeGenOpts.DisableFree)
       BuryPointer(std::move(TM));
   }
 
   std::unique_ptr<TargetMachine> TM;
 
+  /// Set up the assembly helper, including gathering passes
+  void SetupAssemblyHelper(BackendAction Action,
+			   raw_pwrite_stream *OS);
+
+  /// Run only IR optimization passes on a module
+  void ApplyIROptPasses(Module* TheModule);
+
+  /// Run backend passes to generate code
+  void ApplyCodegenPasses(Module* TheModule);
+
   void EmitAssembly(BackendAction Action,
-                    std::unique_ptr<raw_pwrite_stream> OS);
+                    raw_pwrite_stream *OS);
 
   void EmitAssemblyWithNewPassManager(BackendAction Action,
-                                      std::unique_ptr<raw_pwrite_stream> OS);
+                                      raw_pwrite_stream *OS);
 };
 
 // We need this wrapper to access LangOpts and CGOpts from extension functions
@@ -364,6 +414,18 @@ static void addSymbolRewriterPass(const CodeGenOptions &Opts,
   MPM->add(createRewriteSymbolsPass(DL));
 }
 
+static void addPopcornMigPointPasses(const PassManagerBuilder &Builder,
+				     legacy::PassManagerBase &PM) {
+  PM.add(createPopcornCompatibilityPass());
+  PM.add(createSelectMigrationPointsPass());
+}
+
+static void addPopcornAlignmentPasses(const PassManagerBuilder &Builder,
+				      legacy::PassManagerBase &PM) {
+  PM.add(createNameStringLiteralsPass());
+  PM.add(createStaticVarSectionsPass());
+}
+
 static CodeGenOpt::Level getCGOptLevel(const CodeGenOptions &CodeGenOpts) {
   switch (CodeGenOpts.OptimizationLevel) {
   default:
@@ -396,7 +458,7 @@ getCodeModel(const CodeGenOptions &CodeGenOpts) {
 }
 
 static TargetMachine::CodeGenFileType getCodeGenFileType(BackendAction Action) {
-  if (Action == Backend_EmitObj)
+  if (Action == Backend_EmitObj || Action == Backend_EmitMultiObj)
     return TargetMachine::CGFT_ObjectFile;
   else if (Action == Backend_EmitMCNull)
     return TargetMachine::CGFT_Null;
@@ -717,6 +779,29 @@ void EmitAssemblyHelper::CreatePasses(legacy::PassManager &MPM,
   if (!CodeGenOpts.SampleProfileFile.empty())
     PMBuilder.PGOSampleUse = CodeGenOpts.SampleProfileFile;
 
+  // Popcorn Compiler Toolchain passes -- add after IR optimization passes
+  // Select migration points.
+  if (CodeGenOpts.PopcornMigratable) {
+    if (CodeGenOpts.OptimizationLevel > 0)
+      PMBuilder.addExtension(PassManagerBuilder::EP_OptimizerLast,
+			     addPopcornMigPointPasses);
+    else {
+      MPM.add(createPopcornCompatibilityPass());
+      MPM.add(createSelectMigrationPointsPass());
+    }
+  }
+
+  // Adjust global symbol linkage for alignment.
+  if (CodeGenOpts.PopcornAlignment) {
+    if (CodeGenOpts.OptimizationLevel > 0)
+      PMBuilder.addExtension(PassManagerBuilder::EP_OptimizerLast,
+			     addPopcornAlignmentPasses);
+    else {
+      MPM.add(createNameStringLiteralsPass());
+      MPM.add(createStaticVarSectionsPass());
+    }
+  }
+
   PMBuilder.populateFunctionPassManager(FPM);
   PMBuilder.populateModulePassManager(MPM);
 }
@@ -789,8 +874,8 @@ bool EmitAssemblyHelper::AddEmitPasses(legacy::PassManager &CodeGenPasses,
   return true;
 }
 
-void EmitAssemblyHelper::EmitAssembly(BackendAction Action,
-                                      std::unique_ptr<raw_pwrite_stream> OS) {
+void EmitAssemblyHelper::SetupAssemblyHelper(BackendAction Action,
+					     raw_pwrite_stream *OS) {
   TimeRegion Region(FrontendTimesIsEnabled ? &CodeGenerationTime : nullptr);
 
   setCommandLineOpts(CodeGenOpts);
@@ -805,22 +890,17 @@ void EmitAssemblyHelper::EmitAssembly(BackendAction Action,
   if (TM)
     TheModule->setDataLayout(TM->createDataLayout());
 
-  legacy::PassManager PerModulePasses;
-  PerModulePasses.add(
+  getPerModulePasses()->add(
       createTargetTransformInfoWrapperPass(getTargetIRAnalysis()));
 
-  legacy::FunctionPassManager PerFunctionPasses(TheModule);
-  PerFunctionPasses.add(
+  getPerFunctionPasses()->add(
       createTargetTransformInfoWrapperPass(getTargetIRAnalysis()));
 
-  CreatePasses(PerModulePasses, PerFunctionPasses);
+  CreatePasses(*getPerModulePasses(), *getPerFunctionPasses());
 
-  legacy::PassManager CodeGenPasses;
-  CodeGenPasses.add(
+  getCodeGenPasses()->add(
       createTargetTransformInfoWrapperPass(getTargetIRAnalysis()));
 
-  std::unique_ptr<llvm::ToolOutputFile> ThinLinkOS, DwoOS;
-
   switch (Action) {
   case Backend_EmitNothing:
     break;
@@ -833,9 +913,9 @@ void EmitAssemblyHelper::EmitAssembly(BackendAction Action,
           return;
       }
       TheModule->addModuleFlag(Module::Error, "EnableSplitLTOUnit",
-                               CodeGenOpts.EnableSplitLTOUnit);
-      PerModulePasses.add(createWriteThinLTOBitcodePass(
-          *OS, ThinLinkOS ? &ThinLinkOS->os() : nullptr));
+			       CodeGenOpts.EnableSplitLTOUnit);
+      getPerModulePasses()->add(createWriteThinLTOBitcodePass(
+	  *OS, ThinLinkOS ? &ThinLinkOS->os() : nullptr));
     } else {
       // Emit a module summary by default for Regular LTO except for ld64
       // targets
@@ -851,14 +931,14 @@ void EmitAssemblyHelper::EmitAssembly(BackendAction Action,
                                  CodeGenOpts.EnableSplitLTOUnit);
       }
 
-      PerModulePasses.add(createBitcodeWriterPass(
-          *OS, CodeGenOpts.EmitLLVMUseLists, EmitLTOSummary));
+      getPerModulePasses()->add(createBitcodeWriterPass(
+	  *OS, CodeGenOpts.EmitLLVMUseLists, EmitLTOSummary));
     }
     break;
 
   case Backend_EmitLL:
-    PerModulePasses.add(
-        createPrintModulePass(*OS, "", CodeGenOpts.EmitLLVMUseLists));
+    getPerModulePasses()->add(
+	createPrintModulePass(*OS, "", CodeGenOpts.EmitLLVMUseLists));
     break;
 
   default:
@@ -867,35 +947,39 @@ void EmitAssemblyHelper::EmitAssembly(BackendAction Action,
       if (!DwoOS)
         return;
     }
-    if (!AddEmitPasses(CodeGenPasses, Action, *OS,
-                       DwoOS ? &DwoOS->os() : nullptr))
+    if (!AddEmitPasses(*getCodeGenPasses(), Action, *OS,
+		       DwoOS ? &DwoOS->os() : nullptr))
       return;
   }
 
   // Before executing passes, print the final values of the LLVM options.
   cl::PrintOptionValues();
+}
 
+void EmitAssemblyHelper::ApplyIROptPasses(Module* TheModule) {
   // Run passes. For now we do all passes at once, but eventually we
   // would like to have the option of streaming code generation.
 
   {
     PrettyStackTraceString CrashInfo("Per-function optimization");
 
-    PerFunctionPasses.doInitialization();
+    getPerFunctionPasses()->doInitialization();
     for (Function &F : *TheModule)
       if (!F.isDeclaration())
-        PerFunctionPasses.run(F);
-    PerFunctionPasses.doFinalization();
+	getPerFunctionPasses()->run(F);
+    getPerFunctionPasses()->doFinalization();
   }
 
   {
     PrettyStackTraceString CrashInfo("Per-module optimization passes");
-    PerModulePasses.run(*TheModule);
+    getPerModulePasses()->run(*TheModule);
   }
+}
 
+void EmitAssemblyHelper::ApplyCodegenPasses(Module* TheModule) {
   {
     PrettyStackTraceString CrashInfo("Code generation");
-    CodeGenPasses.run(*TheModule);
+    getCodeGenPasses()->run(*TheModule);
   }
 
   if (ThinLinkOS)
@@ -904,6 +988,16 @@ void EmitAssemblyHelper::EmitAssembly(BackendAction Action,
     DwoOS->keep();
 }
 
+void EmitAssemblyHelper::EmitAssembly(BackendAction Action,
+				      raw_pwrite_stream *OS) {
+  SetupAssemblyHelper(Action, OS);
+  ApplyIROptPasses(TheModule);
+  ApplyCodegenPasses(TheModule);
+
+  if (OS)
+    OS->flush();
+}
+
 static PassBuilder::OptimizationLevel mapToLevel(const CodeGenOptions &Opts) {
   switch (Opts.OptimizationLevel) {
   default:
@@ -978,7 +1072,7 @@ static void addSanitizersAtO0(ModulePassManager &MPM,
 /// This API is planned to have its functionality finished and then to replace
 /// `EmitAssembly` at some point in the future when the default switches.
 void EmitAssemblyHelper::EmitAssemblyWithNewPassManager(
-    BackendAction Action, std::unique_ptr<raw_pwrite_stream> OS) {
+    BackendAction Action, raw_pwrite_stream *OS) {
   TimeRegion Region(FrontendTimesIsEnabled ? &CodeGenerationTime : nullptr);
   setCommandLineOpts(CodeGenOpts);
 
@@ -1253,6 +1347,7 @@ void EmitAssemblyHelper::EmitAssemblyWithNewPassManager(
   case Backend_EmitAssembly:
   case Backend_EmitMCNull:
   case Backend_EmitObj:
+  case Backend_EmitMultiObj:
     NeedCodeGen = true;
     CodeGenPasses.add(
         createTargetTransformInfoWrapperPass(getTargetIRAnalysis()));
@@ -1317,7 +1412,7 @@ static void runThinLTOBackend(ModuleSummaryIndex *CombinedIndex, Module *M,
                               const CodeGenOptions &CGOpts,
                               const clang::TargetOptions &TOpts,
                               const LangOptions &LOpts,
-                              std::unique_ptr<raw_pwrite_stream> OS,
+                              raw_pwrite_stream *OS,
                               std::string SampleProfile,
                               std::string ProfileRemapping,
                               BackendAction Action) {
@@ -1372,7 +1467,8 @@ static void runThinLTOBackend(ModuleSummaryIndex *CombinedIndex, Module *M,
     OwnedImports.push_back(std::move(*MBOrErr));
   }
   auto AddStream = [&](size_t Task) {
-    return llvm::make_unique<lto::NativeObjectStream>(std::move(OS));
+    std::unique_ptr<raw_pwrite_stream> uOS = std::unique_ptr<raw_pwrite_stream>(OS);
+    return llvm::make_unique<lto::NativeObjectStream>(std::move(uOS));
   };
   lto::Config Conf;
   if (CGOpts.SaveTempsFilePrefix != "") {
@@ -1449,7 +1545,7 @@ void clang::EmitBackendOutput(DiagnosticsEngine &Diags,
                               const LangOptions &LOpts,
                               const llvm::DataLayout &TDesc, Module *M,
                               BackendAction Action,
-                              std::unique_ptr<raw_pwrite_stream> OS) {
+                              raw_pwrite_stream *OS) {
 
   llvm::TimeTraceScope TimeScope("Backend", StringRef(""));
 
@@ -1474,7 +1570,7 @@ void clang::EmitBackendOutput(DiagnosticsEngine &Diags,
     if (CombinedIndex) {
       if (!CombinedIndex->skipModuleByDistributedBackend()) {
         runThinLTOBackend(CombinedIndex.get(), M, HeaderOpts, CGOpts, TOpts,
-                          LOpts, std::move(OS), CGOpts.SampleProfileFile,
+                          LOpts, OS, CGOpts.SampleProfileFile,
                           CGOpts.ProfileRemappingFile, Action);
         return;
       }
@@ -1493,9 +1589,9 @@ void clang::EmitBackendOutput(DiagnosticsEngine &Diags,
   EmitAssemblyHelper AsmHelper(Diags, HeaderOpts, CGOpts, TOpts, LOpts, M);
 
   if (CGOpts.ExperimentalNewPassManager)
-    AsmHelper.EmitAssemblyWithNewPassManager(Action, std::move(OS));
+    AsmHelper.EmitAssemblyWithNewPassManager(Action, OS);
   else
-    AsmHelper.EmitAssembly(Action, std::move(OS));
+    AsmHelper.EmitAssembly(Action, OS);
 
   // Verify clang's TargetInfo DataLayout against the LLVM TargetMachine's
   // DataLayout.
@@ -1542,6 +1638,48 @@ static const char* getSectionNameForCommandline(const Triple &T) {
   llvm_unreachable("Unimplemented ObjectFormatType");
 }
 
+void clang::ApplyIROptimizations(DiagnosticsEngine &Diags,
+				 const HeaderSearchOptions &HeaderOpts,
+				 const CodeGenOptions &CGOpts,
+				 const clang::TargetOptions &TOpts,
+				 const LangOptions &LOpts, Module *M,
+				 BackendAction Action,
+				 raw_pwrite_stream *OS) {
+  EmitAssemblyHelper AsmHelper(Diags, HeaderOpts, CGOpts, TOpts, LOpts, M);
+  AsmHelper.SetupAssemblyHelper(Action, OS);
+  AsmHelper.ApplyIROptPasses(M);
+
+  if (OS)
+    OS->flush();
+}
+
+void clang::CodegenBackendOutput(DiagnosticsEngine &Diags,
+				 const HeaderSearchOptions &HeaderOpts,
+				 const CodeGenOptions &CGOpts,
+				 const clang::TargetOptions &TOpts,
+				 const LangOptions &LOpts, StringRef TDesc,
+				 Module *M, BackendAction Action,
+				 raw_pwrite_stream *OS) {
+  EmitAssemblyHelper AsmHelper(Diags, HeaderOpts, CGOpts, TOpts, LOpts, M);
+  AsmHelper.SetupAssemblyHelper(Action, OS);
+
+  // If an optional clang TargetInfo description string was passed in, use it to
+  // verify the LLVM TargetMachine's DataLayout.
+  if (AsmHelper.TM && !TDesc.empty()) {
+    std::string DLDesc =
+	AsmHelper.TM->createDataLayout().getStringRepresentation();
+    if (DLDesc != TDesc) {
+      unsigned DiagID = Diags.getCustomDiagID(
+	  DiagnosticsEngine::Error, "backend data layout '%0' does not match "
+				    "expected target description '%1'");
+      Diags.Report(DiagID) << DLDesc << TDesc;
+    }
+  }
+
+  AsmHelper.ApplyCodegenPasses(M);
+  OS->flush();
+}
+
 // With -fembed-bitcode, save a copy of the llvm IR as data in the
 // __LLVM,__bitcode section.
 void clang::EmbedBitcode(llvm::Module *M, const CodeGenOptions &CGOpts,
diff --git a/clang/lib/CodeGen/CGOpenMPRuntime.cpp b/clang/lib/CodeGen/CGOpenMPRuntime.cpp
index 27e7175da84..1efea1c4944 100644
--- a/clang/lib/CodeGen/CGOpenMPRuntime.cpp
+++ b/clang/lib/CodeGen/CGOpenMPRuntime.cpp
@@ -535,6 +535,7 @@ enum OpenMPSchedType {
   OMP_sch_guided_chunked = 36,
   OMP_sch_runtime = 37,
   OMP_sch_auto = 38,
+  OMP_sch_hetprobe = 39,
   /// static with chunk adjustment (e.g., simd)
   OMP_sch_static_balanced_chunked = 45,
   /// Lower bound for 'ordered' versions.
@@ -3425,6 +3426,8 @@ static OpenMPSchedType getRuntimeSchedule(OpenMPScheduleClauseKind ScheduleKind,
     return Ordered ? OMP_ord_runtime : OMP_sch_runtime;
   case OMPC_SCHEDULE_auto:
     return Ordered ? OMP_ord_auto : OMP_sch_auto;
+  case OMPC_SCHEDULE_hetprobe:
+    return OMP_sch_hetprobe;
   case OMPC_SCHEDULE_unknown:
     assert(!Chunked && "chunk was specified but schedule kind not known");
     return Ordered ? OMP_ord_static : OMP_sch_static;
diff --git a/clang/lib/CodeGen/CGStmt.cpp b/clang/lib/CodeGen/CGStmt.cpp
index 40ab79509f9..a6d0447f899 100644
--- a/clang/lib/CodeGen/CGStmt.cpp
+++ b/clang/lib/CodeGen/CGStmt.cpp
@@ -17,6 +17,7 @@
 #include "clang/AST/StmtVisitor.h"
 #include "clang/Basic/Builtins.h"
 #include "clang/Basic/PrettyStackTrace.h"
+#include "clang/CodeGen/PrefetchBuilder.h"
 #include "clang/Basic/TargetInfo.h"
 #include "llvm/ADT/StringExtras.h"
 #include "llvm/IR/DataLayout.h"
@@ -851,6 +852,19 @@ void CodeGenFunction::EmitForStmt(const ForStmt &S,
 
   LexicalScope ForScope(*this, S.getSourceRange());
 
+  if(S.prefetchEnabled()) {
+    const PrefetchAnalysis *PA = getContext().getPrefetchAnalysis(&S);
+    if(PA) {
+      PrefetchBuilder PB(this);
+      const SmallVector<PrefetchRange, 8> &Pref = PA->getArraysToPrefetch();
+      if(Pref.size()) {
+        PB.EmitPrefetchCallDeclarations();
+        for(auto &Range : Pref) PB.EmitPrefetchCall(Range);
+        PB.EmitPrefetchExecuteCall();
+      }
+    }
+  }
+
   // Evaluate the first part before the loop.
   if (S.getInit())
     EmitStmt(S.getInit());
@@ -2304,14 +2318,102 @@ void CodeGenFunction::EmitAsmStmt(const AsmStmt &S) {
   }
 }
 
+void CodeGenFunction::addOffloaded(const CapturedStmt *S, ValueDecl *L,
+                                   ValueDecl *G) {
+  assert(S && isa<CapturedStmt>(S) && "Invalid captured statement");
+  assert(L && G && "Invalid value declarations");
+  OffloadedLocals[S].emplace_back(L, G);
+}
+
+const CodeGenFunction::OffloadList &
+CodeGenFunction::getOffloaded(const CapturedStmt *S) const {
+  assert(S && isa<CapturedStmt>(S) && "Invalid captured statement");
+  OffloadMap::const_iterator it = OffloadedLocals.find(S);
+  assert((it != OffloadedLocals.end()) && "Captured statement not visited?");
+  return it->second;
+}
+
+VarDecl *CodeGenFunction::CreateOffloadedGlobal(const Stmt &S,
+                                                QualType Ty,
+                                                std::string &Name) {
+  ASTContext &AST = getContext();
+  DeclContext *TUC = AST.getTranslationUnitDecl();
+  SourceRange SR = S.getSourceRange();
+  IdentifierInfo *II = &AST.Idents.get("distr_omp_" +
+      std::string(CurFn->getName()) + "_" + Name);
+  if(Ty.isLocalConstQualified()) Ty.removeLocalConst();
+  TypeSourceInfo *TSI = AST.getTrivialTypeSourceInfo(Ty, SR.getBegin());
+  return VarDecl::Create(AST, TUC, SR.getBegin(), SR.getEnd(),
+                         II, Ty, TSI, clang::SC_Static);
+}
+
+VarDecl *CodeGenFunction::CreateOffloadedGlobal(const Stmt &S, const Expr *I) {
+  std::string name(cast<DeclRefExpr>(I)->getDecl()->getNameAsString());
+  return CreateOffloadedGlobal(S, I->getType(), name);
+}
+
+LValue CodeGenFunction::GetVarDeclLValue(QualType SrcType, VarDecl *VD) {
+  if(SrcType.isLocalConstQualified()) SrcType.removeLocalConst();
+  llvm::Type *Ty = CGM.getTypes().ConvertType(SrcType);
+  llvm::Constant *GV = CGM.GetAddrOfGlobalVar(VD, Ty);
+  llvm::GlobalVariable *CastGV = cast<llvm::GlobalVariable>(GV);
+  CastGV->setInitializer(llvm::Constant::getNullValue(Ty));
+  CastGV->setLinkage(llvm::GlobalValue::InternalLinkage);
+  return MakeNaturalAlignAddrLValue(GV, SrcType);
+}
+
+LValue CodeGenFunction::GetVarDeclLValue(const Expr *I, VarDecl *VD) {
+  return GetVarDeclLValue(I->getType(), VD);
+}
+
+DeclRefExpr *CodeGenFunction::GetDeclRefForOffload(ValueDecl *VD) {
+  ASTContext &AST = getContext();
+  QualType Ty = VD->getType();
+  DeclRefExpr *DRE = DeclRefExpr::Create(AST, NestedNameSpecifierLoc(),
+                                         SourceLocation(), VD, false,
+                                         VD->getSourceRange().getBegin(),
+                                         Ty, VK_LValue);
+  DRE->getDecl()->setIsUsed();
+  return DRE;
+}
+
+void CodeGenFunction::RestoreOffloadedLocals(const CapturedStmt *S) {
+  Expr *Global;
+  const OffloadList &Offloaded = getOffloaded(S);
+  for(auto Pair : Offloaded) {
+    if(Pair.first->getType().isLocalConstQualified()) continue;
+    LValue LocalLV(EmitDeclRefLValue(GetDeclRefForOffload(Pair.first)));
+    Global = GetDeclRefForOffload(Pair.second);
+    EmitAnyExprToMem(Global, LocalLV.getAddress(), LocalLV.getQuals(), false);
+  }
+}
+
 LValue CodeGenFunction::InitCapturedStruct(const CapturedStmt &S) {
   const RecordDecl *RD = S.getCapturedRecordDecl();
   QualType RecordTy = getContext().getRecordType(RD);
 
+  // We have to manually track the context number to avoid naming collisions
+  // for files with multiple captured regions
+  static int CtxNum = 0;
+
   // Initialize the captured struct.
-  LValue SlotLV =
-    MakeAddrLValue(CreateMemTemp(RecordTy, "agg.captured"), RecordTy);
+  LValue SlotLV;
+  if(S.offloadShared()) {
+    // Name the captured context corresponding to the generated anonymous
+    // structure type for the context
+    std::string Name("agg.captured");
+    if(CtxNum) Name += "." + std::to_string(CtxNum - 1);
+    CtxNum++;
+
+    VarDecl *GlobalCtx = CreateOffloadedGlobal(S, RecordTy, Name);
+    SlotLV = GetVarDeclLValue(RecordTy, GlobalCtx);
+  }
+  else
+    SlotLV =
+      MakeAddrLValue(CreateMemTemp(RecordTy, "agg.captured"), RecordTy);
 
+  if(S.offloadShared()) OffloadedLocals[&S] = OffloadList();
+ 
   RecordDecl::field_iterator CurField = RD->field_begin();
   for (CapturedStmt::const_capture_init_iterator I = S.capture_init_begin(),
                                                  E = S.capture_init_end();
@@ -2321,7 +2423,17 @@ LValue CodeGenFunction::InitCapturedStruct(const CapturedStmt &S) {
       auto VAT = CurField->getCapturedVLAType();
       EmitStoreThroughLValue(RValue::get(VLASizeMap[VAT->getSizeExpr()]), LV);
     } else {
-      EmitInitializerForField(*CurField, LV, *I);
+      if(S.offloadShared() && isa<DeclRefExpr>(*I)) {
+        // If distributed, create global variable, emit initializer & place
+        // address of new global into capture struct
+        VarDecl *GLD = CreateOffloadedGlobal(S, *I);
+        LValue GLV = GetVarDeclLValue(*I, GLD);
+        EmitAnyExprToMem(*I, GLV.getAddress(), GLV.getQuals(), false);
+        Expr *GlobalRef = GetDeclRefForOffload(GLD);
+        EmitInitializerForField(*CurField, LV, GlobalRef);
+        addOffloaded(&S, cast<DeclRefExpr>(*I)->getDecl(), GLD);
+      }
+      else EmitInitializerForField(*CurField, LV, *I);
     }
   }
 
diff --git a/clang/lib/CodeGen/CGStmtOpenMP.cpp b/clang/lib/CodeGen/CGStmtOpenMP.cpp
index e8fbca5108a..36c9b8cbe9d 100644
--- a/clang/lib/CodeGen/CGStmtOpenMP.cpp
+++ b/clang/lib/CodeGen/CGStmtOpenMP.cpp
@@ -18,6 +18,7 @@
 #include "clang/AST/Stmt.h"
 #include "clang/AST/StmtOpenMP.h"
 #include "clang/AST/DeclOpenMP.h"
+#include "llvm/IR/CallSite.h"
 using namespace clang;
 using namespace CodeGen;
 
@@ -1272,6 +1273,7 @@ static void emitCommonOMPParallelDirective(
   CGF.GenerateOpenMPCapturedVars(*CS, CapturedVars);
   CGF.CGM.getOpenMPRuntime().emitParallelCall(CGF, S.getBeginLoc(), OutlinedFn,
                                               CapturedVars, IfCond);
+  if(CS->offloadShared()) CGF.RestoreOffloadedLocals(CS);
 }
 
 static void emitEmptyBoundParameters(CodeGenFunction &,
@@ -1619,6 +1621,161 @@ static void emitSimdlenSafelenClause(CodeGenFunction &CGF,
   }
 }
 
+/// Ease the variable lookup burden for captures.
+typedef llvm::DenseMap<const VarDecl *, DeclRefExpr *> CaptureMap;
+
+/// Map variable declarations captured in the outer function to their field in
+/// the captured struct.
+static void
+buildCapturedMap(ASTContext &C, CapturedStmt *CS, CaptureMap &Map) {
+  // Captures are in a 1-to-1 correspondence with captured record fields
+  Map.clear();
+  for(auto Child : CS->children()) {
+    if(isa<DeclRefExpr>(Child)) {
+      DeclRefExpr *DRE = cast<DeclRefExpr>(Child);
+      ValueDecl *Decl = DRE->getDecl();
+      if(isa<VarDecl>(Decl)) Map[cast<VarDecl>(Decl)] = DRE;
+    }
+  }
+}
+
+/// Create an expression representing the address of some array index.
+static Expr *getPrefetchAddr(ASTContext &C, Expr *Ptr, Expr *Subscript) {
+  QualType BaseTy = Ptr->getType().getDesugaredType(C), IdxTy;
+
+  // Get an array subscript
+  if(isa<ArrayType>(BaseTy)) IdxTy = cast<ArrayType>(BaseTy)->getElementType();
+  else IdxTy = cast<PointerType>(BaseTy)->getPointeeType();
+  Expr *Index = new (C) ArraySubscriptExpr(Ptr, Subscript, IdxTy,
+                                           VK_RValue, OK_Ordinary,
+                                           SourceLocation());
+
+  // Take the address of the array subscript
+  QualType RePtrTy = C.getPointerType(IdxTy);
+  UnaryOperator *Addr = new (C) UnaryOperator(Index, UO_AddrOf, RePtrTy, VK_LValue,
+					      OK_Ordinary, SourceLocation(), true);
+
+  // Finally, cast to a const void * type
+  QualType VoidPtrTy = C.getPointerType(C.VoidTy.withConst());
+  return ImplicitCastExpr::Create(C, VoidPtrTy, CK_BitCast, Addr, nullptr,
+                                  VK_RValue);
+}
+
+static Expr *getArrayIndexAddr(ASTContext &C, Expr *Arr, const llvm::APInt &Idx) {
+  QualType Ty = Arr->getType();
+  if(Ty->isArrayType()) {
+    Ty = C.getPointerType(cast<ArrayType>(Ty)->getElementType());
+    Arr = ImplicitCastExpr::Create(C, Ty, CK_ArrayToPointerDecay, Arr, nullptr,
+                                   VK_RValue);
+  }
+  Expr *IdxLiteral =
+    IntegerLiteral::Create(C, Idx, C.LongTy, SourceLocation());
+  return getPrefetchAddr(C, Arr, IdxLiteral);
+}
+
+static Expr *getArrayIndexAddr(ASTContext &C, Expr *Arr, int64_t Idx) {
+  return getArrayIndexAddr(C, Arr, llvm::APInt(64, Idx, true));
+}
+
+static llvm::Constant *getPrefetchKind(CodeGenFunction &CGF,
+                                       OpenMPPrefetchClauseKind Kind) {
+  llvm::Type *Ty = llvm::Type::getInt32Ty(CGF.CurFn->getContext());
+  switch(Kind) {
+  case OMPC_PREFETCH_read: return llvm::ConstantInt::get(Ty, 0);
+  case OMPC_PREFETCH_write: return llvm::ConstantInt::get(Ty, 1);
+  //case OMPC_PREFETCH_release: return llvm::ConstantInt::get(Ty, 3);
+  default:
+    llvm_unreachable("Invalid prefetch type\n");
+    return nullptr;
+  }
+}
+
+void CodeGenFunction::EmitOMPPrefetchClauses(const OMPLoopDirective &D) {
+  ASTContext &AST = getContext();
+  CaptureMap AllCaptures;
+  CaptureMap::iterator Captured;
+  const ConstantArrayType *ArrTy;
+  Expr *Base, *Start, *End, *StartAddr, *EndAddr;
+  RValue LoweredStart, LoweredEnd;
+  std::vector<llvm::Value *> Params;
+  std::vector<llvm::Type *> ParamTypes;
+  llvm::FunctionType *FnType;
+  llvm::FunctionCallee Prefetch, Execute;
+
+  bool HasPrefetch = D.hasClausesOfKind<OMPPrefetchClause>();
+  if(HasPrefetch) {
+    // declare void @popcorn_prefetch(i32, i8*, i8*)
+    ParamTypes = { Int32Ty, Int8PtrTy, Int8PtrTy };
+    FnType = llvm::FunctionType::get(VoidTy, ParamTypes, false);
+    Prefetch = CGM.CreateRuntimeFunction(FnType, "popcorn_prefetch");
+
+    // declare i64 @popcorn_prefetch_execute()
+    ParamTypes.clear();
+    FnType = llvm::FunctionType::get(Int64Ty, ParamTypes, false);
+    Execute = CGM.CreateRuntimeFunction(FnType, "popcorn_prefetch_execute");
+
+    // For each prefetched variable, construct start & end range expressions
+    // and call @popcorn_prefetch
+    const auto *CS = cast_or_null<CapturedStmt>(D.getAssociatedStmt());
+    buildCapturedMap(AST, const_cast<CapturedStmt *>(CS), AllCaptures);
+
+    for(const auto *C : D.getClausesOfKind<OMPPrefetchClause>()) {
+      Start = C->getStartOfRange();
+      End = C->getEndOfRange();
+
+      for(auto &V : C->varlists()) {
+        const DeclRefExpr *DR = cast<DeclRefExpr>(V);
+        const VarDecl *VD = cast<VarDecl>(DR->getDecl());
+        Captured = AllCaptures.find(VD);
+
+        // TODO the current mechanism for calculating addresses applies an
+        // "inbounds" tag to the array index addressing expression, but we
+        // don't necessarily know this is true.
+        if(Captured != AllCaptures.end()) {
+          Base = Captured->second;
+          if(Start && End) {
+            // User specified entire range
+            StartAddr = getPrefetchAddr(AST, Base, Start);
+            EndAddr = getPrefetchAddr(AST, Base, End);
+          }
+          else if(Start) {
+            // User specified an expression affine to loop iteration variable
+            // TODO if expression is affine transformation of loop induction
+            // variable, need to re-generate for lower/upper bound variables
+            assert(isa<DeclRefExpr>(Start) &&
+                   "Can't handle transformations on loop variables yet");
+            StartAddr = getPrefetchAddr(AST, Base, D.getLowerBoundVariable());
+            EndAddr = getPrefetchAddr(AST, Base, D.getUpperBoundVariable());
+          }
+          else {
+            // User didn't specify a range, prefetch the entire array (note:
+            // should have type checked it's an array by now).
+            QualType QTy = Base->getType();
+            while(isa<DecayedType>(QTy))
+              QTy = cast<DecayedType>(QTy)->getOriginalType();
+            ArrTy = cast<ConstantArrayType>(QTy);
+            const llvm::APInt &Size = ArrTy->getSize();
+            StartAddr = getArrayIndexAddr(AST, Base, 0);
+            EndAddr = getArrayIndexAddr(AST, Base, Size);
+          }
+
+          LoweredStart = EmitAnyExpr(StartAddr);
+          LoweredEnd = EmitAnyExpr(EndAddr);
+          Params = { getPrefetchKind(*this, C->getPrefetchKind()),
+                     LoweredStart.getScalarVal(),
+                     LoweredEnd.getScalarVal() };
+          EmitCallOrInvoke(Prefetch, Params);
+        }
+        else llvm_unreachable("Invalid prefetch variable");
+      }
+    }
+
+    // Finally, call @popcorn_prefetch_execute to issue requests
+    Params.clear();
+    EmitCallOrInvoke(Execute, Params);
+  }
+}
+
 void CodeGenFunction::EmitOMPSimdInit(const OMPLoopDirective &D,
                                       bool IsMonotonic) {
   // Walk clauses and process safelen/lastprivate.
@@ -2369,6 +2526,8 @@ bool CodeGenFunction::EmitOMPWorksharingLoop(
         // UB = min(UB, GlobalUB);
         if (!StaticChunkedOne)
           EmitIgnoredExpr(S.getEnsureUpperBound());
+        // Popcorn: emit prefetch function declarations & requests
+        if(S.prefetchingEnabled()) EmitOMPPrefetchClauses(S);
         // IV = LB;
         EmitIgnoredExpr(S.getInit());
         // For unchunked static schedule generate:
@@ -3991,6 +4150,7 @@ static void emitOMPAtomicExpr(CodeGenFunction &CGF, OpenMPClauseKind Kind,
   case OMPC_reverse_offload:
   case OMPC_dynamic_allocators:
   case OMPC_atomic_default_mem_order:
+  case OMPC_prefetch:
     llvm_unreachable("Clause is not allowed in 'omp atomic'.");
   }
 }
diff --git a/clang/lib/CodeGen/CMakeLists.txt b/clang/lib/CodeGen/CMakeLists.txt
index 6d1f33b8924..abe5086201e 100644
--- a/clang/lib/CodeGen/CMakeLists.txt
+++ b/clang/lib/CodeGen/CMakeLists.txt
@@ -90,6 +90,8 @@ add_clang_library(clangCodeGen
   ModuleBuilder.cpp
   ObjectFilePCHContainerOperations.cpp
   PatternInit.cpp
+  PopcornUtil.cpp
+  PrefetchBuilder.cpp
   SanitizerMetadata.cpp
   SwiftCallingConv.cpp
   TargetInfo.cpp
diff --git a/clang/lib/CodeGen/CodeGenAction.cpp b/clang/lib/CodeGen/CodeGenAction.cpp
index 0ae9ea427d6..e90376a9c30 100644
--- a/clang/lib/CodeGen/CodeGenAction.cpp
+++ b/clang/lib/CodeGen/CodeGenAction.cpp
@@ -19,6 +19,7 @@
 #include "clang/Basic/TargetInfo.h"
 #include "clang/CodeGen/BackendUtil.h"
 #include "clang/CodeGen/ModuleBuilder.h"
+#include "clang/CodeGen/PopcornUtil.h"
 #include "clang/Driver/DriverDiagnostic.h"
 #include "clang/Frontend/CompilerInstance.h"
 #include "clang/Frontend/FrontendDiagnostic.h"
@@ -41,6 +42,7 @@
 #include "llvm/Support/ToolOutputFile.h"
 #include "llvm/Support/YAMLTraits.h"
 #include "llvm/Transforms/IPO/Internalize.h"
+#include "llvm/Transforms/Utils/Cloning.h"
 
 #include <memory>
 using namespace clang;
@@ -80,16 +82,17 @@ namespace clang {
   };
 
   class BackendConsumer : public ASTConsumer {
+    virtual void anchor();
+  protected:
     using LinkModule = CodeGenAction::LinkModule;
 
-    virtual void anchor();
     DiagnosticsEngine &Diags;
     BackendAction Action;
     const HeaderSearchOptions &HeaderSearchOpts;
     const CodeGenOptions &CodeGenOpts;
     const TargetOptions &TargetOpts;
     const LangOptions &LangOpts;
-    std::unique_ptr<raw_pwrite_stream> AsmOutStream;
+    raw_pwrite_stream *AsmOutStream;
     ASTContext *Context;
 
     Timer LLVMIRGeneration;
@@ -117,11 +120,11 @@ namespace clang {
                     const LangOptions &LangOpts, bool TimePasses,
                     const std::string &InFile,
                     SmallVector<LinkModule, 4> LinkModules,
-                    std::unique_ptr<raw_pwrite_stream> OS, LLVMContext &C,
+                    raw_pwrite_stream *OS, LLVMContext &C,
                     CoverageSourceInfo *CoverageInfo = nullptr)
         : Diags(Diags), Action(Action), HeaderSearchOpts(HeaderSearchOpts),
           CodeGenOpts(CodeGenOpts), TargetOpts(TargetOpts), LangOpts(LangOpts),
-          AsmOutStream(std::move(OS)), Context(nullptr),
+          AsmOutStream(OS), Context(nullptr),
           LLVMIRGeneration("irgen", "LLVM IR Generation Time"),
           LLVMIRGenerationRefCount(0),
           Gen(CreateLLVMCodeGen(Diags, InFile, HeaderSearchOpts, PPOpts,
@@ -226,7 +229,7 @@ namespace clang {
       return false; // success
     }
 
-    void HandleTranslationUnit(ASTContext &C) override {
+    void HandleTranslationUnitCommon(ASTContext &C) {
       {
         PrettyStackTraceString CrashInfo("Per-file LLVM IR generation");
         if (FrontendTimesIsEnabled) {
@@ -245,6 +248,10 @@ namespace clang {
 
         IRGenFinished = true;
       }
+    }
+
+    void HandleTranslationUnit(ASTContext &C) override {
+      HandleTranslationUnitCommon(C);
 
       // Silently ignore if we weren't initialized for some reason.
       if (!getModule())
@@ -302,7 +309,7 @@ namespace clang {
 
       EmitBackendOutput(Diags, HeaderSearchOpts, CodeGenOpts, TargetOpts,
                         LangOpts, C.getTargetInfo().getDataLayout(),
-                        getModule(), Action, std::move(AsmOutStream));
+                        getModule(), Action, AsmOutStream);
 
       Ctx.setInlineAsmDiagnosticHandler(OldHandler, OldContext);
 
@@ -378,6 +385,81 @@ namespace clang {
   };
 
   void BackendConsumer::anchor() {}
+
+  class MultiBackendConsumer : public BackendConsumer {
+  private:
+    virtual void anchor() override;
+    SmallVector<raw_pwrite_stream *, 2> &AsmOutStreams;
+    const SmallVector<std::shared_ptr<TargetOptions>, 2> &AsmTargetOpts;
+    const SmallVector<TargetInfo *, 2> &AsmTargetInfos;
+    const CodeGenOptions &NoOptCodegen;
+  public:
+    MultiBackendConsumer(DiagnosticsEngine &Diags,
+                    const HeaderSearchOptions &HeaderSearchOpts,
+                    const PreprocessorOptions &PPOpts,
+                    const CodeGenOptions &CodeGenOpts,
+                    const CodeGenOptions &NoOptCodegen,
+                    const SmallVector<std::shared_ptr<TargetOptions>, 2> &TargetOpts,
+                    const LangOptions &LangOpts, bool TimePasses,
+                    const std::string &InFile,
+		    SmallVector<LinkModule, 4> LinkModules,
+                    SmallVector<raw_pwrite_stream *, 2> &OSs, LLVMContext &C,
+                    const SmallVector<TargetInfo *, 2> &TargetInfos,
+                    CoverageSourceInfo *CoverageInfo = nullptr)
+        : BackendConsumer(Backend_EmitMultiObj, Diags, HeaderSearchOpts,
+                          PPOpts, CodeGenOpts, *TargetOpts[0], LangOpts,
+                          TimePasses, InFile, std::move(LinkModules), OSs[0], C,
+                          CoverageInfo),
+          AsmOutStreams(OSs), AsmTargetOpts(TargetOpts),
+          AsmTargetInfos(TargetInfos), NoOptCodegen(NoOptCodegen) {}
+
+    void HandleTranslationUnit(ASTContext &C) override {
+      HandleTranslationUnitCommon(C);
+
+      // Silently ignore if we weren't initialized for some reason.
+      if (!getModule())
+        return;
+
+      // Install an inline asm handler so that diagnostics get printed through
+      // our diagnostics hooks.
+      LLVMContext &Ctx = getModule()->getContext();
+      LLVMContext::InlineAsmDiagHandlerTy OldHandler =
+        Ctx.getInlineAsmDiagnosticHandler();
+      void *OldContext = Ctx.getInlineAsmDiagnosticContext();
+      Ctx.setInlineAsmDiagnosticHandler(InlineAsmDiagHandler, this);
+
+      std::unique_ptr<DiagnosticHandler> OldDiagnosticHandler =
+          Ctx.getDiagnosticHandler();
+      Ctx.setDiagnosticHandler(llvm::make_unique<ClangDiagnosticHandler>(
+        CodeGenOpts, this));
+
+      // Apply IR optimizations, but strip target-specific attributes from
+      // all functions added by analyses
+      std::shared_ptr<TargetOptions> IRTargetOpts =
+        Popcorn::GetPopcornTargetOpts(getModule()->getTargetTriple());
+      ApplyIROptimizations(Diags, HeaderSearchOpts, CodeGenOpts, *IRTargetOpts, LangOpts,
+                           getModule(), Action, nullptr);
+      Popcorn::StripTargetAttributes(*getModule());
+
+      // Generate machine code for each target
+      for(size_t i = 0; i < AsmTargetOpts.size(); i++) {
+        std::unique_ptr<llvm::Module> ArchModule = CloneModule(*getModule());
+        ArchModule->setTargetTriple(AsmTargetInfos[i]->getTriple().getTriple());
+        ArchModule->setDataLayout(AsmTargetInfos[i]->getDataLayout());
+        Popcorn::AddArchSpecificTargetFeatures(*ArchModule, AsmTargetOpts[i]);
+        CodegenBackendOutput(Diags, HeaderSearchOpts, NoOptCodegen, *AsmTargetOpts[i], LangOpts,
+                             AsmTargetInfos[i]->getDataLayout().getStringRepresentation(),
+                             ArchModule.release(), Action, AsmOutStreams[i]);
+        //delete ArchModule;
+      }
+
+      Ctx.setInlineAsmDiagnosticHandler(OldHandler, OldContext);
+
+      Ctx.setDiagnosticHandler(std::move(OldDiagnosticHandler));
+    }
+  };
+
+  void MultiBackendConsumer::anchor() {}
 }
 
 bool ClangDiagnosticHandler::handleDiagnostics(const DiagnosticInfo &DI) {
@@ -852,22 +934,14 @@ GetOutputStream(CompilerInstance &CI, StringRef InFile, BackendAction Action) {
   case Backend_EmitMCNull:
     return CI.createNullOutputFile();
   case Backend_EmitObj:
+  case Backend_EmitMultiObj:
     return CI.createDefaultOutputFile(true, InFile, "o");
   }
 
   llvm_unreachable("Invalid action!");
 }
 
-std::unique_ptr<ASTConsumer>
-CodeGenAction::CreateASTConsumer(CompilerInstance &CI, StringRef InFile) {
-  BackendAction BA = static_cast<BackendAction>(Act);
-  std::unique_ptr<raw_pwrite_stream> OS = CI.takeOutputStream();
-  if (!OS)
-    OS = GetOutputStream(CI, InFile, BA);
-
-  if (BA != Backend_EmitNothing && !OS)
-    return nullptr;
-
+SmallVector<clang::CodeGenAction::LinkModule, 4> CodeGenAction::getLinkModuleToUse(CompilerInstance &CI) {
   // Load bitcode modules to link with, if we need to.
   if (LinkModules.empty())
     for (const CodeGenOptions::BitcodeFileToLink &F :
@@ -877,7 +951,7 @@ CodeGenAction::CreateASTConsumer(CompilerInstance &CI, StringRef InFile) {
         CI.getDiagnostics().Report(diag::err_cannot_open_file)
             << F.Filename << BCBuf.getError().message();
         LinkModules.clear();
-        return nullptr;
+        return SmallVector<clang::CodeGenAction::LinkModule, 4>{};
       }
 
       Expected<std::unique_ptr<llvm::Module>> ModuleOrErr =
@@ -888,12 +962,16 @@ CodeGenAction::CreateASTConsumer(CompilerInstance &CI, StringRef InFile) {
               << F.Filename << EIB.message();
         });
         LinkModules.clear();
-        return nullptr;
+        return SmallVector<clang::CodeGenAction::LinkModule, 4>{};
       }
       LinkModules.push_back({std::move(ModuleOrErr.get()), F.PropagateAttrs,
                              F.Internalize, F.LinkFlags});
     }
 
+  return std::move(LinkModules);
+}
+
+CoverageSourceInfo *CodeGenAction::getCoverageInfo(CompilerInstance &CI) {
   CoverageSourceInfo *CoverageInfo = nullptr;
   // Add the preprocessor callback only when the coverage mapping is generated.
   if (CI.getCodeGenOpts().CoverageMapping) {
@@ -902,11 +980,26 @@ CodeGenAction::CreateASTConsumer(CompilerInstance &CI, StringRef InFile) {
                                     std::unique_ptr<PPCallbacks>(CoverageInfo));
   }
 
+  return CoverageInfo;
+}
+
+std::unique_ptr<ASTConsumer>
+CodeGenAction::CreateASTConsumer(CompilerInstance &CI, StringRef InFile) {
+  BackendAction BA = static_cast<BackendAction>(Act);
+  std::unique_ptr<raw_pwrite_stream> OS = CI.takeOutputStream();
+  if (!OS)
+    OS = GetOutputStream(CI, InFile, BA);
+
+  if (BA != Backend_EmitNothing && !OS)
+    return nullptr;
+
+  CoverageSourceInfo *CoverageInfo = getCoverageInfo(CI);
+
   std::unique_ptr<BackendConsumer> Result(new BackendConsumer(
       BA, CI.getDiagnostics(), CI.getHeaderSearchOpts(),
       CI.getPreprocessorOpts(), CI.getCodeGenOpts(), CI.getTargetOpts(),
       CI.getLangOpts(), CI.getFrontendOpts().ShowTimers, InFile,
-      std::move(LinkModules), std::move(OS), *VMContext, CoverageInfo));
+      std::move(LinkModules), OS.release(), *VMContext, CoverageInfo));
   BEConsumer = Result.get();
 
   // Enable generating macro debug info only when debug info is not disabled and
@@ -1012,6 +1105,42 @@ CodeGenAction::loadModule(MemoryBufferRef MBRef) {
   return {};
 }
 
+bool CodeGenAction::ExecuteActionIRCommon(BackendAction &BA,
+                                          CompilerInstance &CI) {
+  bool Invalid;
+  SourceManager &SM = CI.getSourceManager();
+  FileID FID = SM.getMainFileID();
+  const llvm::MemoryBuffer *MainFile = SM.getBuffer(FID, &Invalid);
+  if (Invalid)
+    return true;
+
+  llvm::SMDiagnostic Err;
+  TheModule = parseIR(MainFile->getMemBufferRef(), Err, *VMContext);
+  if (!TheModule) {
+    // Translate from the diagnostic info to the SourceManager location if
+    // available.
+    // TODO: Unify this with ConvertBackendLocation()
+    SourceLocation Loc;
+    if (Err.getLineNo() > 0) {
+      assert(Err.getColumnNo() >= 0);
+      Loc = SM.translateFileLineCol(SM.getFileEntryForID(FID),
+                                    Err.getLineNo(), Err.getColumnNo() + 1);
+    }
+
+    // Strip off a leading diagnostic code if there is one.
+    StringRef Msg = Err.getMessage();
+    if (Msg.startswith("error: "))
+      Msg = Msg.substr(7);
+
+    unsigned DiagID =
+        CI.getDiagnostics().getCustomDiagID(DiagnosticsEngine::Error, "%0");
+
+    CI.getDiagnostics().Report(Loc, DiagID) << Msg;
+    return true;
+  }
+  return false;
+}
+
 void CodeGenAction::ExecuteAction() {
   // If this is an IR file, we have to treat it specially.
   if (getCurrentFileKind().getLanguage() == InputKind::LLVM_IR) {
@@ -1051,7 +1180,7 @@ void CodeGenAction::ExecuteAction() {
     EmitBackendOutput(CI.getDiagnostics(), CI.getHeaderSearchOpts(),
                       CI.getCodeGenOpts(), TargetOpts, CI.getLangOpts(),
                       CI.getTarget().getDataLayout(), TheModule.get(), BA,
-                      std::move(OS));
+                      OS.release());
     return;
   }
 
@@ -1084,3 +1213,155 @@ EmitCodeGenOnlyAction::EmitCodeGenOnlyAction(llvm::LLVMContext *_VMContext)
 void EmitObjAction::anchor() { }
 EmitObjAction::EmitObjAction(llvm::LLVMContext *_VMContext)
   : CodeGenAction(Backend_EmitObj, _VMContext) {}
+
+void EmitMultiObjAction::anchor() { }
+EmitMultiObjAction::EmitMultiObjAction(llvm::LLVMContext *_VMContext)
+  : CodeGenAction(Backend_EmitMultiObj, _VMContext) {}
+
+static std::string AppendArchName(StringRef File, const TargetInfo *TI) {
+  size_t dot = File.rfind('.');
+  std::string NewFile = File.substr(0, dot);
+  NewFile += "_";
+  NewFile += TI->getTriple().getArchName();
+  NewFile += File.substr(dot, StringRef::npos);
+  return NewFile;
+}
+
+static llvm::SmallVector<std::string, 2>
+dedupTargets(llvm::SmallVector<std::string, 2> &Requested) {
+  std::set<std::string> Targets;
+  for(auto &Target : Requested) Targets.insert(Target);
+  llvm::SmallVector<std::string, 2> Ret;
+  for(auto Target : Targets) Ret.push_back(Target);
+  return Ret;
+}
+
+/// Create target information & output streams for each target.
+bool EmitMultiObjAction::InitializeTargets(CompilerInstance &CI,
+                                           StringRef InFile) {
+  // Note: remove output filename from frontend args, as it will override any
+  // special names we try to specify.
+  std::string OutFile(InFile);
+  if(CI.getFrontendOpts().OutputFile != "") {
+    OutFile = CI.getFrontendOpts().OutputFile;
+    CI.getFrontendOpts().OutputFile = "";
+  }
+
+  // Populate list of targets requested.  If none are requested, default to all
+  // supported targets.
+  std::vector<std::string> &Requested = CI.getCodeGenOpts().PopcornTargets;
+  if(Requested.size()) {
+    for(auto &Target : Requested) {
+      if(!Popcorn::SupportedTarget(Target)) {
+        std::string Msg("Popcorn: unsupported target '" + Target + "'");
+        unsigned DiagID =
+            CI.getDiagnostics().getCustomDiagID(DiagnosticsEngine::Error, "%0");
+        CI.getDiagnostics().Report(DiagID) << Msg;
+        return false;
+      }
+      Targets.push_back(Target);
+    }
+    Targets = dedupTargets(Targets);
+  }
+  else Popcorn::GetAllTargets(this->Targets);
+
+  BackendAction BA = Backend_EmitMultiObj;
+  if(Targets.size() > 1) {
+    for(size_t i = 0; i < Targets.size(); i++) {
+      std::shared_ptr<TargetOptions> Opts = Popcorn::GetPopcornTargetOpts(Targets[i]);
+      TargetOpts.push_back(Opts);
+      TargetInfos.push_back(TargetInfo::CreateTargetInfo(CI.getDiagnostics(), Opts));
+
+      std::unique_ptr<raw_pwrite_stream> OS;
+      OS = GetOutputStream(CI, AppendArchName(OutFile, TargetInfos[i]), BA);
+      if (!OS) return false;
+      OutFiles.push_back(OS.release());
+    }
+  }
+  else {
+    std::shared_ptr<TargetOptions> Opts = Popcorn::GetPopcornTargetOpts(Targets[0]);
+    TargetOpts.push_back(Opts);
+    TargetInfos.push_back(TargetInfo::CreateTargetInfo(CI.getDiagnostics(), Opts));
+
+    std::unique_ptr<raw_pwrite_stream> OS = GetOutputStream(CI, OutFile, BA);
+    if (!OS) return false;
+    OutFiles.push_back(OS.release());
+  }
+  CI.getFrontendOpts().OutputFile = OutFile;
+
+  return true;
+}
+
+/// Initialize target information & open output streams for each target.
+std::unique_ptr<ASTConsumer>
+EmitMultiObjAction::CreateASTConsumer(CompilerInstance &CI,
+                                      StringRef InFile) {
+  if(!InitializeTargets(CI, InFile)) return nullptr;
+  SmallVector<clang::CodeGenAction::LinkModule, 4> LinkModuleToUse
+    = getLinkModuleToUse(CI);
+  CoverageSourceInfo *CoverageInfo = getCoverageInfo(CI);
+
+  // Create a MultiBackendConsumer, which is identical to a BackendConsumer
+  // except that it runs the generated IR through multiple backends.
+  std::unique_ptr<MultiBackendConsumer> Result(new MultiBackendConsumer(
+      CI.getDiagnostics(), CI.getHeaderSearchOpts(),
+      CI.getPreprocessorOpts(), CI.getCodeGenOpts(), CI.getCodeGenNoOpts(),
+      TargetOpts, CI.getLangOpts(), CI.getFrontendOpts().ShowTimers, InFile,
+      std::move(LinkModuleToUse), OutFiles, *VMContext, TargetInfos,
+      CoverageInfo));
+  BEConsumer = Result.get();
+  return std::move(Result);
+}
+
+void EmitMultiObjAction::ExecuteAction() {
+  // If this is an IR file, we have to treat it specially.
+  if (getCurrentFileKind().getLanguage() == InputKind::LLVM_IR) {
+    // Initialize targets here because we never called CreateASTConsumer
+    CompilerInstance &CI = getCompilerInstance();
+    StringRef OutFile = getCurrentFile();
+    BackendAction BA = Backend_EmitMultiObj;
+
+    if(!InitializeTargets(CI, OutFile)) {
+      // TODO add diagnostics saying we couldn't initialize targets
+      return;
+    }
+
+    if(ExecuteActionIRCommon(BA, CI)) {
+      // TODO add diagnostics saying we couldn't do common IR work
+      return;
+    }
+
+    // Apply IR optimizations, but strip target-specific attributes from all
+    // functions added by analyses
+    std::shared_ptr<TargetOptions> IRTargetOpts =
+      Popcorn::GetPopcornTargetOpts(TheModule->getTargetTriple());
+    ApplyIROptimizations(CI.getDiagnostics(), CI.getHeaderSearchOpts(),
+			 CI.getCodeGenOpts(),
+                         *IRTargetOpts, CI.getLangOpts(),
+                         TheModule.get(), BA, nullptr);
+    Popcorn::StripTargetAttributes(*TheModule);
+
+    // Emit machine code for all specified architectures
+    for(size_t i = 0; i < Targets.size(); i++) {
+      //std::unique_ptr<Module> ArchModule = CloneModule(TheModule.get());
+      std::unique_ptr<llvm::Module> ArchModule = CloneModule(*TheModule.get());
+      ArchModule->setTargetTriple(Targets[i]);
+      ArchModule->setDataLayout(TargetInfos[i]->getDataLayout());
+      Popcorn::AddArchSpecificTargetFeatures(*ArchModule, TargetOpts[i]);
+      LLVMContext &Ctx = ArchModule->getContext();
+      Ctx.setInlineAsmDiagnosticHandler(BitcodeInlineAsmDiagHandler);
+      CodegenBackendOutput(CI.getDiagnostics(), CI.getHeaderSearchOpts(),
+			   CI.getCodeGenNoOpts(),
+                           *TargetOpts[i], CI.getLangOpts(),
+                           TargetInfos[i]->getDataLayout()
+			       .getStringRepresentation(),
+                           ArchModule.release(), BA, OutFiles[i]);
+      //delete ArchModule;
+    }
+
+    return;
+  }
+
+  // Otherwise follow the normal AST path.
+  this->ASTFrontendAction::ExecuteAction();
+}
diff --git a/clang/lib/CodeGen/CodeGenFunction.h b/clang/lib/CodeGen/CodeGenFunction.h
index c3060d1fb35..907ab7ecab2 100644
--- a/clang/lib/CodeGen/CodeGenFunction.h
+++ b/clang/lib/CodeGen/CodeGenFunction.h
@@ -564,6 +564,15 @@ public:
     return DominatingValue<T>::save(*this, value);
   }
 
+  /// \brief Locals that were offloaded to global memory for a captured
+  /// statement and need to be restored after the statement has executed.
+  /// First declaration in the pair is the local, second is the global.
+  /// Maintain a mapping of captured statements & their offloaded locals.
+  typedef std::pair<ValueDecl *, ValueDecl *> OffloadPair;
+  typedef llvm::SmallVector<OffloadPair, 4> OffloadList;
+  typedef llvm::DenseMap<const CapturedStmt *, OffloadList> OffloadMap;
+  OffloadMap OffloadedLocals;
+
 public:
   /// ObjCEHValueStack - Stack of Objective-C exception values, used for
   /// rethrows.
@@ -2959,6 +2968,17 @@ public:
   void EmitCXXForRangeStmt(const CXXForRangeStmt &S,
                            ArrayRef<const Attr *> Attrs = None);
 
+  /// Offload/restore capabilities for local variables captured in
+  /// capture statements.
+  void addOffloaded(const CapturedStmt *S, ValueDecl *L, ValueDecl *G);
+  const OffloadList &getOffloaded(const CapturedStmt *S) const;
+  VarDecl *CreateOffloadedGlobal(const Stmt &S, QualType Ty, std::string &Name);
+  VarDecl *CreateOffloadedGlobal(const Stmt &S, const Expr *I);
+  LValue GetVarDeclLValue(QualType SrcType, VarDecl *VD);
+  LValue GetVarDeclLValue(const Expr *I, VarDecl *VD);
+  DeclRefExpr *GetDeclRefForOffload(ValueDecl *VD);
+  void RestoreOffloadedLocals(const CapturedStmt *S);
+
   /// Controls insertion of cancellation exit blocks in worksharing constructs.
   class OMPCancelStackRAII {
     CodeGenFunction &CGF;
@@ -3101,6 +3121,10 @@ public:
   /// \return true if at least one linear variable is found that should be
   /// initialized with the value of the original variable, false otherwise.
   bool EmitOMPLinearClauseInit(const OMPLoopDirective &D);
+  /// Emit prefetching requests for loop directive.
+  ///
+  /// \param D Directive (possibly) with the 'prefetch' clause.
+  void EmitOMPPrefetchClauses(const OMPLoopDirective &D);
 
   typedef const llvm::function_ref<void(CodeGenFunction & /*CGF*/,
                                         llvm::Function * /*OutlinedFn*/,
diff --git a/clang/lib/CodeGen/ObjectFilePCHContainerOperations.cpp b/clang/lib/CodeGen/ObjectFilePCHContainerOperations.cpp
index 15a2ab99fda..0bb6c48f5f6 100644
--- a/clang/lib/CodeGen/ObjectFilePCHContainerOperations.cpp
+++ b/clang/lib/CodeGen/ObjectFilePCHContainerOperations.cpp
@@ -297,7 +297,7 @@ public:
           Diags, HeaderSearchOpts, CodeGenOpts, TargetOpts, LangOpts,
           Ctx.getTargetInfo().getDataLayout(), M.get(),
           BackendAction::Backend_EmitLL,
-          llvm::make_unique<llvm::raw_svector_ostream>(Buffer));
+          llvm::make_unique<llvm::raw_svector_ostream>(Buffer).release());
       llvm::dbgs() << Buffer;
     });
 
@@ -305,7 +305,7 @@ public:
     clang::EmitBackendOutput(Diags, HeaderSearchOpts, CodeGenOpts, TargetOpts,
                              LangOpts, Ctx.getTargetInfo().getDataLayout(),
                              M.get(), BackendAction::Backend_EmitObj,
-                             std::move(OS));
+                             OS.release());
 
     // Free the memory for the temporary buffer.
     llvm::SmallVector<char, 0> Empty;
diff --git a/clang/lib/CodeGen/PopcornUtil.cpp b/clang/lib/CodeGen/PopcornUtil.cpp
new file mode 100644
index 00000000000..f09a444ab42
--- /dev/null
+++ b/clang/lib/CodeGen/PopcornUtil.cpp
@@ -0,0 +1,115 @@
+//===--- PopcornUtil.cpp - LLVM Popcorn Linux Utilities -------------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include <clang/CodeGen/PopcornUtil.h>
+#include <llvm/ADT/Triple.h>
+#include <llvm/ADT/SmallVector.h>
+
+using namespace clang;
+using namespace llvm;
+
+const static std::vector<std::string> PopcornSupported = {
+  "aarch64-linux-gnu",
+  //"riscv64-linux-gnu",
+  "x86_64-linux-gnu"
+};
+
+bool Popcorn::SupportedTarget(const StringRef Target) {
+  for(auto SupportedTarget : PopcornSupported)
+    if(Target == SupportedTarget) return true;
+  return false;
+}
+
+void Popcorn::GetAllTargets(SmallVector<std::string, 2> &Targets) {
+  Targets.clear();
+  for(auto Target : PopcornSupported) Targets.push_back(Target);
+}
+
+typedef std::shared_ptr<clang::TargetOptions> TargetOptionsPtr;
+
+TargetOptionsPtr Popcorn::GetPopcornTargetOpts(const StringRef TripleStr) {
+  Triple Triple(Triple::normalize(TripleStr));
+  assert(!Triple.getTriple().empty() && "Invalid target triple");
+
+  TargetOptionsPtr Opts(new TargetOptions);
+  Opts->Triple = Triple.getTriple();
+  Opts->ABI = "";
+  Opts->FPMath = "";
+  Opts->FeaturesAsWritten.clear();
+  Opts->LinkerVersion = "";
+
+  // TODO need to make CPU selectable & add target features according to CPU
+
+  switch(Triple.getArch()) {
+  case Triple::ArchType::aarch64:
+    Opts->ABI = "aapcs";
+    Opts->CPU = "generic";
+    Opts->FeaturesAsWritten.push_back("+neon");
+    break;
+  case Triple::ArchType::riscv64:
+    Opts->ABI = "lp64d";
+    Opts->CPU = "";
+    Opts->Features.push_back("+m");
+    Opts->Features.push_back("+a");
+    Opts->Features.push_back("+f");
+    Opts->Features.push_back("+d");
+    Opts->FeaturesAsWritten.push_back("+m");
+    Opts->FeaturesAsWritten.push_back("+a");
+    Opts->FeaturesAsWritten.push_back("+f");
+    Opts->FeaturesAsWritten.push_back("+d");
+    break;
+  case Triple::ArchType::x86_64:
+    Opts->CPU = "x86-64";
+    Opts->FPMath = "sse";
+    Opts->FeaturesAsWritten.push_back("+sse");
+    Opts->FeaturesAsWritten.push_back("+sse2");
+    Opts->FeaturesAsWritten.push_back("+rtm");
+    break;
+  default: llvm_unreachable("Triple not currently supported on Popcorn");
+  }
+
+  return Opts;
+}
+
+void Popcorn::StripTargetAttributes(Module &M) {
+  /// Target-specific function attributes
+  static SmallVector<std::string, 2> TargetAttributes = {
+    "target-cpu",
+    "target-features"
+  };
+
+  for(Function &F : M) {
+    AttrBuilder AB(F.getAttributes(), llvm::AttributeList::FunctionIndex);
+    for(std::string &Attr : TargetAttributes) {
+      if(F.hasFnAttribute(Attr))
+        AB.removeAttribute(Attr);
+    }
+    F.setAttributes(
+      AttributeList::get(F.getContext(), AttributeList::FunctionIndex, AB));
+  }
+}
+
+void Popcorn::AddArchSpecificTargetFeatures(Module &M,
+                                            TargetOptionsPtr TargetOpts) {
+  static const char *TF = "target-features";
+  std::string AllFeatures("");
+
+  for(auto &Feature : TargetOpts->FeaturesAsWritten)
+    AllFeatures += Feature + ",";
+  AllFeatures = AllFeatures.substr(0, AllFeatures.length() - 1);
+
+  for(Function &F : M) {
+    AttrBuilder AB(F.getAttributes(), AttributeList::FunctionIndex);
+    assert(!F.hasFnAttribute(TF) && "Target features weren't stripped");
+    AB.addAttribute(TF, AllFeatures);
+    F.setAttributes(
+      AttributeList::get(F.getContext(), AttributeList::FunctionIndex, AB));
+  }
+}
+
diff --git a/clang/lib/CodeGen/PrefetchBuilder.cpp b/clang/lib/CodeGen/PrefetchBuilder.cpp
new file mode 100644
index 00000000000..401c75780d3
--- /dev/null
+++ b/clang/lib/CodeGen/PrefetchBuilder.cpp
@@ -0,0 +1,114 @@
+//=- Prefetch.cpp - Prefetching Analysis for Structured Blocks -----------*-==//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements prefetching analysis for structured blocks.  The
+// analysis traverses the AST to determine how arrays are accessed in structured
+// blocks and generates expressions defining ranges of elements accessed.
+//
+//===----------------------------------------------------------------------===//
+
+#include "clang/CodeGen/PrefetchBuilder.h"
+#include "llvm/IR/CallSite.h"
+#include "llvm/Support/Debug.h"
+
+using namespace clang;
+
+//===----------------------------------------------------------------------===//
+// Prefetch builder API
+//
+
+void PrefetchBuilder::EmitPrefetchCallDeclarations() {
+  using namespace clang::CodeGen;
+  std::vector<llvm::Type *> ParamTypes;
+  llvm::FunctionType *FnType;
+
+  // declare void @popcorn_prefetch(i32, i8*, i8*)
+  ParamTypes = { CGF.Int32Ty, CGF.Int8PtrTy, CGF.Int8PtrTy };
+  FnType = llvm::FunctionType::get(CGF.VoidTy, ParamTypes, false);
+  Prefetch = CGM.CreateRuntimeFunction(FnType, "popcorn_prefetch");
+
+  // declare i64 @popcorn_prefetch_execute()
+  ParamTypes.clear();
+  FnType = llvm::FunctionType::get(CGF.Int64Ty, ParamTypes, false);
+  Execute = CGM.CreateRuntimeFunction(FnType, "popcorn_prefetch_execute");
+}
+
+static llvm::Constant *getPrefetchKind(CodeGen::CodeGenFunction &CGF,
+                                       enum PrefetchRange::Type Perm) {
+  llvm::Type *Ty = llvm::Type::getInt32Ty(CGF.CurFn->getContext());
+  if (Perm == PrefetchRange::Read || Perm == PrefetchRange::Write) {
+    llvm_unreachable("Invalid prefetch type\n");
+    return nullptr;
+  }
+  switch(Perm) {
+  case PrefetchRange::Read: return llvm::ConstantInt::get(Ty, 0);
+  case PrefetchRange::Write: return llvm::ConstantInt::get(Ty, 1);
+  }
+}
+
+Expr *PrefetchBuilder::buildArrayIndex(VarDecl *Base, Expr *Subscript) {
+  // Build DeclRefExpr for variable representing base
+  QualType Ty = Base->getType(), ElemTy;
+  DeclRefExpr *DRE = DeclRefExpr::Create(Ctx, NestedNameSpecifierLoc(),
+                                         SourceLocation(), Base, false,
+                                         Base->getSourceRange().getBegin(),
+                                         Ty, VK_LValue);
+
+  // Get an array subscript, e.g., arr[idx]
+  Ty = Ty.getDesugaredType(Ctx);
+  if(isa<ArrayType>(Ty)) ElemTy = cast<ArrayType>(ElemTy)->getElementType();
+  else ElemTy = cast<PointerType>(Ty)->getPointeeType();
+  return new (Ctx) ArraySubscriptExpr(DRE, Subscript, ElemTy, VK_RValue,
+                                      OK_Ordinary, SourceLocation());
+}
+
+Expr *PrefetchBuilder::buildAddrOf(Expr *ArrSub) {
+  // Get the address of the array index, e.g., &arr[idx]
+  QualType RePtrTy = Ctx.getPointerType(ArrSub->getType());
+  UnaryOperator *Addr = new (Ctx) UnaryOperator(ArrSub, UO_AddrOf, RePtrTy,
+                                                VK_LValue, OK_Ordinary,
+                                                SourceLocation(), true);
+
+  // Finally, cast it to a void *, e.g., (void *)&arr[idx]
+  QualType VoidPtrTy = Ctx.getPointerType(Ctx.VoidTy.withConst());
+  return ImplicitCastExpr::Create(Ctx, VoidPtrTy, CK_BitCast, Addr, nullptr,
+                                  VK_RValue);
+}
+
+void PrefetchBuilder::EmitPrefetchCall(const PrefetchRange &P) {
+  Expr *StartAddr, *EndAddr;
+  CodeGen::RValue LoweredStart, LoweredEnd;
+  std::vector<llvm::Value *> Params;
+  VarDecl *Array = P.getArray();
+
+  // TODO this assumes we're only prefetching arrays!
+
+  StartAddr = P.getStart();
+  if(!isa<ArraySubscriptExpr>(StartAddr))
+    StartAddr = buildArrayIndex(Array, StartAddr);
+  StartAddr = buildAddrOf(StartAddr);
+
+  EndAddr = P.getEnd();
+  if(!isa<ArraySubscriptExpr>(EndAddr))
+    EndAddr = buildArrayIndex(Array, EndAddr);
+  EndAddr = buildAddrOf(EndAddr);
+
+  LoweredStart = CGF.EmitAnyExpr(StartAddr);
+  LoweredEnd = CGF.EmitAnyExpr(EndAddr);
+  Params = { getPrefetchKind(CGF, P.getType()),
+             LoweredStart.getScalarVal(),
+             LoweredEnd.getScalarVal() };
+  CGF.EmitCallOrInvoke(Prefetch, Params);
+}
+
+void PrefetchBuilder::EmitPrefetchExecuteCall() {
+  std::vector<llvm::Value *> Params;
+  CGF.EmitCallOrInvoke(Execute, Params);
+}
+
diff --git a/clang/lib/Driver/Driver.cpp b/clang/lib/Driver/Driver.cpp
index a9a273529b4..6163f3ed373 100644
--- a/clang/lib/Driver/Driver.cpp
+++ b/clang/lib/Driver/Driver.cpp
@@ -1048,6 +1048,8 @@ Compilation *Driver::BuildCompilation(ArrayRef<const char *> ArgList) {
   }
   if (const Arg *A = Args.getLastArg(options::OPT_target))
     TargetTriple = A->getValue();
+  if (const Arg *A = Args.getLastArg(options::OPT_popcorn_target))
+    PopcornTargetTriple = A->getValue();
   if (const Arg *A = Args.getLastArg(options::OPT_ccc_install_dir))
     Dir = InstalledDir = A->getValue();
   for (const Arg *A : Args.filtered(options::OPT_B)) {
@@ -4485,7 +4487,14 @@ void Driver::generatePrefixedToolNames(
     StringRef Tool, const ToolChain &TC,
     SmallVectorImpl<std::string> &Names) const {
   // FIXME: Needs a better variable than TargetTriple
-  Names.emplace_back((TargetTriple + "-" + Tool).str());
+
+  // When the -popcorn-target option is specified, use that target to
+  // search for aux. tools.
+  auto target = TargetTriple;
+  if (PopcornTargetTriple != "")
+    target = PopcornTargetTriple;
+
+  Names.emplace_back((target + "-" + Tool).str());
   Names.emplace_back(Tool);
 
   // Allow the discovery of tools prefixed with LLVM's default target triple.
diff --git a/clang/lib/Driver/ToolChains/Arch/RISCV.cpp b/clang/lib/Driver/ToolChains/Arch/RISCV.cpp
index b6768de4d29..98aae88ac17 100644
--- a/clang/lib/Driver/ToolChains/Arch/RISCV.cpp
+++ b/clang/lib/Driver/ToolChains/Arch/RISCV.cpp
@@ -191,9 +191,16 @@ static void getExtensionFeatures(const Driver &D,
 
 void riscv::getRISCVTargetFeatures(const Driver &D, const ArgList &Args,
                                    std::vector<StringRef> &Features) {
-  if (const Arg *A = Args.getLastArg(options::OPT_march_EQ)) {
-    StringRef MArch = A->getValue();
+  const Arg *A = Args.getLastArg(options::OPT_march_EQ);
+  StringRef MArch;
 
+  // POPCORN: set the default architecture to rc64g
+  if (A)
+    MArch = A->getValue();
+  else
+    MArch = "rv64g";
+
+  if (MArch != "") {
     // RISC-V ISA strings must be lowercase.
     if (llvm::any_of(MArch, [](char c) { return isupper(c); })) {
       D.Diag(diag::err_drv_invalid_riscv_arch_name)
@@ -352,8 +359,8 @@ void riscv::getRISCVTargetFeatures(const Driver &D, const ArgList &Args,
     getExtensionFeatures(D, Args, Features, MArch, OtherExts);
   }
 
-  // -mrelax is default, unless -mno-relax is specified.
-  if (Args.hasFlag(options::OPT_mrelax, options::OPT_mno_relax, true))
+  // -mno-relax is default, unless -mrelax is specified.
+  if (Args.hasFlag(options::OPT_mrelax, options::OPT_mno_relax, false))
     Features.push_back("+relax");
   else
     Features.push_back("-relax");
@@ -375,5 +382,6 @@ StringRef riscv::getRISCVABI(const ArgList &Args, const llvm::Triple &Triple) {
   if (Arg *A = Args.getLastArg(options::OPT_mabi_EQ))
     return A->getValue();
 
-  return Triple.getArch() == llvm::Triple::riscv32 ? "ilp32" : "lp64";
+  // POPCORN: default to lp64d
+  return Triple.getArch() == llvm::Triple::riscv32 ? "ilp32" : "lp64d";
 }
diff --git a/clang/lib/Driver/ToolChains/Clang.cpp b/clang/lib/Driver/ToolChains/Clang.cpp
index dd461a1976d..d814f6c51cd 100644
--- a/clang/lib/Driver/ToolChains/Clang.cpp
+++ b/clang/lib/Driver/ToolChains/Clang.cpp
@@ -1862,7 +1862,8 @@ void Clang::AddRISCVTargetArgs(const ArgList &Args,
   else if (Triple.getArch() == llvm::Triple::riscv32)
     ABIName = "ilp32";
   else if (Triple.getArch() == llvm::Triple::riscv64)
-    ABIName = "lp64";
+    // POPCORN: default to lp64d
+    ABIName = "lp64d";
   else
     llvm_unreachable("Unexpected triple!");
 
@@ -5229,6 +5230,58 @@ void Clang::ConstructJob(Compilation &C, const JobAction &JA,
     }
   }
 
+  // Forward Popcorn & other standard compiler flags to -cc1 to generate
+  // multi-ISA binaries
+  if(Args.hasArg(options::OPT_popcorn_migratable) ||
+     Args.hasArg(options::OPT_popcorn_metadata)) {
+    // Full-blown Popcorn migration capabilities, including adding migration
+    // points, symbol alignment and generating stack transformation metadata
+    CmdArgs.push_back("-ffunction-sections");
+    CmdArgs.push_back("-fdata-sections");
+    CmdArgs.push_back("-popcorn-alignment");
+    CmdArgs.push_back("-popcorn-migratable");
+    CmdArgs.push_back("-mllvm");
+    if(Args.hasArg(options::OPT_popcorn_migratable))
+      CmdArgs.push_back("-popcorn-instrument=migration");
+    else
+      CmdArgs.push_back("-popcorn-instrument=metadata");
+    CmdArgs.push_back("-mllvm");
+    CmdArgs.push_back("-optimize-regalloc");
+    CmdArgs.push_back("-mllvm");
+    CmdArgs.push_back("-fast-isel=false");
+    // FIXME: Workaround for RISCV GlobalAddress register allocation
+    CmdArgs.push_back("-mllvm");
+    CmdArgs.push_back("-disable-machine-cse");
+    for(auto Target : Args.getAllArgValues(options::OPT_popcorn_target)) {
+      std::string combined("-popcorn-target=" + Target);
+      CmdArgs.push_back(Args.MakeArgString(combined));
+    }
+  }
+  else if(Args.hasArg(options::OPT_popcorn_libc)) {
+    // Symbol alignment for libc & generate stack transformation metadata for
+    // libc thread start functions
+    CmdArgs.push_back("-ffunction-sections");
+    CmdArgs.push_back("-fdata-sections");
+    CmdArgs.push_back("-popcorn-alignment");
+    CmdArgs.push_back("-mllvm");
+    CmdArgs.push_back("-popcorn-instrument=libc");
+    // FIXME: Workaround for RISCV GlobalAddress register allocation
+    CmdArgs.push_back("-mllvm");
+    CmdArgs.push_back("-disable-machine-cse");
+  }
+  else if(Args.hasArg(options::OPT_popcorn_alignment)) {
+    // Only symbol alignment
+    CmdArgs.push_back("-ffunction-sections");
+    CmdArgs.push_back("-fdata-sections");
+    CmdArgs.push_back("-popcorn-alignment");
+    // FIXME: Workaround for RISCV GlobalAddress register allocation
+    CmdArgs.push_back("-mllvm");
+    CmdArgs.push_back("-disable-machine-cse");
+  }
+
+  if(Args.hasArg(options::OPT_distributed_omp))
+    CmdArgs.push_back("-distributed-omp");
+
   // With -save-temps, we want to save the unoptimized bitcode output from the
   // CompileJobAction, use -disable-llvm-passes to get pristine IR generated
   // by the frontend.
diff --git a/clang/lib/Frontend/CompilerInvocation.cpp b/clang/lib/Frontend/CompilerInvocation.cpp
index bc54e38a1a6..911d08adf04 100644
--- a/clang/lib/Frontend/CompilerInvocation.cpp
+++ b/clang/lib/Frontend/CompilerInvocation.cpp
@@ -638,21 +638,12 @@ static void setPGOUseInstrumentor(CodeGenOptions &Opts,
 static bool ParseCodeGenArgs(CodeGenOptions &Opts, ArgList &Args, InputKind IK,
                              DiagnosticsEngine &Diags,
                              const TargetOptions &TargetOpts,
-                             const FrontendOptions &FrontendOpts) {
+                             const FrontendOptions &FrontendOpts,
+			     unsigned OptLevel) {
   bool Success = true;
   llvm::Triple Triple = llvm::Triple(TargetOpts.Triple);
 
-  unsigned OptimizationLevel = getOptimizationLevel(Args, IK, Diags);
-  // TODO: This could be done in Driver
-  unsigned MaxOptLevel = 3;
-  if (OptimizationLevel > MaxOptLevel) {
-    // If the optimization level is not supported, fall back on the default
-    // optimization
-    Diags.Report(diag::warn_drv_optimization_value)
-        << Args.getLastArg(OPT_O)->getAsString(Args) << "-O" << MaxOptLevel;
-    OptimizationLevel = MaxOptLevel;
-  }
-  Opts.OptimizationLevel = OptimizationLevel;
+  Opts.OptimizationLevel = OptLevel;
 
   // At O0 we want to fully disable inlining outside of cases marked with
   // 'alwaysinline' that are required for correctness.
@@ -1348,9 +1339,40 @@ static bool ParseCodeGenArgs(CodeGenOptions &Opts, ArgList &Args, InputKind IK,
 
   Opts.SymbolPartition = Args.getLastArgValue(OPT_fsymbol_partition_EQ);
 
+  Opts.PopcornAlignment = Args.hasArg(OPT_popcorn_alignment);
+  Opts.PopcornMigratable = Args.hasArg(OPT_popcorn_migratable);
+
+  // TODO: the Popcorn compiler doesn't currently support vectorization
+  if(Opts.PopcornMigratable || Args.hasArg(OPT_popcorn_libc)) {
+    Opts.VectorizeLoop = 0;
+    Opts.VectorizeSLP = 0;
+  }
+
+  if(Opts.PopcornMigratable)
+    for(auto Target : Args.getAllArgValues(OPT_popcorn_target))
+      Opts.PopcornTargets.push_back(Target);
+
   return Success;
 }
 
+static bool ParseCodeGenArgs(CodeGenOptions &Opts, ArgList &Args, InputKind IK,
+                             DiagnosticsEngine &Diags,
+                             const TargetOptions &TargetOpts,
+                             const FrontendOptions &FrontendOpts) {
+  unsigned OptimizationLevel = getOptimizationLevel(Args, IK, Diags);
+  // TODO: This could be done in Driver
+  unsigned MaxOptLevel = 3;
+  if (OptimizationLevel > MaxOptLevel) {
+    // If the optimization level is not supported, fall back on the default
+    // optimization
+    Diags.Report(diag::warn_drv_optimization_value)
+        << Args.getLastArg(OPT_O)->getAsString(Args) << "-O" << MaxOptLevel;
+    OptimizationLevel = MaxOptLevel;
+  }
+  return ParseCodeGenArgs(Opts, Args, IK, Diags, TargetOpts,
+                          FrontendOpts, OptimizationLevel);
+}
+
 static void ParseDependencyOutputArgs(DependencyOutputOptions &Opts,
                                       ArgList &Args) {
   Opts.OutputFile = Args.getLastArgValue(OPT_dependency_file);
@@ -1661,7 +1683,11 @@ static InputKind ParseFrontendArgs(FrontendOptions &Opts, ArgList &Args,
     case OPT_emit_codegen_only:
       Opts.ProgramAction = frontend::EmitCodeGenOnly; break;
     case OPT_emit_obj:
-      Opts.ProgramAction = frontend::EmitObj; break;
+      if(Args.hasArg(OPT_popcorn_migratable))
+        Opts.ProgramAction = frontend::EmitMultiObj;
+      else
+        Opts.ProgramAction = frontend::EmitObj;
+      break;
     case OPT_fixit_EQ:
       Opts.FixItSuffix = A->getValue();
       LLVM_FALLTHROUGH;
@@ -3116,6 +3142,8 @@ static void ParseLangArgs(LangOptions &Opts, ArgList &Args, InputKind IK,
 
   Opts.CompleteMemberPointers = Args.hasArg(OPT_fcomplete_member_pointers);
   Opts.BuildingPCHWithObjectFile = Args.hasArg(OPT_building_pch_with_obj);
+
+  Opts.DistributedOmp = Args.hasArg(OPT_distributed_omp);
 }
 
 static bool isStrictlyPreprocessorAction(frontend::ActionKind Action) {
@@ -3131,6 +3159,7 @@ static bool isStrictlyPreprocessorAction(frontend::ActionKind Action) {
   case frontend::EmitLLVMOnly:
   case frontend::EmitCodeGenOnly:
   case frontend::EmitObj:
+  case frontend::EmitMultiObj:
   case frontend::FixIt:
   case frontend::GenerateModule:
   case frontend::GenerateModuleInterface:
@@ -3312,6 +3341,13 @@ static void ParseTargetArgs(TargetOptions &Opts, ArgList &Args,
     else
       Opts.SDKVersion = Version;
   }
+
+  // POPCORN: Set default CPU and ABI for convenience
+  llvm::Triple T(Opts.Triple);
+  if (T.getArch() == llvm::Triple::riscv64) {
+    //Opts.CPU = "rv64gc";
+    Opts.ABI = "lp64d";
+  }
 }
 
 bool CompilerInvocation::CreateFromArgs(CompilerInvocation &Res,
@@ -3362,6 +3398,11 @@ bool CompilerInvocation::CreateFromArgs(CompilerInvocation &Res,
   ParseTargetArgs(Res.getTargetOpts(), Args, Diags);
   Success &= ParseCodeGenArgs(Res.getCodeGenOpts(), Args, DashX, Diags,
                               Res.getTargetOpts(), Res.getFrontendOpts());
+  // TODO Popcorn: until we can limit optimizations across migration points, we
+  // need to turn off backend optimizations
+  if(Args.hasArg(OPT_popcorn_migratable) || Args.hasArg(OPT_popcorn_metadata))
+    Success &= ParseCodeGenArgs(Res.getCodeGenNoOpts(), Args, DashX, Diags,
+                                Res.getTargetOpts(), Res.getFrontendOpts(), 0);
   ParseHeaderSearchArgs(Res.getHeaderSearchOpts(), Args,
                         Res.getFileSystemOpts().WorkingDir);
   llvm::Triple T(Res.getTargetOpts().Triple);
diff --git a/clang/lib/FrontendTool/ExecuteCompilerInvocation.cpp b/clang/lib/FrontendTool/ExecuteCompilerInvocation.cpp
index 69e773658c5..944eefd967e 100644
--- a/clang/lib/FrontendTool/ExecuteCompilerInvocation.cpp
+++ b/clang/lib/FrontendTool/ExecuteCompilerInvocation.cpp
@@ -56,6 +56,7 @@ CreateFrontendBaseAction(CompilerInstance &CI) {
   case EmitLLVMOnly:           return llvm::make_unique<EmitLLVMOnlyAction>();
   case EmitCodeGenOnly:        return llvm::make_unique<EmitCodeGenOnlyAction>();
   case EmitObj:                return llvm::make_unique<EmitObjAction>();
+  case EmitMultiObj:           return llvm::make_unique<EmitMultiObjAction>();
   case FixIt:                  return llvm::make_unique<FixItAction>();
   case GenerateModule:
     return llvm::make_unique<GenerateModuleFromModuleMapAction>();
diff --git a/clang/lib/Parse/CMakeLists.txt b/clang/lib/Parse/CMakeLists.txt
index b868696eb6b..241be63f028 100644
--- a/clang/lib/Parse/CMakeLists.txt
+++ b/clang/lib/Parse/CMakeLists.txt
@@ -23,6 +23,7 @@ add_clang_library(clangParse
 
   LINK_LIBS
   clangAST
+  clangAnalysis
   clangBasic
   clangLex
   clangSema
diff --git a/clang/lib/Parse/ParseOpenMP.cpp b/clang/lib/Parse/ParseOpenMP.cpp
index 52a68f6d693..d4ccffd59a2 100644
--- a/clang/lib/Parse/ParseOpenMP.cpp
+++ b/clang/lib/Parse/ParseOpenMP.cpp
@@ -11,6 +11,7 @@
 //===----------------------------------------------------------------------===//
 
 #include "clang/AST/ASTContext.h"
+#include "clang/AST/RecursiveASTVisitor.h"
 #include "clang/AST/StmtOpenMP.h"
 #include "clang/Parse/ParseDiagnostic.h"
 #include "clang/Parse/Parser.h"
@@ -83,6 +84,104 @@ static unsigned getOpenMPDirectiveKindEx(StringRef S) {
       .Default(OMPD_unknown);
 }
 
+/// Helper class to find variables declared or used in a statement's
+/// sub-tree.
+class VarFinder : public RecursiveASTVisitor<VarFinder> {
+public:
+  void reset() { Variables.clear(); }
+
+  bool VisitDeclStmt(DeclStmt *D) {
+    for(auto &Child : D->getDeclGroup()) {
+      if(isa<VarDecl>(Child))
+        Variables.insert(cast<VarDecl>(Child));
+    }
+    return true;
+  }
+
+  bool VisitDeclRefExpr(DeclRefExpr *D) {
+    ValueDecl *VD = D->getDecl();
+    if(isa<VarDecl>(VD)) Variables.insert(cast<VarDecl>(VD));
+    return true;
+  }
+
+  bool VisitVarDecl(VarDecl *D) { Variables.insert(D); return true; }
+
+  const llvm::SmallPtrSet<const VarDecl *, 2> &
+  getDeclaredOrReferencedVars() const { return Variables; }
+
+private:
+  llvm::SmallPtrSet<const VarDecl *, 2> Variables;
+};
+
+void Parser::CheckOpenMPPrefetchClauses(StmtResult Directive) {
+  VarFinder CapturedVarFinder, LoopVarFinder, UseFinder;
+
+  OMPLoopDirective *D = dyn_cast<OMPLoopDirective>(Directive.get());
+  if(D) {
+    // OpenMP Standard 4.5, Section 2.6:
+    // A loop has canonical loop form if it conforms to the following:
+    //    for (init-expr; test-expr; incr-expr) structured-block
+    //      init-expr     One of the following:
+    //                    var = lb
+    //                    integer-type var = lb
+    //                    random-access-iterator-type var = lb
+    //                    pointer-type var = lb
+    //
+    // Thus we can examine the initialization statement to find the loop
+    // iteration variable.
+    CapturedStmt *Captured = cast<CapturedStmt>(D->getAssociatedStmt());
+    ForStmt *For = cast<ForStmt>(Captured->getCapturedStmt());
+
+    // Find captured & loop iteration variables
+    CapturedVarFinder.TraverseStmt(Captured);
+    const llvm::SmallPtrSet<const VarDecl *, 2> &CapturedVars =
+      CapturedVarFinder.getDeclaredOrReferencedVars();
+    LoopVarFinder.TraverseStmt(For->getInit());
+    const llvm::SmallPtrSet<const VarDecl *, 2> &LoopVars =
+      LoopVarFinder.getDeclaredOrReferencedVars();
+
+    //for(auto &&I = D->getClausesOfKind<OMPPrefetchClause>(); I; ++I) {
+    for (auto *I: D->getClausesOfKind<OMPPrefetchClause>()) {
+      auto *C = &cast<OMPPrefetchClause>(*I);
+
+      if(C->getPrefetchKind() == OMPC_PREFETCH_smart &&
+         !For->prefetchEnabled()) {
+        // Analyse for-loop to determine what variables to prefetch
+        ASTContext &Ctx = getActions().getASTContext();
+        For->setPrefetchEnabled(true);
+        PrefetchAnalysis PA(&Ctx, For);
+        PA.analyzeStmt();
+        PA.calculatePrefetchRanges();
+        // TODO remove
+        PA.dump();
+        Ctx.addPrefetchAnalysis(For, PA);
+      }
+      else {
+        // Verify we're only prefetching captured variables
+        for(auto E : C->varlists()) {
+          UseFinder.reset();
+          // TODO this cast is nasty
+          UseFinder.TraverseStmt(const_cast<Expr *>(E));
+          for(const auto &V : UseFinder.getDeclaredOrReferencedVars())
+            if(!CapturedVars.count(V))
+              Diag(E->getExprLoc(), diag::err_omp_invalid_prefetch_capture);
+        }
+  
+        // Verify loop iteration variable use
+        Expr *Start = C->getStartOfRange(), *End = C->getEndOfRange();
+        if(Start && !End) {
+          UseFinder.reset();
+          UseFinder.TraverseStmt(Start);
+          for(const auto &V : UseFinder.getDeclaredOrReferencedVars())
+            if(!LoopVars.count(V))
+              Diag(C->getFirstColonLoc(),
+                   diag::err_omp_invalid_prefetch_loop_var);
+        }
+      }
+    }
+  }
+}
+
 static OpenMPDirectiveKind parseOpenMPDirectiveKind(Parser &P) {
   // Array of foldings: F[i][0] F[i][1] ===> F[i][2].
   // E.g.: OMPD_for OMPD_simd ===> OMPD_for_simd
@@ -1465,6 +1564,21 @@ Parser::ParseOpenMPDeclarativeOrExecutableDirective(ParsedStmtContext StmtCtx) {
         DKind, DirName, CancelRegion, Clauses, AssociatedStmt.get(), Loc,
         EndLoc);
 
+    // Enable optimizations for Popcorn Linux, if requested.
+    if(PP.getLangOpts().DistributedOmp) {
+      if((DKind == OMPD_parallel || DKind == OMPD_parallel_for ||
+          DKind == OMPD_parallel_for_simd || DKind == OMPD_parallel_sections)) {
+        CapturedStmt *CS = cast<CapturedStmt>(AssociatedStmt.get());
+        CS->setOffloadShared(true);
+      }
+
+      if(DKind == OMPD_for || DKind == OMPD_parallel_for ||
+         DKind == OMPD_parallel_for_simd) {
+        cast<OMPExecutableDirective>(Directive.get())->setPrefetching(true);
+        CheckOpenMPPrefetchClauses(Directive);
+      }
+    }
+
     // Exit scope.
     Actions.EndOpenMPDSABlock(Directive.get());
     OMPDirectiveScope.Exit();
@@ -1563,7 +1677,7 @@ bool Parser::ParseOpenMPSimpleVarList(
 ///       thread_limit-clause | priority-clause | grainsize-clause |
 ///       nogroup-clause | num_tasks-clause | hint-clause | to-clause |
 ///       from-clause | is_device_ptr-clause | task_reduction-clause |
-///       in_reduction-clause | allocator-clause | allocate-clause
+///       in_reduction-clause | allocator-clause | allocate-clause | prefetch-clause
 ///
 OMPClause *Parser::ParseOpenMPClause(OpenMPDirectiveKind DKind,
                                      OpenMPClauseKind CKind, bool FirstClause) {
@@ -1710,6 +1824,7 @@ OMPClause *Parser::ParseOpenMPClause(OpenMPDirectiveKind DKind,
   case OMPC_use_device_ptr:
   case OMPC_is_device_ptr:
   case OMPC_allocate:
+  case OMPC_prefetch:
     Clause = ParseOpenMPVarListClause(DKind, CKind, WrongDirective);
     break;
   case OMPC_unknown:
@@ -1942,7 +2057,8 @@ OMPClause *Parser::ParseOpenMPSingleExprWithArgClause(OpenMPClauseKind Kind,
       ConsumeAnyToken();
     if ((Arg[ScheduleKind] == OMPC_SCHEDULE_static ||
          Arg[ScheduleKind] == OMPC_SCHEDULE_dynamic ||
-         Arg[ScheduleKind] == OMPC_SCHEDULE_guided) &&
+         Arg[ScheduleKind] == OMPC_SCHEDULE_guided ||
+	 Arg[ScheduleKind] == OMPC_SCHEDULE_hetprobe) &&
         Tok.is(tok::comma))
       DelimLoc = ConsumeAnyToken();
   } else if (Kind == OMPC_dist_schedule) {
@@ -2235,6 +2351,23 @@ bool Parser::ParseOpenMPVarList(OpenMPDirectiveKind DKind,
                                       : diag::warn_pragma_expected_colon)
           << "dependency type";
     }
+  } else if (Kind == OMPC_prefetch) {
+  // Handle permission type for prefetch clause.
+    ColonProtectionRAIIObject ColonRAII(*this);
+    Data.PrefKind = static_cast<OpenMPPrefetchClauseKind>(getOpenMPSimpleClauseType(
+        Kind, Tok.is(tok::identifier) ? PP.getSpelling(Tok) : ""));
+    Data.PrefLoc = Tok.getLocation();
+
+    if (Data.PrefKind == OMPC_PREFETCH_unknown) {
+      SkipUntil(tok::r_paren, tok::annot_pragma_openmp_end, StopBeforeMatch);
+      Diag(Data.PrefLoc, diag::err_omp_invalid_prefetch_kind) << true;
+    } else {
+      ConsumeToken(); // Keyword
+      if(Data.PrefKind == OMPC_PREFETCH_smart) {
+        T.consumeClose();
+      }
+      else ConsumeToken(); // Colon
+    }
   } else if (Kind == OMPC_linear) {
     // Try to parse modifier if any.
     if (Tok.is(tok::identifier) && PP.LookAhead(0).is(tok::l_paren)) {
@@ -2340,7 +2473,8 @@ bool Parser::ParseOpenMPVarList(OpenMPDirectiveKind DKind,
       (Kind == OMPC_reduction && !InvalidReductionId) ||
       (Kind == OMPC_map && Data.MapType != OMPC_MAP_unknown) ||
       (Kind == OMPC_depend && Data.DepKind != OMPC_DEPEND_unknown);
-  const bool MayHaveTail = (Kind == OMPC_linear || Kind == OMPC_aligned);
+  const bool MayHaveTail = (Kind == OMPC_linear || Kind == OMPC_aligned ||
+			    Kind == OMPC_prefetch);
   while (IsComma || (Tok.isNot(tok::r_paren) && Tok.isNot(tok::colon) &&
                      Tok.isNot(tok::annot_pragma_openmp_end))) {
     ColonProtectionRAIIObject ColonRAII(*this, MayHaveTail);
@@ -2385,6 +2519,20 @@ bool Parser::ParseOpenMPVarList(OpenMPDirectiveKind DKind,
                 StopBeforeMatch);
   }
 
+  // Parse ':' end.
+  Data.EndExpr = nullptr;
+  if (Kind == OMPC_prefetch && Tok.is(tok::colon)) {
+    Data.EndColonLoc = Tok.getLocation();
+    ConsumeToken();
+    ExprResult Tail =
+        Actions.CorrectDelayedTyposInExpr(ParseAssignmentExpression());
+    if (Tail.isUsable())
+      Data.EndExpr = Tail.get();
+    else
+      SkipUntil(tok::comma, tok::r_paren, tok::annot_pragma_openmp_end,
+                StopBeforeMatch);
+  }
+
   // Parse ')'.
   Data.RLoc = Tok.getLocation();
   if (!T.consumeClose())
@@ -2438,6 +2586,10 @@ bool Parser::ParseOpenMPVarList(OpenMPDirectiveKind DKind,
 ///       'is_device_ptr' '(' list ')'
 ///    allocate-clause:
 ///       'allocate' '(' [ allocator ':' ] list ')'
+///    prefetch-clause:
+///       'prefetch' '(' read | write ':' list [ ':' start ':' end ] ')'
+///                          or
+///                  '(' smart ')'
 ///
 /// For 'linear' clause linear-list may have the following forms:
 ///  list
@@ -2457,10 +2609,11 @@ OMPClause *Parser::ParseOpenMPVarListClause(OpenMPDirectiveKind DKind,
   if (ParseOnly)
     return nullptr;
   OMPVarListLocTy Locs(Loc, LOpen, Data.RLoc);
+
   return Actions.ActOnOpenMPVarListClause(
-      Kind, Vars, Data.TailExpr, Locs, Data.ColonLoc,
+      Kind, Vars, Data.TailExpr, Data.EndExpr, Locs, Data.ColonLoc, Data.EndColonLoc,
       Data.ReductionOrMapperIdScopeSpec, Data.ReductionOrMapperId, Data.DepKind,
       Data.LinKind, Data.MapTypeModifiers, Data.MapTypeModifiersLoc,
-      Data.MapType, Data.IsMapTypeImplicit, Data.DepLinMapLoc);
+      Data.MapType, Data.IsMapTypeImplicit, Data.DepLinMapLoc, Data.PrefKind, Data.PrefLoc);
 }
 
diff --git a/clang/lib/Parse/ParsePragma.cpp b/clang/lib/Parse/ParsePragma.cpp
index f81ecc738c2..59a59f07167 100644
--- a/clang/lib/Parse/ParsePragma.cpp
+++ b/clang/lib/Parse/ParsePragma.cpp
@@ -262,6 +262,23 @@ struct PragmaAttributeHandler : public PragmaHandler {
   ParsedAttributes AttributesForPragmaAttribute;
 };
 
+struct PragmaNoPopcornHandler: public PragmaHandler {
+  PragmaNoPopcornHandler() : PragmaHandler("popcorn") {}
+  void HandlePragma(Preprocessor &PP,  PragmaIntroducer Introducer,
+                    Token &FirstToken) override;
+};
+
+struct PragmaPopcornHandler : public PragmaHandler {
+  enum Type {
+    Prefetch, // Prefetch for the statement following the pragma
+    None      // Unknown pragma type
+  };
+
+  PragmaPopcornHandler() : PragmaHandler("popcorn") {}
+  void HandlePragma(Preprocessor &PP, PragmaIntroducer Introducer,
+                    Token &FirstToken) override;
+};
+
 }  // end namespace
 
 void Parser::initializePragmaHandlers() {
@@ -382,6 +399,12 @@ void Parser::initializePragmaHandlers() {
   AttributePragmaHandler =
       llvm::make_unique<PragmaAttributeHandler>(AttrFactory);
   PP.AddPragmaHandler("clang", AttributePragmaHandler.get());
+
+  if (getLangOpts().DistributedOmp)
+    PopcornHandler.reset(new PragmaPopcornHandler());
+  else
+    PopcornHandler.reset(new PragmaNoPopcornHandler());
+  PP.AddPragmaHandler(PopcornHandler.get());
 }
 
 void Parser::resetPragmaHandlers() {
@@ -487,6 +510,9 @@ void Parser::resetPragmaHandlers() {
 
   PP.RemovePragmaHandler("clang", AttributePragmaHandler.get());
   AttributePragmaHandler.reset();
+
+  PP.RemovePragmaHandler(PopcornHandler.get());
+  PopcornHandler.reset();
 }
 
 /// Handle the annotation token produced for #pragma unused(...)
@@ -1576,6 +1602,109 @@ void Parser::HandlePragmaAttribute() {
                                         std::move(SubjectMatchRules));
 }
 
+enum PopcornClauseKind {
+  PC_PrefetchIgnore, // Ignore array/pointer variable during prefetch analysis
+  PC_Unknown         // Unknown clause type
+};
+
+static const char *getPopcornClauseName(enum PopcornClauseKind K) {
+  switch(K) {
+  default: return "unknown";
+  case PC_PrefetchIgnore: return "ignore";
+  }
+}
+
+static enum PopcornClauseKind getPopcornClauseKind(llvm::StringRef Name) {
+  return llvm::StringSwitch<PopcornClauseKind>(Name)
+    .Case("ignore", PC_PrefetchIgnore)
+    .Default(PC_Unknown);
+}
+
+void Parser::ParseVarList(llvm::SmallPtrSet<VarDecl *, 4> &Vars) {
+  bool isComma = false;
+  DeclRefExpr *D;
+  VarDecl *VD;
+
+  while(isComma || (Tok.isNot(tok::r_paren) &&
+                    Tok.isNot(tok::annot_pragma_popcorn_prefetch_end))) {
+    // Parse variable
+    ExprResult VarExpr =
+      Actions.CorrectDelayedTyposInExpr(ParseAssignmentExpression());
+
+    if(VarExpr.isUsable()) {
+      Expr *E = VarExpr.get();
+      if((D = dyn_cast<DeclRefExpr>(E)) &&
+         (VD = dyn_cast<VarDecl>(D->getDecl())))
+        Vars.insert(VD);
+      else {
+        PP.Diag(Tok.getLocation(), diag::err_pragma_popcorn_expected_var_name);
+        SkipUntil(tok::comma, tok::r_paren,
+                  tok::annot_pragma_popcorn_prefetch_end);
+      }
+    }
+
+    // Parse ',' if any
+    isComma = Tok.is(tok::comma);
+    if(isComma) ConsumeToken();
+  }
+}
+
+StmtResult Parser::HandlePragmaPopcorn() {
+  llvm::SmallPtrSet<VarDecl *, 4> Ignore;
+
+  assert(Tok.is(tok::annot_pragma_popcorn_prefetch));
+  ConsumeToken(); // The annotation token.
+
+  while(Tok.isNot(tok::annot_pragma_popcorn_prefetch_end)) {
+    // Parse clause type.
+    enum PopcornClauseKind Kind = getPopcornClauseKind(PP.getSpelling(Tok));
+    if(Kind == PC_Unknown) {
+      PP.Diag(Tok.getLocation(),
+              diag::err_pragma_popcorn_invalid_clause) << PP.getSpelling(Tok);
+      SkipUntil(tok::r_paren, tok::annot_pragma_popcorn_prefetch_end,
+                Parser::StopBeforeMatch);
+      continue;
+    }
+    ConsumeToken();
+
+    // Parse '('.
+    BalancedDelimiterTracker T(*this, tok::l_paren,
+                               tok::annot_pragma_popcorn_prefetch_end);
+    if(T.expectAndConsume(diag::err_expected_lparen_after,
+                          getPopcornClauseName(Kind)))
+      return StmtError();
+
+    switch(Kind) {
+    default: llvm_unreachable("Unknown '#pragma popcorn' clause type"); break;
+    case PC_PrefetchIgnore: ParseVarList(Ignore); break;
+    }
+
+    // Parse ')'.
+    T.consumeClose();
+  }
+  ConsumeToken(); // Consume final token.
+
+  StmtResult R = ParseStatement();
+
+  if(R.isInvalid()) return StmtError();
+  else if(isa<ForStmt>(R.get())) {
+    ASTContext &Ctx = getActions().getASTContext();
+    ForStmt *S = cast<ForStmt>(R.get());
+    S->setPrefetchEnabled(true);
+    PrefetchAnalysis PA(&Ctx, R.get());
+    PA.ignoreVars(Ignore);
+    PA.analyzeStmt();
+    PA.calculatePrefetchRanges();
+    // TODO remove
+    PA.dump();
+    Ctx.addPrefetchAnalysis(R.get(), PA);
+  }
+  else PP.Diag(Tok.getLocation(),
+               diag::warn_pragma_popcorn_prefetch_invalid_stmt);
+
+  return R;
+}
+
 // #pragma GCC visibility comes in two variants:
 //   'push' '(' [visibility] ')'
 //   'pop'
@@ -2199,10 +2328,10 @@ void PragmaOpenCLExtensionHandler::HandlePragma(Preprocessor &PP,
 void PragmaNoOpenMPHandler::HandlePragma(Preprocessor &PP,
                                          PragmaIntroducer Introducer,
                                          Token &FirstTok) {
-  if (!PP.getDiagnostics().isIgnored(diag::warn_pragma_omp_ignored,
+  if (!PP.getDiagnostics().isIgnored(diag::warn_pragma_popcorn_ignored,
                                      FirstTok.getLocation())) {
-    PP.Diag(FirstTok, diag::warn_pragma_omp_ignored);
-    PP.getDiagnostics().setSeverity(diag::warn_pragma_omp_ignored,
+    PP.Diag(FirstTok, diag::warn_pragma_popcorn_ignored);
+    PP.getDiagnostics().setSeverity(diag::warn_pragma_popcorn_ignored,
                                     diag::Severity::Ignored, SourceLocation());
   }
   PP.DiscardUntilEndOfDirective();
@@ -3279,3 +3408,66 @@ void PragmaAttributeHandler::HandlePragma(Preprocessor &PP,
   PP.EnterTokenStream(std::move(TokenArray), 1,
                       /*DisableMacroExpansion=*/false, /*IsReinject=*/false);
 }
+
+void PragmaNoPopcornHandler::HandlePragma(Preprocessor &PP,
+                                          PragmaIntroducer Introducer,
+                                          Token &Tok) {
+  if (!PP.getDiagnostics().isIgnored(diag::warn_pragma_omp_ignored,
+                                     Tok.getLocation())) {
+    PP.Diag(Tok, diag::warn_pragma_omp_ignored);
+    PP.getDiagnostics().setSeverity(diag::warn_pragma_omp_ignored,
+                                    diag::Severity::Ignored, SourceLocation());
+  }
+  PP.DiscardUntilEndOfDirective();
+}
+
+void PragmaPopcornHandler::HandlePragma(Preprocessor &PP,
+                                        PragmaIntroducer Introducer,
+                                        Token &Tok) {
+  // Incoming token is "popcorn" for "#pragma popcorn".
+  PP.Lex(Tok);
+  if (Tok.isNot(tok::identifier)) {
+    PP.Diag(Tok.getLocation(), diag::warn_pragma_popcorn_no_arg);
+    return;
+  }
+
+  IdentifierInfo *OptionInfo = Tok.getIdentifierInfo();
+  enum PragmaPopcornHandler::Type Ty =
+    llvm::StringSwitch<enum PragmaPopcornHandler::Type>(OptionInfo->getName())
+                             .Case("prefetch", PragmaPopcornHandler::Prefetch)
+                             .Default(PragmaPopcornHandler::None);
+  if (Ty == PragmaPopcornHandler::None) {
+    PP.Diag(Tok.getLocation(), diag::warn_pragma_popcorn_invalid_option)
+        << OptionInfo->getName();
+    return;
+  }
+
+  switch(Ty) {
+  default: llvm_unreachable("Should have weeded out invalid types"); break;
+  case PragmaPopcornHandler::Prefetch: {
+    // Capture all tokens to be included for prefetching analysis.
+    SmallVector<Token, 16> Pragma;
+    Token NextTok;
+
+    NextTok.startToken();
+    NextTok.setKind(tok::annot_pragma_popcorn_prefetch);
+    NextTok.setLocation(Tok.getLocation());
+    while (NextTok.isNot(tok::eod)) {
+      Pragma.push_back(NextTok);
+      PP.Lex(NextTok);
+    }
+    SourceLocation EodLoc = NextTok.getLocation();
+    NextTok.startToken();
+    NextTok.setKind(tok::annot_pragma_popcorn_prefetch_end);
+    NextTok.setLocation(EodLoc);
+    Pragma.push_back(NextTok);
+
+    auto Toks = llvm::make_unique<Token[]>(Pragma.size());
+    std::copy(Pragma.begin(), Pragma.end(), Toks.get());
+    PP.EnterTokenStream(std::move(Toks), Pragma.size(),
+                        /*DisableMacroExpansion=*/false, /*OwnsTokens=*/true);
+    break;
+  }
+  }
+}
+
diff --git a/clang/lib/Parse/ParseStmt.cpp b/clang/lib/Parse/ParseStmt.cpp
index bf04253ab7f..61c891b3cf6 100644
--- a/clang/lib/Parse/ParseStmt.cpp
+++ b/clang/lib/Parse/ParseStmt.cpp
@@ -391,6 +391,9 @@ Retry:
   case tok::annot_pragma_attribute:
     HandlePragmaAttribute();
     return StmtEmpty();
+
+  case tok::annot_pragma_popcorn_prefetch:
+    return HandlePragmaPopcorn();
   }
 
   // If we reached this code, the statement must end in a semicolon.
diff --git a/clang/lib/Sema/CMakeLists.txt b/clang/lib/Sema/CMakeLists.txt
index 742343583d1..ab27697cf2f 100644
--- a/clang/lib/Sema/CMakeLists.txt
+++ b/clang/lib/Sema/CMakeLists.txt
@@ -23,6 +23,9 @@ add_clang_library(clangSema
   JumpDiagnostics.cpp
   MultiplexExternalSemaSource.cpp
   ParsedAttr.cpp
+  PrefetchAnalysis.cpp
+  PrefetchDataflow.cpp
+  PrefetchExprBuilder.cpp
   Scope.cpp
   ScopeInfo.cpp
   Sema.cpp
diff --git a/clang/lib/Sema/PrefetchAnalysis.cpp b/clang/lib/Sema/PrefetchAnalysis.cpp
new file mode 100644
index 00000000000..738ceb7b9bb
--- /dev/null
+++ b/clang/lib/Sema/PrefetchAnalysis.cpp
@@ -0,0 +1,937 @@
+//=- PrefetchAnalysis.cpp - Prefetching Analysis for Structured Blocks ---*-==//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements prefetching analysis for structured blocks.  The
+// analysis traverses the AST to determine how arrays are accessed in structured
+// blocks and generates expressions defining ranges of elements accessed.
+//
+//===----------------------------------------------------------------------===//
+
+#include "clang/AST/ASTContext.h"
+#include "clang/AST/RecursiveASTVisitor.h"
+#include "clang/Sema/PrefetchAnalysis.h"
+#include "clang/Sema/PrefetchDataflow.h"
+#include "clang/Sema/PrefetchExprBuilder.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/Support/Debug.h"
+
+using namespace clang;
+
+//===----------------------------------------------------------------------===//
+// PrefetchRange API
+//
+
+bool PrefetchRange::equalExceptType(const PrefetchRange &RHS) {
+  if(Array != RHS.Array) return false;
+  else if(!PrefetchExprEquality::exprEqual(Start, RHS.Start)) return false;
+  else if(!PrefetchExprEquality::exprEqual(End, RHS.End)) return false;
+  else return true;
+}
+
+bool PrefetchRange::operator==(const PrefetchRange &RHS) {
+  if(Ty == RHS.Ty && equalExceptType(RHS)) return true;
+  else return false;
+}
+
+//===----------------------------------------------------------------------===//
+// Common utilities
+//
+
+/// Return whether a type is both scalar and integer.
+bool PrefetchAnalysis::isScalarIntType(const QualType &Ty) {
+  return Ty->isIntegerType() && Ty->isScalarType();
+}
+
+/// Get the size in bits for builtin integer types.
+unsigned PrefetchAnalysis::getTypeSize(BuiltinType::Kind K) {
+  switch(K) {
+  case BuiltinType::Bool:
+  case BuiltinType::Char_U: case BuiltinType::UChar:
+  case BuiltinType::Char_S: case BuiltinType::SChar:
+    return 8;
+  case BuiltinType::WChar_U: case BuiltinType::Char16:
+  case BuiltinType::UShort:
+  case BuiltinType::WChar_S:
+  case BuiltinType::Short:
+    return 16;
+  case BuiltinType::Char32:
+  case BuiltinType::UInt:
+  case BuiltinType::Int:
+    return 32;
+  case BuiltinType::ULong:
+  case BuiltinType::ULongLong:
+  case BuiltinType::Long:
+  case BuiltinType::LongLong:
+    return 64;
+  case BuiltinType::UInt128:
+  case BuiltinType::Int128:
+    return 128;
+  default: return UINT32_MAX;
+  }
+}
+
+/// Return the variable declaration if the declared value is a variable and if
+/// it is a scalar integer type or nullptr otherwise.
+VarDecl *PrefetchAnalysis::getVarIfScalarInt(ValueDecl *VD) {
+  if(isa<VarDecl>(VD)) {
+    VarDecl *Var = cast<VarDecl>(VD);
+    if(isScalarIntType(Var->getType())) return Var;
+  }
+  return nullptr;
+}
+
+// Filter functions to only select appropriate operator types.  Return true if
+// the operator is of a type that should be analyzed, or false otherwise.
+typedef bool (*UnaryOpFilter)(UnaryOperator::Opcode);
+typedef bool (*BinaryOpFilter)(BinaryOperator::Opcode);
+
+// Don't analyze *any* operation types.
+bool NoUnaryOp(UnaryOperator::Opcode Op) { return false; }
+bool NoBinaryOp(BinaryOperator::Opcode Op) { return false; }
+
+// Filter out non-assignment binary operations.
+static bool FilterAssignOp(BinaryOperator::Opcode Op) {
+  switch(Op) {
+  case BO_Assign: case BO_MulAssign: case BO_DivAssign: case BO_RemAssign:
+  case BO_AddAssign: case BO_SubAssign: case BO_ShlAssign: case BO_ShrAssign:
+  case BO_AndAssign: case BO_XorAssign: case BO_OrAssign:
+    return true;
+  default: return false;
+  }
+}
+
+// Filter out non-relational binary operations.
+static bool FilterRelationalOp(BinaryOperator::Opcode Op) {
+  switch(Op) {
+  case BO_LT: case BO_GT: case BO_LE: case BO_GE: case BO_EQ: case BO_NE:
+    return true;
+  default: return false;
+  }
+}
+
+// Filter out non-math/logic binary operations.
+static bool FilterMathLogicOp(BinaryOperator::Opcode Op) {
+  switch(Op) {
+  case BO_Mul: case BO_Div: case BO_Rem: case BO_Add: case BO_Sub:
+  case BO_Shl: case BO_Shr: case BO_And: case BO_Xor: case BO_Or:
+    return true;
+  default: return false;
+  }
+}
+
+// Filter out non-math unary operations.
+static bool FilterMathOp(UnaryOperator::Opcode Op) {
+  switch(Op) {
+  case UO_PostInc: case UO_PostDec: case UO_PreInc: case UO_PreDec:
+    return true;
+  default: return false;
+  }
+}
+
+/// Return the statement if it is a scoping statement (e.g., for-loop) or
+/// nullptr otherwise.
+static bool isScopingStmt(Stmt *S) {
+  if(isa<CapturedStmt>(S) || isa<CompoundStmt>(S) || isa<CXXCatchStmt>(S) ||
+     isa<CXXForRangeStmt>(S) || isa<CXXTryStmt>(S) || isa<DoStmt>(S) ||
+     isa<ForStmt>(S) || isa<IfStmt>(S) || isa<OMPExecutableDirective>(S) ||
+     isa<SwitchStmt>(S) || isa<WhileStmt>(S)) return true;
+  else return false;
+}
+
+/// A vector of variable declarations.
+typedef llvm::SmallVector<VarDecl *, 4> VarVec;
+
+//===----------------------------------------------------------------------===//
+// Prefetch analysis -- array accesses
+//
+
+/// Scoping information for array analyses.  A node in a singly-linked list
+/// which allows traversal from innermost scope outwards.  Nodes are reference
+/// counted, so when array accesses which reference the scope (if any) are
+/// deleted, the scoping chain itself gets deleted.
+// TODO verify the chain gets freed correctly
+struct ScopeInfo {
+  Stmt *ScopeStmt;                        // Statement providing scope
+  std::shared_ptr<ScopeInfo> ParentScope; // The parent in the scope chain
+  ScopeInfo(Stmt *ScopeStmt, std::shared_ptr<ScopeInfo> &ParentScope)
+    : ScopeStmt(ScopeStmt), ParentScope(ParentScope) {}
+};
+typedef std::shared_ptr<ScopeInfo> ScopeInfoPtr;
+
+/// An array access.
+class ArrayAccess {
+public:
+  ArrayAccess(PrefetchRange::Type Ty, ArraySubscriptExpr *S,
+              const ScopeInfoPtr &AccessScope)
+    : Valid(true), Ty(Ty), S(S), Base(nullptr), Idx(S),
+      AccessScope(AccessScope) {
+
+    DeclRefExpr *DR;
+    VarDecl *VD;
+
+    // Drill down into subscripts for multi-dimensional arrays, e.g., a[i][j]
+    while(isa<ArraySubscriptExpr>(S->getBase()->IgnoreImpCasts()))
+      S = cast<ArraySubscriptExpr>(S->getBase()->IgnoreImpCasts());
+
+    if(!(DR = dyn_cast<DeclRefExpr>(S->getBase()->IgnoreImpCasts()))) {
+      Valid = false;
+      return;
+    }
+
+    if(!(VD = dyn_cast<VarDecl>(DR->getDecl()))) {
+      Valid = false;
+      return;
+    }
+
+    Base = VD;
+  }
+
+  bool isValid() const { return Valid; }
+  Stmt *getStmt() const { return S; }
+  PrefetchRange::Type getAccessType() const { return Ty; }
+  VarDecl *getBase() const { return Base; }
+  Expr *getIndex() const { return Idx; }
+  const VarVec &getVarsInIdx() const { return VarsInIdx; }
+  const ScopeInfoPtr &getScope() const { return AccessScope; }
+
+  void setInvalid() { Valid = false; }
+  void addVarInIdx(VarDecl *V) { if(V != Base) VarsInIdx.push_back(V); }
+
+  void print(llvm::raw_ostream &O, PrintingPolicy &Policy) const {
+    O << "Array: " << Base->getName() << "\nIndex expression: ";
+    Idx->printPretty(O, nullptr, Policy);
+    O << "\nScoping statement:\n";
+    AccessScope->ScopeStmt->printPretty(O, nullptr, Policy);
+    O << "\nVariables used in index calculation:";
+    for(auto Var : VarsInIdx) O << " " << Var->getName();
+    O << "\n";
+  }
+  void dump(PrintingPolicy &Policy) const { print(llvm::dbgs(), Policy); }
+
+private:
+  bool Valid;               // Is the access valid?
+  PrefetchRange::Type Ty;   // The type of access
+  Stmt *S;                  // The entire array access statement
+  VarDecl *Base;            // The array base
+  Expr *Idx;                // Expression used to calculate index
+  VarVec VarsInIdx;         // Variables used in index calculation
+  ScopeInfoPtr AccessScope; // Scope of the array access
+};
+
+/// Traverse a statement looking for array accesses.
+// TODO *** NEED TO LIMIT TO AFFINE ACCESSES ***
+class ArrayAccessPattern : public RecursiveASTVisitor<ArrayAccessPattern> {
+public:
+  ArrayAccessPattern(llvm::SmallPtrSet<VarDecl *, 4> &Ignore)
+    : Ignore(Ignore) {}
+
+  void InitTraversal() {
+    AssignSide.push_back(TS_RHS);
+    SubscriptSide.push_back(AS_Index);
+    CurAccess.push_back(nullptr);
+  }
+
+  /// Traverse a binary operator & maintain traversal structure to determine if
+  /// we're reading or writing in the array access.  Left-hand side == writing
+  /// and right-hande side == reading.
+  bool TraverseBinaryOperator(BinaryOperator *B) {
+    if(FilterAssignOp(B->getOpcode())) {
+      AssignSide.push_back(TS_LHS);
+      TraverseStmt(B->getLHS());
+      AssignSide.pop_back();
+      AssignSide.push_back(TS_RHS);
+      TraverseStmt(B->getRHS());
+      AssignSide.pop_back();
+    }
+    else RecursiveASTVisitor::TraverseStmt(B);
+    return true;
+  }
+
+  /// Traverse an array subscript & maintain traversal structure to determine if
+  /// we're exploring the base or index of the access.  Don't record subscript
+  /// expressions if we're currently exploring the base of another subscript, as
+  /// it's part of a multi-dimensional access, e.g., a[i][j].
+  bool TraverseArraySubscriptExpr(ArraySubscriptExpr *AS) {
+    // Record array access if we're not exploring a higher-level access' base
+    if(SubscriptSide.back() != AS_Base) VisitArraySubscriptExpr(AS);
+
+    SubscriptSide.push_back(AS_Base);
+    TraverseStmt(AS->getBase());
+    SubscriptSide.pop_back();
+    SubscriptSide.push_back(AS_Index);
+    TraverseStmt(AS->getIdx());
+    SubscriptSide.pop_back();
+
+    // Don't record any more variables for this access
+    if(SubscriptSide.back() != AS_Base) CurAccess.pop_back();
+
+    return true;
+  }
+
+  /// Traverse a statement.  Record scoping information where applicable.
+  bool TraverseStmt(Stmt *S) {
+    bool isScope;
+    BinaryOperator *B;
+
+    if(!S) return true;
+
+    isScope = isScopingStmt(S);
+    if(isScope) CurScope = ScopeInfoPtr(new ScopeInfo(S, CurScope));
+
+    // For some reason RecursiveASTVisitor doesn't redirect binary operations
+    // to TraverseBinaryOperator but instead to individual operation functions
+    // (e.g., TraverseBinAssign).  Instead, redirect those here.
+    if((B = dyn_cast<BinaryOperator>(S))) TraverseBinaryOperator(B);
+    else RecursiveASTVisitor<ArrayAccessPattern>::TraverseStmt(S);
+
+    if(isScope) CurScope = CurScope->ParentScope;
+
+    return true;
+  }
+
+  /// Analyze an array access.
+  bool VisitArraySubscriptExpr(ArraySubscriptExpr *Sub) {
+    PrefetchRange::Type Ty =
+      AssignSide.back() == TS_LHS ? PrefetchRange::Write :
+                                    PrefetchRange::Read;
+    ArrayAccesses.emplace_back(Ty, Sub, CurScope);
+    CurAccess.push_back(&ArrayAccesses.back());
+
+    return true;
+  }
+
+  /// Record variables seen during traversal used to construct indices.
+  bool VisitDeclRefExpr(DeclRefExpr *DR) {
+    ArrayAccess *Back = CurAccess.back();
+    if(Back && Back->isValid()) {
+      VarDecl *VD = dyn_cast<VarDecl>(DR->getDecl());
+      if(VD) Back->addVarInIdx(VD);
+      else Back->setInvalid(); // Can't analyze if decl != variable
+    }
+    return true;
+  }
+
+  /// Rather than removing invalid accesses during traversal (which complicates
+  /// traversal state handling), prune them in one go at the end.
+  void PruneInvalidOrIgnoredAccesses() {
+    llvm::SmallVector<ArrayAccess, 8> Pruned;
+
+    for(auto &Access : ArrayAccesses) {
+      if(Access.isValid() && !Ignore.count(Access.getBase()))
+        Pruned.push_back(Access);
+    }
+    ArrayAccesses = Pruned;
+  }
+
+  const llvm::SmallVector<ArrayAccess, 8> &getArrayAccesses() const
+  { return ArrayAccesses; }
+
+  llvm::SmallVector<ArrayAccess, 8> &getArrayAccesses()
+  { return ArrayAccesses; }
+
+private:
+  /// Which sub-tree of a binary operator we're traversing.  This determines
+  /// whether we're reading or writing the array.
+  enum TraverseStructure { TS_LHS, TS_RHS };
+
+  /// Which part of an array subscript expression we're traversing.
+  enum ArraySubscriptSide { AS_Base, AS_Index };
+
+  llvm::SmallVector<ArrayAccess, 8> ArrayAccesses;
+  ScopeInfoPtr CurScope;
+  llvm::SmallPtrSet<VarDecl *, 4> &Ignore;
+
+  // Traversal state
+  llvm::SmallVector<enum TraverseStructure, 8> AssignSide;
+  llvm::SmallVector<enum ArraySubscriptSide, 8> SubscriptSide;
+  llvm::SmallVector<ArrayAccess *, 8> CurAccess;
+};
+
+void PrefetchAnalysis::pruneArrayAccesses() {
+  llvm::SmallVector<ArrayAccess, 8> &Accesses = ArrAccesses->getArrayAccesses();
+  llvm::SmallVector<ArrayAccess, 8>::iterator Cur, Next;
+  for(Cur = Accesses.begin(); Cur != Accesses.end(); Cur++) {
+    if(!Cur->isValid()) continue;
+    for(Next = Cur + 1; Next != Accesses.end(); Next++) {
+      if(!Next->isValid()) continue;
+      if(Cur->getBase() == Next->getBase() &&
+         PrefetchExprEquality::exprEqual(Cur->getIndex(), Next->getIndex()))
+        Next->setInvalid();
+    }
+  }
+}
+
+void PrefetchAnalysis::mergePrefetchRanges() {
+  // TODO!
+}
+
+void PrefetchAnalysis::prunePrefetchRanges()
+{
+  // TODO we could prevent a bunch of copying if we used a linked list instead
+  // of a vector for ToPrefetch
+  llvm::SmallVector<PrefetchRange, 8>::iterator Cur, Next;
+  for(Cur = ToPrefetch.begin(); Cur != ToPrefetch.end(); Cur++)
+    if(PrefetchExprEquality::exprEqual(Cur->getStart(), Cur->getEnd()))
+      Cur = ToPrefetch.erase(Cur) - 1;
+
+  for(Cur = ToPrefetch.begin(); Cur != ToPrefetch.end(); Cur++) {
+    for(Next = Cur + 1; Next != ToPrefetch.end(); Next++) {
+      if(*Cur == *Next) Next = ToPrefetch.erase(Next) - 1;
+      else if(Cur->equalExceptType(*Next)) {
+        Cur->setType(Cur->getType() > Next->getType() ? Cur->getType() :
+                                                        Next->getType());
+        Next = ToPrefetch.erase(Next) - 1;
+      }
+    }
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// Prefetch analysis -- ForStmts
+//
+
+/// An induction variable and expressions describing its range.
+class InductionVariable {
+public:
+  /// The direction of change for the induction variable
+  enum Direction {
+    Increases, // Update changes variable from lower to higher values
+    Decreases, // Update changes variable from higher to lower values
+    Unknown // Update has an unknown effect, e.g., container interators
+  };
+
+  InductionVariable() : Var(nullptr), Init(nullptr), Cond(nullptr),
+                        Update(nullptr), Dir(Unknown), LowerB(nullptr),
+                        UpperB(nullptr) {}
+
+  InductionVariable(VarDecl *Var, Expr *Init, Expr *Cond, Expr *Update,
+                    ASTContext *Ctx)
+    : Var(Var), Init(Init), Cond(Cond), Update(Update), Dir(Unknown),
+      LowerB(nullptr), UpperB(nullptr) {
+
+    PrefetchExprBuilder::Modifier UpperMod, LowerMod;
+    const UnaryOperator *Unary;
+
+    assert(PrefetchAnalysis::isScalarIntType(Var->getType()) &&
+           "Invalid induction variable");
+
+    // Try to classify update direction to determine which expression specifies
+    // lower and upper bounds
+    if((Unary = dyn_cast<UnaryOperator>(Update)))
+      Dir = classifyUnaryOpDirection(Unary->getOpcode());
+
+    if(Dir == Increases) {
+      LowerMod.ClassifyModifier(Init, Ctx);
+      UpperMod.ClassifyModifier(Cond, Ctx);
+      LowerB = stripInductionVar(Init);
+      UpperB = stripInductionVar(Cond);
+    }
+    else if(Dir == Decreases) {
+      LowerMod.ClassifyModifier(Cond, Ctx);
+      UpperMod.ClassifyModifier(Init, Ctx);
+      LowerB = stripInductionVar(Cond);
+      UpperB = stripInductionVar(Init);
+    }
+
+    if(LowerB && UpperB) {
+      LowerB = PrefetchExprBuilder::cloneAndModifyExpr(LowerB, LowerMod, Ctx);
+      UpperB = PrefetchExprBuilder::cloneAndModifyExpr(UpperB, UpperMod, Ctx);
+    }
+  }
+
+  VarDecl *getVariable() const { return Var; }
+  Expr *getInit() const { return Init; }
+  Expr *getCond() const { return Cond; }
+  Expr *getUpdate() const { return Update; }
+  enum Direction getUpdateDirection() const { return Dir; }
+  Expr *getLowerBound() const { return LowerB; }
+  Expr *getUpperBound() const { return UpperB; }
+
+  void print(llvm::raw_ostream &O, PrintingPolicy &Policy) const {
+    O << "Induction Variable: " << Var->getName() << "\nDirection: ";
+    switch(Dir) {
+    case Increases: O << "increases\n"; break;
+    case Decreases: O << "decreases\n"; break;
+    case Unknown: O << "unknown update direction\n"; break;
+    }
+    if(LowerB && UpperB) {
+      O << "Lower bound: ";
+      LowerB->printPretty(O, nullptr, Policy);
+      O << "\nUpper bound: ";
+      UpperB->printPretty(O, nullptr, Policy);
+    }
+    else O << "-> Could not determine bounds <-";
+    O << "\n";
+  }
+
+  void dump(PrintingPolicy &Policy) const { print(llvm::dbgs(), Policy); }
+
+private:
+  VarDecl *Var;
+  Expr *Init, *Cond, *Update;
+
+  /// Expressions describing the lower & upper bounds of the induction variable
+  /// and its update direction.
+  enum Direction Dir;
+  Expr *LowerB, *UpperB;
+
+  /// Try to classify the induction variable's update direction based on the
+  /// unary operation type.
+  static enum Direction classifyUnaryOpDirection(UnaryOperator::Opcode Op) {
+    switch(Op) {
+    case UO_PostInc:
+    case UO_PreInc:
+      return Increases;
+    case UO_PostDec:
+    case UO_PreDec:
+      return Decreases;
+    default: return Unknown;
+    }
+  }
+
+  Expr *stripInductionVarFromBinOp(BinaryOperator *B) {
+    DeclRefExpr *D;
+    VarDecl *VD;
+
+    D = dyn_cast<DeclRefExpr>(B->getLHS()->IgnoreImpCasts());
+    if(!D) return nullptr;
+    VD = dyn_cast<VarDecl>(D->getDecl());
+    if(!VD) return nullptr;
+    if(VD == Var) return B->getRHS();
+    return nullptr;
+  }
+
+  Expr *stripInductionVarFromExpr(Expr *E) {
+    DeclRefExpr *D;
+    VarDecl *VD;
+
+    D = dyn_cast<DeclRefExpr>(E->IgnoreImpCasts());
+    if(!D) return nullptr;
+    VD = dyn_cast<VarDecl>(D->getDecl());
+    if(!VD) return nullptr;
+    if(VD != Var) return D;
+    return nullptr;
+  }
+
+  /// Remove the induction variable & operator from the expression, leaving
+  /// only a bounds expression.
+  Expr *stripInductionVar(Expr *E) {
+    BinaryOperator *B;
+    IntegerLiteral *L;
+
+    if((B = dyn_cast<BinaryOperator>(E))) return stripInductionVarFromBinOp(B);
+    else if((L = dyn_cast<IntegerLiteral>(E))) return L;
+    else if(E) return stripInductionVarFromExpr(E);
+    else return nullptr;
+  }
+};
+
+/// Syntactic sugar for InductionVariable containers.
+typedef std::shared_ptr<InductionVariable> InductionVariablePtr;
+typedef std::pair<VarDecl *, InductionVariablePtr> IVPair;
+typedef llvm::DenseMap<VarDecl *, InductionVariablePtr> IVMap;
+
+/// Map an induction variable to an expression describing a bound.
+typedef llvm::DenseMap<VarDecl *, Expr *> IVBoundMap;
+typedef std::pair<VarDecl *, Expr *> IVBoundPair;
+
+/// Traversal to find induction variables in loop initialization, condition and
+/// update expressions.
+template<UnaryOpFilter UnaryFilt, BinaryOpFilter BinaryFilt>
+class IVFinder : public RecursiveASTVisitor<IVFinder<UnaryFilt, BinaryFilt>> {
+public:
+  // Visit binary operators to find induction variables.
+  bool VisitBinaryOperator(BinaryOperator *B) {
+    Expr *LHS;
+    DeclRefExpr *DR;
+    VarDecl *Var;
+
+    // Filter out irrelevant operation types
+    if(!BinaryFilt(B->getOpcode())) return true;
+
+    // Look for DeclRefExprs of scalar integer type -- these reference
+    // induction variables
+    LHS = B->getLHS();
+    if(!PrefetchAnalysis::isScalarIntType(LHS->getType())) return true;
+    DR = dyn_cast<DeclRefExpr>(LHS->IgnoreImpCasts());
+    if(!DR) return true;
+
+    // Make sure the expression acting on the induction variable is a scalar
+    // integer (casts may change types)
+    Var = PrefetchAnalysis::getVarIfScalarInt(DR->getDecl());
+    if(!Var) return true;
+    InductionVars[Var] = B;
+    return true;
+  }
+
+  // Visit unary operators to find induction variables.
+  bool VisitUnaryOperator(UnaryOperator *U) {
+    Expr *SubExpr;
+    DeclRefExpr *DR;
+    VarDecl *Var;
+
+    // Filter out irrelevant operation types
+    if(!UnaryFilt(U->getOpcode())) return true;
+
+    // Look for DeclRefExprs of scalar integer type -- these reference
+    // induction variables
+    SubExpr = U->getSubExpr();
+    if(!PrefetchAnalysis::isScalarIntType(SubExpr->getType())) return true;
+    DR = dyn_cast<DeclRefExpr>(SubExpr->IgnoreImpCasts());
+    if(!DR) return true;
+
+    // Make sure the expression acting on the induction variable is a scalar
+    // integer (casts may change types)
+    Var = PrefetchAnalysis::getVarIfScalarInt(DR->getDecl());
+    if(!Var) return true;
+    InductionVars[Var] = U;
+    return true;
+  }
+
+  bool VisitDeclStmt(DeclStmt *D) {
+    VarDecl *Var;
+    for(auto &Child : D->getDeclGroup()) {
+      Var = PrefetchAnalysis::getVarIfScalarInt(dyn_cast<VarDecl>(Child));
+      if(!Var || !Var->hasInit()) continue;
+      InductionVars[Var] = Var->getInit();
+    }
+    return true;
+  }
+
+  /// Return all induction variables found.
+  const IVBoundMap &getInductionVars() const { return InductionVars; }
+
+  /// Return the bounds expression for a given induction variable, or nullptr
+  /// if none was found.
+  Expr *getVarBound(VarDecl *Var) {
+    IVBoundMap::iterator it = InductionVars.find(Var);
+    if(it != InductionVars.end()) return it->second;
+    else return nullptr;
+  }
+
+private:
+  IVBoundMap InductionVars;
+};
+
+/// Structural information about a for-loop, including induction variables and
+/// parent/child loops.
+class ForLoopInfo {
+public:
+  ForLoopInfo(ForStmt *Loop, std::shared_ptr<ForLoopInfo> &Parent, int Level)
+    : Loop(Loop), Parent(Parent), Level(Level) {}
+
+  /// Add an induction variable.
+  void addInductionVar(const InductionVariablePtr &IV)
+  { InductionVars.insert(IVPair(IV->getVariable(), IV)); }
+
+  /// Remove an induction variable if present.  Return true if removed or false
+  /// if we don't have the variable.
+  bool removeInductionVar(const InductionVariablePtr &IV) {
+    IVMap::iterator it = InductionVars.find(IV->getVariable());
+    if(it != InductionVars.end()) {
+      InductionVars.erase(it);
+      return true;
+    }
+    else return false;
+  }
+
+  /// Add a child loop.
+  void addChildLoop(std::shared_ptr<ForLoopInfo> &S) { Children.push_back(S); }
+
+  ForStmt *getLoop() const { return Loop; }
+  const std::shared_ptr<ForLoopInfo> &getParent() const { return Parent; }
+  int getLevel() const { return Level; }
+  const IVMap &getInductionVars() const { return InductionVars; }
+  const llvm::SmallVector<std::shared_ptr<ForLoopInfo>, 4> &getChildren() const
+  { return Children; }
+
+  void print(llvm::raw_ostream &O, PrintingPolicy &Policy) const {
+    O << "Loop @ " << this << "\nDepth: " << Level
+      << "\nParent: " << Parent.get();
+    if(Children.size()) {
+      O << "\nChildren:";
+      for(auto &Child : Children) O << " " << Child.get();
+    }
+    O << "\n";
+    for(auto &IV : InductionVars) IV.second->dump(Policy);
+    O << "\n";
+    Loop->printPretty(O, nullptr, Policy);
+    O << "\n";
+  }
+  void dump(PrintingPolicy &Policy) const { print(llvm::dbgs(), Policy); }
+
+private:
+  ForStmt *Loop;
+  std::shared_ptr<ForLoopInfo> Parent;
+  size_t Level;
+
+  IVMap InductionVars;
+  llvm::SmallVector<std::shared_ptr<ForLoopInfo>, 4> Children;
+};
+typedef std::shared_ptr<ForLoopInfo> ForLoopInfoPtr;
+
+/// Search a sub-tree for loops, calculating induction variables found in any
+/// loops along the way.  We *must* construct tree structural information in
+/// order to correctly handle complex loop nests, e.g.:
+///
+/// int a, b;
+/// for(a = ...; a < ...; a++) {
+///   for(b = 0; b < 10; b++) {
+///     ...
+///   }
+///
+///   for(b = 10; b < 20; b++) {
+///
+///   }
+/// }
+///
+/// In this example, induction variable 'b' has different ranges in each of the
+/// nested loops.
+class LoopNestTraversal : public RecursiveASTVisitor<LoopNestTraversal> {
+public:
+  LoopNestTraversal(ASTContext *Ctx) : Ctx(Ctx) {}
+
+  void InitTraversal() { if(!LoopNest.size()) LoopNest.emplace_back(nullptr); }
+
+  bool VisitForStmt(ForStmt *S) {
+    Expr *InitExpr, *CondExpr, *UpdateExpr;
+    IVFinder<NoUnaryOp, FilterAssignOp> Init;
+    IVFinder<NoUnaryOp, FilterRelationalOp> Cond;
+    IVFinder<FilterMathOp, FilterMathLogicOp> Update;
+
+    // Set up data & tree structure information.
+    LoopNest.emplace_back(
+      new ForLoopInfo(S, LoopNest.back(), LoopNest.size() - 1));
+    ForLoopInfoPtr &Cur = LoopNest.back();
+    Loops[S] = Cur;
+    if(Cur->getParent()) Cur->getParent()->addChildLoop(Cur);
+
+    // Find the induction variables in the loop expressions.
+    Init.TraverseStmt(S->getInit());
+    Cond.TraverseStmt(S->getCond());
+    Update.TraverseStmt(S->getInc());
+
+    // Find induction variables which are referenced in all three parts of the
+    // for-loop header.
+    const IVBoundMap &InitVars = Init.getInductionVars();
+    for(auto Var = InitVars.begin(), E = InitVars.end(); Var != E; Var++) {
+      InitExpr = Var->second;
+      CondExpr = Cond.getVarBound(Var->first),
+      UpdateExpr = Update.getVarBound(Var->first);
+      if(InitExpr && CondExpr && UpdateExpr) {
+        InductionVariablePtr IV(
+          new InductionVariable(Var->first, InitExpr, CondExpr,
+                                UpdateExpr, Ctx));
+        Cur->addInductionVar(std::move(IV));
+      }
+    }
+
+    return true;
+  }
+
+  bool TraverseStmt(Stmt *S) {
+    if(!S) return true;
+    RecursiveASTVisitor<LoopNestTraversal>::TraverseStmt(S);
+    if(isa<ForStmt>(S)) LoopNest.pop_back();
+    return true;
+  }
+
+  /// Prune induction variables so each each loop only maintains its own
+  /// induction variables and not those of any nested loops.
+  // TODO this may not be necessary...
+  void PruneInductionVars() {
+    // Each loop nest is a tree in a forest of all loop nests
+    for(auto &Info : Loops)
+      if(Info.second->getLevel() == 0)
+        PruneInductionVars(Info.second);
+  }
+
+  /// Get all loops discovered during the tree traversal.
+  const llvm::DenseMap<ForStmt *, ForLoopInfoPtr> &getLoops() const
+  { return Loops; }
+
+  /// Get the enclosing loop's information for an array access.
+  const ForLoopInfoPtr getEnclosingLoop(const ArrayAccess &A) const {
+    ScopeInfoPtr S = A.getScope();
+    while(S && !isa<ForStmt>(S->ScopeStmt)) S = S->ParentScope;
+    if(!S) return ForLoopInfoPtr(nullptr);
+
+    llvm::DenseMap<ForStmt *, ForLoopInfoPtr>::const_iterator it
+      = Loops.find(cast<ForStmt>(S->ScopeStmt));
+    if(it != Loops.end()) return it->second;
+    else return ForLoopInfoPtr(nullptr);
+  }
+
+private:
+  ASTContext *Ctx;
+
+  // A stack of nested loops to provide induction variable scoping information.
+  llvm::SmallVector<ForLoopInfoPtr, 4> LoopNest;
+
+  // Map loop statements to information gathered during traversal.
+  llvm::DenseMap<ForStmt *, ForLoopInfoPtr> Loops;
+
+  // Recursively prune induction variables in a bottom-up fashion (post-order
+  // traversal).
+  void PruneInductionVars(ForLoopInfoPtr Loop) {
+    for(auto &Child : Loop->getChildren()) {
+      PruneInductionVars(Child);
+      for(auto &IV : Child->getInductionVars())
+        Loop->removeInductionVar(IV.second);
+    }
+  }
+};
+
+/// Get all induction variables for a scope, including induction variables from
+/// any enclosing scopes.
+static void getAllInductionVars(const ForLoopInfoPtr &Scope, IVMap &IVs) {
+  assert(Scope && "Invalid arguments");
+
+  ForLoopInfoPtr TmpScope = Scope;
+  do {
+    const IVMap &LoopIVs = TmpScope->getInductionVars();
+    for(auto IV : LoopIVs) IVs[IV.first] = IV.second;
+    TmpScope = TmpScope->getParent();
+  } while(TmpScope);
+}
+
+/// A set of variable declarations.
+typedef PrefetchDataflow::VarSet VarSet;
+
+/// Search a for-loop statement for array access patterns based on loop
+/// induction variables that can be prefetched at runtime.
+void PrefetchAnalysis::analyzeForStmt() {
+  // Gather loop nest information, including induction variables
+  Loops->InitTraversal();
+  Loops->TraverseStmt(S);
+  Loops->PruneInductionVars();
+
+  // Find array/pointer accesses.
+  ArrAccesses->InitTraversal();
+  ArrAccesses->TraverseStmt(S);
+  ArrAccesses->PruneInvalidOrIgnoredAccesses();
+}
+
+//===----------------------------------------------------------------------===//
+// Prefetch analysis API
+//
+
+void PrefetchAnalysis::analyzeStmt() {
+  if(!Ctx || !S) return;
+
+  Loops = std::make_shared<LoopNestTraversal>(Ctx);
+  ArrAccesses = std::make_shared<ArrayAccessPattern>(Ignore);
+
+  // TODO other types of statements
+  if(isa<ForStmt>(S)) analyzeForStmt();
+
+  pruneArrayAccesses();
+}
+
+void PrefetchAnalysis::calculatePrefetchRanges() {
+  PrefetchDataflow Dataflow(Ctx);
+  IVMap AllIVs;
+  IVMap::const_iterator IVIt;
+  VarSet VarsToTrack;
+  ExprList VarExprs;
+  ReplaceMap LowerBounds, UpperBounds;
+  Expr *UpperBound, *LowerBound;
+  PrefetchExprBuilder::BuildInfo LowerBuild(Ctx, LowerBounds, true),
+                                 UpperBuild(Ctx, UpperBounds, true);
+
+  if(!Ctx || !S || !Loops || !ArrAccesses) return;
+
+  // TODO the following could probably be optimized to reduce re-computing
+  // induction variable sets.
+
+  // Run the dataflow analysis.  Collect all non-induction variables used to
+  // construct array indices to see if induction variables are used in any
+  // assignment expressions.
+  for(auto &Access : ArrAccesses->getArrayAccesses()) {
+    AllIVs.clear();
+    ForLoopInfoPtr Scope = Loops->getEnclosingLoop(Access);
+    getAllInductionVars(Scope, AllIVs);
+    for(auto &Var : Access.getVarsInIdx()) {
+      if(!AllIVs.count(Var)) VarsToTrack.insert(Var);
+    }
+  }
+
+  Dataflow.runDataflow(cast<ForStmt>(S)->getBody(), VarsToTrack);
+
+  // Reconstruct array subscript expressions with induction variable references
+  // replaced by their bounds.  This includes variables defined using
+  // expressions containing induction variables.
+  for(auto &Access : ArrAccesses->getArrayAccesses()) {
+    LowerBuild.reset();
+    UpperBuild.reset();
+    AllIVs.clear();
+
+    // Get the expressions for replacing upper & lower bounds of induction
+    // variables.  Note that we *must* add all induction variables even if
+    // they're not directly used, as other variables used in the index
+    // calculation may be defined based on induction variables.  For example:
+    //
+    // for(int i = ...; i < ...; i++) {
+    //   int j = i + offset;
+    //   ...
+    //   arr[j] = ...
+    // }
+    //
+    // In this example, 'i' is not directly used in addressing but the dataflow
+    // analysis determines that 'j' is defined based on 'i', and hence we need
+    // to replace 'j' with induction variable bounds expressions.
+    ForLoopInfoPtr Scope = Loops->getEnclosingLoop(Access);
+    getAllInductionVars(Scope, AllIVs);
+    for(auto &Pair : AllIVs) {
+      const InductionVariablePtr &IV = Pair.second;
+      LowerBounds.insert(ReplacePair(IV->getVariable(), IV->getLowerBound()));
+      UpperBounds.insert(ReplacePair(IV->getVariable(), IV->getUpperBound()));
+    }
+
+    // Add other variables used in array calculation that may be defined using
+    // induction variable expressions.
+    for(auto &Var : Access.getVarsInIdx()) {
+      IVIt = AllIVs.find(Var);
+      if(IVIt == AllIVs.end()) {
+        Dataflow.getVariableValues(Var, Access.getStmt(), VarExprs);
+        // TODO currently if a variable used in an index calculation can take
+        // on more than one value due to control flow, we just avoid inserting
+        // prefetch expressions due to all the possible permutations.
+        if(VarExprs.size() == 1) {
+          LowerBounds.insert(ReplacePair(Var, *VarExprs.begin()));
+          UpperBounds.insert(ReplacePair(Var, *VarExprs.begin()));
+        }
+      }
+    }
+
+    // Create array access bounds expressions
+    LowerBound =
+      PrefetchExprBuilder::cloneWithReplacement(Access.getIndex(), LowerBuild),
+    UpperBound =
+      PrefetchExprBuilder::cloneWithReplacement(Access.getIndex(), UpperBuild);
+    if(LowerBound && UpperBound)
+      ToPrefetch.emplace_back(Access.getAccessType(), Access.getBase(),
+                              LowerBound, UpperBound);
+  }
+
+  mergePrefetchRanges();
+  prunePrefetchRanges();
+}
+
+void PrefetchAnalysis::print(llvm::raw_ostream &O) const {
+  PrintingPolicy Policy(Ctx->getLangOpts());
+  for(auto &Range : ToPrefetch) {
+    O << "Array '" << Range.getArray()->getName() << "': ";
+    Range.getStart()->printPretty(O, nullptr, Policy);
+    O << " to ";
+    Range.getEnd()->printPretty(O, nullptr, Policy);
+    O << " (" << Range.getTypeName() << ")\n";
+  }
+}
+
diff --git a/clang/lib/Sema/PrefetchDataflow.cpp b/clang/lib/Sema/PrefetchDataflow.cpp
new file mode 100644
index 00000000000..c2f031b114f
--- /dev/null
+++ b/clang/lib/Sema/PrefetchDataflow.cpp
@@ -0,0 +1,290 @@
+//=- PrefetchDataflow.cpp - Dataflow analysis for prefetching ------------*-==//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements the dataflow of expressions as required for prefetching
+// analysis.  This is required to correctly discover how variables are used in
+// memory accesses in order to construct memory access ranges.
+//
+//===----------------------------------------------------------------------===//
+
+#include "clang/AST/ASTContext.h"
+#include "clang/AST/ParentMap.h"
+#include "clang/AST/RecursiveASTVisitor.h"
+#include "clang/Sema/PrefetchDataflow.h"
+#include "clang/Sema/PrefetchExprBuilder.h"
+#include "llvm/Support/Debug.h"
+#include <queue>
+
+using namespace clang;
+
+//===----------------------------------------------------------------------===//
+// Common utilities
+//
+
+/// Return whether a statement is a loop construct.
+//static inline bool isLoopStmt(const Stmt *S) {
+//  if(isa<DoStmt>(S) || isa<ForStmt>(S) || isa<WhileStmt>(S)) return true;
+//  else return false;
+//}
+
+/// Return whether a binary operator is an assign expression.
+static inline bool isAssign(const BinaryOperator *B) {
+  if(B->getOpcode() == BO_Assign) return true;
+  return false;
+}
+
+/// Return whether a binary operator is an operation + assign expression.
+static inline bool isMathAssign(const BinaryOperator *B) {
+  switch(B->getOpcode()) {
+  case BO_MulAssign: case BO_DivAssign: case BO_RemAssign:
+  case BO_AddAssign: case BO_SubAssign: case BO_ShlAssign:
+  case BO_ShrAssign: case BO_AndAssign: case BO_XorAssign:
+  case BO_OrAssign:
+    return true;
+  default: return false;
+  }
+}
+
+/// Return the variable referenced by the expression E, or a nullptr if none
+/// were referenced.
+static const VarDecl *getVariableIfReference(const Expr *E) {
+  const DeclRefExpr *DR;
+  const VarDecl *VD;
+
+  DR = dyn_cast<DeclRefExpr>(E->IgnoreImpCasts());
+  if(!DR) return nullptr;
+  VD = dyn_cast<VarDecl>(DR->getDecl());
+  return VD;
+}
+
+//===----------------------------------------------------------------------===//
+// Expression dataflow API
+//
+
+PrefetchDataflow::PrefetchDataflow() : Ctx(nullptr) {}
+PrefetchDataflow::PrefetchDataflow(ASTContext *Ctx) : Ctx(Ctx) {}
+PrefetchDataflow::PrefetchDataflow(const PrefetchDataflow &RHS)
+  : Ctx(RHS.Ctx) {}
+
+PrefetchDataflow &PrefetchDataflow::operator=(const PrefetchDataflow &RHS) {
+  Ctx = RHS.Ctx;
+  return *this;
+}
+
+/// Analyze a statement to determine if we're defining a relevant variable.  If
+/// so, clone & store the defining expression.
+static void checkAndUpdateVarDefs(ASTContext *Ctx, const Stmt *S,
+                                  const PrefetchDataflow::VarSet &VarsToTrack,
+                                  SymbolicValueMap &VarExprs) {
+  Expr *Clone;
+  const DeclStmt *DS;
+  const BinaryOperator *BO;
+  const VarDecl *VD;
+
+  // Check for variable declarations with initializers, the initial definition.
+  if((DS = dyn_cast<DeclStmt>(S))) {
+    for(auto D : DS->getDeclGroup()) {
+      VD = dyn_cast<VarDecl>(D);
+      if(VD && VD->hasInit() && VarsToTrack.count(VD)) {
+        // TODO unfortunately CFG & accompanying classes expose statements
+        // & expressions with const qualifiers.  But, we *really* need them
+        // to not be const qualified in order to clone them (in particular,
+        // cloning DeclRefExprs becomes a headache).
+        Clone = PrefetchExprBuilder::clone(const_cast<Expr *>(VD->getInit()), Ctx);
+        if(Clone) VarExprs[VD].insert(Clone);
+      }
+    }
+    return;
+  }
+
+  BO = dyn_cast<BinaryOperator>(S);
+  if(!BO) return;
+
+  // Check for assignment operation to a relevant variable.  If we had
+  // previous expression(s) describing the variable's value the assignment
+  // overwrites them.
+  if(isAssign(BO)) {
+    VD = getVariableIfReference(BO->getLHS());
+    if(VD && VarsToTrack.count(VD)) {
+      ExprList &Exprs = VarExprs[VD];
+      Exprs.clear();
+      Clone = PrefetchExprBuilder::clone(BO->getRHS(), Ctx);
+      if(Clone) Exprs.insert(Clone);
+    }
+  }
+  // TODO we currently don't handle math + assign operations, so the
+  // dataflow analysis clamps to 'unknown' (i.e., no expressions).
+  else if(isMathAssign(BO)) {
+    VD = getVariableIfReference(BO->getLHS());
+    if(VD && VarsToTrack.count(VD)) VarExprs.erase(VD);
+  }
+}
+
+void PrefetchDataflow::runDataflow(Stmt *S, VarSet &VarsToTrack) {
+  const CFGBlock *Block;
+  CFGBlock::const_succ_iterator Succ, SE;
+  CFGBlockSet Seen;
+  std::queue<const CFGBlock *> Work;
+  Optional<CFGStmt> StmtNode;
+  BlockValuesMap::iterator BVIt;
+  SymbolicValueMap CurMap;
+
+  if(!VarsToTrack.size()) return;
+  this->S = S;
+  TheCFG = CFG::buildCFG(nullptr, S, Ctx, CFG::BuildOptions());
+
+  Work.push(&TheCFG->getEntry());
+  while(!Work.empty()) {
+    Block = Work.front();
+    Work.pop();
+    Seen.insert(Block);
+
+    // Find assignment operations within the block.  Because of the forward
+    // dataflow algorithm, predecessors should have already pushed dataflow
+    // expressions, if any, to this block.
+    CurMap = VarValues[Block];
+    for(auto &Elem : *Block) {
+      StmtNode = Elem.getAs<CFGStmt>();
+      if(!StmtNode) continue;
+      checkAndUpdateVarDefs(Ctx, StmtNode->getStmt(), VarsToTrack, CurMap);
+    }
+
+    // Push dataflow expressions to successors & add not-yet visited blocks to
+    // the work queue.
+    for(Succ = Block->succ_begin(), SE = Block->succ_end();
+        Succ != SE; Succ++) {
+      if(!Succ->isReachable() || Seen.count(*Succ)) continue;
+      else {
+        SymbolicValueMap &SuccMap = VarValues[*Succ];
+        for(auto &Pair : CurMap) {
+          ExprList &VarExprs = SuccMap[Pair.first];
+          for(auto Expr : Pair.second) VarExprs.insert(Expr);
+        }
+        Work.push(*Succ);
+      }
+    }
+
+    // TODO do we need to treat sub-scopes, e.g., loops, differently?
+  }
+
+  // Make it easier to look up analysis for statements
+  PMap.reset(new ParentMap(S));
+  StmtToBlock.reset(CFGStmtMap::Build(TheCFG.get(), PMap.get()));
+}
+
+/// Search for statements in sub-trees.
+class StmtFinder : public RecursiveASTVisitor<StmtFinder> {
+public:
+  void initialize(const Stmt *TheStmt) {
+    this->TheStmt = TheStmt;
+    Found = false;
+  }
+
+  bool TraverseStmt(Stmt *S) {
+    if(S == TheStmt) {
+      Found = true;
+      return false;
+    }
+    else return RecursiveASTVisitor::TraverseStmt(S);
+  }
+
+  bool foundStmt() const { return Found; }
+
+private:
+  const Stmt *TheStmt;
+  bool Found;
+};
+
+void PrefetchDataflow::getVariableValues(VarDecl *Var,
+                                         const Stmt *Use,
+                                         ExprList &Exprs) const {
+  SymbolicValueMap TmpMap;
+  VarSet VarsToTrack;
+  Optional<CFGStmt> StmtNode;
+  StmtFinder Finder;
+
+  Exprs.clear();
+
+  // Find analysis for the given variable, if any, at the start of the block
+  // containing the statement.
+  if(!StmtToBlock) return;
+  const CFGBlock *B = StmtToBlock->getBlock(Use);
+  if(!B) return;
+  BlockValuesMap::const_iterator ValIt = VarValues.find(B);
+  if(ValIt == VarValues.end()) return;
+  const SymbolicValueMap &Values = ValIt->second;
+  SymbolicValueMap::const_iterator SymIt = Values.find(Var);
+
+  // Walk through the block to the statement, searching for definitions between
+  // the start of the block and the statement argument
+  VarsToTrack.insert(Var);
+  if(SymIt != Values.end()) TmpMap[Var] = SymIt->second;
+  else TmpMap[Var] = ExprList();
+  for(auto &Elem : *B) {
+    StmtNode = Elem.getAs<CFGStmt>();
+    if(!StmtNode) continue;
+
+    // TODO CFG exposes statements with const qualifiers while the
+    // RecursiveASTVisitor requires non-const qualified statements.
+    Finder.initialize(Use);
+    Finder.TraverseStmt(const_cast<Stmt *>(StmtNode->getStmt()));
+    if(Finder.foundStmt()) {
+      Exprs = TmpMap[Var];
+      return;
+    }
+
+    checkAndUpdateVarDefs(Ctx, StmtNode->getStmt(), VarsToTrack, TmpMap);
+  }
+}
+
+void PrefetchDataflow::reset() {
+  S = nullptr;
+  StmtToBlock.reset();
+  PMap.reset();
+  TheCFG.reset();
+  VarValues.clear();
+}
+
+void PrefetchDataflow::print(llvm::raw_ostream &O) const {
+  if(!S) {
+    O << "<Prefetch Dataflow> No analysis -- did you run with runDataflow()?\n";
+    return;
+  }
+
+  if(!TheCFG) {
+    O << "<Prefetch Dataflow> No variables to track\n";
+    return;
+  }
+
+  if(!VarValues.size()) {
+    O << "<Prefetch Dataflow> No symbolic expressions detected\n";
+    return;
+  }
+
+  O << "<Prefetch Dataflow> Analysis results:\n";
+  PrintingPolicy PP(Ctx->getLangOpts());
+  for(auto Node : *TheCFG) {
+    Node->print(O, TheCFG.get(), Ctx->getLangOpts(), true);
+    O << "\n";
+    BlockValuesMap::const_iterator BVIt = VarValues.find(Node);
+    if(BVIt != VarValues.end()) {
+      for(auto VarValPair : BVIt->second) {
+        O << "Values for '" << VarValPair.first->getName() << "':\n";
+        for(auto E : VarValPair.second) {
+          E->printPretty(O, nullptr, PP);
+          O << "\n";
+        }
+      }
+    }
+    else O << "\n-> No dataflow values <-\n";
+  }
+}
+
+void PrefetchDataflow::dump() const { print(llvm::dbgs()); }
+
diff --git a/clang/lib/Sema/PrefetchExprBuilder.cpp b/clang/lib/Sema/PrefetchExprBuilder.cpp
new file mode 100644
index 00000000000..ec9be7ca800
--- /dev/null
+++ b/clang/lib/Sema/PrefetchExprBuilder.cpp
@@ -0,0 +1,307 @@
+//=- PrefetchExprBuilder.cpp - Prefetching expression builder ------------*-==//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file defines a set of utilities for building expressions for
+// prefetching.
+//
+//===----------------------------------------------------------------------===//
+
+#include "clang/AST/ASTContext.h"
+#include "clang/Sema/PrefetchAnalysis.h"
+#include "clang/Sema/PrefetchExprBuilder.h"
+#include "llvm/Support/Debug.h"
+
+using namespace clang;
+
+//===----------------------------------------------------------------------===//
+// Prefetch expression comparison definitions
+//
+
+static bool
+BinaryOperatorEqual(const BinaryOperator *A, const BinaryOperator *B) {
+  if(A->getOpcode() != B->getOpcode()) return false;
+  else return PrefetchExprEquality::exprEqual(A->getLHS(), B->getLHS()) &&
+              PrefetchExprEquality::exprEqual(A->getRHS(), B->getRHS());
+}
+
+static bool UnaryOperatorEqual(const UnaryOperator *A, const UnaryOperator *B) {
+  if(A->getOpcode() != B->getOpcode()) return false;
+  else return PrefetchExprEquality::exprEqual(A->getSubExpr(), B->getSubExpr());
+}
+
+static bool ArraySubscriptExprEqual(const ArraySubscriptExpr *A,
+                                    const ArraySubscriptExpr *B) {
+  return PrefetchExprEquality::exprEqual(A->getLHS(), B->getLHS()) &&
+         PrefetchExprEquality::exprEqual(A->getRHS(), B->getRHS());
+}
+
+static bool DeclRefExprEqual(const DeclRefExpr *A, const DeclRefExpr *B) {
+  if(A->getDecl() == B->getDecl()) return true;
+  else return false;
+}
+
+static bool
+ImplicitCastExprEqual(const ImplicitCastExpr *A, const ImplicitCastExpr *B) {
+  if(A->getCastKind() != B->getCastKind()) return false;
+  else return PrefetchExprEquality::exprEqual(A->getSubExpr(), B->getSubExpr());
+}
+
+static bool
+IntegerLiteralEqual(const IntegerLiteral *A, const IntegerLiteral *B) {
+  return A->getValue() == B->getValue();
+}
+
+bool PrefetchExprEquality::exprEqual(const Expr *A, const Expr *B) {
+  const BinaryOperator *B_A, *B_B;
+  const UnaryOperator *U_A, *U_B;
+  const ArraySubscriptExpr *A_A, *A_B;
+  const DeclRefExpr *D_A, *D_B;
+  const ImplicitCastExpr *C_A, *C_B;
+  const IntegerLiteral *I_A, *I_B;
+
+  if(!A || ! B) return false;
+
+  // Check common characteristics.  Note that by checking the statement class,
+  // we know the expressions are of the same type and can use cast<> below.
+  if(A->getStmtClass() != B->getStmtClass() ||
+     A->getType() != B->getType() ||
+     A->getValueKind() != B->getValueKind() ||
+     A->getObjectKind() != B->getObjectKind()) return false;
+
+  // TODO better way to switch on type?
+  if((B_A = dyn_cast<BinaryOperator>(A))) {
+    B_B = cast<BinaryOperator>(B);
+    return BinaryOperatorEqual(B_A, B_B);
+  }
+  else if((U_A = dyn_cast<UnaryOperator>(A))) {
+    U_B = cast<UnaryOperator>(B);
+    return UnaryOperatorEqual(U_A, U_B);
+  }
+  else if((A_A = dyn_cast<ArraySubscriptExpr>(A))) {
+    A_B = cast<ArraySubscriptExpr>(B);
+    return ArraySubscriptExprEqual(A_A, A_B);
+  }
+  else if((D_A = dyn_cast<DeclRefExpr>(A))) {
+    D_B = cast<DeclRefExpr>(B);
+    return DeclRefExprEqual(D_A, D_B);
+  }
+  else if((C_A = dyn_cast<ImplicitCastExpr>(A))) {
+    C_B = cast<ImplicitCastExpr>(B);
+    return ImplicitCastExprEqual(C_A, C_B);
+  }
+  else if((I_A = dyn_cast<IntegerLiteral>(A))) {
+    I_B = cast<IntegerLiteral>(B);
+    return IntegerLiteralEqual(I_A, I_B);
+  }
+  else return false;
+}
+
+//===----------------------------------------------------------------------===//
+// Modifier class definitions
+//
+
+void PrefetchExprBuilder::Modifier::ClassifyModifier(const Expr *E,
+                                                     const ASTContext *Ctx) {
+  unsigned Bits;
+  const DeclRefExpr *DR;
+  const BinaryOperator *B;
+  const IntegerLiteral *L;
+  QualType BaseTy;
+
+  Ty = Unknown;
+  if(!E) return;
+
+  E = E->IgnoreImpCasts();
+  if((B = dyn_cast<BinaryOperator>(E))) {
+    // Note: both operands *must* have same type
+    BaseTy = B->getLHS()->getType().getDesugaredType(*Ctx);
+    assert(PrefetchAnalysis::isScalarIntType(BaseTy) &&
+           "Invalid expression type");
+    Bits = PrefetchAnalysis::getTypeSize(cast<BuiltinType>(BaseTy)->getKind());
+
+    switch(B->getOpcode()) {
+    default: Ty = None; break;
+    case BO_LT:
+      Ty = Sub;
+      Val = llvm::APInt(Bits, 1, false);
+      break;
+    case BO_GT:
+      Ty = Add;
+      Val = llvm::APInt(Bits, 1, false);
+      break;
+    // TODO hybrid math/assign operations
+    }
+  }
+  else if((DR = dyn_cast<DeclRefExpr>(E))) Ty = None;
+  else if((L = dyn_cast<IntegerLiteral>(E))) Ty = None;
+}
+
+//===----------------------------------------------------------------------===//
+// Prefetch expression builder definitions
+//
+
+typedef PrefetchExprBuilder::BuildInfo BuildInfo;
+
+Expr *PrefetchExprBuilder::cloneWithReplacement(Expr *E, BuildInfo &Info) {
+  BinaryOperator *B;
+  UnaryOperator *U;
+  ArraySubscriptExpr *A;
+  DeclRefExpr *D;
+  ImplicitCastExpr *C;
+  IntegerLiteral *I;
+
+  if(!E) return nullptr;
+
+  // TODO better way to switch on type?
+  if((B = dyn_cast<BinaryOperator>(E)))
+    return cloneBinaryOperator(B, Info);
+  else if((U = dyn_cast<UnaryOperator>(E)))
+    return cloneUnaryOperator(U, Info);
+  else if((A = dyn_cast<ArraySubscriptExpr>(E)))
+    return cloneArraySubscriptExpr(A, Info);
+  else if((D = dyn_cast<DeclRefExpr>(E)))
+    return cloneDeclRefExpr(D, Info);
+  else if((C = dyn_cast<ImplicitCastExpr>(E)))
+    return cloneImplicitCastExpr(C, Info);
+  else if((I = dyn_cast<IntegerLiteral>(E)))
+    return cloneIntegerLiteral(I, Info);
+  else {
+    // TODO delete
+    llvm::dbgs() << "Unhandled expression:\n";
+    if(Info.dumpInColor) E->dumpColor();
+    else E->dump();
+  }
+
+  return nullptr;
+}
+
+Expr *PrefetchExprBuilder::clone(Expr *E, ASTContext *Ctx) {
+  ReplaceMap Dummy; // No variables, don't replace any DeclRefExprs
+  BuildInfo DummyInfo(Ctx, Dummy, true);
+  return cloneWithReplacement(E, DummyInfo);
+}
+
+Expr *PrefetchExprBuilder::cloneBinaryOperator(BinaryOperator *B,
+                                               BuildInfo &Info) {
+  Expr *LHS = cloneWithReplacement(B->getLHS(), Info),
+       *RHS = cloneWithReplacement(B->getRHS(), Info);
+  if(!LHS || !RHS) return nullptr;
+  return new (*Info.Ctx) BinaryOperator(LHS, RHS, B->getOpcode(),
+                                        B->getType(),
+                                        B->getValueKind(),
+                                        B->getObjectKind(),
+                                        SourceLocation(),
+                                        B->getFPFeatures());
+}
+
+Expr *PrefetchExprBuilder::cloneUnaryOperator(UnaryOperator *U,
+                                              BuildInfo &Info) {
+  Expr *Sub = cloneWithReplacement(U->getSubExpr(), Info);
+  if(!Sub) return nullptr;
+  return new (*Info.Ctx) UnaryOperator(Sub, U->getOpcode(),
+                                       U->getType(),
+                                       U->getValueKind(),
+                                       U->getObjectKind(),
+                                       SourceLocation(),
+				       U->canOverflow());
+}
+
+Expr *PrefetchExprBuilder::cloneArraySubscriptExpr(ArraySubscriptExpr *A,
+                                                   BuildInfo &Info) {
+  Expr *LHS = cloneWithReplacement(A->getLHS(), Info),
+       *RHS = cloneWithReplacement(A->getRHS(), Info);
+  if(!LHS || !RHS) return nullptr;
+  return new (*Info.Ctx) ArraySubscriptExpr(LHS, RHS, A->getType(),
+                                            A->getValueKind(),
+                                            A->getObjectKind(),
+                                            SourceLocation());
+}
+
+Expr *PrefetchExprBuilder::cloneDeclRefExpr(DeclRefExpr *D,
+                                            BuildInfo &Info) {
+  Expr *Clone = nullptr;
+  VarDecl *VD;
+  ReplaceMap::const_iterator it;
+
+  // If the variable is relevant and we haven't replaced it before, replace it
+  // with the specified expression.
+  if((VD = dyn_cast<VarDecl>(D->getDecl())) &&
+     (it = Info.VarReplace.find(VD)) != Info.VarReplace.end() &&
+     !Info.SeenVars.count(VD)) {
+    Info.SeenVars.insert(VD);
+    Clone = cloneWithReplacement(it->second, Info);
+    Info.SeenVars.erase(VD);
+    return Clone;
+  }
+
+  // Clone the DeclRefExpr if the variable isn't relevant or if cloning the
+  // replacement failed.
+  return new (*Info.Ctx) DeclRefExpr(*Info.Ctx, D->getDecl(),
+                                     D->refersToEnclosingVariableOrCapture(),
+                                     D->getType(),
+                                     D->getValueKind(),
+                                     SourceLocation(),
+                                     D->getNameInfo().getInfo());
+}
+
+Expr *PrefetchExprBuilder::cloneImplicitCastExpr(ImplicitCastExpr *C,
+                                                 BuildInfo &Info) {
+  Expr *Sub = cloneWithReplacement(C->getSubExpr(), Info);
+  if(!Sub) return nullptr;
+
+  // Avoid the situation that when replacing an induction variable with another
+  // expression we accidentally chain together 2 implicit casts (which causes
+  // CodeGen to choke).
+  if(C->getCastKind() == CastKind::CK_LValueToRValue &&
+     Sub->getValueKind() == VK_RValue)
+    return Sub;
+  else
+    return new (*Info.Ctx) ImplicitCastExpr(ImplicitCastExpr::OnStack,
+                                            C->getType(),
+                                            C->getCastKind(),
+                                            Sub,
+                                            C->getValueKind());
+}
+
+Expr *PrefetchExprBuilder::cloneIntegerLiteral(IntegerLiteral *L,
+                                               BuildInfo &Info) {
+  return new (*Info.Ctx) IntegerLiteral(*Info.Ctx, L->getValue(),
+                                        L->getType(),
+                                        SourceLocation());
+}
+
+Expr *PrefetchExprBuilder::cloneAndModifyExpr(Expr *E,
+                                              const Modifier &Mod,
+                                              ASTContext *Ctx) {
+  BinaryOperator::Opcode Op;
+  IntegerLiteral *RHS;
+
+  E = clone(E, Ctx);
+  if(!E) return nullptr;
+
+  switch(Mod.getType()) {
+  case Modifier::Add: Op = BO_Add; break;
+  case Modifier::Sub: Op = BO_Sub; break;
+  case Modifier::Mul: Op = BO_Mul; break;
+  case Modifier::Div: Op = BO_Div; break;
+  case Modifier::None: return E; // Nothing to do
+  case Modifier::Unknown: return nullptr; // Couldn't classify
+  }
+
+  RHS = new (*Ctx) IntegerLiteral(*Ctx, Mod.getVal(),
+                                  E->getType(),
+                                  SourceLocation());
+  return new (*Ctx) BinaryOperator(E, RHS, Op,
+                                   E->getType(),
+                                   E->getValueKind(),
+                                   E->getObjectKind(),
+                                   SourceLocation(),
+                                   FPOptions());
+}
+
diff --git a/clang/lib/Sema/SemaOpenMP.cpp b/clang/lib/Sema/SemaOpenMP.cpp
index 4ac87469bf4..3ffa2eb13d4 100644
--- a/clang/lib/Sema/SemaOpenMP.cpp
+++ b/clang/lib/Sema/SemaOpenMP.cpp
@@ -4385,6 +4385,7 @@ StmtResult Sema::ActOnOpenMPExecutableDirective(
       case OMPC_from:
       case OMPC_use_device_ptr:
       case OMPC_is_device_ptr:
+      case OMPC_prefetch:
         continue;
       case OMPC_allocator:
       case OMPC_flush:
@@ -9263,6 +9264,7 @@ OMPClause *Sema::ActOnOpenMPSingleExprClause(OpenMPClauseKind Kind, Expr *Expr,
   case OMPC_reverse_offload:
   case OMPC_dynamic_allocators:
   case OMPC_atomic_default_mem_order:
+  case OMPC_prefetch:
     llvm_unreachable("Clause is not allowed.");
   }
   return Res;
@@ -9805,6 +9807,7 @@ static OpenMPDirectiveKind getOpenMPCaptureRegionForClause(
   case OMPC_reverse_offload:
   case OMPC_dynamic_allocators:
   case OMPC_atomic_default_mem_order:
+  case OMPC_prefetch:
     llvm_unreachable("Unexpected OpenMP clause.");
   }
   return CaptureRegion;
@@ -10198,6 +10201,7 @@ OMPClause *Sema::ActOnOpenMPSimpleClause(
   case OMPC_unified_shared_memory:
   case OMPC_reverse_offload:
   case OMPC_dynamic_allocators:
+  case OMPC_prefetch:
     llvm_unreachable("Clause is not allowed.");
   }
   return Res;
@@ -10376,6 +10380,7 @@ OMPClause *Sema::ActOnOpenMPSingleExprWithArgClause(
   case OMPC_reverse_offload:
   case OMPC_dynamic_allocators:
   case OMPC_atomic_default_mem_order:
+  case OMPC_prefetch:
     llvm_unreachable("Clause is not allowed.");
   }
   return Res;
@@ -10585,6 +10590,7 @@ OMPClause *Sema::ActOnOpenMPClause(OpenMPClauseKind Kind,
   case OMPC_use_device_ptr:
   case OMPC_is_device_ptr:
   case OMPC_atomic_default_mem_order:
+  case OMPC_prefetch:
     llvm_unreachable("Clause is not allowed.");
   }
   return Res;
@@ -10668,13 +10674,14 @@ OMPClause *Sema::ActOnOpenMPDynamicAllocatorsClause(SourceLocation StartLoc,
 
 OMPClause *Sema::ActOnOpenMPVarListClause(
     OpenMPClauseKind Kind, ArrayRef<Expr *> VarList, Expr *TailExpr,
-    const OMPVarListLocTy &Locs, SourceLocation ColonLoc,
-    CXXScopeSpec &ReductionOrMapperIdScopeSpec,
+    Expr *EndExpr, const OMPVarListLocTy &Locs, SourceLocation ColonLoc,
+    SourceLocation EndColonLoc, CXXScopeSpec &ReductionOrMapperIdScopeSpec,
     DeclarationNameInfo &ReductionOrMapperId, OpenMPDependClauseKind DepKind,
     OpenMPLinearClauseKind LinKind,
     ArrayRef<OpenMPMapModifierKind> MapTypeModifiers,
     ArrayRef<SourceLocation> MapTypeModifiersLoc, OpenMPMapClauseKind MapType,
-    bool IsMapTypeImplicit, SourceLocation DepLinMapLoc) {
+    bool IsMapTypeImplicit, SourceLocation DepLinMapLoc,
+    OpenMPPrefetchClauseKind PrefKind, SourceLocation PrefLoc) {
   SourceLocation StartLoc = Locs.StartLoc;
   SourceLocation LParenLoc = Locs.LParenLoc;
   SourceLocation EndLoc = Locs.EndLoc;
@@ -10791,6 +10798,7 @@ OMPClause *Sema::ActOnOpenMPVarListClause(
   case OMPC_reverse_offload:
   case OMPC_dynamic_allocators:
   case OMPC_atomic_default_mem_order:
+  case OMPC_prefetch:
     llvm_unreachable("Clause is not allowed.");
   }
   return Res;
@@ -15307,3 +15315,58 @@ OMPClause *Sema::ActOnOpenMPAllocateClause(
   return OMPAllocateClause::Create(Context, StartLoc, LParenLoc, Allocator,
                                    ColonLoc, EndLoc, Vars);
 }
+
+OMPClause *
+Sema::ActOnOpenMPPrefetchClause(OpenMPPrefetchClauseKind PrefKind,
+                                SourceLocation PrefLoc,
+                                ArrayRef<Expr *> VarList,
+                                Expr *Start, Expr *End,
+                                SourceLocation StartLoc,
+                                SourceLocation LParenLoc,
+                                SourceLocation FirstColonLoc,
+                                SourceLocation SecondColonLoc,
+                                SourceLocation EndLoc) {
+  // Check validity of range expressions & variables which use them
+  if(Start) {
+    if(!Start->getType()->isIntegerType()) {
+      Diag(Start->getExprLoc(), diag::err_omp_invalid_prefetch_range_type);
+      return nullptr;
+    }
+
+    if(End) {
+      if(!End->getType()->isIntegerType()) {
+        Diag(End->getExprLoc(), diag::err_omp_invalid_prefetch_range_type);
+        return nullptr;
+      }
+    }
+
+    for(const auto &Var : VarList) {
+      QualType T = Var->getType();
+
+      if(!T->isArrayType() && !T->isPointerType()) {
+        Diag(Var->getExprLoc(), diag::err_omp_invalid_prefetch_var_type)
+          << "array or pointer";
+        return nullptr;
+      }
+    }
+  }
+  else {
+    for(const auto &Var : VarList) {
+      // Note: array types with known sizes may be decayed to pointer types.
+      // We want the original type (i.e., the array type) but we can't simply
+      // call T.getDesugaredType() as it will return the pointer type.
+      QualType T = Var->getType();
+      while(isa<DecayedType>(T)) T = cast<DecayedType>(T)->getOriginalType();
+
+      if(!T->isConstantArrayType()) {
+        Diag(Var->getExprLoc(), diag::err_omp_invalid_prefetch_var_type)
+          << "constant-sized array";
+        return nullptr;
+      }
+    }
+  }
+
+  return OMPPrefetchClause::Create(Context, PrefKind, PrefLoc, VarList, Start,
+                                   End, StartLoc, LParenLoc, FirstColonLoc,
+                                   SecondColonLoc, EndLoc);
+}
diff --git a/clang/lib/Sema/TreeTransform.h b/clang/lib/Sema/TreeTransform.h
index 8df18b5c278..5b5131271f9 100644
--- a/clang/lib/Sema/TreeTransform.h
+++ b/clang/lib/Sema/TreeTransform.h
@@ -9168,6 +9168,14 @@ TreeTransform<Derived>::TransformOMPIsDevicePtrClause(OMPIsDevicePtrClause *C) {
   return getDerived().RebuildOMPIsDevicePtrClause(Vars, Locs);
 }
 
+template <typename Derived>
+OMPClause *
+TreeTransform<Derived>::TransformOMPPrefetchClause(OMPPrefetchClause *C) {
+  // TODO Rob
+  llvm_unreachable("not yet implmented");
+  return C;
+}
+
 //===----------------------------------------------------------------------===//
 // Expression transformation
 //===----------------------------------------------------------------------===//
diff --git a/clang/lib/Serialization/ASTReaderStmt.cpp b/clang/lib/Serialization/ASTReaderStmt.cpp
index afaaa543bb2..2e3f420ba33 100644
--- a/clang/lib/Serialization/ASTReaderStmt.cpp
+++ b/clang/lib/Serialization/ASTReaderStmt.cpp
@@ -1982,6 +1982,11 @@ void ASTStmtReader::VisitAsTypeExpr(AsTypeExpr *E) {
   E->SrcExpr = Record.readSubExpr();
 }
 
+void OMPClauseReader::VisitOMPPrefetchClause(OMPPrefetchClause *C) {
+  // TODO Rob
+  llvm_unreachable("not yet implemented");
+}
+
 //===----------------------------------------------------------------------===//
 // OpenMP Directives.
 //===----------------------------------------------------------------------===//
diff --git a/clang/lib/Serialization/ASTWriterStmt.cpp b/clang/lib/Serialization/ASTWriterStmt.cpp
index 4fbcbaabe74..f3410338227 100644
--- a/clang/lib/Serialization/ASTWriterStmt.cpp
+++ b/clang/lib/Serialization/ASTWriterStmt.cpp
@@ -1926,6 +1926,11 @@ void ASTStmtWriter::VisitSEHLeaveStmt(SEHLeaveStmt *S) {
   Code = serialization::STMT_SEH_LEAVE;
 }
 
+void OMPClauseWriter::VisitOMPPrefetchClause(OMPPrefetchClause *C) {
+  // TODO Rob
+  llvm_unreachable("not yet implemented");
+}
+
 //===----------------------------------------------------------------------===//
 // OpenMP Directives.
 //===----------------------------------------------------------------------===//
diff --git a/clang/tools/libclang/CIndex.cpp b/clang/tools/libclang/CIndex.cpp
index 1dc961f58a2..165c436b341 100644
--- a/clang/tools/libclang/CIndex.cpp
+++ b/clang/tools/libclang/CIndex.cpp
@@ -2438,6 +2438,10 @@ void OMPClauseEnqueue::VisitOMPUseDevicePtrClause(const OMPUseDevicePtrClause *C
 void OMPClauseEnqueue::VisitOMPIsDevicePtrClause(const OMPIsDevicePtrClause *C) {
   VisitOMPClauseList(C);
 }
+void OMPClauseEnqueue::VisitOMPPrefetchClause(const OMPPrefetchClause *C) {
+  // TODO Rob
+  llvm_unreachable("not yet implemented");
+}
 }
 
 void EnqueueVisitor::EnqueueChildren(const OMPClause *S) {
diff --git a/llvm/docs/StackMaps.rst b/llvm/docs/StackMaps.rst
index 2ceb408097a..eca6a7dbe7c 100644
--- a/llvm/docs/StackMaps.rst
+++ b/llvm/docs/StackMaps.rst
@@ -336,16 +336,20 @@ format of this section follows:
   }
   StkMapRecord[NumRecords] {
     uint64 : PatchPoint ID
+    uint32 : Index of Function Record
     uint32 : Instruction Offset
     uint16 : Reserved (record flags)
     uint16 : NumLocations
     Location[NumLocations] {
-      uint8  : Register | Direct | Indirect | Constant | ConstantIndex
-      uint8  : Reserved (expected to be 0)
-      uint16 : Location Size
-      uint16 : Dwarf RegNum
-      uint16 : Reserved (expected to be 0)
-      int32  : Offset or SmallConstant
+      uint8 (4 bits) : Register | Direct | Indirect | Constant | ConstantIndex
+      uint8 (1 bit)  : Is it a pointer?
+      uint8 (1 bit)  : Is it an alloca?
+      uint8 (1 bit)  : Is it a duplicate record for the same live value?
+      uint8 (1 bit)  : Is it a temporary value created for the stackmap?
+      uint8          : Size in Bytes
+      uint16         : Dwarf RegNum
+      int32          : Offset or SmallConstant
+      uint32         : Size of pointed-to alloca data
     }
     uint32 : Padding (only if required to align to 8 byte)
     uint16 : Padding
@@ -355,6 +359,25 @@ format of this section follows:
       uint8  : Reserved
       uint8  : Size in Bytes
     }
+    uint16 : Padding
+    uint16 : NumArchValues
+    ArchValues[NumArchValues] {
+      Location {
+        uint8 (4 bits) : Register | Indirect
+        uint8 (3 bits) : Padding
+        uint8 (1 bit)  : Is it a pointer?
+        uint8          : Size in Bytes
+        uint16         : Dwarf RegNum
+        int32          : Offset
+      }
+      Value {
+        uint8_t (4 bits) : Instruction
+        uint8_t (4 bits) : Register | Direct | Constant
+        uint8_t          : Size
+        uint16_t         : Dwarf RegNum
+        int64_t          : Offset or Constant
+      }
+    }
     uint32 : Padding (only if required to align to 8 byte)
   }
 
diff --git a/llvm/include/llvm/Analysis/LiveValues.h b/llvm/include/llvm/Analysis/LiveValues.h
new file mode 100644
index 00000000000..40003d008ce
--- /dev/null
+++ b/llvm/include/llvm/Analysis/LiveValues.h
@@ -0,0 +1,208 @@
+/*
+ * Calculate live-value sets for functions.
+ *
+ * Liveness-analysis is based on the non-iterative dataflow algorithm for
+ * reducible graphs by Brandner et. al in:
+ *
+ * "Computing Liveness Sets for SSA-Form Programs"
+ * URL: https://hal.inria.fr/inria-00558509v1/document
+ * Accessed: 5/19/2016
+ *
+ * Author: Rob Lyerly <rlyerly@vt.edu>
+ * Date: 5/19/2016
+ */
+
+#ifndef _LIVE_VALUES_H
+#define _LIVE_VALUES_H
+
+#include <map>
+#include <set>
+#include <list>
+#include "llvm/Pass.h"
+#include "llvm/Analysis/LoopNestingTree.h"
+#include "llvm/IR/Function.h"
+#include "llvm/Support/raw_ostream.h"
+
+namespace llvm {
+
+class LiveValues : public FunctionPass
+{
+public:
+  typedef std::pair<const BasicBlock *, const BasicBlock *> Edge;
+
+  static char ID;
+
+  /**
+   * Default constructor.
+   */
+  LiveValues(void);
+
+  /**
+   * Default destructor.
+   */
+  ~LiveValues(void) {}
+
+  /**
+   * Return whether or not a given type should be included in the analysis.
+   * @return true if the type is included in liveness sets, false otherwise
+   */
+  bool includeAsm(void) const { return inlineasm; }
+  bool includeBitcasts(void) const { return bitcasts; }
+  bool includeComparisons(void) const { return comparisons; }
+  bool includeConstants(void) const { return constants; }
+  bool includeMetadata(void) const { return metadata; }
+
+  /**
+   * Set whether or not to include the specified type in the analysis (all
+   * are set to false by default by the constructor).
+   * @param include true if it should be included, false otherwise
+   */
+  void includeAsm(bool include) { inlineasm = include; }
+  void includeBitcasts(bool include) { bitcasts = include; }
+  void includeComparisons(bool include) { comparisons = include; }
+  void includeConstants(bool include) { constants = include; }
+  void includeMetadata(bool include) { metadata = include; }
+
+  /**
+   * Register which analysis passes we need.
+   * @param AU an analysis usage object
+   */
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const;
+
+  /**
+   * Calculate liveness sets for a function.
+   * @param F a function for which to calculate live values.
+   * @return false, always
+   */
+  virtual bool runOnFunction(Function &F);
+
+  /**
+   * Get the human-readable name of the pass.
+   * @return the pass name
+   */
+  virtual StringRef getPassName() const { return "Live value analysis"; }
+
+  /**
+   * Print a human-readable version of the analysis.
+   * @param O an output stream
+   * @param F the function for which to print analysis
+   */
+  virtual void printF(raw_ostream &O, const Function *F) const;
+
+  /**
+   * Return the live-in set for a basic block.
+   * @param BB a basic block
+   * @return a set of live-in values for the basic block; this set must be
+   *         freed by the user.
+   */
+  std::set<const Value *> *getLiveIn(const BasicBlock *BB) const;
+
+  /**
+   * Return the live-out set for a basic block.
+   * @param BB a basic block
+   * @return a set of live-out values for the basic block; this set must be
+   *         freed by the user.
+   */
+  std::set<const Value *> *getLiveOut(const BasicBlock *BB) const;
+
+  /**
+   * Get the live values across a given instruction, i.e., values live right
+   * after the invocation of the instruction (excluding the value defined by
+   * the instruction itself).
+   * @param inst an instruction
+   * @return the set of values live directly before the instruction; this set
+   *         must be freed by the user.
+   */
+  std::set<const Value *> *
+  getLiveValues(const Instruction *inst) const;
+
+private:
+  /* Should values of each type be included? */
+  bool inlineasm;
+  bool bitcasts;
+  bool comparisons;
+  bool constants;
+  bool metadata;
+
+  /* A loop nesting forest composed of 0 or more loop nesting trees. */
+  typedef std::list<LoopNestingTree> LoopNestingForest;
+
+  /* Maps live values to a basic block. */
+  typedef std::map<const BasicBlock *, std::set<const Value *> > LiveVals;
+  typedef std::pair<const BasicBlock *, std::set<const Value *> > LiveValsPair;
+
+  /* Store analysis for all functions. */
+  std::map<const Function *, LiveVals> FuncBBLiveIn;
+  std::map<const Function *, LiveVals> FuncBBLiveOut;
+
+  /**
+   * Return whether or not a value is a variable that should be tracked.
+   * @param val a value
+   * @return true if the value is a variable to be tracked, false otherwise
+   */
+  bool includeVal(const Value *val) const;
+
+  /**
+   * Insert the values used in phi-nodes at the beginning of basic block S (as
+   * values live from B) into the set uses.
+   * @param B a basic block which passes live values into phi-nodes in S
+   * @param S a basic block, successor to B
+   * @param uses set in which to add values used in phi-nodes in B
+   * @return the number of values added to the set
+   */
+  unsigned phiUses(const BasicBlock *B,
+                   const BasicBlock *S,
+                   std::set<const Value *> &uses);
+
+  /**
+   * Insert the values defined by the phi-nodes at the beginning of basic block
+   * B into the set defs.
+   * @param B a basic block
+   * @param defs set in which to add values defined by phi-nodes in B
+   * @return the number of values added to the set
+   */
+  unsigned phiDefs(const BasicBlock *B,
+                   std::set<const Value *> &defs);
+
+  /**
+   * Do a post-order traversal of the control flow graph to calculate partial
+   * liveness sets.
+   * @param F a function for which to calculate per-basic block partial
+   *          liveness sets
+   * @param liveIn per-basic block live-in values
+   * @param liveOut per-basic block live-out values
+   */
+  void dagDFS(Function &F, LiveVals &liveIn, LiveVals &liveOut);
+
+  /**
+   * Construct the loop-nesting forest for a function.
+   * @param F a function for which to calculate the loop-nesting forest.
+   * @param LNF a loop nesting forest to populate with loop nesting trees.
+   */
+  void constructLoopNestingForest(Function &F, LoopNestingForest &LNF);
+
+  /**
+   * Propagate live values throughout the loop-nesting tree.
+   * @param loopNest a loop-nesting tree
+   * @param liveIn per-basic block live-in values
+   * @param liveOut per-basic block live-out values
+   */
+  void propagateValues(const LoopNestingTree &loopNest,
+                       LiveVals &liveIn,
+                       LiveVals &liveOut);
+
+  /**
+   * Propagate live values within loops for all loop-nesting trees in the
+   * function's loop-nesting forest.
+   * @param LNF a loop nesting forest
+   * @param liveIn per-basic block live-in values
+   * @param liveOut per-basic block live-out values
+   */
+  void loopTreeDFS(LoopNestingForest &LNF,
+                   LiveVals &liveIn,
+                   LiveVals &liveOut);
+};
+
+} /* llvm namespace */
+
+#endif /* _LIVE_VALUES_H */
diff --git a/llvm/include/llvm/Analysis/LoopNestingTree.h b/llvm/include/llvm/Analysis/LoopNestingTree.h
new file mode 100644
index 00000000000..35995cc1cc4
--- /dev/null
+++ b/llvm/include/llvm/Analysis/LoopNestingTree.h
@@ -0,0 +1,190 @@
+/*
+ * Loop-nesting tree for a loop.  The root of a loop-nesting tree is the loop
+ * header of the outermost loop.  The children of any given node (including the
+ * root) are the basic blocks contained within the loop and the loop headers of
+ * nested loops.
+ *
+ * Note: we assume that the control-flow graphs are reducible
+ *
+ * Author: Rob Lyerly <rlyerly@vt.edu>
+ * Date: 5/23/2016
+ */
+
+#ifndef _LOOP_NESTING_TREE_H
+#define _LOOP_NESTING_TREE_H
+
+#include <list>
+#include <vector>
+#include <queue>
+#include "llvm/IR/BasicBlock.h"
+#include "llvm/Analysis/LoopInfo.h"
+#include "llvm/Support/raw_ostream.h"
+
+class LoopNestingTree {
+private:
+  /*
+   * Tree node object.
+   */
+  class Node {
+  public:
+    /**
+     * Construct a node for a basic block.
+     * @param _bb a basic block
+     * @param _parent the parent of this node, i.e. the loop header of the
+     *                containing loop
+     * @param _isLoopHeader is the basic block a loop header?
+     */
+    Node(const llvm::BasicBlock *_bb, const Node *_parent, bool _isLoopHeader)
+      : bb(_bb), parent(_parent), isLoopHeader(_isLoopHeader) {}
+
+    /**
+     * Add a child to the node.
+     * @param child a child to add to the node
+     */
+    void addChild(Node *child) { children.push_back(child); }
+
+    const llvm::BasicBlock *bb; /* Basic block encapsulated by the node. */
+    const Node *parent; /* Parent node, i.e. header of containing loop. */
+    std::list<Node *> children; /* Regular child nodes in the tree. */
+    bool isLoopHeader; /* Is the basic block a loop header? */
+  };
+
+  unsigned _size; /* Number of nodes (i.e., basic blocks) in the tree. */
+  unsigned _depth; /* Number of nested loops in the tree. */
+  Node *_root; /* Root of the tree, i.e. loop header of outermost loop. */
+
+  /**
+   * Print a node & its children.  Recurses into nested loops.
+   * @param O an output stream on which to print the tree
+   * @param node a node to print
+   * @param depth the current depth
+   */
+  void print(llvm::raw_ostream &O, Node *node, unsigned depth) const;
+
+  /**
+   * Delete the node's children & the node itself.  Recurses into nested loops.
+   * @param node the node being deleted
+   */
+  void deleteRecursive(Node *node);
+
+public:
+  /**
+   * Construct a loop-nesting tree from a strongly-connected component of the
+   * control-flow graph.
+   * @param SCC a strongly-connected component of the control-flow graph
+   * @param LI analysis from the loop info pass
+   */
+  LoopNestingTree(const std::vector<llvm::BasicBlock *> &SCC,
+                  const llvm::LoopInfo &LI);
+
+  /**
+   * Destroy a loop-nesting tree.
+   */
+  ~LoopNestingTree() { deleteRecursive(this->_root); }
+
+  /**
+   * Return the size of the loop-nesting tree, that is the number of nodes in
+   * the loop (and all nested loops).
+   * @return the number of nodes in the tree
+   */
+  unsigned size() const { return this->_size; }
+
+  /**
+   * Return the depth of the loop-nesting tree, that is the number of nested
+   * loops.  A value of one indicates that there are no nested loops.
+   * @return the number of nested loops in the tree
+   */
+  unsigned depth() const { return this->_depth; }
+
+  /**
+   * Print the tree.
+   * @param O the output stream on which to print the tree
+   */
+  void print(llvm::raw_ostream &O) const { print(O, this->_root, 0); }
+
+  /*
+   * Loop-node iterator object.  Delivers loop nodes in breadth-first order.
+   */
+  class loop_iterator {
+  public:
+    typedef loop_iterator self_type;
+    typedef const llvm::BasicBlock *value_type;
+    typedef value_type& reference;
+    typedef value_type* pointer;
+    typedef std::forward_iterator_tag iterator_category;
+
+    self_type operator++(void);
+    self_type operator++(int junk);
+    reference operator*(void) { return cur->bb; }
+    pointer operator->(void) { return &cur->bb; }
+    bool operator==(const self_type& rhs) { return cur == rhs.cur; }
+    bool operator!=(const self_type& rhs) { return cur != rhs.cur; }
+
+    friend class LoopNestingTree;
+    friend class child_iterator;
+  private:
+    Node *cur;
+    std::queue<Node *> remaining;
+
+    loop_iterator(Node *start) : cur(start) { addLoopHeaders(); }
+    void addLoopHeaders(void);
+  };
+
+  /*
+   * Child iterator object.  Traverses children of tree nodes.
+   */
+  class child_iterator {
+  public:
+    typedef child_iterator self_type;
+    typedef const llvm::BasicBlock *value_type;
+    typedef value_type& reference;
+    typedef value_type* pointer;
+    typedef std::forward_iterator_tag iterator_category;
+    enum location { BEGIN, END };
+
+    self_type operator++(void)
+      { self_type me = *this; it.operator++(); return me; }
+    self_type operator++(int junk) { it.operator++(junk); return *this; }
+    reference operator*(void) { return (*it)->bb; }
+    pointer operator->(void) { return &(*it)->bb; }
+    bool operator==(const self_type& rhs) { return it == rhs.it; }
+    bool operator!=(const self_type& rhs) { return it != rhs.it; }
+
+    friend class LoopNestingTree;
+  private:
+    std::list<Node *>::const_iterator it;
+
+    child_iterator(loop_iterator &parent, enum location loc);
+  };
+
+  /**
+   * Return an iterator for traversing all loop nodes (i.e., loop header basic
+   * blocks) in the tree.  Delivers nodes in a breadth-first ordering.
+   * @return an iterator to traverse the loop nodes in the tree
+   */
+  loop_iterator loop_begin() const { loop_iterator it(_root); return it; };
+
+  /**
+   * Return an iterator marking the end of the loop nodes in the tree.
+   * @return an iterator marking the end of the traversal
+   */
+  loop_iterator loop_end() const { loop_iterator it(nullptr); return it; };
+
+  /**
+   * Return an iterator for traversing the children of a loop node.
+   * @param an iterator associated with a loop node
+   * @return an iterator to traverse the children of a loop node
+   */
+  child_iterator children_begin(loop_iterator &parent) const
+    { child_iterator it(parent, child_iterator::BEGIN); return it; }
+
+  /**
+   * Return an iterator marking the end of the children of a loop node.
+   * @param an iterator associated with a loop node
+   * @return an iterator marking the end of the traversal
+   */
+  child_iterator children_end(loop_iterator &parent) const
+    { child_iterator it(parent, child_iterator::END); return it; }
+};
+
+#endif /* _LOOP_NESTING_TREE_H */
diff --git a/llvm/include/llvm/Analysis/LoopPaths.h b/llvm/include/llvm/Analysis/LoopPaths.h
new file mode 100644
index 00000000000..352107a48f3
--- /dev/null
+++ b/llvm/include/llvm/Analysis/LoopPaths.h
@@ -0,0 +1,285 @@
+//===- LoopPaths.h - Enumerate paths in loops -------------------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements analysis which enumerates paths in loops.  In
+// particular, this pass calculates all paths in loops which are of the
+// following form:
+//
+//  - Header to backedge, with no equivalence points on the path
+//  - Header to with equivalence point
+//  - Equivalence point to equivalence point
+//  - Equivalence point to backedge
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_ANALYSIS_LOOPPATHS_H
+#define LLVM_ANALYSIS_LOOPPATHS_H
+
+#include <set>
+#include <vector>
+#include <list>
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/ADT/SetVector.h"
+#include "llvm/Analysis/LoopInfo.h"
+#include "llvm/IR/BasicBlock.h"
+#include "llvm/Pass.h"
+#include "llvm/Support/Debug.h"
+
+namespace llvm {
+
+//===----------------------------------------------------------------------===//
+// Utilities
+//===----------------------------------------------------------------------===//
+
+/// Sort loops based on nesting depth, with deeper-nested loops coming first.
+/// If the depths are equal, sort based on pointer value so that distinct loops
+/// with equal depths are not considered equivalent during insertion.
+struct LoopNestCmp {
+  bool operator() (const Loop * const &A, const Loop * const &B) {
+    unsigned DepthA = A->getLoopDepth(), DepthB = B->getLoopDepth();
+    if(DepthA > DepthB) return true;
+    else if(DepthA < DepthB) return false;
+    else return (uint64_t)A < (uint64_t)B;
+  }
+};
+
+/// A loop nest, sorted by depth (deeper loops are first).
+typedef std::set<Loop *, LoopNestCmp> LoopNest;
+
+/// A set of basic blocks.
+typedef SmallPtrSet<const BasicBlock *, 16> BlockSet;
+
+namespace LoopPathUtilities {
+
+/// Populate a LoopNest by traversing the loop L and its children.  Does *not*
+/// traverse loops containing L (e.g., loops for which L is a child).
+void populateLoopNest(Loop *L, LoopNest &Nest);
+
+/// Get blocks contained in all sub-loops of a loop, including loops nested
+/// deeper than those in immediate sub-loops (e.g., blocks of loop depth 3
+/// inside loop depth 1).
+void getSubBlocks(Loop *L, BlockSet &SubBlocks);
+
+}
+
+//===----------------------------------------------------------------------===//
+// LoopPath helper class
+//===----------------------------------------------------------------------===//
+
+/// Nodes along a path in a loop, represented by a basic block.
+class PathNode {
+private:
+  /// The block encapsulated by the node.
+  const BasicBlock *Block;
+
+  /// Whether or not the block is an exiting block from a sub-loop inside of
+  /// the current path.
+  bool SubLoopExit;
+
+public:
+  PathNode() = delete;
+  PathNode(const BasicBlock *Block, bool SubLoopExit = false)
+    : Block(Block), SubLoopExit(SubLoopExit) {}
+
+  const BasicBlock *getBlock() const { return Block; }
+  bool isSubLoopExit() const { return SubLoopExit; }
+  bool operator<(const PathNode &RHS) const { return Block < RHS.Block; }
+};
+
+/// A path through the loop, which begins/ends either on the loop's header, the
+/// loop's backedge(s) or equivalence points.
+class LoopPath {
+private:
+  /// Nodes that comprise the path.  Iteration over the container is equivalent
+  /// to traversing the path, but container has set semantics for quick
+  /// existence checks.
+  // TODO use a DenseSet for the set template argument, which requires defining
+  // a custom comparator
+  SetVector<PathNode, std::vector<PathNode>, std::set<PathNode> > Nodes;
+
+  /// The path begins & ends on specific instructions.  Note that Start *must*
+  /// be inside the starting block and End *must* be inside the ending block.
+  const Instruction *Start, *End;
+
+  /// Does the path start at the loop header?  If not, it by definition starts
+  /// at an equivalence point.
+  bool StartsAtHeader;
+
+  /// Does the path end at a backedge?  If not, it by definition ends at an
+  /// equivalence point.
+  bool EndsAtBackedge;
+
+public:
+  LoopPath() = delete;
+  LoopPath(const std::vector<PathNode> &NodeVector,
+           const Instruction *Start, const Instruction *End,
+           bool StartsAtHeader, bool EndsAtBackedge);
+
+  bool contains(BasicBlock *BB) const { return Nodes.count(PathNode(BB)); }
+  bool contains(const BasicBlock *BB) const
+  { return Nodes.count(PathNode(BB)); }
+
+  /// Get the starting point of the path, guaranteed to be either the loop
+  /// header or an equivalence point.
+  const PathNode &startNode() const { return Nodes.front(); }
+  const Instruction *startInst() const { return Start; }
+
+  /// Get the the ending point of the path, guaranteed to be either an
+  /// equivalence point or a backedge.
+  const PathNode &endNode() const { return Nodes.back(); }
+  const Instruction *endInst() const { return End; }
+
+  /// Iterators over the path's blocks.
+  SetVector<PathNode>::iterator begin() { return Nodes.begin(); }
+  SetVector<PathNode>::iterator end() { return Nodes.end(); }
+  SetVector<PathNode>::const_iterator cbegin() const { return Nodes.begin(); }
+  SetVector<PathNode>::const_iterator cend() const { return Nodes.end(); }
+
+  /// Return whether the path starts at the loop header or equivalence point.
+  bool startsAtHeader() const { return StartsAtHeader; }
+
+  /// Return whether the path ends at a backedge block or equivalence point.
+  bool endsAtBackedge() const { return EndsAtBackedge; }
+
+  /// Return whether this is a spanning path.
+  bool isSpanningPath() const { return StartsAtHeader && EndsAtBackedge; }
+
+  /// Return whether this is an equivalence point path.
+  bool isEqPointPath() const { return !StartsAtHeader || !EndsAtBackedge; }
+
+  std::string toString() const;
+  void print(raw_ostream &O) const;
+  void dump() const { print(dbgs()); }
+};
+
+//===----------------------------------------------------------------------===//
+// Pass implementation
+//===----------------------------------------------------------------------===//
+
+/// Analyze all paths within a loop nest
+class EnumerateLoopPaths : public FunctionPass {
+private:
+  /// Loop information analysis.
+  LoopInfo *LI;
+
+  /// All calculated paths for each analyzed loop.
+  DenseMap<const Loop *, std::vector<LoopPath> > Paths;
+
+  /// Whether there are paths of each type through a basic block in a loop.
+  /// These are *only* maintained for the current loop, not any sub-loops.
+  DenseMap<const Loop *, DenseMap<const BasicBlock *, bool> >
+  HasSpPath, HasEqPointPath;
+
+  /// Information about the loop currently being analyzed
+  Loop *CurLoop;
+  SmallPtrSet<const BasicBlock *, 4> Latches;
+  BlockSet SubLoopBlocks;
+
+  /// Set if analysis had to bail out because there are too many paths through
+  /// the function.
+  bool TooManyPaths;
+
+  /// Set if analysis had to bail out because it found a non-loop cycle due to
+  /// complect control flow (usually due to goto's).
+  bool DetectedCycle;
+
+  /// Depth-first search information for the current path being explored.
+  struct LoopDFSInfo {
+  public:
+    const Instruction *Start;
+    std::vector<PathNode> PathNodes;
+    bool StartsAtHeader;
+  };
+
+  /// Empty all data structures.
+  void reset() {
+    Paths.clear();
+    HasSpPath.clear();
+    HasEqPointPath.clear();
+    CurLoop = nullptr;
+    Latches.clear();
+    SubLoopBlocks.clear();
+  }
+
+  /// Search exit blocks of the loop containing Successor.  Add the terminating
+  /// instruction of the block to either of the two vectors, depending if there
+  /// is a path of either type through the exit block.
+  inline void getSubLoopSuccessors(const BasicBlock *Successor,
+                                   std::vector<const Instruction *> &EqPoint,
+                                   std::vector<const Instruction *> &Spanning);
+
+  /// Run a depth-first search for paths in the loop starting at an instruction.
+  /// Any paths found are added to the vector of paths, and new paths to explore
+  /// are added to the search list.
+  bool loopDFS(const Instruction *I,
+               LoopDFSInfo &DFSI,
+               std::vector<LoopPath> &CurPaths,
+               std::list<const Instruction *> &NewPaths);
+
+  /// Enumerate all paths within a loop, stored in the vector argument.  Return
+  /// true if the analysis was successful or false otherwise.
+  bool analyzeLoop(Loop *L, std::vector<LoopPath> &CurPaths);
+
+public:
+  static char ID;
+  EnumerateLoopPaths() : FunctionPass(ID) {}
+
+  /// Pass interface implementation.
+  void getAnalysisUsage(AnalysisUsage &AU) const override;
+  bool runOnFunction(Function &F) override;
+
+  /// Re-run analysis to enumerate paths through a loop.  Invalidates all APIs
+  /// below which populate containers with paths (for this loop only).
+  void rerunOnLoop(Loop *L);
+
+  /// Query whether analysis failed.
+  bool analysisFailed() const { return TooManyPaths || DetectedCycle; }
+
+  /// Query whether there were too many paths to enumerate in the function.
+  bool tooManyPaths() const { return TooManyPaths; }
+
+  /// Query whether analysis detected a non-loop cycle.
+  bool detectedCycle() const { return DetectedCycle; }
+
+  bool hasPaths(const Loop *L) const { return Paths.count(L); }
+
+  /// Get all the paths through a loop.  Paths in the vector are ordered as
+  /// they were discovered in the depth-first traversal of the loop.
+  void getPaths(const Loop *L, std::vector<const LoopPath *> &P) const;
+
+  /// Get all paths through a loop that end at a backedge.
+  void getBackedgePaths(const Loop *L, std::vector<const LoopPath *> &P) const;
+  void getBackedgePaths(const Loop *L, std::set<const LoopPath *> &P) const;
+
+  /// Get all spanning paths through a loop, where a spanning path is defined
+  /// as starting at the first instruction of the header of the loop and ending
+  /// at the branch in a latch.
+  void getSpanningPaths(const Loop *L, std::vector<const LoopPath *> &P) const;
+  void getSpanningPaths(const Loop *L, std::set<const LoopPath *> &P) const;
+
+  /// Get all the paths through the loop that begin and/or end at an
+  /// equivalence point.
+  void getEqPointPaths(const Loop *L, std::vector<const LoopPath *> &P) const;
+  void getEqPointPaths(const Loop *L, std::set<const LoopPath *> &P) const;
+
+  /// Get all the paths through a loop that contain a given basic block.
+  void getPathsThroughBlock(const Loop *L, BasicBlock *BB,
+                            std::vector<const LoopPath *> &P) const;
+  void getPathsThroughBlock(const Loop *L, BasicBlock *BB,
+                            std::set<const LoopPath *> &P) const;
+
+  /// Return whether there is each type of path through a basic block.
+  bool spanningPathThroughBlock(const Loop *L, const BasicBlock *BB) const;
+  bool eqPointPathThroughBlock(const Loop *L, const BasicBlock *BB) const;
+};
+
+}
+
+#endif
diff --git a/llvm/include/llvm/Analysis/Passes.h b/llvm/include/llvm/Analysis/Passes.h
index d9c97dff8c6..bfb2210b6b0 100644
--- a/llvm/include/llvm/Analysis/Passes.h
+++ b/llvm/include/llvm/Analysis/Passes.h
@@ -103,6 +103,33 @@ namespace llvm {
   //
   FunctionPass *createMustExecutePrinter();
 
+  //===--------------------------------------------------------------------===//
+  //
+  // createPopcornCompatibilityPass - This pass analyzes & warns users about
+  // code features not yet supported by the Popcorn compiler, runtime & OS.
+  //
+  FunctionPass *createPopcornCompatibilityPass();
+
+  //===--------------------------------------------------------------------===//
+  //
+  // createLiveValuesPass - This pass calculates live-value sets for basic
+  // blocks in a function.
+  //
+  FunctionPass *createLiveValuesPass();
+
+  //===--------------------------------------------------------------------===//
+  //
+  // createEnumerateLoopPathsPass - This pass calculates all paths between
+  // equivalence points within a loop.
+  //
+  FunctionPass *createEnumerateLoopPathsPass();
+
+  //===--------------------------------------------------------------------===//
+  //
+  // createSelectMigrationPointsPass - This pass analyzes and marks instructions
+  // inside of functions to be migration points.
+  //
+  FunctionPass *createSelectMigrationPointsPass();
 }
 
 #endif
diff --git a/llvm/include/llvm/Analysis/PopcornUtil.h b/llvm/include/llvm/Analysis/PopcornUtil.h
new file mode 100644
index 00000000000..98da5b9ac49
--- /dev/null
+++ b/llvm/include/llvm/Analysis/PopcornUtil.h
@@ -0,0 +1,187 @@
+//===- LoopPaths.h - Enumerate paths in loops -------------------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file provides Popcorn-specific utility APIs.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_ANALYSIS_POPCORNUTIL_H
+#define LLVM_ANALYSIS_POPCORNUTIL_H
+
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/IR/CallSite.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/Module.h"
+
+namespace llvm {
+namespace Popcorn {
+
+#define POPCORN_META "popcorn"
+#define POPCORN_MIGPOINT "migpoint"
+#define POPCORN_HTM_BEGIN "htmbegin"
+#define POPCORN_HTM_END "htmend"
+
+/// Add named metadata node with string operand to an instruction.
+static inline void addMetadata(Instruction *I, StringRef name, StringRef op) {
+  SmallVector<Metadata *, 2> MetaOps;
+  LLVMContext &C = I->getContext();
+  MDNode *MetaNode = I->getMetadata(name);
+
+  if(MetaNode) {
+    for(auto &Op : MetaNode->operands()) {
+      if(isa<MDString>(Op) && cast<MDString>(Op)->getString() == op) return;
+      else MetaOps.push_back(Op);
+    }
+  }
+
+  MetaOps.push_back(MDString::get(C, op));
+  MetaNode = MDNode::get(C, MetaOps);
+  I->setMetadata(name, MetaNode);
+}
+
+/// Remove string operand from named metadata node.
+static inline void
+removeMetadata(Instruction *I, StringRef name, StringRef op) {
+  SmallVector<Metadata *, 2> MetaOps;
+  MDNode *MetaNode = I->getMetadata(name);
+
+  if(MetaNode) {
+    for(auto &Op : MetaNode->operands()) {
+      if(isa<MDString>(Op) && cast<MDString>(Op)->getString() == op) continue;
+      else MetaOps.push_back(Op);
+    }
+
+    if(MetaOps.size()) {
+      MetaNode = MDNode::get(I->getContext(), MetaOps);
+      I->setMetadata(name, MetaNode);
+    }
+    else I->setMetadata(name, nullptr);
+  }
+}
+
+/// Check to see if instruction has named metadata node with string operand.
+static inline bool
+hasMetadata(const Instruction *I, StringRef name, StringRef op) {
+  const MDNode *MetaNode = I->getMetadata(name);
+  if(MetaNode)
+    for(auto &Op : MetaNode->operands())
+      if(isa<MDString>(Op) && cast<MDString>(Op)->getString() == op)
+        return true;
+  return false;
+}
+
+/// Return whether the instruction is a "true" call site, i.e., not an LLVM
+/// IR-level intrinsic or inline assembly.
+static inline bool isCallSite(const Instruction *I) {
+  if((isa<CallInst>(I) || isa<InvokeInst>(I)) && !isa<IntrinsicInst>(I)) {
+    ImmutableCallSite CS(I);
+    if(!CS.isInlineAsm()) return true;
+  }
+  return false;
+}
+
+/// Add metadata to an instruction marking it as an equivalence point.
+static inline void addEquivalencePointMetadata(Instruction *I) {
+  addMetadata(I, POPCORN_META, POPCORN_MIGPOINT);
+}
+
+/// Remove metadata from an instruction marking it as an equivalence point.
+static inline void removeEquivalencePointMetadata(Instruction *I) {
+  removeMetadata(I, POPCORN_META, POPCORN_MIGPOINT);
+}
+
+static inline bool hasEquivalencePointMetadata(Instruction *I) {
+  return hasMetadata(I , POPCORN_META, POPCORN_MIGPOINT);
+}
+
+/// Return whether an instruction is an equivalence point.  The instruction must
+/// satisfy one of the following:
+///
+/// 1. Is a function call site (not an intrinsic function call)
+/// 2. Analysis has tagged the instruction with appropriate metadata
+static inline bool isEquivalencePoint(const Instruction *I) {
+  if(isCallSite(I)) return true;
+  else return hasMetadata(I, POPCORN_META, POPCORN_MIGPOINT);
+}
+
+/// Add metadata to an instruction marking it as an HTM begin point.
+static inline void addHTMBeginMetadata(Instruction *I) {
+  addMetadata(I, POPCORN_META, POPCORN_HTM_BEGIN);
+}
+
+/// Remove metadata from an instruction marking it as an HTM begin point.
+static inline void removeHTMBeginMetadata(Instruction *I) {
+  removeMetadata(I, POPCORN_META, POPCORN_HTM_BEGIN);
+}
+
+/// Return whether an instruction is an HTM begin point.
+static inline bool isHTMBeginPoint(Instruction *I) {
+  return hasMetadata(I, POPCORN_META, POPCORN_HTM_BEGIN);
+}
+
+/// Add metadata to an instruction marking it as an HTM end point.
+static inline void addHTMEndMetadata(Instruction *I) {
+  addMetadata(I, POPCORN_META, POPCORN_HTM_END);
+}
+
+/// Remove metadata from an instruction marking it as an HTM end point.
+static inline void removeHTMEndMetadata(Instruction *I) {
+  removeMetadata(I, POPCORN_META, POPCORN_HTM_END);
+}
+
+/// Return whether an instruction is an HTM end point.
+static inline bool isHTMEndPoint(Instruction *I) {
+  return hasMetadata(I, POPCORN_META, POPCORN_HTM_END);
+}
+
+#define POPCORN_INST_KEY "popcorn-inst-ty"
+
+/// Type of instrumentation.
+enum InstrumentType {
+  HTM = 0,
+  Cycles,
+  None,
+  NumVals // Don't use!
+};
+
+/// Mark the module as having a certain instrumentation type.
+static inline void
+setInstrumentationType(Module &M, enum InstrumentType Ty) {
+  switch(Ty) {
+  case HTM:
+    M.addModuleFlag(Module::Error, POPCORN_INST_KEY, HTM);
+    break;
+  case Cycles:
+    M.addModuleFlag(Module::Error, POPCORN_INST_KEY, Cycles);
+    break;
+  case None:
+    M.addModuleFlag(Module::Error, POPCORN_INST_KEY, None);
+    break;
+  default: llvm_unreachable("Unknown instrumentation type"); break;
+  }
+}
+
+/// Get the type of instrumentation.
+static inline enum InstrumentType getInstrumentationType(Module &M) {
+  Metadata *MD = M.getModuleFlag(POPCORN_INST_KEY);
+  if(MD) {
+    ConstantAsMetadata *Val = cast<ConstantAsMetadata>(MD);
+    ConstantInt *IntVal = cast<ConstantInt>(Val->getValue());
+    uint64_t RawVal = IntVal->getZExtValue();
+    assert(RawVal < NumVals && "Invalid instrumentation type");
+    return (enum InstrumentType)RawVal;
+  }
+  else return None;
+}
+
+}
+}
+
+#endif
diff --git a/llvm/include/llvm/CodeGen/AsmPrinter.h b/llvm/include/llvm/CodeGen/AsmPrinter.h
index d110f8b01cb..c4e131336b7 100644
--- a/llvm/include/llvm/CodeGen/AsmPrinter.h
+++ b/llvm/include/llvm/CodeGen/AsmPrinter.h
@@ -22,6 +22,8 @@
 #include "llvm/CodeGen/AsmPrinterHandler.h"
 #include "llvm/CodeGen/DwarfStringPoolEntry.h"
 #include "llvm/CodeGen/MachineFunctionPass.h"
+#include "llvm/CodeGen/MachineModuleInfo.h"
+#include "llvm/CodeGen/MachineModuleInfoImpls.h"
 #include "llvm/IR/InlineAsm.h"
 #include "llvm/IR/LLVMContext.h"
 #include "llvm/Support/ErrorHandling.h"
@@ -212,8 +214,11 @@ public:
   /// Return information about object file lowering.
   const TargetLoweringObjectFile &getObjFileLowering() const;
 
+  // CJP: StackMap workaround.
   /// Return information about data layout.
-  const DataLayout &getDataLayout() const;
+  const DataLayout &getDataLayout() const {
+    return MMI->getModule()->getDataLayout();
+  }
 
   /// Return the pointer size from the TargetMachine
   unsigned getPointerSize() const;
@@ -293,9 +298,10 @@ public:
 
   /// Emit the specified function out to the OutStreamer.
   bool runOnMachineFunction(MachineFunction &MF) override {
+    bool modified = TagCallSites(MF);
     SetupMachineFunction(MF);
     EmitFunctionBody();
-    return false;
+    return modified;
   }
 
   //===------------------------------------------------------------------===//
@@ -433,6 +439,13 @@ public:
   /// instructions in verbose mode.
   virtual void emitImplicitDef(const MachineInstr *MI) const;
 
+  /// Some machine instructions encapsulate a call with follow-on boilerplate
+  /// instructions, meaning labels emitted after the "instruction" do not
+  /// capture the call's true return address.  Return an offset for correcting
+  /// these labels to refer to the call's actual return address.
+  virtual int getCanonicalReturnAddr(const MachineInstr *Call) const
+  { return 0; }
+
   //===------------------------------------------------------------------===//
   // Symbol Lowering Routines.
   //===------------------------------------------------------------------===//
@@ -504,6 +517,14 @@ public:
   /// Emit something like ".long Label + Offset".
   void EmitDwarfOffset(const MCSymbol *Label, uint64_t Offset) const;
 
+  /// Find the stackmap intrinsic associated with a function call
+  MachineInstr *FindPcnStackMap(MachineBasicBlock &MBB,
+				MachineInstr *MI) const;
+
+  /// Move stackmap intrinsics directly after calls to correctly capture
+  /// return addresses
+  bool TagCallSites(MachineFunction &MF);
+
   //===------------------------------------------------------------------===//
   // Dwarf Emission Helper Routines
   //===------------------------------------------------------------------===//
diff --git a/llvm/include/llvm/CodeGen/FastISel.h b/llvm/include/llvm/CodeGen/FastISel.h
index f09b59daf4d..d7779f3353c 100644
--- a/llvm/include/llvm/CodeGen/FastISel.h
+++ b/llvm/include/llvm/CodeGen/FastISel.h
@@ -529,7 +529,7 @@ protected:
   bool selectBinaryOp(const User *I, unsigned ISDOpcode);
   bool selectFNeg(const User *I, const Value *In);
   bool selectGetElementPtr(const User *I);
-  bool selectStackmap(const CallInst *I);
+  bool selectStackmap(const CallInst *I, unsigned Opcode);
   bool selectPatchpoint(const CallInst *I);
   bool selectCall(const User *I);
   bool selectIntrinsicCall(const IntrinsicInst *II);
diff --git a/llvm/include/llvm/CodeGen/MachineFrameInfo.h b/llvm/include/llvm/CodeGen/MachineFrameInfo.h
index 761735120a6..b0eb4eccca5 100644
--- a/llvm/include/llvm/CodeGen/MachineFrameInfo.h
+++ b/llvm/include/llvm/CodeGen/MachineFrameInfo.h
@@ -232,6 +232,10 @@ private:
   /// to builtin \@llvm.experimental.stackmap.
   bool HasStackMap = false;
 
+  /// This boolean keeps track of whether there is a call
+  /// to builtin \@llvm.experimental.pcn_stackmap.
+  bool HasPcnStackMap = false;
+  
   /// This boolean keeps track of whether there is a call
   /// to builtin \@llvm.experimental.patchpoint.
   bool HasPatchPoint = false;
@@ -377,6 +381,12 @@ public:
   bool hasStackMap() const { return HasStackMap; }
   void setHasStackMap(bool s = true) { HasStackMap = s; }
 
+  /// This method may be called any time after instruction
+  /// selection is complete to determine if there is a call to builtin
+  /// \@llvm.experimental.pcn.stackmap.
+  bool hasPcnStackMap() const { return HasPcnStackMap; }
+  void setHasPcnStackMap(bool s = true) { HasPcnStackMap = s; }
+
   /// This method may be called any time after instruction
   /// selection is complete to determine if there is a call to builtin
   /// \@llvm.experimental.patchpoint.
diff --git a/llvm/include/llvm/CodeGen/MachineFunction.h b/llvm/include/llvm/CodeGen/MachineFunction.h
index 60c13468542..176de872b4f 100644
--- a/llvm/include/llvm/CodeGen/MachineFunction.h
+++ b/llvm/include/llvm/CodeGen/MachineFunction.h
@@ -30,6 +30,8 @@
 #include "llvm/CodeGen/MachineBasicBlock.h"
 #include "llvm/CodeGen/MachineInstr.h"
 #include "llvm/CodeGen/MachineMemOperand.h"
+#include "llvm/CodeGen/StackTransformTypes.h"
+#include "llvm/IR/Instructions.h"
 #include "llvm/Support/Allocator.h"
 #include "llvm/Support/ArrayRecycler.h"
 #include "llvm/Support/AtomicOrdering.h"
@@ -341,6 +343,16 @@ class MachineFunction {
 
   EHPersonality PersonalityTypeCache = EHPersonality::Unknown;
 
+  /// Information about changes in number of IR operands to number of machine
+  /// /// operands due to legalization.
+  SMToOpLegalizeMap SMOpLegalizeChanges;
+
+  /// Duplicate live value locations for stackmap operands
+  InstToOperands SMDuplicateLocs;
+
+  /// Architecture-specific live value locations for each stackmap
+  InstToArchLiveValues SMArchSpecificLocs;
+
   /// \}
 
   /// Clear all the members of this MachineFunction, but the ones used
@@ -783,6 +795,9 @@ public:
   /// Allocate and initialize a register mask with @p NumRegister bits.
   uint32_t *allocateRegMask();
 
+  /// Is a register caller-saved?
+  bool isCallerSaved(unsigned Reg) const;
+
   /// Allocate and construct an extra info structure for a `MachineInstr`.
   ///
   /// This is allocated on the function's allocator and so lives the life of
@@ -986,6 +1001,46 @@ public:
   /// call instruction with new one.
   void updateCallSiteInfo(const MachineInstr *Old,
                           const MachineInstr *New = nullptr);
+
+  //===--------------------------------------------------------------------===//
+  // Additional stack transformation metadata
+  //
+
+  /// Add a note indicating that machine operand number OpNo was changed to Num
+  /// operands in stackmap SMID.
+  void addOpLegalizeChange(int64_t SMID, unsigned OpNo, unsigned Num);
+
+  /// Add an IR/architecture-specific location mapping for a stackmap operand
+  void addSMOpLocation(const CallInst *SM, const Value *Val,
+                       const MachineLiveLoc &MLL);
+  void addSMOpLocation(const CallInst *SM, unsigned Op,
+                       const MachineLiveLoc &MLL);
+
+  /// Add an architecture-specific live value & location for a stackmap
+  void addSMArchSpecificLocation(const CallInst *SM,
+                                 const MachineLiveLoc &MLL,
+                                 const MachineLiveVal &MLV);
+
+  /// Update stack slot references to new indexes after stack slot coloring
+  void updateSMStackSlotRefs(SmallDenseMap<int, int, 16> &Changes);
+
+  /// Return the number of machine operands corresponding to a given IR operand.
+  unsigned getNumLegalizedOps(int64_t SMID, unsigned OpNo) const;
+
+  /// Are there any architecture-specific locations for operand Val in stackmap
+  /// SM?
+  bool hasSMOpLocations(const CallInst *SM, const Value *Val) const;
+
+  /// Are there any architecture-specific locations for stackmap SM?
+  bool hasSMArchSpecificLocations(const CallInst *SM) const;
+
+  /// Return the architecture-specific locations for a stackmap operand.
+  const MachineLiveLocs &getSMOpLocations(const CallInst *SM,
+                                          const Value *Val) const;
+
+  /// Return the architecture-specific locations for a stackmap that are not
+  /// associated with any operand.
+  const ArchLiveValues &getSMArchSpecificLocations(const CallInst *SM) const;
 };
 
 //===--------------------------------------------------------------------===//
diff --git a/llvm/include/llvm/CodeGen/Passes.h b/llvm/include/llvm/CodeGen/Passes.h
index d92ee93268e..01bc142484a 100644
--- a/llvm/include/llvm/CodeGen/Passes.h
+++ b/llvm/include/llvm/CodeGen/Passes.h
@@ -129,6 +129,10 @@ namespace llvm {
   // instruction and update the MachineFunctionInfo with that information.
   extern char &ShrinkWrapID;
 
+  /// Stack transformation metadata pass.  Gather additional stack
+  /// transformation metadata from machine functions.
+  extern char &StackTransformMetadataID;
+
   /// LiveRangeShrink pass. Move instruction close to its definition to shrink
   /// the definition's live range.
   extern char &LiveRangeShrinkID;
@@ -407,6 +411,17 @@ namespace llvm {
   /// protect against stack-based overflow vulnerabilities.
   FunctionPass *createSafeStackPass();
 
+  /// \brief This pass inserts equivalence points into functions.
+  FunctionPass *createMigrationPointsPass();
+
+  /// \brief This pass inserts stack map intrinsics at equivalence points in
+  /// order to record live value locations
+  ModulePass *createInsertStackMapsPass();
+
+  /// \brief This pass inserts stack map intrinsics similarly to InsertStackMaps,
+  /// but only in thread start functions inside of libc
+  ModulePass *createLibcStackMapsPass();
+
   /// This pass detects subregister lanes in a virtual register that are used
   /// independently of other lanes and splits them into separate virtual
   /// registers.
diff --git a/llvm/include/llvm/CodeGen/StackMaps.h b/llvm/include/llvm/CodeGen/StackMaps.h
index d7d88de6f68..d3bd1e2830e 100644
--- a/llvm/include/llvm/CodeGen/StackMaps.h
+++ b/llvm/include/llvm/CodeGen/StackMaps.h
@@ -12,6 +12,7 @@
 #include "llvm/ADT/MapVector.h"
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/CodeGen/MachineInstr.h"
+#include "llvm/CodeGen/StackTransformTypes.h"
 #include "llvm/IR/CallingConv.h"
 #include "llvm/MC/MCSymbol.h"
 #include "llvm/Support/Debug.h"
@@ -27,6 +28,7 @@ class MCExpr;
 class MCStreamer;
 class raw_ostream;
 class TargetRegisterInfo;
+class UnwindInfo;
 
 /// MI-level stackmap operands.
 ///
@@ -201,14 +203,35 @@ public:
       Constant,
       ConstantIndex
     };
-    LocationType Type = Unprocessed;
-    unsigned Size = 0;
-    unsigned Reg = 0;
-    int64_t Offset = 0;
-
-    Location() = default;
+    LocationType Type;
+    unsigned Size;
+    unsigned Reg;
+    int64_t Offset;
+    bool Ptr;
+    bool Alloca;
+    bool Duplicate;
+    bool Temporary;
+    unsigned AllocaSize;
+
+    Location() {
+      Type = Unprocessed;
+      Size = 0;
+      Reg = 0;
+      Offset = 0;
+      Ptr = false;
+      Alloca = false;
+      Duplicate = false;
+      Temporary = false;
+      AllocaSize = 0;
+    }
     Location(LocationType Type, unsigned Size, unsigned Reg, int64_t Offset)
         : Type(Type), Size(Size), Reg(Reg), Offset(Offset) {}
+    Location(LocationType Type, unsigned Size, unsigned Reg, int64_t Offset,
+	     bool Ptr, bool Alloca, bool Duplicate, bool Temporary,
+	     unsigned AllocaSize)
+        : Type(Type), Size(Size), Reg(Reg), Offset(Offset), Ptr(Ptr),
+          Alloca(Alloca), Duplicate(Duplicate), Temporary(Temporary),
+          AllocaSize(AllocaSize) {}
   };
 
   struct LiveOutReg {
@@ -222,10 +245,34 @@ public:
         : Reg(Reg), DwarfRegNum(DwarfRegNum), Size(Size) {}
   };
 
+  struct Operation {
+    ValueGenInst::InstType InstType;
+    Location::LocationType OperandType;
+    unsigned Size;
+    unsigned DwarfReg;
+    int64_t Constant;
+    bool isGenerated;
+    bool isSymbol;
+    const MCSymbol *Symbol;
+
+    Operation() {
+      Size = 0;
+      DwarfReg = 0;
+      Constant = 0;
+      isGenerated = false;
+      isSymbol = false;
+      Symbol = nullptr;
+    }
+    Operation(unsigned Size, unsigned DwarfReg, int64_t, bool isGenerated,
+	      bool isSymbol)
+      : Size(0), DwarfReg(0), Constant(0), isGenerated(false),
+        isSymbol(false), Symbol(nullptr) {}
+  };
+
   // OpTypes are used to encode information about the following logical
   // operand (which may consist of several MachineOperands) for the
   // OpParser.
-  using OpType = enum { DirectMemRefOp, IndirectMemRefOp, ConstantOp };
+  using OpType = enum { DirectMemRefOp, IndirectMemRefOp, ConstantOp, TemporaryOp };
 
   StackMaps(AsmPrinter &AP);
 
@@ -247,17 +294,28 @@ public:
     explicit FunctionInfo(uint64_t StackSize) : StackSize(StackSize) {}
   };
 
+  using ArchValue = std::pair<Location, Operation>;
+  using ArchValues = SmallVector<ArchValue, 8>;
+
   struct CallsiteInfo {
+    const MCSymbol *Func = nullptr;
     const MCExpr *CSOffsetExpr = nullptr;
     uint64_t ID = 0;
     LocationVec Locations;
     LiveOutVec LiveOuts;
+    ArchValues Vals;
 
     CallsiteInfo() = default;
     CallsiteInfo(const MCExpr *CSOffsetExpr, uint64_t ID,
                  LocationVec &&Locations, LiveOutVec &&LiveOuts)
         : CSOffsetExpr(CSOffsetExpr), ID(ID), Locations(std::move(Locations)),
           LiveOuts(std::move(LiveOuts)) {}
+    CallsiteInfo(const MCSymbol *Func, const MCExpr *CSOffsetExpr,
+		 uint64_t ID, LocationVec &&Locations,
+		 LiveOutVec &&LiveOuts, ArchValues &&Vals)
+        : Func(Func), CSOffsetExpr(CSOffsetExpr), ID(ID),
+	  Locations(std::move(Locations)),
+          LiveOuts(std::move(LiveOuts)), Vals(std::move(Vals)) {}
   };
 
   using FnInfoMap = MapVector<const MCSymbol *, FunctionInfo>;
@@ -268,6 +326,9 @@ public:
   /// MI must be a raw STACKMAP, not a PATCHPOINT.
   void recordStackMap(const MachineInstr &MI);
 
+  /// MI must be a raw PCN_STACKMAP, not a STACKMAP.
+  void recordPcnStackMap(const MachineInstr &MI);
+  
   /// Generate a stackmap record for a patchpoint instruction.
   void recordPatchPoint(const MachineInstr &MI);
 
@@ -278,6 +339,7 @@ public:
   /// the map info into it. This clears the stack map data structures
   /// afterwards.
   void serializeToStackMapSection();
+  void serializeToPcnStackMapSection(const UnwindInfo *UI = nullptr);
 
   /// Get call site info.
   CallsiteInfoList &getCSInfos() { return CSInfos; }
@@ -293,6 +355,23 @@ private:
   ConstantPool ConstPool;
   FnInfoMap FnInfos;
 
+  /// Get stackmap information for register location
+  void getRegLocation(unsigned Phys, unsigned &Dwarf, unsigned &Offset) const;
+
+  /// Get pointer typing information for stackmap operand
+  void getPointerInfo(const Value *Op, const DataLayout &DL, bool &isPtr,
+                      bool &isAlloca, unsigned &AllocaSize) const;
+
+  /// Add duplicate target-specific locations for a stackmap operand
+  void addDuplicateLocs(const CallInst *StackMap, const Value *Oper,
+                        LocationVec &Locs, unsigned Size, bool Ptr,
+                        bool Alloca, unsigned AllocaSize) const;
+
+  MachineInstr::const_mop_iterator
+  parsePcnOperand(MachineInstr::const_mop_iterator MOI,
+               MachineInstr::const_mop_iterator MOE, LocationVec &Locs,
+               LiveOutVec &LiveOuts, User::const_op_iterator &Op) const;
+
   MachineInstr::const_mop_iterator
   parseOperand(MachineInstr::const_mop_iterator MOI,
                MachineInstr::const_mop_iterator MOE, LocationVec &Locs,
@@ -306,6 +385,15 @@ private:
   /// registers that need to be recorded in the stackmap.
   LiveOutVec parseRegisterLiveOutMask(const uint32_t *Mask) const;
 
+  /// Convert a list of instructions used to generate an architecture-specific
+  /// live value into multiple individual records.
+  void genArchValsFromInsts(ArchValues &AV,
+                            Location &Loc,
+                            const MachineLiveVal &MLV);
+
+  /// Add architecture-specific locations for the stackmap.
+  void addArchLiveVals(const CallInst *SM, ArchValues &AV);
+
   /// This should be called by the MC lowering code _immediately_ before
   /// lowering the MI to an MCInst. It records where the operands for the
   /// instruction are stored, and outputs a label to record the offset of
@@ -316,17 +404,24 @@ private:
                            MachineInstr::const_mop_iterator MOE,
                            bool recordResult = false);
 
+  void recordPcnStackMapOpers(const MachineInstr &MI, uint64_t ID,
+			      MachineInstr::const_mop_iterator MOI,
+			      MachineInstr::const_mop_iterator MOE,
+			      bool recordResult = false);
+
   /// Emit the stackmap header.
   void emitStackmapHeader(MCStreamer &OS);
 
   /// Emit the function frame record for each function.
   void emitFunctionFrameRecords(MCStreamer &OS);
+  void emitPcnFunctionFrameRecords(MCStreamer &OS, const UnwindInfo *UI);
 
   /// Emit the constant pool.
   void emitConstantPoolEntries(MCStreamer &OS);
 
   /// Emit the callsite info for each stackmap/patchpoint intrinsic call.
   void emitCallsiteEntries(MCStreamer &OS);
+  void emitPcnCallsiteEntries(MCStreamer &OS);
 
   void print(raw_ostream &OS);
   void debug() { print(dbgs()); }
diff --git a/llvm/include/llvm/CodeGen/StackTransformTypes.def b/llvm/include/llvm/CodeGen/StackTransformTypes.def
new file mode 100644
index 00000000000..79f2d86cb8a
--- /dev/null
+++ b/llvm/include/llvm/CodeGen/StackTransformTypes.def
@@ -0,0 +1,35 @@
+//===-- llvm/Target/StackTransformTypes.def - Generator Opcodes -*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Macros which define the set of available instructions for the ISA-agnostic
+// value generator.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_CODEGEN_STACKTRANSFORMTYPES_DEF
+#define LLVM_CODEGEN_STACKTRANSFORMTYPES_DEF
+
+// TODO generate using TableGen rather than X macros
+
+/// Each instruction is defined by a mnemonic and an operand (represented using
+/// the various ValueGenInst types).
+#define VALUE_GEN_INST \
+  X(Set)             /* Set the destination to another value */ \
+  X(Add)             /* Add a value to the destination */ \
+  X(Subtract)        /* Subtract a value from the destination */ \
+  X(Multiply)        /* Multiply the destination by another value */ \
+  X(Divide)          /* Divide the destination by another value */ \
+  X(LeftShift)       /* Left-shift the destination */ \
+  X(RightShiftLog)   /* Right-shift (logical) the destination */ \
+  X(RightShiftArith) /* Right-shift (arithmetic) the destination */ \
+  X(Mask)            /* Apply bit mask to the destination */ \
+  X(Load32)          /* Load 32 bits from memory */ \
+  X(Load64)          /* Load 64 bits from memory */
+
+#endif
diff --git a/llvm/include/llvm/CodeGen/StackTransformTypes.h b/llvm/include/llvm/CodeGen/StackTransformTypes.h
new file mode 100644
index 00000000000..ca765bc956a
--- /dev/null
+++ b/llvm/include/llvm/CodeGen/StackTransformTypes.h
@@ -0,0 +1,586 @@
+//===------- StackTransformTypes.h - Stack Transform Types ------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_CODEGEN_STACKTRANFORMTYPES_H
+#define LLVM_CODEGEN_STACKTRANFORMTYPES_H
+
+#include <cstdint>
+#include <string>
+#include <vector>
+#include <map>
+#include <memory>
+#include "llvm/CodeGen/MachineOperand.h"
+#include "llvm/CodeGen/StackTransformTypes.def"
+#include "llvm/ADT/SmallVector.h"
+
+namespace llvm {
+
+class AsmPrinter;
+class Instruction;
+class MachineInstr;
+class MCSymbol;
+class Value;
+
+//===----------------------------------------------------------------------===//
+// Types for generating more complex architecture-specific live values
+//
+
+#define INV_INST_TYPE "Invalid instruction type"
+
+/// ValueGenInst - an instruction for the transformation runtime which generates
+/// a live value.  These instructions specify a simple operation (e.g., add) and
+/// an operand.  These instructions, coupled together with a destination
+/// location (i.e., a register or stack slot), allow the runtime to construct
+/// more complex live values like bit-shifts or pointers into arrays of structs.
+class ValueGenInst {
+public:
+  virtual ~ValueGenInst() {}
+
+  /// Instruction types
+  enum InstType {
+#define X(type) type,
+    VALUE_GEN_INST
+#undef X
+  };
+  virtual InstType type() const = 0;
+
+  /// Operand types
+  enum OpType { Register, Immediate, Reference };
+  virtual OpType opType() const = 0;
+
+  /// Equivalence checking.  Depends on both instruction & operand type, and
+  /// any operand-specific information.
+  virtual bool operator==(const ValueGenInst &RHS) const = 0;
+
+  /// Get a human-readable name for the instruction type
+  static const char *getInstName(enum InstType Type);
+  static std::string getInstNameStr(enum InstType Type);
+
+  /// Get a human-readable description of the instruction & operand
+  virtual std::string str() const = 0;
+private:
+  static const char *InstTypeStr[];
+};
+
+/// RegInstructionBase - base class for register operand instructions.  The
+/// register is stored as an architecture-specific physical register.
+class RegInstructionBase : public ValueGenInst {
+public:
+  virtual OpType opType() const { return Register; }
+  unsigned getReg() const { return Reg; }
+  void setReg(unsigned Reg) { this->Reg = Reg; }
+
+protected:
+  /// The register used in the instruction
+  // Note: will be converted to DWARF during metadata emission
+  unsigned Reg;
+
+  RegInstructionBase(unsigned Reg) : Reg(Reg) {}
+};
+
+/// RegInstruction<T> - register-based instructions.  Instructions are
+/// specified via template argument.
+template<ValueGenInst::InstType Type>
+class RegInstruction : public RegInstructionBase {
+  static_assert(Type == Set || Type == Add || Type == Subtract ||
+                Type == Multiply || Type == Divide,
+                INV_INST_TYPE " for register instruction");
+public:
+  RegInstruction(unsigned Reg) : RegInstructionBase(Reg) {}
+
+  virtual InstType type() const { return Type; }
+
+  virtual bool operator==(const ValueGenInst &RHS) const {
+    if(RHS.type() == Type && RHS.opType() == Register) {
+      const RegInstruction<Type> &RI = (const RegInstruction<Type> &)RHS;
+      if(RI.Reg == Reg) return true;
+    }
+    return false;
+  }
+
+  virtual std::string str() const
+  { return getInstNameStr(Type) + " register " + std::to_string(Reg); }
+};
+
+/// ImmInstructionBase - base class for immediate operand instructions.
+class ImmInstructionBase : public ValueGenInst {
+public:
+  virtual OpType opType() const { return Immediate; }
+  unsigned getImmSize() const { return Size; }
+  int64_t getImm() const { return Imm; }
+  void setImm(unsigned Size, int64_t Imm)
+  { this->Size = Size; this->Imm = Imm; }
+
+protected:
+  unsigned Size; // in bytes
+  int64_t Imm;
+
+  ImmInstructionBase(unsigned Size, int64_t Imm) : Size(Size), Imm(Imm) {}
+};
+
+/// ImmInstruction<T> - immediate-based instructions.  Instructions are
+/// specified via template argument.
+template<ValueGenInst::InstType Type>
+class ImmInstruction : public ImmInstructionBase {
+public:
+  ImmInstruction(unsigned Size, int64_t Imm) : ImmInstructionBase(Size, Imm) {}
+
+  virtual InstType type() const { return Type; }
+
+  virtual bool operator==(const ValueGenInst &RHS) const {
+    if(RHS.type() == Type && RHS.opType() == Immediate) {
+      const ImmInstruction<Type> &II = (const ImmInstruction<Type> &)RHS;
+      if(II.Imm == Imm && II.Size == Size) return true;
+    }
+    return false;
+  }
+
+  virtual std::string str() const
+  { return getInstNameStr(Type) + " immediate " + std::to_string(Imm); }
+};
+
+/// ReferenceInstruction - references to symbols.  Only supports set
+/// instructions.
+class RefInstruction : public ValueGenInst {
+public:
+  RefInstruction(const MachineOperand &Symbol) : Symbol(Symbol) {}
+
+  virtual InstType type() const { return Set; }
+  virtual OpType opType() const { return Reference; }
+  MCSymbol *getReference(AsmPrinter &AP) const;
+  virtual bool operator==(const ValueGenInst &RHS) const {
+    if(RHS.type() == Set && RHS.opType() == Reference) {
+      const RefInstruction &RI = (const RefInstruction &)RHS;
+      if(&RI.Symbol == &Symbol) return true;
+    }
+    return false;
+  }
+  virtual std::string str() const;
+
+private:
+  // MCSymbols may not exist yet, so instead store the operand to look up the
+  // MCSymbol at metadata emission time.
+  // Note: store hard-copy (not reference) because optimizations may convert
+  // symbol reference to a different type, e.g., register
+  const MachineOperand Symbol;
+};
+
+/// Wrap raw pointers to ValueGenInst in smart pointers.  Use shared_ptr so we
+/// can use copy constructors for containers of these instructions.
+typedef std::shared_ptr<ValueGenInst> ValueGenInstPtr;
+
+/// A list of instructions used to generate a value
+typedef std::vector<ValueGenInstPtr> ValueGenInstList;
+
+#undef INV_INST_TYPE
+
+//===----------------------------------------------------------------------===//
+// Machine-specific live values
+//
+// These are the live values used to populate an architecture-specific location,
+// e.g., a reference to a global symbol or an immediate value
+
+/// MachineLiveVal - A machine-specific live value
+class MachineLiveVal {
+public:
+  /// Constructors & destructors.
+  // Note: create child class objects rather than objects of this class.
+  virtual ~MachineLiveVal() {}
+  virtual MachineLiveVal *copy() const = 0;
+
+  /// Possible live value types
+  enum Type { SymbolRef, ConstPoolRef, StackObject, Immediate, Generated };
+
+  /// Determine the live value's type
+  virtual enum Type getType() const = 0;
+  virtual bool isReference() const { return false; }
+  virtual bool isSymbolRef() const { return false; }
+  virtual bool isConstPoolRef() const { return false; }
+  virtual bool isStackObject() const { return false; }
+  virtual bool isImm() const { return false; }
+  virtual bool isGenerated() const { return false; }
+
+  /// Equivalence checking
+  virtual bool operator==(const MachineLiveVal &RHS) const = 0;
+
+  /// Generate a human-readable string describing the value
+  virtual std::string toString() const = 0;
+
+  /// Get the machine instruction which defines the live value
+  const MachineInstr *getDefiningInst() const { return DefMI; }
+
+  /// Return whether this value is a pointer
+  // Note: if the value *could* be a pointer, this should be set so the runtime
+  // can do pointer-to-stack checks
+  bool isPtr() const { return Ptr; }
+
+protected:
+  /// Defining instruction for live value
+  const MachineInstr *DefMI;
+
+  /// Is this a pointer?
+  // Note: if the value *could* be a pointer, this should be set so the runtime
+  // can do pointer-to-stack checks
+  bool Ptr;
+
+  MachineLiveVal(const MachineInstr *DefMI, bool Ptr)
+    : DefMI(DefMI), Ptr(Ptr) {}
+  MachineLiveVal(const MachineLiveVal &C) : DefMI(C.DefMI), Ptr(C.Ptr) {}
+};
+
+/// MachineReference - a reference to some binary object outside of thread
+/// local storage
+class MachineReference : public MachineLiveVal {
+public:
+  virtual bool isReference() const { return true; }
+
+  /// Get a symbol reference for label generation
+  virtual MCSymbol *getReference(AsmPrinter &AP) const = 0;
+
+  /// Return whether we are to load the reference's value
+  virtual bool isLoad() const { return false; }
+
+protected:
+  MachineReference(const MachineInstr *DefMI)
+    : MachineLiveVal(DefMI, true) {}
+  MachineReference(const MachineReference &C) : MachineLiveVal(C) {}
+};
+
+/// MachineSymbolRef - a reference to a global symbol
+class MachineSymbolRef : public MachineReference {
+public:
+  MachineSymbolRef(const MachineOperand &Symbol,
+                   bool Load,
+                   const MachineInstr *DefMI)
+    : MachineReference(DefMI), Symbol(Symbol), Load(Load) {}
+  MachineSymbolRef(const MachineSymbolRef &C)
+    : MachineReference(C), Symbol(C.Symbol), Load(C.Load) {}
+  virtual MachineLiveVal *copy() const { return new MachineSymbolRef(*this); }
+
+  virtual enum Type getType() const { return SymbolRef; }
+  virtual bool isSymbolRef() const { return true; }
+
+  virtual bool operator==(const MachineLiveVal &RHS) const;
+  virtual std::string toString() const;
+  virtual MCSymbol *getReference(AsmPrinter &AP) const;
+  virtual bool isLoad() const { return Load; }
+
+private:
+  // MCSymbols may not exist yet, so instead store the operand to look up the
+  // MCSymbol at metadata emission time.
+  // Note: store hard-copy (not reference) because optimizations may convert
+  // symbol reference to a different type, e.g., register
+  const MachineOperand Symbol;
+
+  // Should we load the reference's value?
+  bool Load;
+};
+
+/// MachineConstPoolRef - a reference to a constant pool entry
+class MachineConstPoolRef : public MachineReference {
+public:
+  MachineConstPoolRef(int Index, const MachineInstr *DefMI)
+    : MachineReference(DefMI), Index(Index) {}
+  MachineConstPoolRef(const MachineConstPoolRef &C)
+    : MachineReference(C), Index(C.Index) {}
+  virtual MachineLiveVal *copy() const
+  { return new MachineConstPoolRef(*this); }
+
+  virtual enum Type getType() const { return ConstPoolRef; }
+  virtual bool isConstPoolRef() const { return true; }
+
+  virtual bool operator==(const MachineLiveVal &RHS) const;
+
+  virtual std::string toString() const
+  { return "reference to constant pool index " + std::to_string(Index); }
+
+  virtual MCSymbol *getReference(AsmPrinter &AP) const;
+
+private:
+  int Index;
+};
+
+/// MachineStackObject - an object on the stack
+class MachineStackObject : public MachineLiveVal {
+public:
+  MachineStackObject(int Index,
+                     bool Load,
+                     const MachineInstr *DefMI,
+                     bool Ptr = false)
+    : MachineLiveVal(DefMI, Ptr), Index(Index), Load(Load) {}
+  MachineStackObject(const MachineStackObject &C)
+    : MachineLiveVal(C), Index(C.Index), Load(C.Load) {}
+  virtual MachineLiveVal *copy() const
+  { return new MachineStackObject(*this); }
+
+  /// Objects common across stack frames for all supported architectures
+  enum Common { None, ReturnAddr = INT32_MAX };
+
+  virtual enum Type getType() const { return StackObject; }
+  virtual bool isStackObject() const { return true; }
+  virtual enum Common getCommonObjectType() const { return None; }
+  virtual bool isCommonObject() const { return false; }
+
+  virtual bool operator==(const MachineLiveVal &RHS) const;
+
+  virtual std::string toString() const;
+
+  /// Return the object's offset from a base register (returned in BR)
+  virtual int getOffsetFromReg(AsmPrinter &AP, unsigned &BR) const;
+
+  int getIndex() const { return Index; }
+  void setIndex(int Index) { this->Index = Index; }
+  bool isLoad() const { return Load; }
+  void setLoad(bool Load) { this->Load = Load; }
+
+private:
+  /// The stack slot index of a stack object
+  int Index;
+
+  /// Are we generating a reference to a stack object or loading a value from
+  /// the stack slot?
+  bool Load;
+};
+
+/// ReturnAddress - the return address stored on the stack
+class ReturnAddress : public MachineStackObject {
+public:
+  ReturnAddress(const MachineInstr *DefMI)
+    : MachineStackObject(ReturnAddr, true, DefMI, false) {}
+  ReturnAddress(const ReturnAddress &C) : MachineStackObject(C) {}
+  virtual MachineLiveVal *copy() const { return new ReturnAddress(*this); }
+
+  virtual enum Common getCommonObjectType() const { return ReturnAddr; }
+  virtual bool isCommonObject() const { return true; }
+
+  virtual std::string toString() const
+  { return "function return address"; }
+
+  virtual int getOffsetFromReg(AsmPrinter &AP, unsigned &BR) const;
+};
+
+/// MachineImmediate - an immediate value
+class MachineImmediate : public MachineLiveVal {
+public:
+  MachineImmediate(unsigned Size,
+                   uint64_t Value,
+                   const MachineInstr *DefMI,
+                   bool Ptr = false);
+  MachineImmediate(const MachineImmediate &C)
+    : MachineLiveVal(C), Size(C.Size), Value(C.Value) {}
+  virtual MachineLiveVal *copy() const { return new MachineImmediate(*this); }
+
+  virtual enum Type getType() const { return Immediate; }
+  virtual bool isImm() const { return true; }
+
+  virtual bool operator==(const MachineLiveVal &RHS) const;
+
+  virtual std::string toString() const
+  { return "immediate value: " + std::to_string(Value); }
+
+  unsigned getSize() const { return Size; }
+  uint64_t getValue() const { return Value; }
+
+private:
+  unsigned Size; // in bytes
+  uint64_t Value;
+};
+
+/// MachineGeneratedVal - a value generated through a set of small operations
+class MachineGeneratedVal : public MachineLiveVal {
+public:
+  MachineGeneratedVal(const ValueGenInstList &VG,
+                      const MachineInstr *DefMI,
+                      bool Ptr)
+    : MachineLiveVal(DefMI, Ptr), VG(VG) {}
+  virtual MachineLiveVal *copy() const
+  { return new MachineGeneratedVal(*this); }
+
+  enum Type getType() const { return Generated; }
+  bool isGenerated() const { return true; }
+
+  virtual bool operator==(const MachineLiveVal &RHS) const;
+
+  virtual std::string toString() const
+  { return "generated value, " + std::to_string(VG.size()) + " instruction(s)"; }
+
+  const ValueGenInstList &getInstructions() const { return VG; }
+
+private:
+  ValueGenInstList VG;
+};
+
+// TODO add API to generate "Operation"
+
+//===----------------------------------------------------------------------===//
+// Machine-specific locations
+//
+// These are locations to be populated with the live values, e.g., a register or
+// stack slot.
+//
+// Note: these represent a live value's *destination*, not the live value
+// itself.  For example, don't confuse MachineStackObject above (a live value
+// to be copied from a stack slot) versus MachineLiveStackSlot below (the
+// location where a live value will be stored).
+
+/// MachineLiveLoc - an architecture-specific location for a live value
+class MachineLiveLoc {
+public:
+  /// Constructors & destructors.
+  // Note: create child class objects rather than objects of this class.
+  virtual ~MachineLiveLoc() {}
+  virtual MachineLiveLoc *copy() const = 0;
+  virtual bool operator==(const MachineLiveLoc &R) const = 0;
+
+  /// Determine the live value location type
+  virtual bool isReg() const { return false; }
+  virtual bool isStackAddr() const { return false; }
+  virtual bool isStackSlot() const { return false; }
+
+  virtual std::string toString() const = 0;
+};
+
+/// MachineLiveReg - a live value stored in a register.  Stores the register
+/// number as an architecture-specific physical register.
+class MachineLiveReg : public MachineLiveLoc {
+public:
+  MachineLiveReg(unsigned Reg) : Reg(Reg) {}
+  MachineLiveReg(const MachineLiveReg &C) : Reg(C.Reg) {}
+  virtual MachineLiveLoc *copy() const { return new MachineLiveReg(*this); }
+
+  virtual bool isReg() const { return true; }
+
+  virtual bool operator==(const MachineLiveLoc &RHS) const;
+
+  unsigned getReg() const { return Reg; }
+  void setReg(unsigned Reg) { this->Reg = Reg; }
+
+  virtual std::string toString() const
+  { return "live value in register " + std::to_string(Reg); }
+
+private:
+  unsigned Reg;
+};
+
+/// MachineLiveStackAddr - a live value stored at a known stack address.  Can
+/// be used for stack objects at hard-coded offsets, e.g., the TOC pointer save
+/// location for PowerPC/ELFv2 ABI.
+class MachineLiveStackAddr : public MachineLiveLoc {
+public:
+  MachineLiveStackAddr() : Offset(INT32_MAX), Reg(UINT32_MAX), Size(0) {}
+  MachineLiveStackAddr(int Offset, unsigned Reg, unsigned Size)
+    : Offset(Offset), Reg(Reg), Size(Size) {}
+  MachineLiveStackAddr(const MachineLiveStackAddr &C)
+    : Offset(C.Offset), Reg(C.Reg), Size(C.Size) {}
+  virtual MachineLiveLoc *copy() const
+  { return new MachineLiveStackAddr(*this); }
+
+  virtual bool isStackAddr() const { return true; }
+
+  virtual bool operator==(const MachineLiveLoc &RHS) const;
+
+  int getOffset() const { return Offset; }
+  void setOffset(int Offset) { this->Offset = Offset; }
+  unsigned getReg() const { return Reg; }
+  void setReg(unsigned Reg) { this->Reg = Reg; }
+  void setSize(unsigned Size) { this->Size = Size; }
+
+  // Calculate the final position of the stack object.  Return the object's
+  // location as an offset from a base pointer register.
+  virtual int calcAndGetRegOffset(const AsmPrinter &AP, unsigned &BP)
+  { BP = Reg; return Offset; }
+
+  // The size of a stack object may need to be determined by code emission
+  // metadata in child classes, hence the AsmPrinter argument
+  virtual unsigned getSize(const AsmPrinter &AP) { return Size; }
+
+  virtual std::string toString() const
+  {
+    return "live value at register " + std::to_string(Reg) +
+           " + " + std::to_string(Offset);
+  }
+
+protected:
+  // The object is referenced by an offset from a (physical) register's value.
+  int Offset;
+  unsigned Reg, Size;
+};
+
+/// MachineLiveStackSlot - a live value stored in a stack slot.  A more
+/// abstract version of MachineLiveStackAddr, where the value is in a virtual
+/// stack slot whose address won't be determined until instruction emission.
+class MachineLiveStackSlot : public MachineLiveStackAddr {
+public:
+  MachineLiveStackSlot(int Index) : Index(Index) {}
+  MachineLiveStackSlot(const MachineLiveStackSlot &C)
+    : MachineLiveStackAddr(C), Index(C.Index) {}
+  virtual MachineLiveLoc *copy() const
+  { return new MachineLiveStackSlot(*this); }
+
+  virtual bool isStackSlot() const { return true; }
+
+  virtual bool operator==(const MachineLiveLoc &RHS) const;
+
+  unsigned getStackSlot() const { return Index; }
+  void setStackSlot(int Index) { this->Index = Index; }
+  virtual int calcAndGetRegOffset(const AsmPrinter &AP, unsigned &BP);
+  virtual unsigned getSize(const AsmPrinter &AP);
+
+  virtual std::string toString() const
+  { return "live value in stack slot " + std::to_string(Index); }
+
+private:
+  int Index;
+};
+
+/// Useful typedefs for data structures needed to store additional stack
+/// transformation metadata not captured in the stackmap instructions.
+
+/// Tidy up objects defined above into smart pointers
+typedef std::unique_ptr<MachineLiveVal> MachineLiveValPtr;
+typedef std::unique_ptr<MachineLiveLoc> MachineLiveLocPtr;
+
+/// Map an IR operand number to the number of Machine IR operands in the lowered
+/// stackmap instruction.  Required as legalizing some types (e.g., i128) may
+/// require changing the IR value into more/fewer multiple machine values.
+typedef std::map<unsigned, unsigned> OpNumberMap;
+typedef std::pair<unsigned, unsigned> OpNumberPair;
+
+/// Map stackmaps to information about how operands changed during legalization.
+typedef std::map<int64_t, OpNumberMap> SMToOpLegalizeMap;
+typedef std::pair<int64_t, OpNumberMap> SMToOpLegalizePair;
+
+/// A vector of architecture-specific live value locations
+// Note: we could use a set instead (because we want unique live values), but
+// because we're using MachineLiveLoc pointers the set would only uniquify
+// based on the pointer, not the pointed-to value.
+typedef SmallVector<MachineLiveLocPtr, 4> MachineLiveLocs;
+
+/// Map IR value to a list of architecture-specific live value locations.
+/// Usually used to store duplicate locations for an IR value.
+typedef std::map<const Value *, MachineLiveLocs> IRToMachineLocs;
+typedef std::pair<const Value *, MachineLiveLocs> IRMachineLocPair;
+
+/// Map an IR instruction to the metadata about its IR operands (and their
+/// associated architecture-specific live values locations).
+typedef std::map<const Instruction *, IRToMachineLocs> InstToOperands;
+typedef std::pair<const Instruction *, IRToMachineLocs> InstOperandPair;
+
+/// A pair to couple an architecture-specific location to the value used to
+/// populate it, and a vector for storing several of them.
+typedef std::pair<MachineLiveLocPtr, MachineLiveValPtr> ArchLiveValue;
+typedef SmallVector<ArchLiveValue, 8> ArchLiveValues;
+
+// Map an IR instruction to architecture-specific live values
+typedef std::map<const Instruction *, ArchLiveValues> InstToArchLiveValues;
+typedef std::pair<const Instruction *, ArchLiveValues> InstArchLiveValuePair;
+
+}
+
+#endif
diff --git a/llvm/include/llvm/CodeGen/TargetFrameLowering.h b/llvm/include/llvm/CodeGen/TargetFrameLowering.h
index 878c9ffd2b5..2ea9c0dedea 100644
--- a/llvm/include/llvm/CodeGen/TargetFrameLowering.h
+++ b/llvm/include/llvm/CodeGen/TargetFrameLowering.h
@@ -281,6 +281,14 @@ public:
     return getFrameIndexReference(MF, FI, FrameReg);
   }
 
+  /// Same as above, except that the 'frame register' will always be the ISA's
+  /// frame pointer (which can be different from the variable 'frame register'
+  /// which may be the stack pointer, frame pointer, etc.)
+  virtual int getFrameIndexReferenceFromFP(const MachineFunction &MF, int FI,
+                                           unsigned &FrameReg) const {
+    return getFrameIndexReference(MF, FI, FrameReg);
+  }
+
   /// This method determines which of the registers reported by
   /// TargetRegisterInfo::getCalleeSavedRegs() should actually get saved.
   /// The default implementation checks populates the \p SavedRegs bitset with
diff --git a/llvm/include/llvm/CodeGen/TargetPassConfig.h b/llvm/include/llvm/CodeGen/TargetPassConfig.h
index 0bd82aafac3..7d4e38c1713 100644
--- a/llvm/include/llvm/CodeGen/TargetPassConfig.h
+++ b/llvm/include/llvm/CodeGen/TargetPassConfig.h
@@ -15,6 +15,7 @@
 
 #include "llvm/Pass.h"
 #include "llvm/Support/CodeGen.h"
+#include "llvm/Target/TargetMachine.h"
 #include <cassert>
 #include <string>
 
@@ -136,6 +137,15 @@ protected:
   /// preparation passes on IR.
   bool addCoreISelPasses();
 
+  /// Add equivalence points into the application
+  bool AddMigrationPoints = false;
+
+  /// Add stackmaps at function call sites & equivalence points
+  bool AddStackMaps = false;
+
+  /// Add stackmaps describing stack state in libc thread start functions
+  bool AddLibcStackMaps = false;
+
 public:
   TargetPassConfig(LLVMTargetMachine &TM, PassManagerBase &pm);
   // Dummy constructor.
@@ -155,6 +165,9 @@ public:
 
   CodeGenOpt::Level getOptLevel() const;
 
+  CodeGenOpt::Level getArchIROptLevel() const
+  { return TM->getArchIROptLevel(); }
+
   /// Returns true if one of the `-start-after`, `-start-before`, `-stop-after`
   /// or `-stop-before` options is set.
   static bool hasLimitedCodeGenPipeline();
@@ -174,6 +187,26 @@ public:
   bool getEnableTailMerge() const { return EnableTailMerge; }
   void setEnableTailMerge(bool Enable) { setOpt(EnableTailMerge, Enable); }
 
+  /// Return whether we should instrument the code with equivalence points.
+  bool addMigrationPoints() const { return AddMigrationPoints; }
+
+  /// Return whether we should emit stack transformation metadata by
+  /// instrumenting the code with IR-level StackMaps.
+  bool addStackMaps() const { return AddStackMaps; }
+
+  /// Return whether we should emit transformation metadata (via IR-level
+  /// StackMaps) for libc thread start functions.
+  bool addLibcStackMaps() const { return AddLibcStackMaps; }
+
+  /// \brief Enable/disable adding equivalence points.
+  void setAddMigrationPoints(bool Set) { AddMigrationPoints = Set; }
+
+  /// \brief Enable/disable adding StackMaps.
+  void setAddStackMaps(bool Set) { AddStackMaps = Set; }
+
+  /// \brief Enable/disable adding StackMaps to libc thread start function.
+  void setAddLibcStackMaps(bool Set) { AddLibcStackMaps = Set; }
+  
   bool requiresCodeGenSCCOrder() const { return RequireCodeGenSCCOrder; }
   void setRequiresCodeGenSCCOrder(bool Enable = true) {
     setOpt(RequireCodeGenSCCOrder, Enable);
@@ -211,6 +244,9 @@ public:
   /// has not be overriden on the command line with '-regalloc=...'
   bool usingDefaultRegAlloc() const;
 
+  /// Add Popcorn-specific IR passes for code generation.
+  void addPopcornPasses();
+
   /// High level function that adds all passes necessary to go from llvm IR
   /// representation to the MI representation.
   /// Adds IR based lowering and target specific optimization passes and finally
diff --git a/llvm/include/llvm/CodeGen/TargetRegisterInfo.h b/llvm/include/llvm/CodeGen/TargetRegisterInfo.h
index ddbd677b3ea..86248592c6e 100644
--- a/llvm/include/llvm/CodeGen/TargetRegisterInfo.h
+++ b/llvm/include/llvm/CodeGen/TargetRegisterInfo.h
@@ -1005,6 +1005,17 @@ public:
                                    const MachineRegisterInfo &MRI) const {
     return nullptr;
   }
+
+  /// getReturnAddrLoc - This method should return the location of the saved
+  /// return address on the stack, expressed as a base register (returned via
+  /// BaseReg) and an offset
+  virtual int getReturnAddrLoc(const MachineFunction &MF,
+                               unsigned &BaseReg) const
+  {
+    llvm_unreachable("Not implemented for target!");
+    BaseReg = 0;
+    return INT32_MAX;
+  }
 };
 
 //===----------------------------------------------------------------------===//
diff --git a/llvm/include/llvm/CodeGen/TargetSubtargetInfo.h b/llvm/include/llvm/CodeGen/TargetSubtargetInfo.h
index 037fc3ed324..98819e59a9f 100644
--- a/llvm/include/llvm/CodeGen/TargetSubtargetInfo.h
+++ b/llvm/include/llvm/CodeGen/TargetSubtargetInfo.h
@@ -51,6 +51,7 @@ class TargetLowering;
 class TargetRegisterClass;
 class TargetRegisterInfo;
 class TargetSchedModel;
+class TargetValues;
 class Triple;
 
 //===----------------------------------------------------------------------===//
@@ -134,6 +135,13 @@ public:
     return nullptr;
   }
 
+  /// getValues - Returns the value generator object for the target or specific
+  /// subtarget
+  ///
+  virtual const TargetValues *getValues() const {
+    return nullptr;
+  };
+
   /// Resolve a SchedClass at runtime, where SchedClass identifies an
   /// MCSchedClassDesc with the isVariant property. This may return the ID of
   /// another variant SchedClass, but repeated invocation must quickly terminate
diff --git a/llvm/include/llvm/CodeGen/UnwindInfo.h b/llvm/include/llvm/CodeGen/UnwindInfo.h
new file mode 100644
index 00000000000..ce9d46166b5
--- /dev/null
+++ b/llvm/include/llvm/CodeGen/UnwindInfo.h
@@ -0,0 +1,112 @@
+//===----------------- UnwindInfo.h - UnwindInfo ---------------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Generate unwinding information for stack transformation runtime.  Note that
+// this is implemented assuming the function uses a frame base pointer (FBP).
+// This requirement is guaranteed to be satisfied if the function has a
+// stackmap, which are the only functions for which we want to generate
+// unwinding information.
+//
+//===---------------------------------------------------------------------===//
+
+#ifndef LLVM_CODEGEN_UNWINDINFO_H
+#define LLVM_CODEGEN_UNWINDINFO_H
+
+#include "llvm/CodeGen/AsmPrinter.h"
+#include "llvm/CodeGen/MachineFrameInfo.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/MC/MCContext.h"
+#include "llvm/MC/MCStreamer.h"
+#include "llvm/Support/Debug.h"
+#include <map>
+
+namespace llvm {
+
+class UnwindInfo {
+public:
+  /// Per-function unwinding metadata classes & typedefs
+  class FuncUnwindInfo {
+  public:
+    uint32_t SecOffset; // Offset into unwinding record section
+    uint32_t NumUnwindRecord; // Number of unwinding records
+
+    FuncUnwindInfo() : SecOffset(UINT32_MAX), NumUnwindRecord(0) {}
+    FuncUnwindInfo(uint32_t SecOffset, uint32_t NumUnwindRecord)
+      : SecOffset(SecOffset), NumUnwindRecord(NumUnwindRecord) {}
+  };
+
+  typedef std::pair<const MCSymbol *, FuncUnwindInfo> FuncUnwindPair;
+  typedef std::map<const MCSymbol *, FuncUnwindInfo> FuncUnwindMap;
+
+  /// Unwinding record classes & typedefs
+  class RegOffset {
+  public:
+    uint32_t DwarfReg;
+    int32_t Offset;
+
+    RegOffset() : DwarfReg(0), Offset(0) {}
+    RegOffset(uint32_t DwarfReg, int32_t Offset) :
+      DwarfReg(DwarfReg), Offset(Offset) {}
+  };
+
+  typedef SmallVector<RegOffset, 32> CalleeSavedRegisters;
+  typedef std::pair<const MCSymbol *, CalleeSavedRegisters> FuncCalleePair;
+  typedef std::map<const MCSymbol *, CalleeSavedRegisters> FuncCalleeMap;
+
+  /// \brief Constructors
+  UnwindInfo() = delete;
+  UnwindInfo(AsmPrinter &AP)
+    : AP(AP), OutContext(AP.OutStreamer->getContext()), Emitted(false) {};
+
+  /// \bried Clear all saved unwinding information
+  void reset() {
+    Emitted = false;
+    FuncCalleeSaved.clear();
+    FuncUnwindMetadata.clear();
+  }
+
+  /// \brief Store unwinding information for a function
+  void recordUnwindInfo(const MachineFunction &MF);
+
+  /// \brief Add a register restore offset for a function.  MachineReg will get
+  /// converted to a DWARF register internally.
+  void addRegisterUnwindInfo(const MachineFunction &MF,
+                             uint32_t MachineReg,
+                             int32_t Offset);
+
+  /// Create an unwinding information section and serialize the map info into
+  /// it.
+  ///
+  /// Note: unlike StackMaps::serializeToStackMapSection, this function *does
+  /// not* clear out the data structures.  This is so that the stack map
+  /// machinery can access per-function unwinding information.
+  void serializeToUnwindInfoSection();
+
+  /// Get unwinding section metadata for a function
+  const FuncUnwindInfo &getUnwindInfo(const MCSymbol *Func) const;
+
+private:
+  AsmPrinter &AP;
+  MCContext &OutContext;
+  FuncCalleeMap FuncCalleeSaved;
+  FuncUnwindMap FuncUnwindMetadata;
+  bool Emitted;
+
+  /// \brief Emit the unwind info for each function.
+  void emitUnwindInfo(MCStreamer &OS);
+
+  /// \brief Emit the address range info for each function.
+  void emitAddrRangeInfo(MCStreamer &OS);
+
+  void print(raw_ostream &OS);
+  void debug() { print(dbgs()); }
+};
+}
+
+#endif
diff --git a/llvm/include/llvm/IR/DiagnosticInfo.h b/llvm/include/llvm/IR/DiagnosticInfo.h
index 373663289db..6e303d9a4a7 100644
--- a/llvm/include/llvm/IR/DiagnosticInfo.h
+++ b/llvm/include/llvm/IR/DiagnosticInfo.h
@@ -65,6 +65,7 @@ enum DiagnosticKind {
   DK_OptimizationRemarkAnalysisFPCommute,
   DK_OptimizationRemarkAnalysisAliasing,
   DK_OptimizationFailure,
+  DK_OptimizationError,
   DK_FirstRemark = DK_OptimizationRemark,
   DK_LastRemark = DK_OptimizationFailure,
   DK_MachineOptimizationRemark,
@@ -523,6 +524,29 @@ protected:
   int FirstExtraArgIndex = -1;
 };
 
+/// Diagnostic information for show-stopping failures.
+class DiagnosticInfoOptimizationError
+    : public DiagnosticInfoOptimizationBase {
+public:
+  /// \p Fn is the function where the diagnostic is being emitted. \p DLoc is
+  /// the location information to use in the diagnostic. If line table
+  /// information is available, the diagnostic will include the source code
+  /// location. \p Msg is the message to show. Note that this class does not
+  /// copy this message, so this reference must be valid for the whole life time
+  /// of the diagnostic.
+  DiagnosticInfoOptimizationError(const Function &Fn, const DebugLoc &DLoc,
+                                  const StringRef &Msg)
+      : DiagnosticInfoOptimizationBase(DK_OptimizationError, DS_Error,
+                                       nullptr, Msg, Fn, DLoc) {}
+
+  static bool classof(const DiagnosticInfo *DI) {
+    return DI->getKind() == DK_OptimizationError;
+  }
+
+  /// \see DiagnosticInfoOptimizationBase::isEnabled.
+  bool isEnabled() const override;
+};
+
 /// Allow the insertion operator to return the actual remark type rather than a
 /// common base class.  This allows returning the result of the insertion
 /// directly by value, e.g. return OptimizationRemarkAnalysis(...) << "blah".
diff --git a/llvm/include/llvm/IR/Intrinsics.td b/llvm/include/llvm/IR/Intrinsics.td
index d660f827843..feaefe0803c 100644
--- a/llvm/include/llvm/IR/Intrinsics.td
+++ b/llvm/include/llvm/IR/Intrinsics.td
@@ -928,6 +928,10 @@ def int_strip_invariant_group : Intrinsic<[llvm_anyptr_ty],
 def int_experimental_stackmap : Intrinsic<[],
                                   [llvm_i64_ty, llvm_i32_ty, llvm_vararg_ty],
                                   [Throws]>;
+def int_experimental_pcn_stackmap : Intrinsic<[],
+                          	              [llvm_i64_ty, llvm_i32_ty,
+					       llvm_vararg_ty],
+                                 	       [Throws]>;
 def int_experimental_patchpoint_void : Intrinsic<[],
                                                  [llvm_i64_ty, llvm_i32_ty,
                                                   llvm_ptr_ty, llvm_i32_ty,
diff --git a/llvm/include/llvm/InitializePasses.h b/llvm/include/llvm/InitializePasses.h
index 164d0be2855..af13eb34881 100644
--- a/llvm/include/llvm/InitializePasses.h
+++ b/llvm/include/llvm/InitializePasses.h
@@ -136,7 +136,9 @@ void initializeEarlyMachineLICMPass(PassRegistry&);
 void initializeEarlyTailDuplicatePass(PassRegistry&);
 void initializeEdgeBundlesPass(PassRegistry&);
 void initializeEliminateAvailableExternallyLegacyPassPass(PassRegistry&);
+void initializeMigrationPointsPass(PassRegistry&);
 void initializeEntryExitInstrumenterPass(PassRegistry&);
+void initializeEnumerateLoopPathsPass(PassRegistry&);
 void initializeExpandMemCmpPassPass(PassRegistry&);
 void initializeExpandPostRAPass(PassRegistry&);
 void initializeExpandReductionsPass(PassRegistry&);
@@ -178,6 +180,7 @@ void initializeIndirectBrExpandPassPass(PassRegistry&);
 void initializeInferAddressSpacesPass(PassRegistry&);
 void initializeInferFunctionAttrsLegacyPassPass(PassRegistry&);
 void initializeInlineCostAnalysisPass(PassRegistry&);
+void initializeInsertStackMapsPass(PassRegistry&);
 void initializeInstCountPass(PassRegistry&);
 void initializeInstNamerPass(PassRegistry&);
 void initializeInstSimplifyLegacyPassPass(PassRegistry &);
@@ -203,6 +206,7 @@ void initializeLegacyLoopSinkPassPass(PassRegistry&);
 void initializeLegalizerPass(PassRegistry&);
 void initializeGISelCSEAnalysisWrapperPassPass(PassRegistry &);
 void initializeLibCallsShrinkWrapLegacyPassPass(PassRegistry&);
+void initializeLibcStackMapsPass(PassRegistry&);
 void initializeLintPass(PassRegistry&);
 void initializeLiveDebugValuesPass(PassRegistry&);
 void initializeLiveDebugVariablesPass(PassRegistry&);
@@ -211,6 +215,7 @@ void initializeLiveRangeShrinkPass(PassRegistry&);
 void initializeLiveRegMatrixPass(PassRegistry&);
 void initializeLiveStacksPass(PassRegistry&);
 void initializeLiveVariablesPass(PassRegistry&);
+void initializeLiveValuesPass(PassRegistry&);
 void initializeLoadStoreVectorizerLegacyPassPass(PassRegistry&);
 void initializeLoaderPassPass(PassRegistry&);
 void initializeLocalStackSlotPassPass(PassRegistry&);
@@ -288,6 +293,7 @@ void initializeModuleDebugInfoPrinterPass(PassRegistry&);
 void initializeModuleSummaryIndexWrapperPassPass(PassRegistry&);
 void initializeMustExecutePrinterPass(PassRegistry&);
 void initializeNameAnonGlobalLegacyPassPass(PassRegistry&);
+void initializeNameStringLiteralsPass(PassRegistry&);
 void initializeNaryReassociateLegacyPassPass(PassRegistry&);
 void initializeNewGVNLegacyPassPass(PassRegistry&);
 void initializeObjCARCAAWrapperPassPass(PassRegistry&);
@@ -313,6 +319,7 @@ void initializePhiValuesWrapperPassPass(PassRegistry&);
 void initializePhysicalRegisterUsageInfoPass(PassRegistry&);
 void initializePlaceBackedgeSafepointsImplPass(PassRegistry&);
 void initializePlaceSafepointsPass(PassRegistry&);
+void initializePopcornCompatibilityPass(PassRegistry&);
 void initializePostDomOnlyPrinterPass(PassRegistry&);
 void initializePostDomOnlyViewerPass(PassRegistry&);
 void initializePostDomPrinterPass(PassRegistry&);
@@ -366,6 +373,7 @@ void initializeScalarizeMaskedMemIntrinPass(PassRegistry&);
 void initializeScalarizerLegacyPassPass(PassRegistry&);
 void initializeScavengerTestPass(PassRegistry&);
 void initializeScopedNoAliasAAWrapperPassPass(PassRegistry&);
+void initializeSelectMigrationPointsPass(PassRegistry&);
 void initializeSeparateConstOffsetFromGEPPass(PassRegistry&);
 void initializeShadowStackGCLoweringPass(PassRegistry&);
 void initializeShrinkWrapPass(PassRegistry&);
@@ -383,6 +391,8 @@ void initializeStackProtectorPass(PassRegistry&);
 void initializeStackSafetyGlobalInfoWrapperPassPass(PassRegistry &);
 void initializeStackSafetyInfoWrapperPassPass(PassRegistry &);
 void initializeStackSlotColoringPass(PassRegistry&);
+void initializeStackTransformMetadataPass(PassRegistry&);
+void initializeStaticVarSectionsPass(PassRegistry&);
 void initializeStraightLineStrengthReducePass(PassRegistry&);
 void initializeStripDeadDebugInfoPass(PassRegistry&);
 void initializeStripDeadPrototypesLegacyPassPass(PassRegistry&);
diff --git a/llvm/include/llvm/LinkAllPasses.h b/llvm/include/llvm/LinkAllPasses.h
index 675d179eb22..d4f488430ec 100644
--- a/llvm/include/llvm/LinkAllPasses.h
+++ b/llvm/include/llvm/LinkAllPasses.h
@@ -101,6 +101,7 @@ namespace {
       (void) llvm::createDomOnlyViewerPass();
       (void) llvm::createDomViewerPass();
       (void) llvm::createGCOVProfilerPass();
+      (void) llvm::createMigrationPointsPass();
       (void) llvm::createPGOInstrumentationGenLegacyPass();
       (void) llvm::createPGOInstrumentationUseLegacyPass();
       (void) llvm::createPGOInstrumentationGenCreateVarLegacyPass();
@@ -110,6 +111,7 @@ namespace {
       (void) llvm::createFunctionImportPass();
       (void) llvm::createFunctionInliningPass();
       (void) llvm::createAlwaysInlinerLegacyPass();
+      (void) llvm::createEnumerateLoopPathsPass();
       (void) llvm::createGlobalDCEPass();
       (void) llvm::createGlobalOptimizerPass();
       (void) llvm::createGlobalsAAWrapperPass();
@@ -119,12 +121,15 @@ namespace {
       (void) llvm::createIPSCCPPass();
       (void) llvm::createInductiveRangeCheckEliminationPass();
       (void) llvm::createIndVarSimplifyPass();
+      (void) llvm::createInsertStackMapsPass();
       (void) llvm::createInstSimplifyLegacyPass();
       (void) llvm::createInstructionCombiningPass();
       (void) llvm::createInternalizePass();
       (void) llvm::createLCSSAPass();
       (void) llvm::createLegacyDivergenceAnalysisPass();
+      (void) llvm::createLibcStackMapsPass();
       (void) llvm::createLICMPass();
+      (void) llvm::createLiveValuesPass();
       (void) llvm::createLoopSinkPass();
       (void) llvm::createLazyValueInfoPass();
       (void) llvm::createLoopExtractorPass();
@@ -144,6 +149,7 @@ namespace {
       (void) llvm::createLowerInvokePass();
       (void) llvm::createLowerSwitchPass();
       (void) llvm::createNaryReassociatePass();
+      (void) llvm::createNameStringLiteralsPass();
       (void) llvm::createObjCARCAAWrapperPass();
       (void) llvm::createObjCARCAPElimPass();
       (void) llvm::createObjCARCExpandPass();
@@ -153,6 +159,7 @@ namespace {
       (void) llvm::createPromoteMemoryToRegisterPass();
       (void) llvm::createDemoteRegisterToMemoryPass();
       (void) llvm::createPruneEHPass();
+      (void) llvm::createPopcornCompatibilityPass();
       (void) llvm::createPostDomOnlyPrinterPass();
       (void) llvm::createPostDomPrinterPass();
       (void) llvm::createPostDomOnlyViewerPass();
@@ -167,6 +174,7 @@ namespace {
       (void) llvm::createSafeStackPass();
       (void) llvm::createSROAPass();
       (void) llvm::createSingleLoopExtractorPass();
+      (void) llvm::createStaticVarSectionsPass();
       (void) llvm::createStripSymbolsPass();
       (void) llvm::createStripNonDebugSymbolsPass();
       (void) llvm::createStripDeadDebugInfoPass();
@@ -212,6 +220,7 @@ namespace {
       (void) llvm::createLoadStoreVectorizerPass();
       (void) llvm::createPartiallyInlineLibCallsPass();
       (void) llvm::createScalarizerPass();
+      (void) llvm::createSelectMigrationPointsPass();
       (void) llvm::createSeparateConstOffsetFromGEPPass();
       (void) llvm::createSpeculativeExecutionPass();
       (void) llvm::createSpeculativeExecutionIfHasBranchDivergencePass();
diff --git a/llvm/include/llvm/MC/MCObjectFileInfo.h b/llvm/include/llvm/MC/MCObjectFileInfo.h
index abc87bf2774..fee49b75c91 100644
--- a/llvm/include/llvm/MC/MCObjectFileInfo.h
+++ b/llvm/include/llvm/MC/MCObjectFileInfo.h
@@ -153,9 +153,16 @@ protected:
   /// Null if this target doesn't support a BSS section. ELF and MachO only.
   MCSection *TLSBSSSection; // Defaults to ".tbss".
 
+  /// Unwinding address ranges & register location sections.
+  MCSection *UnwindAddrRangeSection;
+  MCSection *UnwindInfoSection;
+
   /// StackMap section.
   MCSection *StackMapSection;
 
+  /// Pcn StackMap section.
+  MCSection *PcnStackMapSection;
+
   /// FaultMap section.
   MCSection *FaultMapSection;
 
@@ -315,7 +322,11 @@ public:
   const MCSection *getTLSDataSection() const { return TLSDataSection; }
   MCSection *getTLSBSSSection() const { return TLSBSSSection; }
 
+  MCSection *getUnwindInfoSection() const { return UnwindInfoSection; }
+  MCSection *getUnwindAddrRangeSection() const { return UnwindAddrRangeSection; }
+
   MCSection *getStackMapSection() const { return StackMapSection; }
+  MCSection *getPcnStackMapSection() const { return PcnStackMapSection; }
   MCSection *getFaultMapSection() const { return FaultMapSection; }
   MCSection *getRemarksSection() const { return RemarksSection; }
 
diff --git a/llvm/include/llvm/Support/TargetOpcodes.def b/llvm/include/llvm/Support/TargetOpcodes.def
index 598c1064efd..dd639742864 100644
--- a/llvm/include/llvm/Support/TargetOpcodes.def
+++ b/llvm/include/llvm/Support/TargetOpcodes.def
@@ -110,6 +110,7 @@ HANDLE_TARGET_OPCODE(LIFETIME_END)
 /// position in the instruction stream. It is followed by a shadow of bytes
 /// that must lie within the function and not contain another stackmap.
 HANDLE_TARGET_OPCODE(STACKMAP)
+HANDLE_TARGET_OPCODE(PCN_STACKMAP)
 
 /// FEntry all - This is a marker instruction which gets translated into a raw fentry call.
 HANDLE_TARGET_OPCODE(FENTRY_CALL)
diff --git a/llvm/include/llvm/Target/Target.td b/llvm/include/llvm/Target/Target.td
index d58662e128e..1392d52da49 100644
--- a/llvm/include/llvm/Target/Target.td
+++ b/llvm/include/llvm/Target/Target.td
@@ -1092,6 +1092,14 @@ def STACKMAP : StandardPseudoInstruction {
   let mayLoad = 1;
   let usesCustomInserter = 1;
 }
+def PCN_STACKMAP : StandardPseudoInstruction {
+  let OutOperandList = (outs);
+  let InOperandList = (ins i64imm:$id, i32imm:$nbytes, variable_ops);
+  let hasSideEffects = 1;
+  let isCall = 1;
+  let mayLoad = 1;
+  let usesCustomInserter = 1;
+}
 def PATCHPOINT : StandardPseudoInstruction {
   let OutOperandList = (outs unknown:$dst);
   let InOperandList = (ins i64imm:$id, i32imm:$nbytes, unknown:$callee,
diff --git a/llvm/include/llvm/Target/TargetMachine.h b/llvm/include/llvm/Target/TargetMachine.h
index cdf9f8bfd5e..be354f14426 100644
--- a/llvm/include/llvm/Target/TargetMachine.h
+++ b/llvm/include/llvm/Target/TargetMachine.h
@@ -89,6 +89,11 @@ protected: // Can only create subclasses.
   CodeModel::Model CMModel = CodeModel::Small;
   CodeGenOpt::Level OptLevel = CodeGenOpt::Default;
 
+  /// ArchIROptLevel - Optimization level (architecture-specific IR
+  /// optimizations only).  Defaults to OptLevel.
+  ///
+  CodeGenOpt::Level ArchIROptLevel = CodeGenOpt::Default;
+
   /// Contains target specific asm information.
   std::unique_ptr<const MCAsmInfo> AsmInfo;
   std::unique_ptr<const MCRegisterInfo> MRI;
@@ -221,9 +226,12 @@ public:
   /// Returns the optimization level: None, Less, Default, or Aggressive.
   CodeGenOpt::Level getOptLevel() const;
 
+  CodeGenOpt::Level getArchIROptLevel() const { return ArchIROptLevel; }
+
   /// Overrides the optimization level.
   void setOptLevel(CodeGenOpt::Level Level);
 
+  void setArchIROptLevel(CodeGenOpt::Level Level) { ArchIROptLevel = Level; }
   void setFastISel(bool Enable) { Options.EnableFastISel = Enable; }
   bool getO0WantsFastISel() { return O0WantsFastISel; }
   void setO0WantsFastISel(bool Enable) { O0WantsFastISel = Enable; }
diff --git a/llvm/include/llvm/Target/TargetValues.h b/llvm/include/llvm/Target/TargetValues.h
new file mode 100644
index 00000000000..2a4823f0d54
--- /dev/null
+++ b/llvm/include/llvm/Target/TargetValues.h
@@ -0,0 +1,92 @@
+//===----- llvm/Target/TargetValues.h - Value Properties ----*- C++ -----*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file provides an API for detecting properties of architecture-specific
+// values & for generating a series of simple metadata instructions for
+// reconstituting a value.  This is used by the stack transformation runtime to
+// set up architecture-specific live values.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_TARGET_TARGETVAL_H
+#define LLVM_TARGET_TARGETVAL_H
+
+#include <memory>
+#include "llvm/CodeGen/StackTransformTypes.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/VirtRegMap.h"
+#include "llvm/IR/Instructions.h"
+
+namespace llvm {
+
+/// A value created by the backend to satisfy the stackmap.  Note that this is
+/// not necessarily an error -- some backends may create and save the value
+/// whereas other backends may materialize the value as needed.
+struct TemporaryValue {
+  enum type {
+    StackSlotRef
+  };
+
+  enum type Type;
+  unsigned Size;
+
+  /// The virtual register used if the temporary is stored in a register
+  unsigned Vreg;
+
+  /// If the value is of type StackSlotRef, the stack slot and offset into the
+  /// stack slot being referenced
+  int StackSlot;
+  int64_t Offset;
+};
+
+typedef std::unique_ptr<TemporaryValue> TemporaryValuePtr;
+
+//===----------------------------------------------------------------------===//
+// Superclass for ISA-specific values
+//
+
+class TargetValues {
+public:
+  TargetValues(const TargetValues &) = delete;
+  void operator=(const TargetValues &) = delete;
+  virtual ~TargetValues() {};
+
+  /// The code generator may have materialized a temporary value solely to
+  /// satisfy the stackmap if the value is materialized as-needed elsewhere.
+  /// Return metadata describing the temporary value in this situation.
+  virtual TemporaryValuePtr getTemporaryValue(const MachineInstr *MI,
+                                              const VirtRegMap *VRM) const
+  { return nullptr; }
+
+  /// Return a machine-specific value generated by a machine instruction.
+  virtual MachineLiveValPtr getMachineValue(const MachineInstr *MI) const = 0;
+
+  /// Add any required architecture-specific live values, e.g., the TOC pointer
+  /// on PowerPC.
+  virtual void addRequiredArchLiveValues(MachineFunction *MF,
+                                         const MachineInstr *MIStackMap,
+                                         const CallInst *IRStackMap) const
+  { return; }
+
+protected:
+  TargetValues() {}
+
+  /// Return whether or not the operand is some type of symbol reference.
+  static bool isSymbolValue(const MachineOperand &MO)
+  { return MO.isGlobal() || MO.isSymbol() || MO.isMCSymbol(); }
+  static bool isSymbolValue(const MachineOperand *MO)
+  { return isSymbolValue(*MO); }
+  static bool isSymbolValueConstant(const MachineOperand &MO);
+  static bool isSymbolValueConstant(const MachineOperand *MO)
+  { return isSymbolValueConstant(*MO); }
+};
+
+} // End llvm namespace
+
+#endif
diff --git a/llvm/include/llvm/Transforms/Utils.h b/llvm/include/llvm/Transforms/Utils.h
index 6e03453babf..52ece9075ad 100644
--- a/llvm/include/llvm/Transforms/Utils.h
+++ b/llvm/include/llvm/Transforms/Utils.h
@@ -119,6 +119,19 @@ ModulePass *createStripNonLineTableDebugInfoPass();
 // number of conditional branches in the hot paths based on profiles.
 //
 FunctionPass *createControlHeightReductionLegacyPass();
+
+//===----------------------------------------------------------------------===//
+//
+// NameStringLiterals - Give symbol names to anonymous string literals so they
+// can be aligned at link-time
+//
+ModulePass *createNameStringLiteralsPass();
+
+//===----------------------------------------------------------------------===//
+//
+// StaticVarSections - Put static global variables into their own sections
+//
+ModulePass *createStaticVarSectionsPass();
 }
 
 #endif
diff --git a/llvm/lib/Analysis/Analysis.cpp b/llvm/lib/Analysis/Analysis.cpp
index d46a8d8e306..cdbe79223a3 100644
--- a/llvm/lib/Analysis/Analysis.cpp
+++ b/llvm/lib/Analysis/Analysis.cpp
@@ -42,8 +42,10 @@ void llvm::initializeAnalysis(PassRegistry &Registry) {
   initializeDomViewerPass(Registry);
   initializeDomPrinterPass(Registry);
   initializeDomOnlyViewerPass(Registry);
+  initializeEnumerateLoopPathsPass(Registry);
   initializePostDomViewerPass(Registry);
   initializeDomOnlyPrinterPass(Registry);
+  initializePopcornCompatibilityPass(Registry);
   initializePostDomPrinterPass(Registry);
   initializePostDomOnlyViewerPass(Registry);
   initializePostDomOnlyPrinterPass(Registry);
@@ -58,6 +60,7 @@ void llvm::initializeAnalysis(PassRegistry &Registry) {
   initializeLazyValueInfoPrinterPass(Registry);
   initializeLegacyDivergenceAnalysisPass(Registry);
   initializeLintPass(Registry);
+  initializeLiveValuesPass(Registry);
   initializeLoopInfoWrapperPassPass(Registry);
   initializeMemDepPrinterPass(Registry);
   initializeMemDerefPrinterPass(Registry);
@@ -76,6 +79,7 @@ void llvm::initializeAnalysis(PassRegistry &Registry) {
   initializeRegionOnlyPrinterPass(Registry);
   initializeSCEVAAWrapperPassPass(Registry);
   initializeScalarEvolutionWrapperPassPass(Registry);
+  initializeSelectMigrationPointsPass(Registry);
   initializeStackSafetyGlobalInfoWrapperPassPass(Registry);
   initializeStackSafetyInfoWrapperPassPass(Registry);
   initializeTargetTransformInfoWrapperPassPass(Registry);
diff --git a/llvm/lib/Analysis/CMakeLists.txt b/llvm/lib/Analysis/CMakeLists.txt
index dd5d6251414..eca3b23de3b 100644
--- a/llvm/lib/Analysis/CMakeLists.txt
+++ b/llvm/lib/Analysis/CMakeLists.txt
@@ -47,12 +47,15 @@ add_llvm_library(LLVMAnalysis
   LazyValueInfo.cpp
   LegacyDivergenceAnalysis.cpp
   Lint.cpp
+  LiveValues.cpp
   Loads.cpp
   LoopAccessAnalysis.cpp
   LoopAnalysisManager.cpp
   LoopUnrollAnalyzer.cpp
   LoopInfo.cpp
+  LoopNestingTree.cpp
   LoopPass.cpp
+  LoopPaths.cpp
   MemDepPrinter.cpp
   MemDerefPrinter.cpp
   MemoryBuiltins.cpp
@@ -71,6 +74,7 @@ add_llvm_library(LLVMAnalysis
   OrderedInstructions.cpp
   PHITransAddr.cpp
   PhiValues.cpp
+  PopcornCompatibility.cpp
   PostDominators.cpp
   ProfileSummaryInfo.cpp
   PtrUseVisitor.cpp
@@ -81,6 +85,7 @@ add_llvm_library(LLVMAnalysis
   ScalarEvolutionAliasAnalysis.cpp
   ScalarEvolutionExpander.cpp
   ScalarEvolutionNormalization.cpp
+  SelectMigrationPoints.cpp
   StackSafetyAnalysis.cpp
   SyncDependenceAnalysis.cpp
   SyntheticCountsUtils.cpp
diff --git a/llvm/lib/Analysis/LiveValues.cpp b/llvm/lib/Analysis/LiveValues.cpp
new file mode 100644
index 00000000000..1b562800cdc
--- /dev/null
+++ b/llvm/lib/Analysis/LiveValues.cpp
@@ -0,0 +1,385 @@
+#include "llvm/Analysis/LiveValues.h"
+#include "llvm/IR/Metadata.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/CFG.h"
+#include "llvm/Analysis/CFG.h"
+#include "llvm/Analysis/LoopInfo.h"
+#include "llvm/ADT/PostOrderIterator.h"
+#include "llvm/ADT/SCCIterator.h"
+#include "llvm/Support/Debug.h"
+
+#define DEBUG_TYPE "live-values"
+
+using namespace llvm;
+
+char LiveValues::ID = 0;
+INITIALIZE_PASS_BEGIN(LiveValues, "live-values", 
+                    "Live-value set calculation", true, true)
+INITIALIZE_PASS_DEPENDENCY(LoopInfoWrapperPass)
+INITIALIZE_PASS_END(LiveValues, "live-values", 
+                    "Live-value set calculation", true, true)
+
+namespace llvm {
+  FunctionPass *createLiveValuesPass() { return new LiveValues(); }
+}
+
+///////////////////////////////////////////////////////////////////////////////
+// Public API
+///////////////////////////////////////////////////////////////////////////////
+
+LiveValues::LiveValues(void)
+  : FunctionPass(ID), inlineasm(false), bitcasts(true), comparisons(true),
+    constants(false), metadata(false) {}
+
+void LiveValues::getAnalysisUsage(AnalysisUsage &AU) const
+{
+  AU.addRequired<LoopInfoWrapperPass>();
+  AU.setPreservesAll();
+}
+
+bool LiveValues::runOnFunction(Function &F)
+{
+  if(FuncBBLiveIn.count(&F))
+  {
+    LLVM_DEBUG(
+      errs() << "\nFound previous analysis for " << F.getName() << "\n\n";
+      printF(errs(), &F);
+    );
+  }
+  else
+  {
+    LLVM_DEBUG(errs() << "\n********** Beginning LiveValues **********\n"
+                 << "********** Function: " << F.getName() << " **********\n\n"
+                    "LiveValues: performing bottom-up dataflow analysis\n");
+
+    LoopNestingForest LNF;
+    FuncBBLiveIn.emplace(&F, LiveVals());
+    FuncBBLiveOut.emplace(&F, LiveVals());
+
+    /* 1. Compute partial liveness sets using a postorder traversal. */
+    dagDFS(F, FuncBBLiveIn[&F], FuncBBLiveOut[&F]);
+
+    LLVM_DEBUG(errs() << "LiveValues: constructing loop-nesting forest\n");
+
+    /* 2. Construct loop-nesting forest. */
+    constructLoopNestingForest(F, LNF);
+
+    LLVM_DEBUG(errs() << "LiveValues: propagating values within loop-nests\n");
+
+    /* 3. Propagate live variables within loop bodies. */
+    loopTreeDFS(LNF, FuncBBLiveIn[&F], FuncBBLiveOut[&F]);
+
+    LLVM_DEBUG(
+      printF(errs(), &F);
+      errs() << "LiveValues: finished analysis\n"
+    );
+  }
+
+  return false;
+}
+
+void
+LiveValues::printF(raw_ostream &O, const Function *F) const
+{
+  LiveVals::const_iterator bbIt;
+  std::set<const Value *>::const_iterator valIt;
+  const Module *M = F->getParent();
+
+  O << "LiveValues: results of live-value analysis\n";
+
+  if(!FuncBBLiveIn.count(F) || !FuncBBLiveOut.count(F))
+  {
+    if(F->hasName())
+      O << "No liveness information for function " << F->getName() << "\n";
+    else
+      O << "No liveness information for requested function\n";
+  }
+  else
+  {
+    for(bbIt = FuncBBLiveIn.at(F).cbegin();
+        bbIt != FuncBBLiveIn.at(F).cend();
+        bbIt++)
+    {
+      const BasicBlock *bb = bbIt->first;
+      const std::set<const Value *> &liveInVals = bbIt->second;
+      const std::set<const Value *> &liveOutVals = FuncBBLiveOut.at(F).at(bb);
+
+      bb->printAsOperand(O, false, M);
+      O << "\n  Live-in:\n    ";
+      for(valIt = liveInVals.cbegin(); valIt != liveInVals.cend(); valIt++)
+      {
+        (*valIt)->printAsOperand(O, false, M);
+        O << " ";
+      }
+
+      O << "\n  Live-out:\n    ";
+      for(valIt = liveOutVals.cbegin(); valIt != liveOutVals.cend(); valIt++)
+      {
+        (*valIt)->printAsOperand(O, false, M);
+        O << " ";
+      }
+
+      O << "\n";
+    }
+  }
+}
+
+std::set<const Value *> *LiveValues::getLiveIn(const BasicBlock *BB) const
+{
+  const Function *F = BB->getParent();
+  return new std::set<const Value *>(FuncBBLiveIn.at(F).at(BB));
+}
+
+std::set<const Value *> *LiveValues::getLiveOut(const BasicBlock *BB) const
+{
+  const Function *F = BB->getParent();
+  return new std::set<const Value *>(FuncBBLiveOut.at(F).at(BB));
+}
+
+std::set<const Value *>
+*LiveValues::getLiveValues(const Instruction *inst) const
+{
+  const BasicBlock *BB = inst->getParent();
+  const Function *F = BB->getParent();
+  BasicBlock::const_reverse_iterator ri, rie;
+  std::set<const Value *> *live = nullptr;
+
+  // Note: some functions have unreachable basic blocks (e.g., functions that
+  // call exit and then return a value).  If we don't have analysis for the
+  // block, return an empty set.
+  const LiveVals &Blocks = FuncBBLiveOut.at(F);
+  if(Blocks.count(BB)) live = new std::set<const Value *>(Blocks.at(BB));
+  else return new std::set<const Value *>;
+
+  for(ri = BB->rbegin(), rie = BB->rend(); ri != rie; ri++)
+  {
+    if(&*ri == inst) break;
+
+    live->erase(&*ri);
+    for(User::const_op_iterator op = ri->op_begin();
+        op != ri->op_end();
+        op++)
+      if(includeVal(*op))
+        live->insert(*op);
+  }
+
+  live->erase(&*ri);
+  for(User::const_op_iterator op = ri->op_begin();
+      op != ri->op_end();
+      op++)
+    if(includeVal(*op))
+      live->insert(*op);
+
+  return live;
+}
+
+///////////////////////////////////////////////////////////////////////////////
+// Private API
+///////////////////////////////////////////////////////////////////////////////
+
+bool LiveValues::includeVal(const llvm::Value *val) const
+{
+  bool include = true;
+
+  // TODO other values that should be filtered out?
+  if(isa<BasicBlock>(val))
+    include = false;
+  else if(isa<InlineAsm>(val) && !inlineasm)
+    include = false;
+  else if(isa<BitCastInst>(val) && !bitcasts)
+    include = false;
+  else if(isa<CmpInst>(val) && !comparisons)
+    include = false;
+  else if(isa<Constant>(val) && !constants)
+    include = false;
+  else if(isa<MetadataAsValue>(val) && !metadata)
+    include = false;
+
+  return include;
+}
+
+unsigned LiveValues::phiUses(const BasicBlock *B,
+                             const BasicBlock *S,
+                             std::set<const Value *> &uses)
+{
+  const PHINode *phi;
+  unsigned added = 0;
+
+  for(BasicBlock::const_iterator it = S->begin(); it != S->end(); it++)
+  {
+    if((phi = dyn_cast<PHINode>(&*it))) {
+      for(unsigned i = 0; i < phi->getNumIncomingValues(); i++)
+        if(phi->getIncomingBlock(i) == B &&
+           includeVal(phi->getIncomingValue(i)))
+          if(uses.insert(phi->getIncomingValue(i)).second)
+            added++;
+    }
+    else break; // phi-nodes are always at the start of the basic block
+  }
+
+  return added;
+}
+
+unsigned LiveValues::phiDefs(const BasicBlock *B,
+                             std::set<const Value *> &uses)
+{
+  const PHINode *phi;
+  unsigned added = 0;
+
+  for(BasicBlock::const_iterator it = B->begin(); it != B->end(); it++)
+  {
+    if((phi = dyn_cast<PHINode>(&*it))) {
+      if(includeVal(phi))
+        if(uses.insert(&*it).second)
+          added++;
+    }
+    else break; // phi-nodes are always at the start of the basic block
+  }
+
+  return added;
+}
+
+void LiveValues::dagDFS(Function &F, LiveVals &liveIn, LiveVals &liveOut)
+{
+  std::set<const Value *> live, phiDefined;
+  std::set<Edge> loopEdges;
+  SmallVector<Edge, 16> loopEdgeVec;
+
+  /* Find loop edges & convert to set for existence checking. */
+  FindFunctionBackedges(F, loopEdgeVec);
+  for(SmallVectorImpl<Edge>::const_iterator eit = loopEdgeVec.begin();
+      eit != loopEdgeVec.end();
+      eit++)
+    loopEdges.insert(*eit);
+
+  /* Calculate partial liveness sets for CFG nodes. */
+  for(auto B = po_iterator<const BasicBlock *>::begin(&F.getEntryBlock());
+      B != po_iterator<const BasicBlock *>::end(&F.getEntryBlock());
+      B++)
+  {
+    /* Calculate live-out set (lines 4-7 of Algorithm 2). */
+    for(succ_const_iterator S = succ_begin(*B); S != succ_end(*B); S++)
+    {
+      // Note: skip self-loop-edges, as adding values from phi-uses of this
+      // block causes use-def violations, and LLVM will complain.  This
+      // shouldn't matter, as phi-defs will cover this case.
+      if(*S == *B) continue;
+
+      phiUses(*B, *S, live);
+      if(!loopEdges.count(Edge(*B, *S)))
+      {
+        phiDefs(*S, phiDefined);
+        for(std::set<const Value *>::const_iterator vi = liveIn[*S].begin();
+            vi != liveIn[*S].end();
+            vi++)
+          if(!phiDefined.count(*vi) && includeVal(*vi)) live.insert(*vi);
+        phiDefined.clear();
+      }
+    }
+    liveOut.insert(LiveValsPair(*B, std::set<const Value *>(live)));
+
+    /* Calculate live-in set (lines 8-11 of Algorithm 2). */
+    for(BasicBlock::const_reverse_iterator inst = (*B)->rbegin();
+        inst != (*B)->rend();
+        inst++)
+    {
+      if(isa<PHINode>(&*inst)) break;
+
+      live.erase(&*inst);
+      for(User::const_op_iterator op = inst->op_begin();
+          op != inst->op_end();
+          op++)
+        if(includeVal(*op)) live.insert(*op);
+    }
+    phiDefs(*B, live);
+    liveIn.insert(LiveValsPair(*B, std::set<const Value *>(live)));
+
+    live.clear();
+
+    LLVM_DEBUG(
+      errs() << "  ";
+      (*B)->printAsOperand(errs(), false);
+      errs() << ":\n";
+      errs() << "    Live-in:\n      ";
+      std::set<const Value *>::const_iterator it;
+      for(it = liveIn[*B].begin(); it != liveIn[*B].end(); it++)
+      {
+        (*it)->printAsOperand(errs(), false);
+        errs() << " ";
+      }
+      errs() << "\n    Live-out:\n      ";
+      for(it = liveOut[*B].begin(); it != liveOut[*B].end(); it++)
+      {
+        (*it)->printAsOperand(errs(), false);
+        errs() << " ";
+      }
+      errs() << "\n";
+    );
+  }
+}
+
+void LiveValues::constructLoopNestingForest(Function &F,
+                                            LoopNestingForest &LNF)
+{
+  LoopInfo &LI = getAnalysis<LoopInfoWrapperPass>().getLoopInfo();
+
+  for(scc_iterator<Function *> scc = scc_begin(&F);
+      scc != scc_end(&F);
+      ++scc)
+  {
+    const std::vector<BasicBlock *> &SCC = *scc;
+    LNF.emplace_back(SCC, LI);
+
+    LLVM_DEBUG(
+      errs() << "Loop nesting tree: "
+             << LNF.back().size() << " node(s), loop-nesting depth: "
+             << LNF.back().depth() << "\n";
+      LNF.back().print(errs());
+      errs() << "\n"
+    );
+  }
+}
+
+void LiveValues::propagateValues(const LoopNestingTree &loopNest,
+                                 LiveVals &liveIn,
+                                 LiveVals &liveOut)
+{
+  std::set<const Value *> liveLoop, phiDefined;
+
+  /* Iterate over all loop nodes. */
+  for(LoopNestingTree::loop_iterator loop = loopNest.loop_begin();
+      loop != loopNest.loop_end();
+      loop++)
+  {
+    /* Calculate LiveLoop (lines 3 & 4 of Algorithm 3). */
+    phiDefs(*loop, phiDefined);
+    for(std::set<const Value *>::const_iterator it = liveIn[*loop].begin();
+        it != liveIn[*loop].end();
+        it++)
+      if(!phiDefined.count(*it) && includeVal(*it))
+        liveLoop.insert(*it);
+
+    /* Propagate values to children (lines 5-8 of Algorithm 3). */
+    for(LoopNestingTree::child_iterator child = loopNest.children_begin(loop);
+        child != loopNest.children_end(loop);
+        child++) {
+      for(std::set<const Value *>::const_iterator it = liveLoop.begin();
+          it != liveLoop.end();
+          it++) {
+        liveIn[*child].insert(*it);
+        liveOut[*child].insert(*it);
+      }
+    }
+
+    liveLoop.clear();
+  }
+}
+
+void LiveValues::loopTreeDFS(LoopNestingForest &LNF,
+                             LiveVals &liveIn,
+                             LiveVals &liveOut)
+{
+  LoopNestingForest::const_iterator it;
+  for(it = LNF.begin(); it != LNF.end(); it++)
+    propagateValues(*it, liveIn, liveOut);
+}
diff --git a/llvm/lib/Analysis/LoopNestingTree.cpp b/llvm/lib/Analysis/LoopNestingTree.cpp
new file mode 100644
index 00000000000..af62cdcc435
--- /dev/null
+++ b/llvm/lib/Analysis/LoopNestingTree.cpp
@@ -0,0 +1,147 @@
+#include "llvm/Analysis/LoopNestingTree.h"
+#include "llvm/Support/raw_ostream.h"
+
+using namespace llvm;
+
+///////////////////////////////////////////////////////////////////////////////
+// Public API
+///////////////////////////////////////////////////////////////////////////////
+
+LoopNestingTree::LoopNestingTree(const std::vector<BasicBlock *> &SCC,
+                                 const LoopInfo &LI)
+  : _size(1), _depth(1), _root(nullptr)
+{
+  unsigned depth = 1, nodeDepth;
+  const Loop *loop = nullptr;
+  Node *loopHeader = nullptr, *newHeader = nullptr;
+  std::list<Node *> work;
+
+  /* Bootstrap by grabbing the loop of the first basic block encountered. */
+  loop = LI[SCC.front()];
+  if(!loop) // Is the SCC actually a loop?
+  {
+    _root = new Node(SCC.front(), nullptr, true);
+    return;
+  }
+
+  /* Get header of outermost loop, the tree's root. */
+  while(loop->getLoopDepth() > 1)
+    loop = loop->getParentLoop();
+  _root = new Node(loop->getHeader(), nullptr, true);
+  work.push_back(_root);
+
+  /* Parse the loop-headers of the SCC into the tree. */
+  while(work.size())
+  {
+    loopHeader = work.front();
+    work.pop_front();
+    loop = LI[loopHeader->bb];
+    depth = LI.getLoopDepth(loopHeader->bb);
+    _depth = (depth > _depth ? depth : _depth);
+
+    /* Add children of the loop header. */
+    for(auto bbi = loop->block_begin() + 1; bbi != loop->block_end(); bbi++)
+    {
+      nodeDepth = LI.getLoopDepth(*bbi);
+      if(nodeDepth == depth) // Regular child node
+      {
+        loopHeader->addChild(new Node(*bbi, loopHeader, false));
+        _size++;
+      }
+      else if(nodeDepth == (depth + 1) && // Header of nested loop
+              LI.isLoopHeader(*bbi))
+      {
+        newHeader = new Node(*bbi, loopHeader, true);
+        loopHeader->addChild(newHeader);
+        work.push_back(newHeader);
+        _size++;
+      }
+    }
+  }
+}
+
+LoopNestingTree::loop_iterator LoopNestingTree::loop_iterator::operator++()
+{
+  loop_iterator me = *this;
+  if(remaining.size())
+  {
+    cur = remaining.front();
+    remaining.pop();
+    addLoopHeaders();
+  }
+  else cur = nullptr;
+  return me;
+}
+
+LoopNestingTree::loop_iterator
+LoopNestingTree::loop_iterator::operator++(int junk)
+{
+  if(remaining.size())
+  {
+    cur = remaining.front();
+    remaining.pop();
+    addLoopHeaders();
+  }
+  else cur = nullptr;
+  return *this;
+}
+
+LoopNestingTree::child_iterator::child_iterator(loop_iterator &parent,
+                                                enum location loc)
+{
+  if(loc == BEGIN) it = parent.cur->children.begin();
+  else it = parent.cur->children.end();
+}
+
+///////////////////////////////////////////////////////////////////////////////
+// Private API
+///////////////////////////////////////////////////////////////////////////////
+
+void LoopNestingTree::print(raw_ostream &O, Node *node, unsigned depth) const
+{
+  for(unsigned i = 0; i < depth; i++) O << " ";
+  node->bb->printAsOperand(O, false);
+  O << "\n";
+  if(node->children.size())
+  {
+    for(unsigned i = 0; i < depth; i++) O << " ";
+    O << "\\\n";
+
+    for(std::list<Node *>::const_iterator it = node->children.begin();
+        it != node->children.end();
+        it++)
+    {
+      if((*it)->isLoopHeader) print(O, (*it), depth + 1);
+      else
+      {
+        for(unsigned i = 0; i < depth + 1; i++) O << " ";
+        (*it)->bb->printAsOperand(O, false);
+        O << "\n";
+      }
+    }
+  }
+}
+
+void LoopNestingTree::deleteRecursive(Node *node)
+{
+  for(std::list<Node *>::iterator it = node->children.begin();
+      it != node->children.end();
+      it++)
+  {
+    if((*it)->isLoopHeader) deleteRecursive(*it);
+    else delete *it;
+  }
+  delete node;
+}
+
+void LoopNestingTree::loop_iterator::addLoopHeaders()
+{
+  if(cur != nullptr)
+  {
+    for(std::list<Node *>::const_iterator it = cur->children.begin();
+        it != cur->children.end();
+        it++)
+      if((*it)->isLoopHeader)
+        remaining.push(*it);
+  }
+}
diff --git a/llvm/lib/Analysis/LoopPaths.cpp b/llvm/lib/Analysis/LoopPaths.cpp
new file mode 100644
index 00000000000..b9020b0891c
--- /dev/null
+++ b/llvm/lib/Analysis/LoopPaths.cpp
@@ -0,0 +1,549 @@
+//===- LoopPaths.h - Enumerate paths in loops -------------------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements analysis which enumerates paths in loops.  In
+// particular, this pass calculates all paths in loops which are of the
+// following form:
+//
+//  - Header to backedge block, with no equivalence points on the path
+//  - Header to block with equivalence point
+//  - Block with equivalence point to block with equivalence point
+//  - Block with equivalence point to backedge block
+//
+// Note that backedge blocks may or may not also be exit blocks.
+//
+//===----------------------------------------------------------------------===//
+
+#include <queue>
+#include "llvm/Analysis/LoopPaths.h"
+#include "llvm/Analysis/PopcornUtil.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "looppaths"
+
+const static cl::opt<unsigned>
+MaxNumPaths("max-num-paths", cl::Hidden, cl::init(10000),
+  cl::desc("Max number of paths to analyze"));
+
+void LoopPathUtilities::populateLoopNest(Loop *L, LoopNest &Nest) {
+  std::queue<Loop *> ToVisit;
+  Nest.clear();
+  Nest.insert(L);
+  ToVisit.push(L);
+
+  while(ToVisit.size()) {
+    const Loop *Sub = ToVisit.front();
+    ToVisit.pop();
+    for(auto L : Sub->getSubLoops()) {
+      Nest.insert(L);
+      ToVisit.push(L);
+    }
+  }
+}
+
+void LoopPathUtilities::getSubBlocks(Loop *L, BlockSet &SubBlocks) {
+  SubBlocks.clear();
+  LoopNest Nest;
+
+  for(auto Sub : L->getSubLoops()) {
+    populateLoopNest(Sub, Nest);
+    for(auto Nested : Nest)
+      for(auto BB : Nested->getBlocks())
+        SubBlocks.insert(BB);
+  }
+}
+
+LoopPath::LoopPath(const std::vector<PathNode> &NodeVector,
+                   const Instruction *Start, const Instruction *End,
+                   bool StartsAtHeader, bool EndsAtBackedge)
+  : Start(Start), End(End), StartsAtHeader(StartsAtHeader),
+    EndsAtBackedge(EndsAtBackedge) {
+  assert(NodeVector.size() && "Trivial path");
+  assert(Start && Start->getParent() == NodeVector.front().getBlock() &&
+         "Invalid starting instruction");
+  assert(End && End->getParent() == NodeVector.back().getBlock() &&
+         "Invalid ending instruction");
+
+  for(auto Node : NodeVector) Nodes.insert(Node);
+}
+
+std::string LoopPath::toString() const {
+  std::string buf = "Path with " + std::to_string(Nodes.size()) + " nodes(s)\n";
+
+  buf += "  Start: ";
+  if(Start->hasName()) buf += Start->getName();
+  else buf += "<unnamed instruction>";
+  buf += "\n";
+
+  buf += "  End: ";
+  if(End->hasName()) buf += End->getName();
+  else buf += "<unnamed instruction>";
+  buf += "\n";
+
+  buf += "  Nodes:\n";
+  for(auto Node : Nodes) {
+    buf += "    ";
+    const BasicBlock *BB = Node.getBlock();
+    if(BB->hasName()) buf += BB->getName();
+    else buf += "<unnamed block>";
+    if(Node.isSubLoopExit()) buf += " (sub-loop exit)";
+    buf += "\n";
+  }
+
+  return buf;
+}
+
+void LoopPath::print(raw_ostream &O) const {
+  O << "    Path with " << std::to_string(Nodes.size()) << " nodes(s)\n";
+  O << "    Start:"; Start->print(O); O << "\n";
+  O << "    End:"; End->print(O); O << "\n";
+  O << "    Nodes:\n";
+  for(auto Node : Nodes) {
+    const BasicBlock *BB = Node.getBlock();
+    if(BB->hasName()) O << "      " << BB->getName();
+    else O << "      <unnamed block>";
+    if(Node.isSubLoopExit()) O << " (sub-loop exit)";
+    O << "\n";
+  }
+}
+
+void EnumerateLoopPaths::getAnalysisUsage(AnalysisUsage &AU) const {
+  AU.addRequired<LoopInfoWrapperPass>();
+  AU.setPreservesAll();
+}
+
+/// Search over the instructions in a basic block (starting at I) for
+/// equivalence points.  Return an equivalence point if found, or null
+/// otherwise.
+static const Instruction *hasEquivalencePoint(const Instruction *I) {
+  if(!I) return nullptr;
+  for(BasicBlock::const_iterator it(I), e = I->getParent()->end();
+      it != e; ++it)
+    if(Popcorn::isEquivalencePoint(&*it)) return &*it;
+  return nullptr;
+}
+
+/// Add a value to a list if it's not already contained in the list.  This is
+/// used to unique new instructions at which to start searches, as multiple
+/// paths may end at the same equivalence point (but we don't need to search
+/// it multiple times).
+static inline void pushIfNotPresent(const Instruction *I,
+                                    std::list<const Instruction *> &List) {
+  if(std::find(List.begin(), List.end(), I) == List.end()) List.push_back(I);
+}
+
+/// Return whether the current path contains a basic block.
+static inline bool pathContains(const std::vector<PathNode> &Path,
+                                const BasicBlock *BB) {
+  for(auto &Node : Path)
+    if(Node.getBlock() == BB) return true;
+  return false;
+}
+
+// TODO this needs to be converted to iteration rather than recursion
+
+void
+EnumerateLoopPaths::getSubLoopSuccessors(const BasicBlock *Successor,
+                                   std::vector<const Instruction *> &EqPoint,
+                                   std::vector<const Instruction *> &Spanning) {
+  const Instruction *Term;
+  const Loop *SubLoop;
+  SmallVector<BasicBlock *, 4> ExitBlocks;
+
+  EqPoint.clear();
+  Spanning.clear();
+  assert(CurLoop->contains(Successor) && SubLoopBlocks.count(Successor) &&
+         "Invalid sub-loop block");
+
+  SubLoop = LI->getLoopFor(Successor);
+  SubLoop->getExitingBlocks(ExitBlocks);
+  for(auto Exit : ExitBlocks) {
+    Term = Exit->getTerminator();
+    if(HasSpPath[SubLoop][Exit]) Spanning.push_back(Term);
+    if(HasEqPointPath[SubLoop][Exit]) EqPoint.push_back(Term);
+  }
+}
+
+static inline void printNewPath(raw_ostream &O, const LoopPath &Path) {
+  O << "Found path that start at ";
+  if(Path.startsAtHeader()) O << "the header";
+  else O << "an equivalence point";
+  O << " and ends at ";
+  if(Path.endsAtBackedge()) O << "a loop backedge";
+  else O << "an equivalence point";
+  Path.print(O);
+}
+
+bool EnumerateLoopPaths::loopDFS(const Instruction *I,
+                                 LoopDFSInfo &DFSI,
+                                 std::vector<LoopPath> &CurPaths,
+                                 std::list<const Instruction *> &NewPaths) {
+  const Instruction *EqPoint;
+  const BasicBlock *BB = I->getParent(), *PathBlock;
+  std::vector<const Instruction *> EqPointInsts, SpanningInsts;
+
+  if(!SubLoopBlocks.count(BB)) {
+    if(pathContains(DFSI.PathNodes, BB)) {
+      DetectedCycle = true;
+      return false;
+    }
+
+    DFSI.PathNodes.emplace_back(BB, false);
+
+    if((EqPoint = hasEquivalencePoint(I))) {
+      CurPaths.emplace_back(DFSI.PathNodes, DFSI.Start, EqPoint,
+                            DFSI.StartsAtHeader, false);
+
+      if(CurPaths.size() > MaxNumPaths) {
+        TooManyPaths = true;
+        return false;
+      }
+
+      for(auto Node : DFSI.PathNodes) {
+        PathBlock = Node.getBlock();
+        if(!SubLoopBlocks.count(PathBlock))
+          HasEqPointPath[CurLoop][PathBlock] = true;
+      }
+
+      // Add instruction after equivalence point (or at start of successor
+      // basic blocks if EqPoint is the last instruction in its block) as start
+      // of new equivalence point path to be searched.
+      if(!EqPoint->isTerminator())
+        pushIfNotPresent(EqPoint->getNextNode(), NewPaths);
+      else {
+        for(auto Succ : successors(BB)) {
+          if(!CurLoop->contains(Succ) || // Skip exit blocks & latches
+             Succ == CurLoop->getHeader()) continue;
+          else if(!SubLoopBlocks.count(Succ)) // Successor is in same outer loop
+            pushIfNotPresent(&Succ->front(), NewPaths);
+          else { // Successor is in sub-loop
+            getSubLoopSuccessors(Succ, EqPointInsts, SpanningInsts);
+            for(auto SLE : EqPointInsts) pushIfNotPresent(SLE, NewPaths);
+            for(auto SLE : SpanningInsts)
+              if(!loopDFS(SLE, DFSI, CurPaths, NewPaths)) return false;
+          }
+        }
+      }
+
+      LLVM_DEBUG(printNewPath(dbgs(), CurPaths.back()));
+    }
+    else if(Latches.count(BB)) {
+      CurPaths.emplace_back(DFSI.PathNodes, DFSI.Start, BB->getTerminator(),
+                            DFSI.StartsAtHeader, true);
+
+      if(CurPaths.size() > MaxNumPaths) {
+        TooManyPaths = true;
+        return false;
+      }
+
+      if(DFSI.StartsAtHeader) {
+        for(auto Node : DFSI.PathNodes) {
+          PathBlock = Node.getBlock();
+          if(!SubLoopBlocks.count(PathBlock))
+            HasSpPath[CurLoop][PathBlock] = true;
+        }
+      }
+      else {
+        for(auto Node : DFSI.PathNodes) {
+          PathBlock = Node.getBlock();
+          if(!SubLoopBlocks.count(PathBlock))
+            HasEqPointPath[CurLoop][PathBlock] = true;
+        }
+      }
+
+      LLVM_DEBUG(printNewPath(dbgs(), CurPaths.back()));
+    }
+    else {
+      for(auto Succ : successors(BB)) {
+        if(!CurLoop->contains(Succ)) continue;
+        else if(!SubLoopBlocks.count(Succ)) {
+          if(!loopDFS(&Succ->front(), DFSI, CurPaths, NewPaths))
+            return false;
+        }
+        else {
+          getSubLoopSuccessors(Succ, EqPointInsts, SpanningInsts);
+          for(auto SLE : EqPointInsts) {
+            // Rather than stopping the path at the equivalence point inside
+            // of a sub-loop, stop it at the end of the current block
+            // TODO this can create duplicates for a path that reaches a
+            // sub-loop with multiple exiting blocks, but the analysis in
+            // MigrationPoints doesn't care about paths that don't end at a
+            // backedge anyway
+            CurPaths.emplace_back(DFSI.PathNodes, DFSI.Start,
+                                  BB->getTerminator(),
+                                  DFSI.StartsAtHeader, false);
+
+            if(CurPaths.size() > MaxNumPaths) {
+              TooManyPaths = true;
+              return false;
+            }
+
+            pushIfNotPresent(SLE, NewPaths);
+            LLVM_DEBUG(printNewPath(dbgs(), CurPaths.back()));
+          }
+          for(auto SLE : SpanningInsts)
+            if(!loopDFS(SLE, DFSI, CurPaths, NewPaths)) return false;
+        }
+      }
+    }
+    DFSI.PathNodes.pop_back();
+  }
+  else {
+    if(pathContains(DFSI.PathNodes, BB)) {
+      DetectedCycle = true;
+      return false;
+    }
+
+    // This is a sub-loop block; we only want to explore successors who are not
+    // contained in this sub-loop but are still contained in the current loop.
+    DFSI.PathNodes.emplace_back(BB, true);
+
+    const Loop *WeedOutLoop = LI->getLoopFor(BB);
+    for(auto Succ : successors(BB)) {
+      if(WeedOutLoop->contains(Succ) || !CurLoop->contains(Succ)) continue;
+      else if(!SubLoopBlocks.count(Succ)) {
+        if(!loopDFS(&Succ->front(), DFSI, CurPaths, NewPaths))
+          return false;
+      }
+      else {
+        getSubLoopSuccessors(Succ, EqPointInsts, SpanningInsts);
+        for(auto SLE : EqPointInsts) {
+          // TODO this can create duplicates for a path that reaches a sub-loop
+          // with multiple exiting blocks, but the analysis in MigrationPoints
+          // doesn't care about paths that don't end at a backedge anyway
+          CurPaths.emplace_back(DFSI.PathNodes, DFSI.Start,
+                                BB->getTerminator(),
+                                DFSI.StartsAtHeader, false);
+
+          if(CurPaths.size() > MaxNumPaths) {
+            TooManyPaths = true;
+            return false;
+          }
+
+          LLVM_DEBUG(printNewPath(dbgs(), CurPaths.back()));
+          pushIfNotPresent(SLE, NewPaths);
+        }
+        for(auto SLE: SpanningInsts)
+          if(!loopDFS(SLE, DFSI, CurPaths, NewPaths)) return false;
+      }
+    }
+    DFSI.PathNodes.pop_back();
+  }
+
+  return true;
+}
+
+bool EnumerateLoopPaths::analyzeLoop(Loop *L, std::vector<LoopPath> &CurPaths) {
+  std::list<const Instruction *> NewPaths;
+  SmallVector<BasicBlock *, 4> LatchVec;
+  LoopDFSInfo DFSI;
+
+  CurPaths.clear();
+  HasSpPath[L].clear();
+  HasEqPointPath[L].clear();
+
+  LLVM_DEBUG(
+    DebugLoc DL(L->getStartLoc());
+    dbgs() << "Enumerating paths";
+    if(DL) {
+      dbgs() << " for loop at ";
+      DL.print(dbgs());
+    }
+    dbgs() << ": "; L->dump();
+  );
+
+  // Store information about the current loop, it's backedges, and sub-loops
+  CurLoop = L;
+  Latches.clear();
+  L->getLoopLatches(LatchVec);
+  for(auto L : LatchVec) Latches.insert(L);
+  LoopPathUtilities::getSubBlocks(L, SubLoopBlocks);
+
+  assert(Latches.size() && "No backedges, not a loop?");
+  assert(!SubLoopBlocks.count(L->getHeader()) && "Header is in sub-loop?");
+
+  DFSI.Start = &L->getHeader()->front();
+  DFSI.StartsAtHeader = true;
+  if(!loopDFS(DFSI.Start, DFSI, CurPaths, NewPaths)) return false;
+  assert(DFSI.PathNodes.size() == 0 && "Invalid traversal");
+
+  DFSI.StartsAtHeader = false;
+  while(!NewPaths.empty()) {
+    DFSI.Start = NewPaths.front();
+    NewPaths.pop_front();
+    if(!loopDFS(DFSI.Start, DFSI, CurPaths, NewPaths)) return false;
+    assert(DFSI.PathNodes.size() == 0 && "Invalid traversal");
+  }
+
+  return true;
+}
+
+bool EnumerateLoopPaths::runOnFunction(Function &F) {
+  LLVM_DEBUG(dbgs() << "\n********** ENUMERATE LOOP PATHS **********\n"
+               << "********** Function: " << F.getName() << "\n\n");
+
+  reset();
+  TooManyPaths = DetectedCycle = false;
+  LI = &getAnalysis<LoopInfoWrapperPass>().getLoopInfo();
+  std::vector<LoopNest> Nests;
+
+  // Discover all loop nests.
+  for(LoopInfo::iterator I = LI->begin(), E = LI->end(); I != E; ++I) {
+    if((*I)->getLoopDepth() != 1) continue;
+    Nests.push_back(LoopNest());
+    LoopPathUtilities::populateLoopNest(*I, Nests.back());
+  }
+
+  // Search all loops within all loop nests.
+  for(auto Nest : Nests) {
+    LLVM_DEBUG(dbgs() << "Analyzing nest with " << std::to_string(Nest.size())
+                 << " loops\n");
+
+    for(auto L : Nest) {
+      std::vector<LoopPath> &CurPaths = Paths[L];
+      assert(CurPaths.size() == 0 && "Re-processing loop?");
+      if(!analyzeLoop(L, CurPaths)) break;
+    }
+
+    if(analysisFailed()) break;
+  }
+
+  if(TooManyPaths) {
+    LLVM_DEBUG(dbgs() << "WARNING: too many paths, bailing on analysis\n");
+    reset();
+  }
+
+  if(DetectedCycle) {
+    LLVM_DEBUG(dbgs() << "WARNING: detected a cycle, bailing on analysis\n");
+    reset();
+  }
+
+  return false;
+}
+
+void EnumerateLoopPaths::rerunOnLoop(Loop *L) {
+  // We *should* be analyzing a loop for the 2+ time
+  std::vector<LoopPath> &CurPaths = Paths[L];
+  LLVM_DEBUG(if(!CurPaths.size()) dbgs() << "  -> No previous analysis?\n");
+  if(!analyzeLoop(L, CurPaths)) reset();
+}
+
+void EnumerateLoopPaths::getPaths(const Loop *L,
+                                  std::vector<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second) P.push_back(&Path);
+}
+
+void
+EnumerateLoopPaths::getBackedgePaths(const Loop *L,
+                                     std::vector<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.endsAtBackedge()) P.push_back(&Path);
+}
+
+void
+EnumerateLoopPaths::getBackedgePaths(const Loop *L,
+                                     std::set<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.endsAtBackedge()) P.insert(&Path);
+}
+
+void
+EnumerateLoopPaths::getSpanningPaths(const Loop *L,
+                                     std::vector<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.isSpanningPath()) P.push_back(&Path);
+}
+
+void
+EnumerateLoopPaths::getSpanningPaths(const Loop *L,
+                                     std::set<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.isSpanningPath()) P.insert(&Path);
+}
+
+void
+EnumerateLoopPaths::getEqPointPaths(const Loop *L,
+                                    std::vector<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.isEqPointPath()) P.push_back(&Path);
+}
+
+void
+EnumerateLoopPaths::getEqPointPaths(const Loop *L,
+                                    std::set<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.isEqPointPath()) P.insert(&Path);
+}
+
+void
+EnumerateLoopPaths::getPathsThroughBlock(const Loop *L, BasicBlock *BB,
+                                         std::vector<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  assert(L->contains(BB) && "Loop does not contain basic block");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.contains(BB)) P.push_back(&Path);
+}
+
+void
+EnumerateLoopPaths::getPathsThroughBlock(const Loop *L, BasicBlock *BB,
+                                         std::set<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  assert(L->contains(BB) && "Loop does not contain basic block");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.contains(BB)) P.insert(&Path);
+}
+
+bool EnumerateLoopPaths::spanningPathThroughBlock(const Loop *L,
+                                                  const BasicBlock *BB) const {
+  assert(hasPaths(L) && "No paths for loop");
+  assert(L->contains(BB) && "Loop does not contain basic block");
+  return HasSpPath.find(L)->second.find(BB)->second;
+}
+
+bool EnumerateLoopPaths::eqPointPathThroughBlock(const Loop *L,
+                                                 const BasicBlock *BB) const {
+  assert(hasPaths(L) && "No paths for loop");
+  assert(L->contains(BB) && "Loop does not contain basic block");
+  return HasEqPointPath.find(L)->second.find(BB)->second;
+}
+
+char EnumerateLoopPaths::ID = 0;
+INITIALIZE_PASS_BEGIN(EnumerateLoopPaths, "looppaths",
+                      "Enumerate paths in loops",
+                      false, true)
+INITIALIZE_PASS_DEPENDENCY(LoopInfoWrapperPass)
+INITIALIZE_PASS_END(EnumerateLoopPaths, "looppaths",
+                    "Enumerate paths in loops",
+                    false, true)
+
+
+namespace llvm {
+  FunctionPass *createEnumerateLoopPathsPass()
+  { return new EnumerateLoopPaths(); }
+}
diff --git a/llvm/lib/Analysis/PopcornCompatibility.cpp b/llvm/lib/Analysis/PopcornCompatibility.cpp
new file mode 100644
index 00000000000..398edb29055
--- /dev/null
+++ b/llvm/lib/Analysis/PopcornCompatibility.cpp
@@ -0,0 +1,141 @@
+//===- PopcornCompatibility.cpp -------------------------------------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Looks for code features which are not currently handled by the Popcorn
+// compiler/stack transformation process.  These code features either *might*
+// cause issues during stack transformation (and hence the compiler will issue
+// a warning), or are guaranteed to not be handled correctly and will cause
+// compilation to abort.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Pass.h"
+#include "llvm/IR/CallSite.h"
+#include "llvm/IR/DiagnosticInfo.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/LLVMContext.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "popcorn-compat"
+
+namespace {
+
+class PopcornCompatibility : public FunctionPass
+{
+public:
+  static char ID;
+
+  PopcornCompatibility() : FunctionPass(ID) {
+    initializePopcornCompatibilityPass(*PassRegistry::getPassRegistry());
+  }
+  ~PopcornCompatibility() {}
+
+  virtual StringRef getPassName() const
+  { return "Popcorn compatibility checking"; }
+
+  //===--------------------------------------------------------------------===//
+  // Warning & error printing
+  //===--------------------------------------------------------------------===//
+
+  /// Emit a warning message for a given location, denoted by an instruction.
+  static void warn(const Instruction *I, const std::string &Msg) {
+    const Function *F = I->getParent()->getParent();
+    std::string Warning("Popcorn compatibility");
+    if(F->hasName()) {
+      Warning += " in function '";
+      Warning += F->getName();
+      Warning += "'";
+    }
+    Warning += ": " + Msg;
+    DiagnosticInfoOptimizationFailure DI(*F, I->getDebugLoc(), Warning);
+    I->getContext().diagnose(DI);
+  }
+
+  /// Emit a warning message for a function.
+  static void warn(const Function &F, const std::string &Msg)
+  { warn(&*F.getEntryBlock().begin(), Msg); }
+
+  /// Emit an error message for a given location, denoted by an instruction.
+  static void error(const Instruction *I, const std::string &Msg) {
+    const Function *F = I->getParent()->getParent();
+    std::string Error("Popcorn compatibility");
+    if(F->hasName()) {
+      Error += " in function '";
+      Error += F->getName();
+      Error += "'";
+    }
+    Error += ": " + Msg;
+    DiagnosticInfoOptimizationError DI(*F, I->getDebugLoc(), Error);
+    I->getContext().diagnose(DI);
+  }
+
+  //===--------------------------------------------------------------------===//
+  // Properties of instructions
+  //===--------------------------------------------------------------------===//
+
+  /// Return whether the alloca is dynamically-sized.
+  static bool isVariableSizedAlloca(const Instruction &I) {
+    const AllocaInst *AI;
+    if((AI = dyn_cast<AllocaInst>(&I)) && !AI->isStaticAlloca()) return true;
+    else return false;
+  }
+
+  static bool isInlineAsm(const Instruction &I) {
+    if((isa<CallInst>(I) || isa<InvokeInst>(I)) && !isa<IntrinsicInst>(I)) {
+      ImmutableCallSite CS(&I);
+      if(CS.isInlineAsm()) return true;
+    }
+    return false;
+  }
+
+  //===--------------------------------------------------------------------===//
+  // The main show
+  //===--------------------------------------------------------------------===//
+
+  virtual bool runOnFunction(Function &F) {
+    std::string Msg;
+
+    if(!F.isDeclaration() && !F.isIntrinsic()) {
+      if(F.isVarArg()) warn(F, "function takes a variable number of arguments");
+      for(auto &BB : F) {
+        for(auto &I : BB) {
+          if(isVariableSizedAlloca(I)) {
+            Msg = "stack variable '";
+            Msg += I.getName();
+            Msg += "' is dynamically sized (will cause "
+                   "issues during code generation)";
+            error(&I, Msg);
+          }
+
+          if(isInlineAsm(I))
+            warn(&I, "inline assembly may have unanalyzable side-effects");
+
+          if(isa<VAArgInst>(I) || isa<VACopyInst>(I) || isa<VAEndInst>(I))
+            warn(&I, "va_arg not transformable across architectures");
+        }
+      }
+    }
+    return false;
+  }
+
+private:
+};
+
+} /* end anonymous namespace */
+
+char PopcornCompatibility::ID = 0;
+
+INITIALIZE_PASS(PopcornCompatibility, "popcorn-compat",
+                "Analyze code for compatibility issues", false, true)
+
+namespace llvm {
+  FunctionPass *createPopcornCompatibilityPass()
+  { return new PopcornCompatibility(); }
+}
diff --git a/llvm/lib/Analysis/SelectMigrationPoints.cpp b/llvm/lib/Analysis/SelectMigrationPoints.cpp
new file mode 100644
index 00000000000..808526a3a80
--- /dev/null
+++ b/llvm/lib/Analysis/SelectMigrationPoints.cpp
@@ -0,0 +1,1838 @@
+//===- SelectMigrationPoints.cpp ------------------------------------------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Select code locations to instrument with migration points, which are
+// locations where threads make calls to invoke the migration process in
+// addition to any other instrumentation (e.g., hardware transactional memory,
+// HTM, stops & starts).  Migration points only occur at equivalence points, or
+// locations in the program code where there is a direct mapping between
+// architecture-specific execution state, like registers and stack, across
+// different ISAs.  In our implementation, every function call site is an
+// equivalence point; hence, calls inserted to invoke the migration by
+// definition create equivalence points at the migration point.  Thus, all
+// migration points are equivalence points, but not all equivalence points are
+// migration points.
+//
+// By default, the pass only inserts migration points at the beginning and end
+// of a function.  More advanced analyses can be used to instrument function
+// bodies (in particular, loops) with more migration points and HTM execution.
+//
+// More details about equivalence points can be found in the paper "A Unified
+// Model of Pointwise Migration of Procedural Computations" by von Bank et. al
+// (http://dl.acm.org/citation.cfm?id=197402).
+//
+//===----------------------------------------------------------------------===//
+
+#include <cmath>
+#include <map>
+#include <memory>
+#include "llvm/Pass.h"
+#include "llvm/ADT/PostOrderIterator.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/StringSet.h"
+#include "llvm/ADT/SCCIterator.h"
+#include "llvm/Analysis/LoopInfo.h"
+#include "llvm/Analysis/LoopIterator.h"
+#include "llvm/Analysis/LoopPaths.h"
+#include "llvm/Analysis/PopcornUtil.h"
+#include "llvm/Analysis/ScalarEvolution.h"
+#include "llvm/Analysis/ScalarEvolutionExpressions.h"
+#include "llvm/IR/CallSite.h"
+#include "llvm/IR/DiagnosticInfo.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/Intrinsics.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/Module.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_os_ostream.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "migration-points"
+
+/// Insert more migration points into the body of a function.  Analyze
+/// execution behavior & attempt to instrument the code to reduce the time
+/// until the thread reaches a migration point.
+const static cl::opt<bool>
+MoreMigPoints("more-mig-points", cl::Hidden, cl::init(false),
+  cl::desc("Add additional migration points into the body of functions"));
+
+/// By default we assume that loops will execute "enough iterations as to
+/// require instrumentation".  That's not necessarily true, so contrain N in
+/// hitting migration point every N iterations.  If analysis determines that
+/// we need to hit analysis for some number larger than N, don't instrument
+/// the loop.
+const static cl::opt<unsigned>
+MaxItersPerMigPoint("max-iters-per-migpoint", cl::Hidden, cl::init(UINT32_MAX),
+  cl::desc("Max iterations per migration point"));
+
+/// Percent of capacity (determined by analysis type, e.g., HTM buffer size) at
+/// which point weight objects will request a new migration point be inserted.
+const static cl::opt<unsigned>
+CapacityThreshold("cap-threshold", cl::Hidden, cl::init(80),
+  cl::desc("Percent of capacity at which point a new migration point should "
+           "be inserted (only applies to -more-mig-points)"));
+
+/// Per-function capacity threshold.
+const static cl::list<std::string>
+FuncCapThreshold("func-cap", cl::Hidden, cl::ZeroOrMore,
+  cl::desc("Function-specific capacity threshold in function,value pairs"));
+
+/// Normally we instrument function entry points with migration points.  If
+/// we're below some percent of capacity at all exit points & we haven't added
+/// instrumentation into the body (i.e., nothing depends on a clean slate to
+/// start), skip this instrumentation.
+const static cl::opt<unsigned>
+StartThreshold("start-threshold", cl::Hidden, cl::init(5),
+  cl::desc("Don't instrument function entry points under a percent of "
+           "capacity (only applies to -more-mig-points), used for "
+           "small functions"));
+
+/// Per-function starting threshold.
+const static cl::list<std::string>
+FuncStartThreshold("func-start", cl::Hidden, cl::ZeroOrMore,
+  cl::desc("Function-specific start threshold in function,value pairs"));
+
+/// Normally we instrument function exit points with migration points.  If
+/// we're below some percent of capacity, skip this instrumentation (useful for
+/// very small/short-lived functions).
+const static cl::opt<unsigned>
+RetThreshold("ret-threshold", cl::Hidden, cl::init(5),
+  cl::desc("Don't instrument function exit points under a percent of "
+           "capacity (only applies to -more-mig-points)"));
+
+/// Per-function return threshold.
+const static cl::list<std::string>
+FuncRetThreshold("func-ret", cl::Hidden, cl::ZeroOrMore,
+  cl::desc("Function-specific return threshold in function,value pairs"));
+
+/// Don't instrument a specific function with extra migration points.
+const static cl::list<std::string>
+FuncNoInst("func-no-inst", cl::Hidden, cl::ZeroOrMore,
+  cl::desc("Don't instrument a particular function with migration points"));
+
+/// Target cycles between migration points when instrumenting applications with
+/// more migration points (but without HTM).  Allows tuning trade off between
+/// migration point response time and overhead.
+const static cl::opt<unsigned>
+MillionCyclesBetweenMigPoints("migpoint-cycles", cl::Hidden, cl::init(50),
+  cl::desc("Cycles between migration points, in millions of cycles"));
+
+/// Cover the application in transactional execution by inserting HTM
+/// stop/start instructions at migration points.  Tailors the analysis to
+/// reduce capacity aborts by estimating memory access behavior.
+const static cl::opt<bool>
+HTMExec("htm-execution", cl::NotHidden, cl::init(false),
+  cl::desc("Instrument migration points with HTM execution "
+           "(only supported on PowerPC 64-bit & x86-64)"));
+
+/// Disable wrapping mem<set, copy, move> instructions for which we don't know
+/// the size.
+const static cl::opt<bool>
+NoWrapUnknownMem("htm-no-wrap-unknown-mem", cl::Hidden, cl::init(false),
+  cl::desc("Disable wrapping mem<set, copy, move> of unknown size with HTM"));
+
+/// Disable wrapping libc functions which are likely to cause HTM aborts with
+/// HTM stop/start intrinsics.  Wrapping happens by default with HTM execution.
+const static cl::opt<bool>
+NoWrapLibc("htm-no-wrap-libc", cl::Hidden, cl::init(false),
+  cl::desc("Disable wrapping libc functions with HTM stop/start"));
+
+/// HTM memory read buffer size for tuning analysis when inserting additional
+/// migration points.
+const static cl::opt<unsigned>
+HTMReadBufSizeArg("htm-buf-read", cl::Hidden, cl::init(32),
+  cl::desc("HTM analysis tuning - HTM read buffer size, in kilobytes"),
+  cl::value_desc("size"));
+
+/// HTM memory write buffer size for tuning analysis when inserting additional
+/// migration points.
+const static cl::opt<unsigned>
+HTMWriteBufSizeArg("htm-buf-write", cl::Hidden, cl::init(8),
+  cl::desc("HTM analysis tuning - HTM write buffer size, in kilobytes"),
+  cl::value_desc("size"));
+
+#define KB 1024
+#define HTMReadBufSize (HTMReadBufSizeArg * KB)
+#define HTMWriteBufSize (HTMWriteBufSizeArg * KB)
+
+#define MILLION 1000000
+#define CyclesBetweenMigPoints \
+  ((unsigned long)MillionCyclesBetweenMigPoints * MILLION)
+#define MEM_WEIGHT 40
+
+STATISTIC(LoopsTransformed, "Number of loops transformed");
+STATISTIC(NumIVsAdded, "Number of induction variables added");
+
+namespace {
+
+/// Get the integer size of a value, if statically known.
+static int64_t getValueSize(const Value *V) {
+  if(isa<ConstantInt>(V)) return cast<ConstantInt>(V)->getSExtValue();
+  return -1;
+}
+
+/// Return a percentage of a value.
+static inline size_t getValuePercent(size_t V, unsigned P) {
+  assert(P <= 100 && "Invalid percentage");
+  return floor(((double)V) * (((double)P) / 100.0));
+}
+
+/// Return the number of cache lines accessed for a given number of
+/// (assumed contiguous) bytes.
+static inline size_t getNumCacheLines(size_t Bytes, unsigned LineSize) {
+  return ceil((double)Bytes / (double)LineSize);
+}
+
+/// Abstract weight metric.  Child classes implement for analyzing different
+/// resource capacities, e.g., HTM buffer sizes.
+class Weight {
+protected:
+  /// Number of times the weight was reset.
+  size_t Resets;
+
+  Weight() : Resets(0) {}
+  Weight(const Weight &C) : Resets(C.Resets) {}
+
+public:
+  virtual ~Weight() {};
+  virtual Weight *copy() const = 0;
+
+  /// Expose types of child implementations.
+  virtual bool isCycleWeight() const { return false; }
+  virtual bool isHTMWeight() const { return false; }
+
+  /// Analyze an instruction & update accounting.
+  virtual void analyze(const Instruction *I, const DataLayout *DL) = 0;
+
+  /// Return whether or not we should add a migration point.  This is tuned
+  /// based on the resource capacity and percentage threshold options.
+  virtual bool shouldAddMigPoint(unsigned percent) const {
+    return !underPercentOfThreshold(percent);
+  }
+
+  /// Reset the weight.
+  virtual void reset() { Resets++; }
+
+  /// Update this weight with the max of this weight and another.
+  virtual void max(const Weight *RHS) = 0;
+  virtual void max(const std::unique_ptr<Weight> &RHS) { max(RHS.get()); }
+
+  /// Multiply the weight by a factor, e.g., a number of loop iterations.
+  virtual void multiply(size_t factor) = 0;
+
+  /// Add another weight to this weight.
+  virtual void add(const Weight *RHS) = 0;
+  virtual void add(const std::unique_ptr<Weight> &RHS) { add(RHS.get()); }
+
+  /// Number of times this weight "fits" into the resource capacity before we
+  /// need to place a migration point.  This is used for calculating how many
+  /// iterations of a loop can be executed between migration points.
+  virtual size_t numIters(unsigned percent) const = 0;
+
+  /// Return whether or not the weight is within some percent (0-100) of the
+  /// resource capacity for a type of weight.
+  virtual bool underPercentOfThreshold(unsigned percent) const = 0;
+
+  /// Return a human-readable string describing weight information.
+  virtual std::string toString() const = 0;
+};
+
+typedef std::unique_ptr<Weight> WeightPtr;
+
+/// Weight metrics for HTM analysis, which basically depend on the number
+/// of bytes loaded & stored.
+class HTMWeight : public Weight {
+private:
+  // The number of bytes loaded & stored, respectively.
+  size_t LoadBytes, StoreBytes;
+
+  // Statistics about when the weight was reset (i.e., at HTM stop/starts).
+  size_t ResetLoad, ResetStore;
+
+public:
+  HTMWeight(size_t LoadBytes = 0, size_t StoreBytes = 0)
+    : LoadBytes(LoadBytes), StoreBytes(StoreBytes), ResetLoad(0),
+      ResetStore(0) {}
+  HTMWeight(const HTMWeight &C)
+    : Weight(C), LoadBytes(C.LoadBytes), StoreBytes(C.StoreBytes),
+      ResetLoad(C.ResetLoad), ResetStore(C.ResetStore) {}
+  virtual Weight *copy() const { return new HTMWeight(*this); }
+
+  virtual bool isHTMWeight() const { return true; }
+
+  /// Analyze an instruction for memory operations.
+  virtual void analyze(const Instruction *I, const DataLayout *DL) {
+    Type *Ty;
+
+    // TODO do extractelement, insertelement, shufflevector, extractvalue, or
+    // insertvalue read/write memory?
+    // TODO Need to handle the following instructions/instrinsics (also see
+    // Instruction::mayLoad() / Instruction::mayStore()):
+    //   llvm.masked.load
+    //   llvm.masked.store
+    //   llvm.masked.gather
+    //   llvm.masked.store
+    switch(I->getOpcode()) {
+    default: break;
+    case Instruction::Load: {
+      const LoadInst *LI = cast<LoadInst>(I);
+      Ty = LI->getPointerOperand()->getType()->getPointerElementType();
+      LoadBytes += DL->getTypeStoreSize(Ty);
+      break;
+    }
+
+    case Instruction::Store: {
+      const StoreInst *SI = cast<StoreInst>(I);
+      Ty = SI->getValueOperand()->getType();
+      StoreBytes += DL->getTypeStoreSize(Ty);
+      break;
+    }
+
+    case Instruction::AtomicCmpXchg: {
+      const AtomicCmpXchgInst *Cmp = cast<AtomicCmpXchgInst>(I);
+      Ty = Cmp->getPointerOperand()->getType()->getPointerElementType();
+      LoadBytes += DL->getTypeStoreSize(Ty);
+      StoreBytes += DL->getTypeStoreSize(Ty);
+      break;  // CJP: should this break or fall through?
+    }
+
+    case Instruction::AtomicRMW: {
+      const AtomicRMWInst *RMW = cast<AtomicRMWInst>(I);
+      Ty = RMW->getPointerOperand()->getType()->getPointerElementType();
+      LoadBytes += DL->getTypeStoreSize(Ty);
+      StoreBytes += DL->getTypeStoreSize(Ty);
+      break;  // CJP: should this break or fall through?
+    }
+
+    case Instruction::Call: {
+      const IntrinsicInst *II = dyn_cast<IntrinsicInst>(I);
+      bool Loads = false, Stores = false;
+      int64_t Size = 0;
+
+      if(!II) break;
+      switch(II->getIntrinsicID()) {
+      default: break;
+      case Intrinsic::memcpy:
+      case Intrinsic::memmove:
+        // Arguments: i8* dest, i8* src, i<x> len, i32 align, i1 isvolatile
+        Loads = Stores = true;
+        Size = getValueSize(II->getArgOperand(2));
+        break;
+      case Intrinsic::memset:
+        // Arguments: i8* dest, i8 val, i<x> len, i32 align, i1 isvolatile
+        Stores = true;
+        Size = getValueSize(II->getArgOperand(2));
+        break;
+      }
+
+      // Size > 0: we know the size statically
+      // Size < 0: we can't determine the size statically
+      // Size == 0: some intrinsic we don't care about
+      if(Size > 0) {
+        if(Loads) LoadBytes += Size;
+        if(Stores) StoreBytes += Size;
+      }
+      else if(Size < 0) {
+        // Assume we're doing heavy reading & writing -- may need to revise if
+        // transaction begin/ends are too expensive.
+        if(Loads) LoadBytes += HTMReadBufSize;
+        if(Stores) StoreBytes += HTMWriteBufSize;
+      }
+
+      break;
+    }
+    }
+  }
+
+  virtual void reset() {
+    Weight::reset();
+    ResetLoad += LoadBytes;
+    ResetStore += StoreBytes;
+    LoadBytes = StoreBytes = 0;
+  }
+
+  /// The max value for HTM weights is the max of the two weights' LoadBytes
+  /// and StoreBytes (maintained separately).
+  virtual void max(const Weight *RHS) {
+    assert(RHS->isHTMWeight() && "Cannot mix weight types");
+    const HTMWeight *W = (const HTMWeight *)RHS;
+    if(W->LoadBytes > LoadBytes) LoadBytes = W->LoadBytes;
+    if(W->StoreBytes > StoreBytes) StoreBytes = W->StoreBytes;
+  }
+
+  virtual void multiply(size_t factor) {
+    LoadBytes *= factor;
+    StoreBytes *= factor;
+  }
+
+  virtual void add(const Weight *RHS) {
+    assert(RHS->isHTMWeight() && "Cannot mix weight types");
+    const HTMWeight *W = (const HTMWeight *)RHS;
+    LoadBytes += W->LoadBytes;
+    StoreBytes += W->StoreBytes;
+  }
+
+  /// The number of times this weight's load & stores could be executed without
+  /// overflowing the capacity threshold of the HTM buffers.
+  virtual size_t numIters(unsigned percent) const {
+    size_t NumLoadIters = UINT64_MAX, NumStoreIters = UINT64_MAX,
+      FPHtmReadSize = getValuePercent(HTMReadBufSize, percent),
+      FPHtmWriteSize = getValuePercent(HTMWriteBufSize, percent);
+
+    if(!LoadBytes && !StoreBytes) return 1024; // Return a safe value
+    else {
+      if(LoadBytes) NumLoadIters = FPHtmReadSize / LoadBytes;
+      if(StoreBytes) NumStoreIters = FPHtmWriteSize / StoreBytes;
+
+      if(!NumLoadIters && !NumStoreIters) return 1;
+      else return NumLoadIters < NumStoreIters ? NumLoadIters : NumStoreIters;
+    }
+  }
+
+  virtual bool underPercentOfThreshold(unsigned percent) const {
+    if(LoadBytes <= getValuePercent(HTMReadBufSize, percent) &&
+       StoreBytes <= getValuePercent(HTMWriteBufSize, percent))
+      return true;
+    else return false;
+  }
+
+  virtual std::string toString() const {
+    return std::to_string(LoadBytes) + " byte(s) loaded, " +
+           std::to_string(StoreBytes) + " byte(s) stored";
+  }
+};
+
+/// Weight metric for temporally-spaced migration points.
+class CycleWeight : public Weight {
+private:
+  // An estimate of the number of cycles since the last migration point.
+  size_t Cycles;
+
+  // Statistics about when the weight was reset (i.e., at migration points).
+  size_t ResetCycles;
+
+public:
+  CycleWeight(size_t Cycles = 0) : Cycles(Cycles), ResetCycles(0) {}
+  CycleWeight(const CycleWeight &C)
+    : Weight(C), Cycles(C.Cycles), ResetCycles(C.ResetCycles) {}
+  virtual CycleWeight *copy() const { return new CycleWeight(*this); }
+
+  virtual bool isCycleWeight() const { return true; }
+
+  virtual void analyze(const Instruction *I, const DataLayout *DL) {
+    Type *Ty;
+
+    // Cycles are estimated using Agner Fog's instruction latency guide at
+    // http://www.agner.org/optimize/instruction_tables.pdf for "Broadwell".
+    switch(I->getOpcode()) {
+    default: break;
+    // Terminator instructions
+    // TODO Ret, Invoke, Resume
+    case Instruction::Br: Cycles += 2; break;
+    case Instruction::Switch: Cycles += 2; break;
+    case Instruction::IndirectBr: Cycles += 2; break;
+
+    // Binary instructions
+    case Instruction::Add: Cycles++; break;
+    case Instruction::FAdd: Cycles += 3; break;
+    case Instruction::Sub: Cycles++; break;
+    case Instruction::FSub: Cycles += 3; break;
+    case Instruction::Mul: Cycles += 2; break;
+    case Instruction::FMul: Cycles += 3; break;
+    case Instruction::UDiv: Cycles += 73; break;
+    case Instruction::SDiv: Cycles += 81; break;
+    case Instruction::FDiv: Cycles += 14; break;
+    case Instruction::URem: Cycles += 73; break;
+    case Instruction::SRem: Cycles += 81; break;
+    case Instruction::FRem: Cycles += 14; break;
+
+    // Logical operators
+    case Instruction::Shl: Cycles += 2; break;
+    case Instruction::LShr: Cycles += 2; break;
+    case Instruction::AShr: Cycles += 2; break;
+    case Instruction::And: Cycles += 1; break;
+    case Instruction::Or: Cycles += 1; break;
+    case Instruction::Xor: Cycles += 1; break;
+
+    // Memory instructions
+    case Instruction::Load: {
+      const LoadInst *LI = cast<LoadInst>(I);
+      Ty = LI->getPointerOperand()->getType()->getPointerElementType();
+      Cycles += getNumCacheLines(DL->getTypeStoreSize(Ty), 64) * MEM_WEIGHT;
+      break;
+    }
+    case Instruction::Store: {
+      const StoreInst *SI = cast<StoreInst>(I);
+      Ty = SI->getValueOperand()->getType();
+      Cycles += getNumCacheLines(DL->getTypeStoreSize(Ty), 64) * MEM_WEIGHT;
+      break;
+    }
+    case Instruction::GetElementPtr: Cycles++; break;
+    case Instruction::Fence: Cycles += 33; break;
+    case Instruction::AtomicCmpXchg: Cycles += 21; break;
+    case Instruction::AtomicRMW: Cycles += 21; break;
+
+    // Cast instructions
+    case Instruction::Trunc: Cycles++; break;
+    case Instruction::ZExt: Cycles++; break;
+    case Instruction::SExt: Cycles++; break;
+    case Instruction::FPToUI: Cycles += 4; break;
+    case Instruction::FPToSI: Cycles += 4; break;
+    case Instruction::UIToFP: Cycles += 5; break;
+    case Instruction::SIToFP: Cycles += 5; break;
+    case Instruction::FPTrunc: Cycles += 4; break;
+    case Instruction::FPExt: Cycles += 2; break;
+
+    // Other instructions
+    // TODO VAArg, ExtractElement, InsertElement, ShuffleVector, ExtractValue,
+    // InsertValue, LandingPad
+    case Instruction::ICmp: Cycles++; break;
+    case Instruction::FCmp: Cycles += 3; break;
+    case Instruction::Call: {
+      const IntrinsicInst *II = dyn_cast<IntrinsicInst>(I);
+      int64_t Size = 0;
+
+      if(!II) Cycles += 3;
+      else {
+        switch(II->getIntrinsicID()) {
+        default: break;
+        case Intrinsic::memcpy:
+        case Intrinsic::memmove:
+        case Intrinsic::memset:
+          // Arguments: i8* dest, i8* src, i<x> len, i32 align, i1 isvolatile
+          Size = getValueSize(II->getArgOperand(2));
+          break;
+        }
+
+        if(Size > 0) Cycles += getNumCacheLines(Size, 64) * MEM_WEIGHT;
+      }
+      break;
+    }
+    case Instruction::Select: Cycles += 3; break;
+    }
+  }
+
+  virtual void reset() {
+    Weight::reset();
+    ResetCycles += Cycles;
+    Cycles = 0;
+  }
+
+  virtual void max(const Weight *RHS) {
+    assert(RHS->isCycleWeight() && "Cannot mix weight types");
+    const CycleWeight *W = (const CycleWeight *)RHS;
+    if(W->Cycles > Cycles) Cycles = W->Cycles;
+  }
+
+  virtual void multiply(size_t factor) { Cycles *= factor; }
+  virtual void add(const Weight *RHS) {
+    assert(RHS->isCycleWeight() && "Cannot mix weight types");
+    const CycleWeight *W = (const CycleWeight *)RHS;
+    Cycles += W->Cycles;
+  }
+
+  virtual size_t numIters(unsigned percent) const {
+    if(!Cycles) return 1048576; // Return a safe value
+    else {
+      size_t FPCycleCap = getValuePercent(CyclesBetweenMigPoints, percent);
+      size_t Iters = FPCycleCap / Cycles;
+      return Iters ? Iters : 1;
+    }
+  }
+
+  virtual bool underPercentOfThreshold(unsigned percent) const {
+    if(Cycles <= getValuePercent(CyclesBetweenMigPoints, percent)) return true;
+    else return false;
+  }
+
+  virtual std::string toString() const {
+    return std::to_string(Cycles) + " cycles";
+  }
+};
+
+/// Get a weight object with zero-initialized weight based on the type of
+/// analysis being used to instrument the application.
+///
+/// Note: returns a dynamically allocated object to be managed by the caller
+static Weight *getZeroWeight() {
+  if(HTMExec) return new HTMWeight();
+  else return new CycleWeight();
+}
+
+/// SelectMigrationPoints - select locations at which to insert migration
+/// points into functions.
+class SelectMigrationPoints : public FunctionPass
+{
+public:
+  static char ID;
+
+  SelectMigrationPoints() : FunctionPass(ID) {
+    initializeSelectMigrationPointsPass(*PassRegistry::getPassRegistry());
+  }
+  ~SelectMigrationPoints() {}
+
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const {
+    AU.addRequired<LoopInfoWrapperPass>();
+    AU.addRequired<EnumerateLoopPaths>();
+    AU.addRequired<ScalarEvolutionWrapperPass>();
+  }
+
+  virtual StringRef getPassName() const
+  { return "Select migration point locations"; }
+
+  static inline unsigned splitFuncValPair(const std::string &Pair,
+                                          std::string &Func) {
+    unsigned Val;
+    size_t Comma = Pair.rfind(',');
+    Func = Pair.substr(0, Comma);
+    Val = stoul(Pair.substr(Comma + 1));
+    assert(Val <= 100 && "Invalid percentage");
+    return Val;
+  }
+
+  /// Parse per-function threshold values from the command line.
+  void parsePerFuncThresholds() {
+    unsigned Val;
+    std::string Name;
+
+    FuncCapList.clear();
+    FuncStartList.clear();
+    FuncRetList.clear();
+    NoInstFuncs.clear();
+
+    for(auto Pair : FuncCapThreshold) {
+      Val = splitFuncValPair(Pair, Name);
+      FuncCapList[Name] = Val;
+    }
+    for(auto Pair : FuncStartThreshold) {
+      Val = splitFuncValPair(Pair, Name);
+      FuncStartList[Name] = Val;
+    }
+    for(auto Pair : FuncRetThreshold) {
+      Val = splitFuncValPair(Pair, Name);
+      FuncRetList[Name] = Val;
+    }
+    for(auto Func : FuncNoInst) NoInstFuncs.insert(Func);
+  }
+
+  virtual bool doInitialization(Module &M) {
+    DL = &M.getDataLayout();
+    addPopcornFnAttributes(M);
+    if(MoreMigPoints) parsePerFuncThresholds();
+    if(HTMExec) Popcorn::setInstrumentationType(M, Popcorn::HTM);
+    else Popcorn::setInstrumentationType(M, Popcorn::Cycles);
+    return false;
+  }
+
+  /// Select where to insert migration points into functions.
+  virtual bool runOnFunction(Function &F)
+  {
+    LLVM_DEBUG(dbgs() << "\n********** SELECT MIGRATION POINTS **********\n"
+                 << "********** Function: " << F.getName() << "\n\n");
+
+    if(F.hasFnAttribute("popcorn-noinstr") ||
+       NoInstFuncs.find(F.getName()) != NoInstFuncs.end()) return false;
+
+    initializeAnalysis(F);
+
+    // Some operations (e.g., big memory copies, I/O) will cause aborts.
+    // Instrument these operations to stop & resume transactions afterwards.
+    if(HTMExec) {
+      bool AddedMigPoint = wrapWithHTM(F, isBigMemoryOp,
+        "memory operations that overflow HTM buffers");
+      if(!NoWrapLibc)
+        AddedMigPoint |= wrapWithHTM(F, isLibcIO, "I/O functions");
+      if(AddedMigPoint) LP->runOnFunction(F);
+    }
+
+    if(MoreMigPoints && !LP->analysisFailed()) {
+      StringRef FuncName = F.getName();
+      StringMap<unsigned>::const_iterator It;
+      if((It = FuncCapList.find(FuncName)) != FuncCapList.end())
+        CurCapThresh = It->second;
+      else CurCapThresh = CapacityThreshold;
+      if((It = FuncStartList.find(FuncName)) != FuncStartList.end())
+        CurStartThresh = It->second;
+      else CurStartThresh = StartThreshold;
+      if((It = FuncRetList.find(FuncName)) != FuncRetList.end())
+        CurRetThresh = It->second;
+      else CurRetThresh = RetThreshold;
+
+      LLVM_DEBUG(
+        dbgs() << "\n-> Analyzing function body to add migration points <-\n"
+               << "\nCapacity threshold: " << std::to_string(CurCapThresh)
+               << "\nStart threshold: " << std::to_string(CurStartThresh)
+               << "\nReturn threshold: " << std::to_string(CurRetThresh)
+               << "\nMaximum iterations/migration point: "
+               << std::to_string(MaxItersPerMigPoint);
+
+        if(HTMExec)
+          dbgs() << "\nAnalyzing for HTM Instrumentation"
+                 << "\n  HTM read buffer size: "
+                 << std::to_string(HTMReadBufSizeArg) << "kb"
+                 << "\n  HTM write buffer size: "
+                 << std::to_string(HTMWriteBufSizeArg) << "kb\n";
+        else
+          dbgs() << "\nAnalyzing for migration call out instrumentation"
+                 << "\n  Target millions of cycles between migration points: "
+                 << std::to_string(MillionCyclesBetweenMigPoints) << "\n";
+      );
+
+      // We by default mark the function start as a migration point, but if we
+      // don't add any instrumentation & the function's exit weights are
+      // sufficiently small avoid instrumentation altogether.
+      bool MarkStart = false;
+      if(!analyzeFunctionBody(F)) {
+        for(Function::iterator BB = F.begin(), E = F.end(); BB != E; BB++)
+          if(isa<ReturnInst>(BB->getTerminator()) &&
+             !BBWeights[&*BB].BlockWeight->underPercentOfThreshold(CurStartThresh))
+            MarkStart = true;
+      }
+      else MarkStart = true;
+
+      if(MarkStart) {
+        LLVM_DEBUG(dbgs() << "-> Marking function entry as a migration point <-\n");
+        markAsMigPoint(&*F.getEntryBlock().getFirstInsertionPt(), true, true);
+      }
+      else { LLVM_DEBUG(dbgs() << "-> Eliding instrumenting function entry <-\n"); }
+    }
+    else {
+      if(MoreMigPoints) {
+        std::string Msg = "too many paths to instrument function with more "
+                          "migration points -- falling back to instrumenting "
+                          "function entry/exit";
+        DiagnosticInfoOptimizationFailure DI(F, nullptr, Msg);
+        F.getContext().diagnose(DI);
+      }
+
+      LLVM_DEBUG(dbgs() << "-> Marking function entry as a migration point <-\n");
+      markAsMigPoint(&*F.getEntryBlock().getFirstInsertionPt(), true, true);
+
+      // Instrument function exit point(s)
+      LLVM_DEBUG(dbgs() << "-> Marking function exit(s) as a migration point <-\n");
+      for(Function::iterator BB = F.begin(), E = F.end(); BB != E; BB++)
+        if(isa<ReturnInst>(BB->getTerminator()))
+          markAsMigPoint(BB->getTerminator(), true, true);
+    }
+
+    // Finally, apply transformations to loops headers according to analysis.
+    transformLoopHeaders(F);
+
+    return true;
+  }
+
+  /// Reset all analysis.
+  void initializeAnalysis(const Function &F) {
+    SE = &getAnalysis<ScalarEvolutionWrapperPass>();
+    LI = &getAnalysis<LoopInfoWrapperPass>().getLoopInfo();
+    LP = &getAnalysis<EnumerateLoopPaths>();
+    BBWeights.clear();
+    LoopWeights.clear();
+    TransformLoops.clear();
+    MigPointInsts.clear();
+    HTMBeginInsts.clear();
+    HTMEndInsts.clear();
+  }
+
+private:
+  //===--------------------------------------------------------------------===//
+  // Types & fields
+  //===--------------------------------------------------------------------===//
+
+  /// Configuration for the function currently being analyzed.
+  unsigned CurCapThresh;
+  unsigned CurStartThresh;
+  unsigned CurRetThresh;
+
+  /// The current architecture - used to access architecture-specific HTM calls
+  const DataLayout *DL;
+
+  /// Parsed per-function thresholds.
+  StringMap<unsigned> FuncCapList;
+  StringMap<unsigned> FuncStartList;
+  StringMap<unsigned> FuncRetList;
+  StringSet<> NoInstFuncs;
+
+  /// Analyses on which we depend
+  ScalarEvolutionWrapperPass *SE;
+  LoopInfo *LI;
+  EnumerateLoopPaths *LP;
+
+  /// libc functions which are likely to cause an HTM abort through a syscall
+  const static StringSet<> LibcIO;
+
+  /// Weight information for basic blocks.
+  class BasicBlockWeightInfo {
+  public:
+    /// Weight of the basic block at the end of its execution.  If the block has
+    /// a migration point, the weight *only* captures the instructions following
+    /// the migration point (migration points "reset" the weight).
+    WeightPtr BlockWeight;
+
+    BasicBlockWeightInfo() : BlockWeight(nullptr) {}
+    BasicBlockWeightInfo(const Weight *BlockWeight)
+      : BlockWeight(BlockWeight->copy()) {}
+    BasicBlockWeightInfo(const WeightPtr &BlockWeight)
+      : BlockWeight(BlockWeight->copy()) {}
+
+    std::string toString() const {
+      if(BlockWeight) return BlockWeight->toString();
+      else return "<uninitialized basic block weight info>";
+    }
+  };
+
+  /// Weight information for loops.  Maintains weights at loop exit points as
+  /// well as path-specific weight information for the loop & exit blocks.
+  class LoopWeightInfo {
+  private:
+    /// The weight of the loop upon entry.  Zero in the default case, but may
+    /// be set if analysis elides instrumentation in and around the loop.
+    WeightPtr EntryWeight;
+
+    /// The maximum weight when exiting the loop at each of its exit blocks.
+    /// Automatically recalculated when any of its ingredients are changed.
+    DenseMap<const BasicBlock *, WeightPtr> ExitWeights;
+
+    /// Whether the loop has either of the two types of paths, and if so the
+    /// maximum weight of each type.  Note that the spanning path weight is
+    /// *not* scaled by the number of iterations, ItersPerMigPoint.
+    bool LoopHasSpanningPath, LoopHasEqPointPath;
+    WeightPtr LoopSpanningPathWeight, LoopEqPointPathWeight;
+
+    /// Number of iterations between migration points if the loop has one or
+    /// more spanning paths, or zero otherwise.
+    size_t ItersPerMigPoint;
+
+    /// Whether there are either of the two types of paths through each exit
+    /// block, and if so the maximum weight of each type.
+    DenseMap<const BasicBlock *, bool> ExitHasSpanningPath, ExitHasEqPointPath;
+    DenseMap<const BasicBlock *, WeightPtr> ExitSpanningPathWeights,
+                                            ExitEqPointPathWeights;
+
+    /// Calculate the exit block's maximum weight, which is the max of both the
+    /// spanning path exit weight and equivalence point path exit weight.
+    void computeExitWeight(const BasicBlock *BB) {
+      // Note: these operations are in a specific order -- change with care!
+
+      // Calculate the loop weight up until the current iteration
+      WeightPtr BBWeight(getZeroWeight());
+      if(LoopHasSpanningPath) BBWeight->max(getLoopSpanningPathWeight(true));
+      if(LoopHasEqPointPath) BBWeight->max(LoopEqPointPathWeight);
+
+      // Calculate the maximum possible value of the current iteration:
+      //   - Spanning path: loop weight + current path weight
+      //   - Equivalence point path: current weight path
+      if(ExitHasSpanningPath[BB]) BBWeight->add(ExitSpanningPathWeights[BB]);
+      if(ExitHasEqPointPath[BB]) BBWeight->max(ExitEqPointPathWeights[BB]);
+
+      ExitWeights[BB] = std::move(BBWeight);
+    }
+
+    void computeAllExitWeights() {
+      for(auto I = ExitWeights.begin(), E = ExitWeights.end(); I != E; I++)
+        computeExitWeight(I->first);
+    }
+
+  public:
+    LoopWeightInfo() = delete;
+    LoopWeightInfo(const Loop *L)
+      : EntryWeight(getZeroWeight()), LoopHasSpanningPath(false),
+        LoopHasEqPointPath(false), ItersPerMigPoint(0) {
+      SmallVector<BasicBlock *, 4> ExitBlocks;
+      L->getExitingBlocks(ExitBlocks);
+      for(auto Block : ExitBlocks) {
+        ExitHasSpanningPath[Block] = false;
+        ExitHasEqPointPath[Block] = false;
+      }
+    }
+
+    /// Set the weight upon entering the loop & recompute all exit weights.
+    void setEntryWeight(const WeightPtr &W) {
+      EntryWeight.reset(W->copy());
+      computeAllExitWeights();
+    }
+
+    /// Get the number of iterations between migration points, or zero if there
+    /// are no spanning paths through the loop.
+    size_t getItersPerMigPoint() const {
+      return ItersPerMigPoint;
+    }
+
+    /// Get the loop's spanning path weight, scaled based on the number of
+    /// iterations.  Also includes loop entry weight if requested.
+    WeightPtr getLoopSpanningPathWeight(bool AddEntry) const {
+      assert(LoopHasSpanningPath && "No spanning path weight for loop");
+      WeightPtr Ret(LoopSpanningPathWeight->copy());
+      Ret->multiply(ItersPerMigPoint - 1);
+      if(AddEntry) Ret->add(EntryWeight);
+      return Ret;
+    }
+
+    /// Set the loop's spanning path weight & recompute all exit weights.
+    ///  - W: the maximum weight of a single spanning path iteration
+    ///  - I: the number of iterations per migration point
+    void setLoopSpanningPathWeight(const WeightPtr &W, size_t I) {
+      LoopHasSpanningPath = true;
+      LoopSpanningPathWeight.reset(W->copy());
+      ItersPerMigPoint = I;
+      computeAllExitWeights();
+    }
+
+    /// Get the loop's equivalence point path weight.
+    WeightPtr getLoopEqPointPathWeight() const
+    {
+      assert(LoopHasEqPointPath && "No equivalence point path weight for loop");
+      return WeightPtr(LoopEqPointPathWeight->copy());
+    }
+
+    /// Set the loop's equivalence point path weight & recompute all exit
+    /// weights.
+    void setLoopEqPointPathWeight(const WeightPtr &W) {
+      LoopHasEqPointPath = true;
+      LoopEqPointPathWeight.reset(W->copy());
+      computeAllExitWeights();
+    }
+
+    /// Get an exit block's spanning path weight.  This is the raw weight for
+    /// a single iteration of paths through this exiting block, it does *not*
+    /// incorporate loop weights.
+    WeightPtr getExitSpanningPathWeight(const BasicBlock *BB) const
+    {
+      assert(ExitHasSpanningPath.find(BB)->second &&
+             "No spanning path weight for exit block");
+      return WeightPtr(ExitSpanningPathWeights.find(BB)->second->copy());
+    }
+
+    /// Set the exit block's spanning path weight & recompute the exit block's
+    /// overall maximum weight.
+    void setExitSpanningPathWeight(const BasicBlock *BB, const WeightPtr &W)
+    {
+      ExitHasSpanningPath[BB] = true;
+      ExitSpanningPathWeights[BB].reset(W->copy());
+      computeExitWeight(BB);
+    }
+
+    /// Get an exit block's equivalence point path weight.  This is the raw
+    /// weight for a single iteration of paths through this exiting block, it
+    /// does *not* incorporate loop weights.
+    WeightPtr getExitEqPointPathWeight(const BasicBlock *BB) const
+    {
+      assert(ExitHasEqPointPath.find(BB)->second &&
+             "No equivalence point path weight for exit block");
+      return WeightPtr(ExitEqPointPathWeights.find(BB)->second->copy());
+    }
+
+    /// Set the equivalence point path exit block weight & recompute the exit
+    /// block's overall maximum weight.
+    void setExitEqPointPathWeight(const BasicBlock *BB, const WeightPtr &W)
+    {
+      ExitHasEqPointPath[BB] = true;
+      ExitEqPointPathWeights[BB].reset(W->copy());
+      computeExitWeight(BB);
+    }
+
+    /// Return whether the loop/exit block has spanning and equivalence point
+    /// paths through it.
+    bool loopHasSpanningPath() const { return LoopHasSpanningPath; }
+    bool loopHasEqPointPath() const { return LoopHasEqPointPath; }
+    bool exitHasSpanningPath(const BasicBlock *BB) const
+    { return ExitHasSpanningPath.find(BB)->second; }
+    bool exitHasEqPointPath(const BasicBlock *BB) const
+    { return ExitHasEqPointPath.find(BB)->second; }
+
+    /// Return the weight of a given exiting basic block.
+    const WeightPtr &getExitWeight(const BasicBlock *BB) const {
+      assert(ExitWeights.count(BB) && "Invalid exit basic block");
+      return ExitWeights.find(BB)->second;
+    }
+
+    const WeightPtr &operator[](const BasicBlock *BB) const
+    { return getExitWeight(BB); }
+
+    std::string toString() const {
+      if(!ExitWeights.size()) return "<uninitialized loop weight info>";
+      else {
+        std::string buf = "Exit block weights:\n";
+        for(auto It = ExitWeights.begin(), E = ExitWeights.end();
+            It != E; ++It) {
+          buf += "    ";
+          if(It->first->hasName()) {
+            buf += It->first->getName();
+            buf += ": ";
+          }
+          buf += It->second->toString() + "\n";
+        }
+        return buf;
+      }
+    }
+  };
+
+  /// Weight information gathered by analyses for basic blocks & loops
+  typedef std::map<const BasicBlock *, BasicBlockWeightInfo> BlockWeightMap;
+  typedef std::map<const Loop *, LoopWeightInfo> LoopWeightMap;
+  BlockWeightMap BBWeights;
+  LoopWeightMap LoopWeights;
+
+  /// Code locations marked for instrumentation.
+  SmallPtrSet<Loop *, 16> TransformLoops;
+  SmallPtrSet<Instruction *, 32> MigPointInsts;
+  SmallPtrSet<Instruction *, 32> HTMBeginInsts;
+  SmallPtrSet<Instruction *, 32> HTMEndInsts;
+
+  //===--------------------------------------------------------------------===//
+  // Analysis implementation
+  //===--------------------------------------------------------------------===//
+
+  /// Add Popcorn-related function attributes where appropriate.
+  void addPopcornFnAttributes(Module &M) const {
+    auto GlobalAnnos = M.getNamedGlobal("llvm.global.annotations");
+    if(GlobalAnnos) {
+      auto a = cast<ConstantArray>(GlobalAnnos->getOperand(0));
+      for(unsigned int i = 0; i < a->getNumOperands(); i++) {
+        auto e = cast<ConstantStruct>(a->getOperand(i));
+        if(auto fn = dyn_cast<Function>(e->getOperand(0)->getOperand(0))) {
+          auto Anno = cast<ConstantDataArray>(
+                        cast<GlobalVariable>(
+                          e->getOperand(1)->getOperand(0)
+                        )->getOperand(0)
+                      )->getAsCString();
+          fn->addFnAttr(Anno);
+        }
+      }
+    }
+  }
+
+  /// Return whether the instruction requires HTM begin instrumentation.
+  bool shouldAddHTMBegin(Instruction *I) const {
+    if(Popcorn::isHTMBeginPoint(I)) return true;
+    else return HTMBeginInsts.count(I);
+  }
+
+  /// Return whether the instruction requires HTM end instrumentation.
+  bool shouldAddHTMEnd(Instruction *I) const {
+    if(Popcorn::isHTMEndPoint(I)) return true;
+    else return HTMEndInsts.count(I);
+  }
+
+  /// Return whether the instruction is a migration point.  We assume that all
+  /// called functions have migration points internally.
+  bool isMigrationPoint(Instruction *I) const {
+    if(Popcorn::isEquivalencePoint(I)) return true;
+    else return MigPointInsts.count(I);
+  }
+
+  /// Return whether the instruction is marked for any instrumentation.
+  bool isMarkedForInstrumentation(Instruction *I) const {
+    return isMigrationPoint(I) || shouldAddHTMBegin(I) || shouldAddHTMEnd(I);
+  }
+
+  /// Mark an instruction to be instrumented with an HTM begin, directly before
+  /// the instruction
+  bool markAsHTMBegin(Instruction *I) {
+    if(!HTMExec) return false;
+    LLVM_DEBUG(dbgs() << "  + Marking"; I->print(dbgs());
+          dbgs() << " as HTM begin\n");
+    HTMBeginInsts.insert(I);
+    Popcorn::addHTMBeginMetadata(I);
+    return true;
+  }
+
+  /// Mark an instruction to be instrumented with an HTM end, directly before
+  /// the instruction
+  bool markAsHTMEnd(Instruction *I) {
+    if(!HTMExec) return false;
+    LLVM_DEBUG(dbgs() << "  + Marking"; I->print(dbgs());
+          dbgs() << " as HTM end\n");
+    HTMEndInsts.insert(I);
+    Popcorn::addHTMEndMetadata(I);
+    return true;
+  }
+
+  /// Mark an instruction to be instrumented with a migration point, directly
+  /// before the instruction.  Optionally mark instruction as needing HTM
+  /// start/stop intrinsics.
+  bool markAsMigPoint(Instruction *I, bool AddHTMBegin, bool AddHTMEnd) {
+    // Don't clobber any existing instrumentation
+    if(isMarkedForInstrumentation(I)) return false;
+    LLVM_DEBUG(dbgs() << "  + Marking"; I->print(dbgs());
+          dbgs() << " as a migration point\n");
+    MigPointInsts.insert(I);
+    Popcorn::addEquivalencePointMetadata(I);
+    if(AddHTMBegin) markAsHTMBegin(I);
+    if(AddHTMEnd) markAsHTMEnd(I);
+    return true;
+  }
+
+  /// Instruction matching function type.
+  typedef bool (*InstMatch)(const Instruction *, unsigned Thresh);
+
+  /// Return whether the instruction is a memory operation that will overflow
+  /// HTM buffers.
+  static bool isBigMemoryOp(const Instruction *I, unsigned Thresh) {
+    if(!I || !isa<IntrinsicInst>(I)) return false;
+    const IntrinsicInst *II = cast<IntrinsicInst>(I);
+    int64_t Size = 0;
+    switch(II->getIntrinsicID()) {
+    default: return false;
+    case Intrinsic::memcpy: case Intrinsic::memmove: case Intrinsic::memset:
+      // Arguments: i8* dest, i8* src, i<x> len, i32 align, i1 isvolatile
+      Size = getValueSize(II->getArgOperand(2));
+      break;
+    }
+
+    if(Size >= 0) { // We know the size
+      size_t USize = (size_t)Size;
+      return USize >= getValuePercent(HTMReadBufSize, Thresh) ||
+             USize >= getValuePercent(HTMWriteBufSize, Thresh);
+    }
+    else return !NoWrapUnknownMem;
+  }
+
+  /// Return whether the instruction is a libc I/O call.
+  static bool isLibcIO(const Instruction *I, unsigned Thresh) {
+    if(!I || !Popcorn::isCallSite(I)) return false;
+    const ImmutableCallSite CS(I);
+    const Function *CalledFunc = CS.getCalledFunction();
+    if(CalledFunc && CalledFunc->hasName())
+      return LibcIO.find(CalledFunc->getName()) != LibcIO.end();
+    return false;
+  }
+
+  /// Search for & wrap operations that match a certain criteria.
+  bool wrapWithHTM(Function &F, InstMatch Matcher, const char *Desc) {
+    bool AddedMigPoint = false;
+
+    LLVM_DEBUG(dbgs() << "\n-> Wrapping " << Desc << " with HTM stop/start <-\n");
+    for(Function::iterator BB = F.begin(), BE = F.end(); BB != BE; BB++) {
+      if(LI->getLoopFor(&*BB)) continue; // Don't do this in loops!
+      for(BasicBlock::iterator I = BB->begin(), E = BB->end(); I != E; I++) {
+        if(Matcher(&*I, CurCapThresh)) {
+          markAsHTMEnd(&*I);
+
+          // Search subsequent instructions for other libc calls to prevent
+          // pathological transaction stop/starts.
+          const static size_t searchSpan = 10;
+          BasicBlock::iterator NextI(I->getNextNode());
+          for(size_t rem = searchSpan; rem > 0 && NextI != E; rem--, NextI++) {
+            if(Matcher(&*NextI, CurCapThresh)) {
+              LLVM_DEBUG(dbgs() << "  - Found another match:"; NextI->dump());
+              I = NextI;
+              rem = searchSpan;
+            }
+          }
+
+          // TODO analyze successor blocks as well
+
+          AddedMigPoint |= markAsMigPoint(I->getNextNode(), true, false);
+        }
+      }
+    }
+
+    return AddedMigPoint;
+  }
+
+  /// Get the starting weight for a basic block based on the max weights of its
+  /// predecessors.
+  ///
+  /// Note: returns a dynamically allocated object to be managed by the caller
+  Weight *getInitialWeight(const BasicBlock *BB) const {
+    Weight *PredWeight = getZeroWeight();
+    const Loop *L = LI->getLoopFor(BB);
+    bool BBIsHeader = L && (BB == L->getHeader());
+    unsigned LDepth = L ? L->getLoopDepth() : 0;
+
+    for(auto Pred : predecessors(BB)) {
+      const Loop *PredLoop = LI->getLoopFor(Pred);
+
+      // We *only* gather header initial weights when analyzing whether to
+      // instrument loop entry, which doesn't depend on latches.
+      if(BBIsHeader && PredLoop == L) continue;
+
+      // Determine if the predecessor is an exit block from another loop:
+      //
+      //   1. The predecessor is in a loop
+      //   2. The predecessor's loop is not BB's loop
+      //   3. The nesting depth of the predecessor's loop is >= BB's loop*
+      //
+      // If it's an exit block, use the loop weight info to get the exit
+      // weight.  Otherwise, use the basic block weight info.
+      //
+      // *Note: if the predecessor's nesting depth is < BB's, then BB is in a
+      // child loop inside the predecessor's loop, and the predecessor is NOT a
+      // loop exiting block.
+      if(PredLoop && PredLoop != L && PredLoop->getLoopDepth() >= LDepth) {
+        assert(LoopWeights.count(PredLoop) &&
+               "Invalid reverse post-order traversal");
+        PredWeight->max(LoopWeights.at(PredLoop)[Pred]);
+      }
+      else {
+        assert(BBWeights.count(Pred) && "Invalid reverse post-order traversal");
+        PredWeight->max(BBWeights.at(Pred).BlockWeight);
+      }
+    }
+
+    return PredWeight;
+  }
+
+  /// Analyze a single basic block with an initial starting weight and update
+  /// it with the block's ending weight.  Return whether or not a migration
+  /// point was added.
+  bool traverseBlock(BasicBlock *BB, Weight *CurWeight) {
+    bool AddedMigPoint = false;
+
+    LLVM_DEBUG(
+      dbgs() << "      Analyzing basic block";
+      if(BB->hasName()) dbgs() << " '" << BB->getName() << "'";
+      dbgs() << "\n";
+    );
+
+    // TODO this doesn't respect spans identified by wrapWithHTM()!
+
+    for(BasicBlock::iterator I = BB->begin(), E = BB->end(); I != E; I++) {
+      if(isa<PHINode>(I)) continue;
+
+      // Check if there is or there should be a migration point before the
+      // instruction, and if so, reset the weight.  Note: markAsMigPoint()
+      // internally avoids tampering with existing instrumentation.
+      if(isMigrationPoint(&*I)) CurWeight->reset();
+      else if(CurWeight->shouldAddMigPoint(CurCapThresh)) {
+        AddedMigPoint |= markAsMigPoint(&*I, true, true);
+        CurWeight->reset();
+      }
+
+      CurWeight->analyze(&*I, DL);
+    }
+
+    LLVM_DEBUG(dbgs() << "       - Weight: " << CurWeight->toString() << "\n");
+
+    return AddedMigPoint;
+  }
+
+  bool traverseBlock(BasicBlock *BB, WeightPtr &Initial)
+  { return traverseBlock(BB, Initial.get()); }
+
+  /// Mark loop predecessors, i.e., all branches into the loop header, as
+  /// migration points.  Return whether or not a migration point was added.
+  bool markLoopPredecessors(const Loop *L) {
+    bool AddedMigPoint = false;
+    BasicBlock *Header = L->getHeader();
+    for(auto Pred : predecessors(Header)) {
+      // Weed out latches
+      if(!L->contains(Pred)) {
+        // Avoid adding migration points in bodies of predecessor loops when
+        // exiting from one loop directly into the header of another, e.g.,
+        //
+        //   for.body:  ;Body of first loop
+        //     ...
+        //     br i1 %cmp, for.body, for.body.2
+        //
+        //   for.body.2: ;Body of second, completely independent loop
+        //     ...
+        const Loop *PredL = LI->getLoopFor(Pred);
+        if(PredL == nullptr || (PredL->getLoopDepth() < L->getLoopDepth()))
+          AddedMigPoint |= markAsMigPoint(Pred->getTerminator(), true, true);
+      }
+    }
+    return AddedMigPoint;
+  }
+
+  /// Analyze & mark loop entry with migration points.  Avoid instrumenting if
+  /// we can execute the entire loop & any entry code without overflowing our
+  /// resource capacity.
+  bool traverseLoopEntry(Loop *L) {
+    // We don't need to instrument around the loop if we're instrumenting the
+    // header, as we'll hit a migration point at the beginning of the loop.
+    if(TransformLoops.count(L)) return false;
+
+    assert(LoopWeights.count(L) && "Invalid reverse post-order traversal");
+    LoopWeightInfo &LWI = LoopWeights.at(L);
+
+    // If the loop only has equivalence point paths, assume that we'll hit an
+    // equivalence point before we abort -- may need to revise if there are too
+    // many capacity aborts.
+    if(!LWI.loopHasSpanningPath()) {
+      LLVM_DEBUG(dbgs() << "       - Loop only has equivalence point paths, "
+                      "can elide instrumenting loop entry points\n");
+      return false;
+    }
+
+    // TODO what if it's an irreducible loop, i.e., > 1 header?
+    BasicBlock *Header = L->getHeader();
+    WeightPtr HeaderWeight(getInitialWeight(Header));
+
+    LLVM_DEBUG(dbgs() << "       + Analyzing loop entry points to "
+                 << Header->getName() << ", header weight: "
+                 << HeaderWeight->toString() << "\n");
+
+    // See if any of the exit spanning path weights are too heavy to include
+    // the entry point weight (entry point weights don't affect equivalence
+    // point paths).
+    bool InstrumentLoopEntry = false;
+    SmallVector<BasicBlock *, 4> ExitBlocks;
+    L->getExitingBlocks(ExitBlocks);
+    for(auto Exit : ExitBlocks) {
+      if(LWI.exitHasSpanningPath(Exit)) {
+        WeightPtr SpExitWeight(LWI.getLoopSpanningPathWeight(false));
+        SpExitWeight->add(LWI.getExitSpanningPathWeight(Exit));
+        SpExitWeight->add(HeaderWeight);
+        if(SpExitWeight->shouldAddMigPoint(CurCapThresh))
+          InstrumentLoopEntry = true;
+      }
+    }
+
+    if(InstrumentLoopEntry) {
+      LLVM_DEBUG(dbgs() << "       - One or more spanning path(s) were too heavy, "
+                      "instrumenting loop entry points\n");
+      return markLoopPredecessors(L);
+    }
+    else {
+      LLVM_DEBUG(dbgs() << "       + Can elide instrumenting loop entry points\n");
+      LWI.setEntryWeight(HeaderWeight);
+      return false;
+    }
+  }
+
+  /// Traverse a loop and instrument with migration points on paths that are
+  /// too "heavy".  Return whether or not a migration point was added.
+  bool traverseLoop(Loop *L) {
+    bool AddedMigPoint = false;
+    LoopBlocksDFS DFS(L); DFS.perform(LI);
+    LoopBlocksDFS::RPOIterator Block = DFS.beginRPO(), E = DFS.endRPO();
+    SmallPtrSet<const Loop *, 4> MarkedLoops;
+    Loop *BlockLoop;
+
+    assert(Block != E && "Loop with no basic blocks");
+
+    LLVM_DEBUG(
+      dbgs() << "  + Analyzing "; L->dump();
+      dbgs() << "    - At "; L->getStartLoc().dump();
+    );
+
+    // TODO what if it's an irreducible loop, i.e., > 1 header?
+    BasicBlock *CurBB = *Block;
+    WeightPtr HdrWeight(getZeroWeight());
+    AddedMigPoint |= traverseBlock(CurBB, HdrWeight);
+    BBWeights[CurBB] = std::move(HdrWeight);
+
+    for(++Block; Block != E; ++Block) {
+      CurBB = *Block;
+      BlockLoop = LI->getLoopFor(CurBB);
+      if(BlockLoop == L) { // Block is in same loop & nesting depth
+        WeightPtr PredWeight(getInitialWeight(CurBB));
+        AddedMigPoint |= traverseBlock(CurBB, PredWeight);
+        BBWeights[CurBB] = std::move(PredWeight);
+      }
+      else if(!MarkedLoops.count(BlockLoop)) {
+        // Block is in a sub-loop, analyze & mark sub-loop's entry.  Only
+        // analyze direct sub-loops, as deeper-nested (2+) loops will have
+        // already been analyzed by their parents.
+        if(BlockLoop->getLoopDepth() - L->getLoopDepth() == 1)
+          AddedMigPoint |= traverseLoopEntry(BlockLoop);
+        MarkedLoops.insert(BlockLoop);
+      }
+    }
+
+    LLVM_DEBUG(dbgs() << "    Finished analyzing loop\n");
+
+    return AddedMigPoint;
+  }
+
+  /// Analyze a path in a loop up until a particular end instruction and return
+  /// its weight.  Doesn't do any marking.
+  ///
+  /// Note: returns a dynamically allocated object to be managed by the caller
+  Weight *traversePathInternal(const LoopPath *LP,
+                               const Instruction *PathEnd,
+                               bool &ActuallyEqPoint) const {
+    assert(LP->cbegin() != LP->cend() && "Trivial loop path, no blocks");
+    assert(LP->contains(PathEnd->getParent()) && "Invalid end instruction");
+    ActuallyEqPoint = false;
+
+    Loop *SubLoop;
+    Weight *PathWeight = getZeroWeight();
+    SetVector<PathNode>::const_iterator Node = LP->cbegin(),
+                                        EndNode = LP->cend();
+    const BasicBlock *NodeBlock = Node->getBlock(),
+                     *EndBlock = PathEnd->getParent();
+    BasicBlock::const_iterator Inst, EndInst, PathEndInst(PathEnd);
+
+    if(Node->isSubLoopExit()) {
+      // Since the sub-loop exit block is the start of the path, it's by
+      // definition exiting from an equivalence point path.
+      SubLoop = LI->getLoopFor(NodeBlock);
+      assert(LoopWeights.count(SubLoop) && "Invalid traversal");
+      const LoopWeightInfo &LWI = LoopWeights.at(SubLoop);
+      PathWeight->add(LWI.getExitEqPointPathWeight(NodeBlock));
+    }
+    else {
+      for(Inst = BasicBlock::const_iterator(LP->startInst()), EndInst = NodeBlock->end();
+          Inst != EndInst && Inst != PathEndInst; Inst++)
+        PathWeight->analyze(&*Inst, DL);
+    }
+
+    if(NodeBlock == EndBlock) {
+      PathWeight->analyze(&*PathEndInst, DL);
+      return PathWeight;
+    }
+
+    for(Node++; Node != EndNode; Node++) {
+      NodeBlock = Node->getBlock();
+      if(Node->isSubLoopExit()) {
+        // Since the sub-loop exit block is in the middle of the path, it's by
+        // definition exiting from a spanning path.  EnumerateLoopPaths doesn't
+        // know about loops we've marked for transformation, however, so reset
+        // the path weight for loops that'll have a migration point added to
+        // their header.
+        SubLoop = LI->getLoopFor(NodeBlock);
+        assert(LoopWeights.count(SubLoop) && "Invalid traversal");
+        const LoopWeightInfo &LWI = LoopWeights.at(SubLoop);
+        if(TransformLoops.count(SubLoop)) {
+          ActuallyEqPoint = true;
+          PathWeight->reset();
+        }
+
+        // TODO we need to ultimately deal with the following situation more
+        // gracefully:
+        //
+        //   loop 1: all spanning paths, contains loop 2
+        //     loop 2: all spanning paths, contains loop 3
+        //       loop 3: all spanning paths, to be instrumented
+        //
+        // Analysis determines loop 3 needs to be instrumented.  If all paths
+        // in loop 2 go through loop 3, then loop 2 no longer has spanning
+        // paths but only equivalence point paths.  The previous if statement
+        // detects this, and reports it to calculateLoopExitWeights().  However
+        // when analyzing paths through loop 1, we can't detect that loop 2
+        // only has equivalence points paths.
+
+        if(LWI.loopHasSpanningPath()) {
+          PathWeight->add(LWI.getLoopSpanningPathWeight(false));
+          PathWeight->add(LWI.getExitSpanningPathWeight(NodeBlock));
+        }
+        else {
+          ActuallyEqPoint = true;
+          PathWeight->reset();
+          PathWeight->add(LWI[NodeBlock]);
+        }
+      }
+      else {
+        for(Inst = NodeBlock->begin(), EndInst = NodeBlock->end();
+            Inst != EndInst && Inst != PathEndInst; Inst++)
+          PathWeight->analyze(&*Inst, DL);
+      }
+
+      if(NodeBlock == EndBlock) break;
+    }
+    PathWeight->analyze(&*PathEndInst, DL);
+
+    return PathWeight;
+  }
+
+  /// Analyze a path in a loop and return its weight.  Doesn't do any marking.
+  ///
+  /// Note: returns a dynamically allocated object to be managed by the caller
+  Weight *traversePath(const LoopPath *LP, bool &ActuallyEqPoint) const {
+    LLVM_DEBUG(dbgs() << "  + Analyzing loop path: "; LP->dump());
+    return traversePathInternal(LP, LP->endInst(), ActuallyEqPoint);
+  }
+
+  /// Analyze a path until a given exit block & return path's weight up until
+  /// the exit point.
+  ///
+  /// Note: returns a dynamically allocated object to be managed by the caller
+  Weight *traversePathUntilExit(const LoopPath *LP,
+                                BasicBlock *Exit,
+                                bool &ActuallyEqPoint) const
+  { return traversePathInternal(LP, Exit->getTerminator(), ActuallyEqPoint); }
+
+  /// Get the loop trip count if available and less than UINT32_MAX, or 0
+  /// otherwise.
+  ///
+  /// Note: ported from ScalarEvolution::getSmallConstantMaxTripCount() in
+  /// later LLVM releases.
+  unsigned getTripCount(const Loop *L) const {
+    const SCEVConstant *MaxExitCount =
+      dyn_cast<SCEVConstant>(SE->getSE().getMaxBackedgeTakenCount(L));
+    if(!MaxExitCount) return 0;
+    ConstantInt *ExitConst = MaxExitCount->getValue();
+    if(ExitConst->getValue().getActiveBits() > 32) return 0;
+    else return ((unsigned)ExitConst->getZExtValue()) + 1;
+  }
+
+  /// Calculate the exit weights of a loop at all exit points.
+  void calculateLoopExitWeights(Loop *L) {
+    assert(!LoopWeights.count(L) && "Previously analyzed loop?");
+
+    bool HasSpPath = false, HasEqPointPath = false, ActuallyEqPoint;
+    std::vector<const LoopPath *> Paths;
+    LoopWeights.emplace(L, LoopWeightInfo(L));
+    LoopWeightInfo &LWI = LoopWeights.at(L);
+    SmallVector<BasicBlock *, 4> ExitBlocks;
+    WeightPtr SpanningWeight(getZeroWeight()),
+              EqPointWeight(getZeroWeight());
+    LP->getBackedgePaths(L, Paths);
+
+    LLVM_DEBUG(dbgs() << "\n    Calculating loop path weights: "
+                 << std::to_string(Paths.size()) << " backedge path(s)\n");
+
+    // Analyze weights of individual paths through the loop that end at a
+    // backedge, as these will dictate the loop's weight.
+    for(auto Path : Paths) {
+      WeightPtr PathWeight(traversePath(Path, ActuallyEqPoint));
+      LLVM_DEBUG(dbgs() << "    Path weight: " << PathWeight->toString() << " ");
+      if(Path->isSpanningPath() && !ActuallyEqPoint) {
+        HasSpPath = true;
+        SpanningWeight->max(PathWeight);
+        LLVM_DEBUG(dbgs() << "(spanning path)\n");
+      }
+      else {
+        HasEqPointPath = true;
+        EqPointWeight->max(PathWeight);
+        LLVM_DEBUG(dbgs() << "(equivalence point path)\n");
+      }
+    }
+
+    // Calculate/store the loop's spanning and equivalence point path weights.
+    if(HasSpPath) {
+      // Optimization: if the loop trip count is smaller than the number of
+      // iterations between migration points, elide loop instrumentation.
+      size_t NumIters = SpanningWeight->numIters(CurCapThresh);
+      unsigned TripCount = getTripCount(L);
+      assert(NumIters > 0 && "Should have added a migration point");
+      if(TripCount && TripCount < NumIters) {
+        LLVM_DEBUG(dbgs() << "  Eliding loop instrumentation, loop trip count: "
+                     << std::to_string(TripCount) << "\n");
+        NumIters = TripCount;
+      }
+      else if(L->getLoopDepth() > 1 &&
+              NumIters > (size_t)MaxItersPerMigPoint) {
+        LLVM_DEBUG(dbgs() << "  Eliding loop instrumentation (exceeded maximum "
+                        " iterations per migration point), loop trip count: "
+                     << std::to_string(MaxItersPerMigPoint) << "\n");
+        NumIters = MaxItersPerMigPoint;
+      }
+      // TODO mark first insertion point in loop header as migration point,
+      // propagate whether we added a migration point as return value
+      else TransformLoops.insert(L);
+      LWI.setLoopSpanningPathWeight(SpanningWeight, NumIters);
+
+      LLVM_DEBUG(
+        dbgs() << "  Loop spanning path weight: " << SpanningWeight->toString()
+               << ", " << std::to_string(NumIters)
+               << " iteration(s)/migration point\n";
+      );
+    }
+    if(HasEqPointPath) {
+      LWI.setLoopEqPointPathWeight(EqPointWeight);
+
+      LLVM_DEBUG(dbgs() << "  Loop equivalence point path weight: "
+                   << EqPointWeight->toString() << "\n");
+    }
+
+    LLVM_DEBUG(dbgs() << "\n    Calculating loop exit weights");
+
+    // Calculate the weight of the loop at every exit point.  Maintain separate
+    // spanning & equivalence point path exit weights so that if we avoid
+    // instrumenting loop boundaries in traverseLoopEntry() we can update the
+    // exit weights.
+    L->getExitingBlocks(ExitBlocks);
+    for(auto Exit : ExitBlocks) {
+      HasSpPath = HasEqPointPath = false;
+      SpanningWeight.reset(getZeroWeight());
+      EqPointWeight.reset(getZeroWeight());
+
+      LP->getPathsThroughBlock(L, Exit, Paths);
+      for(auto Path : Paths) {
+        WeightPtr PathWeight(traversePathUntilExit(Path, Exit, ActuallyEqPoint));
+        if(Path->isSpanningPath() && !ActuallyEqPoint) {
+          HasSpPath = true;
+          SpanningWeight->max(PathWeight);
+        }
+        else {
+          HasEqPointPath = true;
+          EqPointWeight->max(PathWeight);
+        }
+      }
+
+      if(HasSpPath) LWI.setExitSpanningPathWeight(Exit, SpanningWeight);
+      if(HasEqPointPath) LWI.setExitEqPointPathWeight(Exit, EqPointWeight);
+    }
+  }
+
+  /// Analyze loop nests & mark locations for migration points.  Return whether
+  /// or not a migration point was added.
+  bool traverseLoopNest(const std::vector<BasicBlock *> &SCC) {
+    bool AddedMigPoint = false;
+    Loop *L;
+    LoopNest Nest;
+
+    // Get outermost loop in loop nest & enumerate the rest of the nest
+    assert(LI->getLoopFor(SCC.front()) && "No loop in SCC");
+    L = LI->getLoopFor(SCC.front());
+    while(L->getLoopDepth() != 1) L = L->getParentLoop();
+    LoopPathUtilities::populateLoopNest(L, Nest);
+
+    LLVM_DEBUG(
+      dbgs() << " + Analyzing loop nest at "; L->getStartLoc().print(dbgs());
+      dbgs() << " with " << std::to_string(Nest.size()) << " loop(s)\n\n";
+    );
+
+    for(auto CurLoop : Nest) {
+      // Note: if migration points were added to any sub-loo(s) then we need to
+      // re-run the LoopPaths analysis on the outer loop.
+      // TODO this is a little overzealous, sibling loops (e.g., 2 sub-loops at
+      // the same depth and contained in the same outer loop) can cause
+      // unnecessary re-enumerations.
+      if(traverseLoop(CurLoop) || AddedMigPoint) {
+        AddedMigPoint = true;
+        LP->rerunOnLoop(CurLoop);
+      }
+
+      // TODO if we are instrumenting the loop header, re-enumerate paths
+      calculateLoopExitWeights(CurLoop);
+
+      LLVM_DEBUG(dbgs() << "\n  Loop analysis: "
+                   << LoopWeights.at(CurLoop).toString() << "\n");
+    }
+
+    LLVM_DEBUG(dbgs() << " - Finished loop nest\n");
+
+    return AddedMigPoint;
+  }
+
+  /// Analyze the function's body to add migration points.  Return whether or
+  /// not a migration point was added.
+  bool analyzeFunctionBody(Function &F) {
+    std::set<const Loop *> MarkedLoops;
+    bool AddedMigPoint = false;
+    Loop *BlockLoop;
+
+    // Analyze & mark paths through loop nests
+    LLVM_DEBUG(dbgs() << "\n-> Analyzing loop nests <-\n");
+    for(scc_iterator<Function *> SCC = scc_begin(&F), E = scc_end(&F);
+        SCC != E; ++SCC)
+      if(SCC.hasLoop()) AddedMigPoint |= traverseLoopNest(*SCC);
+
+    // Analyze the rest of the function body
+    LLVM_DEBUG(dbgs() << "\n-> Analyzing the rest of the function body <-\n");
+    ReversePostOrderTraversal<Function *> RPOT(&F);
+    for(auto BB = RPOT.begin(), BE = RPOT.end(); BB != BE; ++BB) {
+      BlockLoop = LI->getLoopFor(*BB);
+      if(!BlockLoop) {
+        WeightPtr PredWeight(getInitialWeight(*BB));
+        AddedMigPoint |= traverseBlock(*BB, PredWeight);
+        BBWeights[*BB] = std::move(PredWeight);
+      }
+      else if(!MarkedLoops.count(BlockLoop)) {
+        // Block is in a loop, analyze & mark loop's boundaries
+        AddedMigPoint |= traverseLoopEntry(BlockLoop);
+        MarkedLoops.insert(BlockLoop);
+      }
+    }
+
+    // Finally, determine if we should add a migration point at exit block(s).
+    for(Function::iterator BB = F.begin(), E = F.end(); BB != E; BB++) {
+      if(isa<ReturnInst>(BB->getTerminator())) {
+        assert(!LI->getLoopFor(&*BB) && "Returning inside a loop");
+        assert(BBWeights.count(&*BB) && "Missing block weight");
+        const BasicBlockWeightInfo &BBWI = BBWeights[&*BB].BlockWeight;
+        if(!BBWI.BlockWeight->underPercentOfThreshold(CurRetThresh)) {
+          LLVM_DEBUG(dbgs() << " - Not under weight threshold, marking return\n");
+          markAsMigPoint(BB->getTerminator(), true, true);
+        }
+      }
+    }
+
+    return AddedMigPoint;
+  }
+
+  //===--------------------------------------------------------------------===//
+  // Instrumentation implementation
+  //===--------------------------------------------------------------------===//
+
+  /// Either find an existing induction variable (and its stride), or create
+  /// one for a loop.
+  Instruction *getInductionVariable(Loop *L, size_t &Stride) {
+    BasicBlock *H = L->getHeader();
+    const SCEVAddRecExpr *Induct;
+    const SCEVConstant *StrideExpr;
+    Type *IVTy;
+
+    // Search for the induction variable & it's stride
+    for(BasicBlock::iterator I = H->begin(), E = H->end(); I != E; ++I) {
+      if(!isa<PHINode>(*I)) break;
+      IVTy = I->getType();
+      if(IVTy->isPointerTy() || !SE->getSE().isSCEVable(IVTy)) continue;
+      Induct = dyn_cast<SCEVAddRecExpr>(SE->getSE().getSCEV(&*I));
+      if(Induct && isa<SCEVConstant>(Induct->getStepRecurrence(SE->getSE()))) {
+        StrideExpr = cast<SCEVConstant>(Induct->getStepRecurrence(SE->getSE()));
+        Stride = std::abs(StrideExpr->getValue()->getSExtValue());
+
+        // TODO if stride != 1, it's hard to ensure we're hitting a migration
+        // point every n iterations unless we know the *exact* number at which
+        // it starts.  For example, if stride = 4 but we start at 1, the
+        // migration point checking logic has to add checks for 1, 5, 9, etc.
+        // It's easier to just create our own induction variable.
+        if(Stride != 1) continue;
+
+        LLVM_DEBUG(dbgs() << "Found induction variable with loop stride of "
+                     << std::to_string(Stride) << ":"; I->print(dbgs());
+              dbgs() << "\n");
+
+        return &*I;
+      }
+    }
+
+    LLVM_DEBUG(dbgs() << "No induction variable, adding'migpoint.iv."
+                 << std::to_string(NumIVsAdded) << "' to the loop\n");
+
+    LLVMContext &C = H->getContext();
+    Type *Int32Ty = Type::getInt32Ty(C);
+    IRBuilder<> PhiBuilder(&*H->getFirstInsertionPt());
+    PHINode *IV = PhiBuilder.CreatePHI(Int32Ty, 0,
+      "migpoint.iv." + std::to_string(NumIVsAdded++));
+    Constant *One = ConstantInt::get(Int32Ty, 1, 0),
+             *Zero = ConstantInt::get(Int32Ty, 0, 0);
+    for(auto Pred : predecessors(H)) {
+      IRBuilder<> AddRecBuilder(Pred->getTerminator());
+      if(L->contains(Pred)) { // Backedge
+        Value *RecVal = AddRecBuilder.CreateAdd(IV, One);
+        IV->addIncoming(RecVal, Pred);
+      }
+      else IV->addIncoming(Zero, Pred);
+    }
+
+    Stride = 1;
+    return IV;
+  }
+
+  /// Round a value down to the nearest power of 2.  Stolen/modified from
+  /// https://graphics.stanford.edu/~seander/bithacks.html#RoundUpPowerOf2
+  unsigned roundDownPowerOf2(unsigned Count) {
+    unsigned Starting = Count;
+    Count--;
+    Count |= Count >> 1;
+    Count |= Count >> 2;
+    Count |= Count >> 4;
+    Count |= Count >> 8;
+    Count |= Count >> 16;
+    Count++;
+
+    // If we're already a power of 2, then the above math returns the same
+    // value.  Otherwise, we've rounded *up* to the nearest power of 2 and need
+    // to divide by 2 to round *down*.
+    if(Count != Starting) Count >>= 1;
+    return Count;
+  }
+
+  /// Transform a loop header so that migration points (and any concomitant
+  /// costs) are only experienced every nth iteration, based on weight metrics
+  void transformLoopHeader(Loop *L) {
+    BasicBlock *Header = L->getHeader();
+    size_t ItersPerMigPoint, Stride = 0, InstrStride;
+
+    // If the first instruction has already been marked due to heuristics that
+    // bookend libc I/O & big memory operations, then there's nothing to do.
+    Instruction *First = &*Header->getFirstInsertionPt();
+    if(isMarkedForInstrumentation(First)) return;
+
+    LLVM_DEBUG(dbgs() << "+ Instrumenting "; L->dump());
+
+    assert(LoopWeights.count(L) && "No loop analysis");
+    ItersPerMigPoint = LoopWeights.at(L).getItersPerMigPoint();
+
+    if(ItersPerMigPoint > 1) {
+      BasicBlock *NewSuccBB, *MigPointBB;
+      Instruction *IV = getInductionVariable(L, Stride);
+
+      IntegerType *IVType = cast<IntegerType>(IV->getType());
+      Function *CurF = Header->getParent();
+      LLVMContext &C = Header->getContext();
+
+      // Create new successor for all instructions after migration point
+      NewSuccBB = Header->splitBasicBlock(Header->getFirstInsertionPt(),
+        "l.postmigpoint" + std::to_string(LoopsTransformed));
+
+      // Create new block for migration point
+      MigPointBB = BasicBlock::Create(C,
+        "l.migpoint" + std::to_string(LoopsTransformed), CurF, NewSuccBB);
+      IRBuilder<> MigPointWorker(MigPointBB);
+      Instruction *Br = cast<Instruction>(MigPointWorker.CreateBr(NewSuccBB));
+      markAsMigPoint(Br, true, true);
+
+      // Add check and branch to migration point only every nth iteration.
+      // Round down to nearest power-of-2, which allows us to use a simple
+      // bitmask for migration point check (URem instructions can cause
+      // non-negligible overhead in tight-loops).
+      IRBuilder<> Worker(Header->getTerminator());
+      InstrStride = roundDownPowerOf2(ItersPerMigPoint * Stride) - 1;
+      assert(InstrStride > 0 && "Invalid migration point stride");
+      Constant *N = ConstantInt::get(IVType, InstrStride, IVType->getSignBit()),
+               *Zero = ConstantInt::get(IVType, 0, IVType->getSignBit());
+      Value *Rem = Worker.CreateAnd(IV, N);
+      Value *Cmp = Worker.CreateICmpEQ(Rem, Zero);
+      Worker.CreateCondBr(Cmp, MigPointBB, NewSuccBB);
+      Header->getTerminator()->eraseFromParent();
+
+      LLVM_DEBUG(dbgs() << "Instrumenting to hit migration point every "
+                   << std::to_string(InstrStride + 1) << " iterations\n");
+    }
+    else {
+      LLVM_DEBUG(dbgs() << "Instrumenting to hit migration point every iteration\n");
+      markAsMigPoint(&*Header->getFirstInsertionPt(), true, true);
+    }
+  }
+
+  /// Insert migration points & HTM instrumentation for instructions.
+  void transformLoopHeaders(Function &F) {
+    LLVM_DEBUG(dbgs() << "\n-> Transforming loop headers <-\n");
+    for(auto Loop : TransformLoops) {
+      transformLoopHeader(Loop);
+      LoopsTransformed++;
+    }
+  }
+};
+
+} /* end anonymous namespace */
+
+char SelectMigrationPoints::ID = 0;
+
+INITIALIZE_PASS_BEGIN(SelectMigrationPoints, "select-migration-points",
+                      "Select migration points locations", true, false)
+INITIALIZE_PASS_DEPENDENCY(LoopInfoWrapperPass)
+INITIALIZE_PASS_DEPENDENCY(EnumerateLoopPaths)
+INITIALIZE_PASS_DEPENDENCY(ScalarEvolutionWrapperPass)
+INITIALIZE_PASS_END(SelectMigrationPoints, "select-migration-points",
+                    "Select migration points locations", true, false)
+
+const StringSet<> SelectMigrationPoints::LibcIO = {
+  "fopen", "freopen", "fclose", "fflush", "fwide",
+  "setbuf", "setvbuf", "fread", "fwrite",
+  "fgetc", "getc", "fgets", "fputc", "putc", "fputs",
+  "getchar", "gets", "putchar", "puts", "ungetc",
+  "fgetwc", "getwc", "fgetws", "fputwc", "putwc", "fputws",
+  "getwchar", "putwchar", "ungetwc",
+  "scanf", "fscanf", "vscanf", "vfscanf",
+  "printf", "fprintf", "vprintf", "vfprintf",
+  "wscanf", "fwscanf", "vwscanf", "vfwscanf",
+  "wprintf", "fwprintf", "vwprintf", "vfwprintf",
+  "ftell", "fgetpos", "fseek", "fsetpos", "rewind",
+  "clearerr", "feof", "ferror", "perror",
+  "remove", "rename", "tmpfile", "tmpnam",
+  "__isoc99_fscanf", "exit"
+};
+
+namespace llvm {
+  FunctionPass *createSelectMigrationPointsPass()
+  { return new SelectMigrationPoints(); }
+}
diff --git a/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp b/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp
index 54f6cc2d557..d1f4301aca3 100644
--- a/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp
+++ b/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp
@@ -215,9 +215,10 @@ const TargetLoweringObjectFile &AsmPrinter::getObjFileLowering() const {
   return *TM.getObjFileLowering();
 }
 
-const DataLayout &AsmPrinter::getDataLayout() const {
-  return MMI->getModule()->getDataLayout();
-}
+// CJP: moving to AsmPrinter.h to workaround a stackmaps dependency
+// const DataLayout &AsmPrinter::getDataLayout() const {
+//   return MMI->getModule()->getDataLayout();
+// }
 
 // Do not use the cached DataLayout because some client use it without a Module
 // (dsymutil, llvm-dwarfdump).
@@ -1677,6 +1678,38 @@ MCSymbol *AsmPrinter::getCurExceptionSym() {
   return CurExceptionSym;
 }
 
+MachineInstr *AsmPrinter::FindPcnStackMap(MachineBasicBlock &MBB,
+                                       MachineInstr *MI) const {
+  for(auto MI = MBB.instr_begin(), MIE = MBB.instr_end(); MI != MIE; MI++) {
+    if(MI->getOpcode() == TargetOpcode::PCN_STACKMAP)
+      return &*MI;
+    else if (MI->isCall())
+      break;
+  }
+
+  // Call site without a stackmap implies that either the call was generated by
+  // the backend or the LLVM bitcode was never instrumented by the StackInfo
+  // pass.  This is not necessarily an error!
+  return nullptr;
+}
+
+bool AsmPrinter::TagCallSites(MachineFunction &MF) {
+  bool tagged = false;
+  for(auto MBB = MF.begin(), MBBE = MF.end(); MBB != MBBE; MBB++) {
+    for(auto MI = MBB->instr_begin(), MIE = MBB->instr_end(); MI != MIE; MI++) {
+      if(MI->isCall() && !MI->isPseudo()) {
+        MachineInstr *SMI = FindPcnStackMap(*MBB, &*MI);
+        if(SMI != nullptr) {
+          MBB->remove(SMI);
+          MI = MBB->insert(++MI, SMI);
+          tagged = true;
+        }
+      }
+    }
+  }
+  return tagged;
+}
+
 void AsmPrinter::SetupMachineFunction(MachineFunction &MF) {
   this->MF = &MF;
   // Get the function symbol.
diff --git a/llvm/lib/CodeGen/CMakeLists.txt b/llvm/lib/CodeGen/CMakeLists.txt
index 5d8a1382867..29970c8faf8 100644
--- a/llvm/lib/CodeGen/CMakeLists.txt
+++ b/llvm/lib/CodeGen/CMakeLists.txt
@@ -144,6 +144,8 @@ add_llvm_library(LLVMCodeGen
   StackMaps.cpp
   StackProtector.cpp
   StackSlotColoring.cpp
+  StackTransformMetadata.cpp
+  StackTransformTypes.cpp
   SwiftErrorValueTracking.cpp
   SwitchLoweringUtils.cpp
   TailDuplication.cpp
@@ -159,6 +161,7 @@ add_llvm_library(LLVMCodeGen
   TargetSubtargetInfo.cpp
   TwoAddressInstructionPass.cpp
   UnreachableBlockElim.cpp
+  UnwindInfo.cpp
   ValueTypes.cpp
   VirtRegMap.cpp
   WasmEHPrepare.cpp
@@ -171,6 +174,22 @@ add_llvm_library(LLVMCodeGen
 
   LINK_LIBS ${LLVM_PTHREAD_LIB}
 
+  LINK_COMPONENTS
+  Core
+  Support
+  MC
+  Analysis
+  TransformUtils
+  Target
+  BitWriter
+  BitReader
+  ScalarOpts
+  Instrumentation
+  ProfileData
+#  AArch64AsmParser
+#  RISCVAsmParser
+#  X86AsmParser 
+
   DEPENDS
   intrinsics_gen
   )
diff --git a/llvm/lib/CodeGen/CodeGen.cpp b/llvm/lib/CodeGen/CodeGen.cpp
index c37ed57781d..6691a24c820 100644
--- a/llvm/lib/CodeGen/CodeGen.cpp
+++ b/llvm/lib/CodeGen/CodeGen.cpp
@@ -51,6 +51,7 @@ void llvm::initializeCodeGen(PassRegistry &Registry) {
   initializeLiveStacksPass(Registry);
   initializeLiveVariablesPass(Registry);
   initializeLocalStackSlotPassPass(Registry);
+  initializeStackTransformMetadataPass(Registry);
   initializeLowerIntrinsicsPass(Registry);
   initializeMIRCanonicalizerPass(Registry);
   initializeMachineBlockFrequencyInfoPass(Registry);
diff --git a/llvm/lib/CodeGen/InlineSpiller.cpp b/llvm/lib/CodeGen/InlineSpiller.cpp
index 41ae8061a91..fd11a773d1d 100644
--- a/llvm/lib/CodeGen/InlineSpiller.cpp
+++ b/llvm/lib/CodeGen/InlineSpiller.cpp
@@ -804,6 +804,7 @@ foldMemoryOperand(ArrayRef<std::pair<MachineInstr *, unsigned>> Ops,
   bool SpillSubRegs = TII.isSubregFoldable() ||
                       MI->getOpcode() == TargetOpcode::STATEPOINT ||
                       MI->getOpcode() == TargetOpcode::PATCHPOINT ||
+                      MI->getOpcode() == TargetOpcode::PCN_STACKMAP ||
                       MI->getOpcode() == TargetOpcode::STACKMAP;
 
   // TargetInstrInfo::foldMemoryOperand only expects explicit, non-tied
diff --git a/llvm/lib/CodeGen/LLVMTargetMachine.cpp b/llvm/lib/CodeGen/LLVMTargetMachine.cpp
index 886ae7e94ad..dd995e7a423 100644
--- a/llvm/lib/CodeGen/LLVMTargetMachine.cpp
+++ b/llvm/lib/CodeGen/LLVMTargetMachine.cpp
@@ -38,6 +38,27 @@ static cl::opt<bool> EnableTrapUnreachable("trap-unreachable",
   cl::Hidden, cl::ZeroOrMore, cl::init(false),
   cl::desc("Enable generating trap for unreachable"));
 
+// Popcorn-specific IR-level instrumentation
+enum PopcornInstrumentation {
+  none, // No instrumentation
+  metadata, // Only generate migration metadata, don't insert migration points
+  migpoints, // Only add migration points, don't generate rewriting metadata
+  migration, // Add migration points & generate rewriting metadata for migration
+  libc // Generate rewriting metadata for libc thread start functions
+};
+
+static cl::opt<PopcornInstrumentation> PopcornInstrument("popcorn-instrument",
+  cl::desc("Add Popcorn-specific instrumentation to applications"),
+  cl::init(none),
+  cl::values(
+    clEnumValN(none, "none", "No instrumentation (default)"),
+    clEnumValN(metadata, "metadata", "Only generate migration metadata (no migration points"),
+    clEnumValN(migpoints, "migpoints", "Only add migration points (no migration metadata)"),
+    clEnumValN(migration, "migration", "Add migration points & generate migration metadata"),
+    clEnumValN(libc, "libc", "Instrument libc thread start functions for migration")
+  )
+);
+
 void LLVMTargetMachine::initAsmInfo() {
   MRI.reset(TheTarget.createMCRegInfo(getTargetTriple().str()));
   MII.reset(TheTarget.createMCInstrInfo());
@@ -102,7 +123,35 @@ addPassesToGenerateCode(LLVMTargetMachine &TM, PassManagerBase &PM,
   TargetPassConfig *PassConfig = TM.createPassConfig(PM);
   // Set PassConfig options provided by TargetMachine.
   PassConfig->setDisableVerify(DisableVerify);
+
+  /// Popcorn compiler - multi-ISA binary configurations.  Requires that IR
+  /// passed to backends is identical, save for certain architecture-specific
+  /// quirks like atomic operations or intrinsics
+  if(PopcornInstrument != PopcornInstrumentation::none) {
+    TM.setArchIROptLevel(CodeGenOpt::None);
+
+    switch(PopcornInstrument) {
+    case PopcornInstrumentation::metadata:
+      PassConfig->setAddStackMaps(true);
+      break;
+    case PopcornInstrumentation::migpoints:
+      PassConfig->setAddMigrationPoints(true);
+      break;
+    case PopcornInstrumentation::migration:
+      PassConfig->setAddMigrationPoints(true);
+      PassConfig->setAddStackMaps(true);
+      break;
+    case PopcornInstrumentation::libc:
+      PassConfig->setAddLibcStackMaps(true);
+      break;
+    default:
+      llvm_unreachable("Invalid instrumentation type");
+      break;
+    }
+  }
+
   PM.add(PassConfig);
+  PassConfig->addPopcornPasses();
   PM.add(&MMI);
 
   if (PassConfig->addISelPasses())
diff --git a/llvm/lib/CodeGen/LocalStackSlotAllocation.cpp b/llvm/lib/CodeGen/LocalStackSlotAllocation.cpp
index b14d76a585f..3e324a64534 100644
--- a/llvm/lib/CodeGen/LocalStackSlotAllocation.cpp
+++ b/llvm/lib/CodeGen/LocalStackSlotAllocation.cpp
@@ -306,7 +306,8 @@ bool LocalStackSlotPass::insertFrameReferenceRegisters(MachineFunction &Fn) {
       // range, so they don't need any updates.
       if (MI.isDebugInstr() || MI.getOpcode() == TargetOpcode::STATEPOINT ||
           MI.getOpcode() == TargetOpcode::STACKMAP ||
-          MI.getOpcode() == TargetOpcode::PATCHPOINT)
+          MI.getOpcode() == TargetOpcode::PATCHPOINT ||
+	  MI.getOpcode() == TargetOpcode::PCN_STACKMAP)
         continue;
 
       // For now, allocate the base register(s) within the basic block
diff --git a/llvm/lib/CodeGen/MachineFunction.cpp b/llvm/lib/CodeGen/MachineFunction.cpp
index faae944f591..9fed0e7ee6b 100644
--- a/llvm/lib/CodeGen/MachineFunction.cpp
+++ b/llvm/lib/CodeGen/MachineFunction.cpp
@@ -116,6 +116,15 @@ void MachineFunctionProperties::print(raw_ostream &OS) const {
 // MachineFunction implementation
 //===----------------------------------------------------------------------===//
 
+/// Is a register caller-saved?
+bool MachineFunction::isCallerSaved(unsigned Reg) const {
+  assert(TargetRegisterInfo::isPhysicalRegister(Reg) && "Invalid register");
+  CallingConv::ID CC = F.getCallingConv();
+  const uint32_t *Mask =
+    RegInfo->getTargetRegisterInfo()->getCallPreservedMask(*this, CC);
+  return !((Mask[Reg / 32] >> Reg % 32) & 1);
+}
+
 // Out-of-line virtual method.
 MachineFunctionInfo::~MachineFunctionInfo() = default;
 
@@ -856,6 +865,188 @@ void MachineFunction::updateCallSiteInfo(const MachineInstr *Old,
     CallSitesInfo[New] = CSInfo;
 }
 
+/// Get the value for key K in Map M, or create a new one if it doesn't exist.
+template<typename Key, typename Val, typename Map>
+static Val &getOrCreateMapping(const Key *K, Map &M) {
+  typename Map::iterator It;
+  if((It = M.find(K)) == M.end())
+    It = M.insert(std::pair<const Key *, Val>(K, Val())).first;
+  return It->second;
+}
+
+void MachineFunction::addOpLegalizeChange(int64_t SMID,
+                                          unsigned OpNo,
+                                          unsigned Num) {
+  // TODO Popcorn: we currently assume operands are legalized in increasing
+  // operand number order, otherwise OpNo may be invalid or invalidate other
+  // operand numbers
+  OpNumberMap &OpChanges = SMOpLegalizeChanges[SMID];
+  assert(std::max_element(OpChanges.begin(), OpChanges.end())->first < OpNo &&
+         "Invalid legalization sequence");
+  SMOpLegalizeChanges[SMID][OpNo] = Num;
+}
+
+/// Add an IR/architecture-specific location mapping for a stackmap operand
+void MachineFunction::addSMOpLocation(const CallInst *SM,
+                                      const Value *Val,
+                                      const MachineLiveLoc &MLL) {
+  auto containsLoc = [](const MachineLiveLocs &Vals,
+                        const MachineLiveLoc &Cur) -> bool {
+    for(auto &LV : Vals) if(Cur == *LV) return true;
+    return false;
+  };
+
+  assert(SM && "Invalid stackmap");
+  assert(Val && "Invalid stackmap operand");
+
+  IRToMachineLocs &IRMap =
+    getOrCreateMapping<Instruction,
+                       IRToMachineLocs,
+                       InstToOperands>(SM, SMDuplicateLocs);
+  MachineLiveLocs &Vals =
+    getOrCreateMapping<Value,
+                       MachineLiveLocs,
+                       IRToMachineLocs>(Val, IRMap);
+  if(!containsLoc(Vals, MLL))
+    Vals.push_back(MachineLiveLocPtr(MLL.copy()));
+}
+
+/// Add an IR/architecture-specific location mapping for a stackmap operand
+void MachineFunction::addSMOpLocation(const CallInst *SM,
+                                      unsigned Op,
+                                      const MachineLiveLoc &MLL) {
+  assert(SM && "Invalid stackmap");
+  assert(Op < SM->getNumArgOperands() && "Invalid operand number");
+  addSMOpLocation(SM, SM->getArgOperand(Op), MLL);
+}
+
+
+/// Add an architecture-specific live value & location for a stackmap
+void MachineFunction::addSMArchSpecificLocation(const CallInst *SM,
+                                                const MachineLiveLoc &MLL,
+                                                const MachineLiveVal &MLV) {
+  auto containsLoc = [](const ArchLiveValues &Vals,
+                        const MachineLiveLoc &Cur) -> bool {
+    for(auto &LV : Vals) if(Cur == *LV.first) return true;
+    return false;
+  };
+
+  assert(SM && "Invalid stackmap");
+  ArchLiveValues &Vals =
+    getOrCreateMapping<Instruction,
+                       ArchLiveValues,
+                       InstToArchLiveValues>(SM, SMArchSpecificLocs);
+  if(!containsLoc(Vals, MLL))
+    Vals.push_back(ArchLiveValue(MachineLiveLocPtr(MLL.copy()),
+                                 MachineLiveValPtr(MLV.copy())));
+}
+
+/// Update stack slot references to new indexes after stack slot coloring
+void
+MachineFunction::updateSMStackSlotRefs(SmallDenseMap<int, int, 16> &Changes) {
+
+  if(Changes.size()) {
+    LLVM_DEBUG(dbgs() << "Updating stackmap stack slot references\n";);
+
+    int SS;
+    SmallDenseMap<int, int, 16>::iterator Change;
+
+    // Iterate over all operand duplicate locations
+    for(auto &InstIt : SMDuplicateLocs) {
+      for(auto &IRIt : InstIt.second) {
+        for(auto &MLL : IRIt.second) {
+          if(MLL->isStackSlot()) {
+            MachineLiveStackSlot &LSS = (MachineLiveStackSlot &)*MLL;
+            SS = LSS.getStackSlot();
+            Change = Changes.find(SS);
+            if(Change != Changes.end())
+              LSS.setStackSlot(Change->second);
+          }
+        }
+      }
+    }
+
+    // Iterate over all architecture-specific locations.
+    // Note: both the destination & source location can be stack slots
+    for(auto &InstIt : SMArchSpecificLocs) {
+      for(auto &MLL : InstIt.second) {
+        if(MLL.first->isStackSlot()) {
+          MachineLiveStackSlot &LSS = (MachineLiveStackSlot &)*MLL.first;
+          SS = LSS.getStackSlot();
+          Change = Changes.find(SS);
+          if(Change != Changes.end())
+            LSS.setStackSlot(Change->second);
+        }
+
+        if(MLL.second->isStackObject()) {
+          MachineStackObject &MSO = (MachineStackObject &)*MLL.second;
+          SS = MSO.getIndex();
+          Change = Changes.find(SS);
+          if(Change != Changes.end())
+            MSO.setIndex(Change->second);
+        }
+      }
+    }
+  }
+}
+
+/// Return the number of machine operands corresponding to a given IR operand.
+unsigned
+MachineFunction::getNumLegalizedOps(int64_t SMID, unsigned OpNo) const {
+  SMToOpLegalizeMap::const_iterator SM = SMOpLegalizeChanges.find(SMID);
+  if(SM != SMOpLegalizeChanges.end()) {
+    const OpNumberMap &OpChanges = SM->second;
+    OpNumberMap::const_iterator Op = OpChanges.find(OpNo);
+    if(Op != OpChanges.end()) return Op->second;
+  }
+  return 1;
+}
+
+/// Are there any architecture-specific locations for operand Val in stackmap
+/// SM?
+bool
+MachineFunction::hasSMOpLocations(const CallInst *SM, const Value *Val) const {
+  assert(SM && "Invalid stackmap");
+  assert(Val && "Invalid stackmap operand");
+  InstToOperands::const_iterator InstIt;
+  if((InstIt = SMDuplicateLocs.find(SM)) != SMDuplicateLocs.end())
+    return InstIt->second.find(Val) != InstIt->second.end();
+  return false;
+}
+
+/// Are there any architecture-specific locations for stackmap SM?
+bool
+MachineFunction::hasSMArchSpecificLocations(const llvm::CallInst *SM) const {
+  assert(SM && "Invalid stackmap");
+  return SMArchSpecificLocs.find(SM) != SMArchSpecificLocs.end();
+}
+
+/// Return the architecture-specific locations for a stackmap operand.
+const MachineLiveLocs &
+MachineFunction::getSMOpLocations(const CallInst *SM,
+                                  const Value *Val ) const {
+  assert(SM && "Invalid stackmap");
+  assert(Val && "Invalid stackmap operand");
+  InstToOperands::const_iterator InstIt = SMDuplicateLocs.find(SM);
+  assert(InstIt != SMDuplicateLocs.end() &&
+         "No duplicate locations for stackmap");
+  IRToMachineLocs::const_iterator IRIt = InstIt->second.find(Val);
+  assert(IRIt != InstIt->second.end() &&
+         "No duplicate locations for stackmap operand");
+  return IRIt->second;
+}
+
+/// Return the architecture-specific locations for a stackmap that are not
+/// associated with any operand.
+const ArchLiveValues &
+MachineFunction::getSMArchSpecificLocations(const CallInst *SM) const {
+  assert(SM && "Invalid stackmap");
+  InstToArchLiveValues::const_iterator InstIt = SMArchSpecificLocs.find(SM);
+  assert(InstIt != SMArchSpecificLocs.end() &&
+         "No architecture-specific locations for stackmap");
+  return InstIt->second;
+}
+
 /// \}
 
 //===----------------------------------------------------------------------===//
diff --git a/llvm/lib/CodeGen/RegAllocFast.cpp b/llvm/lib/CodeGen/RegAllocFast.cpp
index 2ffa5e389f8..3e1bb31138a 100644
--- a/llvm/lib/CodeGen/RegAllocFast.cpp
+++ b/llvm/lib/CodeGen/RegAllocFast.cpp
@@ -1291,6 +1291,13 @@ void RegAllocFast::allocateBasicBlock(MachineBasicBlock &MBB) {
 }
 
 bool RegAllocFast::runOnMachineFunction(MachineFunction &MF) {
+  // TODO the fast register allocator behaves poorly for stackmaps with lots
+  // of operands, and since it doesn't use the VirtRegRewriter pass we can't
+  // capture correct stackmap operand locations
+  if(MF.getFrameInfo().hasPcnStackMap())
+    llvm_unreachable("Fast register allocator not supported for stack"
+                     "transformation");
+
   LLVM_DEBUG(dbgs() << "********** FAST REGISTER ALLOCATION **********\n"
                     << "********** Function: " << MF.getName() << '\n');
   MRI = &MF.getRegInfo();
diff --git a/llvm/lib/CodeGen/SelectionDAG/FastISel.cpp b/llvm/lib/CodeGen/SelectionDAG/FastISel.cpp
index 5ac3606dc66..361e297e763 100644
--- a/llvm/lib/CodeGen/SelectionDAG/FastISel.cpp
+++ b/llvm/lib/CodeGen/SelectionDAG/FastISel.cpp
@@ -787,7 +787,7 @@ bool FastISel::addStackMapLiveVars(SmallVectorImpl<MachineOperand> &Ops,
   return true;
 }
 
-bool FastISel::selectStackmap(const CallInst *I) {
+bool FastISel::selectStackmap(const CallInst *I, unsigned id) {
   // void @llvm.experimental.stackmap(i64 <id>, i32 <numShadowBytes>,
   //                                  [live variables...])
   assert(I->getCalledFunction()->getReturnType()->isVoidTy() &&
@@ -804,6 +804,8 @@ bool FastISel::selectStackmap(const CallInst *I) {
   // CALLSEQ_END(0, 0)
   //
   SmallVector<MachineOperand, 32> Ops;
+  unsigned Opcode = id == Intrinsic::experimental_stackmap
+    ? TargetOpcode::STACKMAP : TargetOpcode::PCN_STACKMAP;
 
   // Add the <id> and <numBytes> constants.
   assert(isa<ConstantInt>(I->getOperand(PatchPointOpers::IDPos)) &&
@@ -843,7 +845,7 @@ bool FastISel::selectStackmap(const CallInst *I) {
 
   // Issue STACKMAP.
   MachineInstrBuilder MIB = BuildMI(*FuncInfo.MBB, FuncInfo.InsertPt, DbgLoc,
-                                    TII.get(TargetOpcode::STACKMAP));
+                                    TII.get(Opcode));
   for (auto const &MO : Ops)
     MIB.add(MO);
 
@@ -854,7 +856,10 @@ bool FastISel::selectStackmap(const CallInst *I) {
       .addImm(0);
 
   // Inform the Frame Information that we have a stackmap in this function.
-  FuncInfo.MF->getFrameInfo().setHasStackMap();
+  if (id == Intrinsic::experimental_stackmap)
+    FuncInfo.MF->getFrameInfo().setHasStackMap();
+  else
+    FuncInfo.MF->getFrameInfo().setHasPcnStackMap();
 
   return true;
 }
@@ -1480,7 +1485,8 @@ bool FastISel::selectIntrinsicCall(const IntrinsicInst *II) {
     return true;
   }
   case Intrinsic::experimental_stackmap:
-    return selectStackmap(II);
+  case Intrinsic::experimental_pcn_stackmap:
+    return selectStackmap(II, II->getIntrinsicID());
   case Intrinsic::experimental_patchpoint_void:
   case Intrinsic::experimental_patchpoint_i64:
     return selectPatchpoint(II);
diff --git a/llvm/lib/CodeGen/SelectionDAG/InstrEmitter.cpp b/llvm/lib/CodeGen/SelectionDAG/InstrEmitter.cpp
index 9bc07d35dfc..5b8efc2c3ba 100644
--- a/llvm/lib/CodeGen/SelectionDAG/InstrEmitter.cpp
+++ b/llvm/lib/CodeGen/SelectionDAG/InstrEmitter.cpp
@@ -815,7 +815,8 @@ EmitMachineNode(SDNode *Node, bool IsClone, bool IsCloned,
   const MCPhysReg *ScratchRegs = nullptr;
 
   // Handle STACKMAP and PATCHPOINT specially and then use the generic code.
-  if (Opc == TargetOpcode::STACKMAP || Opc == TargetOpcode::PATCHPOINT) {
+  if (Opc == TargetOpcode::STACKMAP || Opc == TargetOpcode::PATCHPOINT
+      || Opc == TargetOpcode::PCN_STACKMAP) {
     // Stackmaps do not have arguments and do not preserve their calling
     // convention. However, to simplify runtime support, they clobber the same
     // scratch registers as AnyRegCC.
diff --git a/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp b/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp
index 15ac45c37c6..cf02f2f114e 100644
--- a/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp
+++ b/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp
@@ -1161,6 +1161,9 @@ bool DAGTypeLegalizer::PromoteIntegerOperand(SDNode *N, unsigned OpNo) {
   case ISD::VECREDUCE_SMIN:
   case ISD::VECREDUCE_UMAX:
   case ISD::VECREDUCE_UMIN: Res = PromoteIntOp_VECREDUCE(N); break;
+
+  case (uint16_t)~TargetOpcode::PCN_STACKMAP:
+    Res = PromoteIntOp_STACKMAP(N, OpNo); break;
   }
 
   // If the result is null, the sub-method took care of registering results etc.
@@ -1397,6 +1400,30 @@ SDValue DAGTypeLegalizer::PromoteIntOp_SINT_TO_FP(SDNode *N) {
                                 SExtPromotedInteger(N->getOperand(0))), 0);
 }
 
+SDValue DAGTypeLegalizer::PromoteIntOp_STACKMAP(SDNode *N, unsigned OpNo) {
+  std::vector<SDValue> Ops(N->getNumOperands());
+  SDLoc dl(N);
+
+  for(unsigned i = 0; i < N->getNumOperands(); i++) {
+    if(i == OpNo) {
+      EVT NVT = TLI.getTypeToTransformTo(*DAG.getContext(),
+					 N->getOperand(OpNo)->getValueType(0));
+      if(N->getOperand(i).getValueType() == MVT::i1 ||
+         N->getOperand(i).getValueType() == MVT::i8 ||
+         N->getOperand(i).getValueType() == MVT::i16 ||
+	 N->getOperand(i).getValueType() == MVT::i32) {
+        if(N->getOperand(i).getOpcode() == ISD::TRUNCATE)
+          Ops[i] = N->getOperand(i)->getOperand(0);
+        else
+          Ops[i] = DAG.getNode(ISD::ZERO_EXTEND, dl, NVT, N->getOperand(i));
+      }
+    }
+    else Ops[i] = N->getOperand(i);
+  }
+
+  return SDValue(DAG.UpdateNodeOperands(N, Ops), 0);
+}
+
 SDValue DAGTypeLegalizer::PromoteIntOp_STORE(StoreSDNode *N, unsigned OpNo){
   assert(ISD::isUNINDEXEDStore(N) && "Indexed store during type legalization!");
   SDValue Ch = N->getChain(), Ptr = N->getBasePtr();
@@ -3499,6 +3526,9 @@ bool DAGTypeLegalizer::ExpandIntegerOperand(SDNode *N, unsigned OpNo) {
   case ISD::FRAMEADDR:         Res = ExpandIntOp_RETURNADDR(N); break;
 
   case ISD::ATOMIC_STORE:      Res = ExpandIntOp_ATOMIC_STORE(N); break;
+
+  case (uint16_t)~TargetOpcode::PCN_STACKMAP:
+    Res = ExpandIntOp_STACKMAP(N, OpNo); break;
   }
 
   // If the result is null, the sub-method took care of registering results etc.
@@ -3937,6 +3967,36 @@ SDValue DAGTypeLegalizer::ExpandIntOp_ATOMIC_STORE(SDNode *N) {
   return Swap.getValue(1);
 }
 
+SDValue DAGTypeLegalizer::ExpandIntOp_STACKMAP(SDNode *N, unsigned OpNo) {
+  std::vector<SDValue> Ops;
+  SDLoc dl(N);
+  const SDValue &InvOp = N->getOperand(OpNo);
+
+  switch(InvOp.getSimpleValueType().SimpleTy) {
+  default: llvm_unreachable("Unhandled operand type in stackmap");
+  case MVT::i128:
+    if(InvOp->getOpcode() == ISD::BUILD_PAIR) {
+      // If the i128 is the result of a buildpair i64, i64, replace the i128
+      // operand with the two i64 operands in the stackmap
+      LLVM_DEBUG(dbgs() << "Stackmap: replacing i128 operand with i64 pair\n");
+
+      Ops.reserve(N->getNumOperands() + 1);
+      for(unsigned i = 0; i < N->getNumOperands(); i++) {
+        if(i == OpNo) {
+          Ops.push_back(InvOp->getOperand(0));
+          Ops.push_back(InvOp->getOperand(1));
+        }
+        else Ops.push_back(N->getOperand(i));
+      }
+
+      int64_t SMID = cast<ConstantSDNode>(N->getOperand(0))->getSExtValue();
+      DAG.getMachineFunction().addOpLegalizeChange(SMID, OpNo, 2);
+    }
+    break;
+  }
+
+  return SDValue(DAG.MorphNodeTo(N, N->getOpcode(), N->getVTList(), Ops), 0);
+}
 
 SDValue DAGTypeLegalizer::PromoteIntRes_EXTRACT_SUBVECTOR(SDNode *N) {
 
diff --git a/llvm/lib/CodeGen/SelectionDAG/LegalizeTypes.h b/llvm/lib/CodeGen/SelectionDAG/LegalizeTypes.h
index 1d489b1b3a3..2241bbf5c02 100644
--- a/llvm/lib/CodeGen/SelectionDAG/LegalizeTypes.h
+++ b/llvm/lib/CodeGen/SelectionDAG/LegalizeTypes.h
@@ -369,6 +369,7 @@ private:
   SDValue PromoteIntOp_Shift(SDNode *N);
   SDValue PromoteIntOp_SIGN_EXTEND(SDNode *N);
   SDValue PromoteIntOp_SINT_TO_FP(SDNode *N);
+  SDValue PromoteIntOp_STACKMAP(SDNode *N, unsigned OpNo);
   SDValue PromoteIntOp_STORE(StoreSDNode *N, unsigned OpNo);
   SDValue PromoteIntOp_TRUNCATE(SDNode *N);
   SDValue PromoteIntOp_UINT_TO_FP(SDNode *N);
@@ -464,6 +465,7 @@ private:
   SDValue ExpandIntOp_UINT_TO_FP(SDNode *N);
   SDValue ExpandIntOp_RETURNADDR(SDNode *N);
   SDValue ExpandIntOp_ATOMIC_STORE(SDNode *N);
+  SDValue ExpandIntOp_STACKMAP(SDNode *N, unsigned OpNo);
 
   void IntegerExpandSetCCOperands(SDValue &NewLHS, SDValue &NewRHS,
                                   ISD::CondCode &CCCode, const SDLoc &dl);
diff --git a/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp b/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp
index 4120a401b69..3ad287c3398 100644
--- a/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp
+++ b/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp
@@ -6574,6 +6574,9 @@ void SelectionDAGBuilder::visitIntrinsicCall(const CallInst &I,
   case Intrinsic::experimental_stackmap:
     visitStackmap(I);
     return;
+  case Intrinsic::experimental_pcn_stackmap:
+    visitPcnStackmap(I);
+    return;
   case Intrinsic::experimental_patchpoint_void:
   case Intrinsic::experimental_patchpoint_i64:
     visitPatchpoint(&I);
@@ -8657,6 +8660,144 @@ void SelectionDAGBuilder::visitStackmap(const CallInst &CI) {
   FuncInfo.MF->getFrameInfo().setHasStackMap();
 }
 
+/// Lower llvm.experimental.stackmap directly to its target opcode.
+void SelectionDAGBuilder::visitPcnStackmap(const CallInst &CI) {
+  // void @llvm.experimental.stackmap(i32 <id>, i32 <numShadowBytes>,
+  //                                  [live variables...])
+
+  assert(CI.getType()->isVoidTy() && "Stackmap cannot return a value.");
+
+  SDValue Chain, InFlag, Callee, NullPtr;
+  SmallVector<SDValue, 32> Ops;
+
+  SDLoc DL = getCurSDLoc();
+  Callee = getValue(CI.getCalledValue());
+  NullPtr = DAG.getIntPtrConstant(0, DL, true);
+
+  // Popcorn: we need to insert the stackmap directly after the call
+  // instruction, so grab the chain from the CALLSEQ_END node.  Note that
+  // although we're moving the stackmap between the call & return value
+  // copy-out, the stackmap doesn't generate code so we're not clobbering
+  // return values.
+  Chain = getRoot();
+  SDNode *CSE = Chain.getNode(), *RetCopyOut = nullptr;
+  while(CSE && CSE->getOpcode() != ISD::CALLSEQ_END) {
+    RetCopyOut = CSE;
+    CSE = CSE->getGluedNode();
+  }
+
+  // It's possible the function call that induced this stackmap to be generated
+  // got lowered directly to a machine instruction, which can lead to weird
+  // cases like trying to glue this stackmap to another.  This causes the
+  // instruction scheduler to choke, so avoid that situation.
+  // TODO why does gluing 2 stackmaps cause it to die?  It complains about
+  // NumLiveRegs already being zero for releasing call dependencies in
+  // ScheduleDAGRRList.cpp:759
+  if(CSE && CSE->getOpcode() == ISD::CALLSEQ_END) {
+    SDNode *Check = CSE;
+    do {
+      if(Check->isMachineOpcode() &&
+         Check->getMachineOpcode() == TargetOpcode::PCN_STACKMAP) {
+        CSE = nullptr;
+        break;
+      }
+    } while((Check = Check->getGluedNode()));
+  }
+
+  if(CSE && CSE->getOpcode() == ISD::CALLSEQ_END) Chain = SDValue(CSE, 0);
+  else {
+    LLVM_DEBUG(dbgs() << "WARNING: no call node for: "; Chain->dump());
+    RetCopyOut = nullptr;
+  }
+  assert(Chain.getSimpleValueType() == MVT::Other && "Invalid chain");
+
+  // The stackmap intrinsic only records the live variables (the arguemnts
+  // passed to it) and emits NOPS (if requested). Unlike the patchpoint
+  // intrinsic, this won't be lowered to a function call. This means we don't
+  // have to worry about calling conventions and target specific lowering code.
+  // Instead we perform the call lowering right here.
+  //
+  // chain, flag = CALLSEQ_START(chain, 0, 0)
+  // chain, flag = STACKMAP(id, nbytes, ..., chain, flag)
+  // chain, flag = CALLSEQ_END(chain, 0, 0, flag)
+  //
+  Chain = DAG.getCALLSEQ_START(Chain, 0, 0, DL);
+  InFlag = Chain.getValue(1);
+
+  // Popcorn: add glue operand to CALLSEQ_START to tie the stackmap to the
+  // call sequence (note that it already produces a glue value).
+  if(CSE) {
+    SmallVector<EVT, 2> VTs(Chain->value_begin(), Chain->value_end());
+    SDVTList VTList = DAG.getVTList(VTs);
+    for(auto &Op : Chain->ops()) Ops.push_back(Op);
+    assert(Ops.back().getSimpleValueType() != MVT::Glue &&
+           "Already have glue");
+    Ops.push_back(SDValue(CSE, 1));
+    assert(Ops.back().getSimpleValueType() == MVT::Glue &&
+           "Invalid glue operand");
+    Chain.setNode(DAG.MorphNodeTo(Chain.getNode(), Chain->getOpcode(),
+                                  VTList, Ops));
+    InFlag = Chain.getValue(1);
+    Ops.clear();
+  }
+
+  // Add the <id> and <numBytes> constants.
+  SDValue IDVal = getValue(CI.getOperand(PatchPointOpers::IDPos));
+  Ops.push_back(DAG.getTargetConstant(
+                  cast<ConstantSDNode>(IDVal)->getZExtValue(), DL, MVT::i64));
+  SDValue NBytesVal = getValue(CI.getOperand(PatchPointOpers::NBytesPos));
+  Ops.push_back(DAG.getTargetConstant(
+                  cast<ConstantSDNode>(NBytesVal)->getZExtValue(), DL,
+                  MVT::i32));
+
+  // Push live variables for the stack map.
+  addStackMapLiveVars(&CI, 2, DL, Ops, *this);
+
+  // We are not pushing any register mask info here on the operands list,
+  // because the stackmap doesn't clobber anything.
+
+  // Push the chain and the glue flag.
+  Ops.push_back(Chain);
+  Ops.push_back(InFlag);
+
+  // Create the STACKMAP node.
+  SDVTList NodeTys = DAG.getVTList(MVT::Other, MVT::Glue);
+  SDNode *SM = DAG.getMachineNode(TargetOpcode::PCN_STACKMAP, DL, NodeTys, Ops);
+  Chain = SDValue(SM, 0);
+  InFlag = Chain.getValue(1);
+
+  Chain = DAG.getCALLSEQ_END(Chain, NullPtr, NullPtr, InFlag, DL);
+  InFlag = Chain.getValue(1);
+
+  // Stackmaps don't generate values, so nothing goes into the NodeMap.
+
+  // Popcorn: fix-up the glue so the return-value copy outs happen after
+  // the stackmap's CALLSEQ_END.  Note that this is *required*, otherwise
+  // the backend won't correctly track physical register outputs from call.
+  if(RetCopyOut) {
+    unsigned NOpts = RetCopyOut->getNumOperands();
+    Ops.clear();
+    for(size_t i = 0; i < NOpts; i++) {
+      if(RetCopyOut->getOperand(i).getSimpleValueType() == MVT::Other)
+        Ops.push_back(Chain);
+      else if(RetCopyOut->getOperand(i).getSimpleValueType() == MVT::Glue)
+        Ops.push_back(InFlag);
+      else Ops.push_back(RetCopyOut->getOperand(i));
+    }
+    RetCopyOut = DAG.UpdateNodeOperands(RetCopyOut, Ops);
+
+    while(RetCopyOut->getGluedUser()) RetCopyOut = RetCopyOut->getGluedUser();
+    Chain = SDValue(RetCopyOut, NOpts - 2);
+    assert(Chain.getSimpleValueType() == MVT::Other && "Invalid chain");
+  }
+
+  // Set the root to the target-lowered call chain.
+  DAG.setRoot(Chain);
+
+  // Inform the Frame Information that we have a stackmap in this function.
+  FuncInfo.MF->getFrameInfo().setHasPcnStackMap();
+}
+
 /// Lower llvm.experimental.patchpoint directly to its target opcode.
 void SelectionDAGBuilder::visitPatchpoint(ImmutableCallSite CS,
                                           const BasicBlock *EHPadBB) {
diff --git a/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.h b/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.h
index 0072e33f23b..b5ad137bcab 100644
--- a/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.h
+++ b/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.h
@@ -753,6 +753,7 @@ private:
   void visitVAEnd(const CallInst &I);
   void visitVACopy(const CallInst &I);
   void visitStackmap(const CallInst &I);
+  void visitPcnStackmap(const CallInst &I);
   void visitPatchpoint(ImmutableCallSite CS,
                        const BasicBlock *EHPadBB = nullptr);
 
diff --git a/llvm/lib/CodeGen/StackMaps.cpp b/llvm/lib/CodeGen/StackMaps.cpp
index ae9401b8970..4ec29c1854f 100644
--- a/llvm/lib/CodeGen/StackMaps.cpp
+++ b/llvm/lib/CodeGen/StackMaps.cpp
@@ -15,15 +15,20 @@
 #include "llvm/CodeGen/MachineFunction.h"
 #include "llvm/CodeGen/MachineInstr.h"
 #include "llvm/CodeGen/MachineOperand.h"
+#include "llvm/CodeGen/TargetFrameLowering.h"
 #include "llvm/CodeGen/TargetOpcodes.h"
 #include "llvm/CodeGen/TargetRegisterInfo.h"
 #include "llvm/CodeGen/TargetSubtargetInfo.h"
+#include "llvm/CodeGen/UnwindInfo.h"
 #include "llvm/IR/DataLayout.h"
+#include "llvm/IR/DiagnosticInfo.h"
+#include "llvm/IR/IntrinsicInst.h"
 #include "llvm/MC/MCContext.h"
 #include "llvm/MC/MCExpr.h"
 #include "llvm/MC/MCObjectFileInfo.h"
 #include "llvm/MC/MCRegisterInfo.h"
 #include "llvm/MC/MCStreamer.h"
+#include "llvm/MC/MCSymbol.h"
 #include "llvm/Support/CommandLine.h"
 #include "llvm/Support/Debug.h"
 #include "llvm/Support/ErrorHandling.h"
@@ -39,6 +44,16 @@ using namespace llvm;
 
 #define DEBUG_TYPE "stackmaps"
 
+#define TYPE_AND_FLAGS(type, ptr, alloca, dup, temp) \
+  ((uint8_t)type) << 4 | ((uint8_t)ptr) << 3 | \
+  ((uint8_t)alloca) << 2 | ((uint8_t)dup << 1) | \
+  ((uint8_t)temp)
+
+#define ARCH_TYPE_AND_FLAGS(type, ptr) ((uint8_t)type) << 4 | ((uint8_t)ptr)
+
+#define ARCH_OP_TYPE(inst, gen, op) \
+  ((uint8_t)inst) << 4 | ((uint8_t)gen) << 3 | (uint8_t)op
+
 static cl::opt<int> StackMapVersion(
     "stackmap-version", cl::init(3), cl::Hidden,
     cl::desc("Specify the stackmap encoding version (default = 3)"));
@@ -98,6 +113,212 @@ static unsigned getDwarfRegNum(unsigned Reg, const TargetRegisterInfo *TRI) {
   return (unsigned)RegNum;
 }
 
+/// If the instruction is simply casting a pointer to another type, return
+/// the value used as the source of the cast.  This is required because allocas
+/// may be cast to different pointer types (which appear as non-allocas) but
+/// may still be represented by a direct memory reference in the stackmap.
+static inline const Value *getPointerCastSrc(const Value *Inst) {
+  assert(Inst->getType()->isPointerTy() && "Not a pointer type");
+
+  if(isa<BitCastInst>(Inst)) {
+    // Ensure that we're casting to another pointer type
+    const BitCastInst *BC = cast<BitCastInst>(Inst);
+    if(!BC->getSrcTy()->isPointerTy()) return nullptr;
+    else return BC->getOperand(0);
+  }
+  else if(isa<GetElementPtrInst>(Inst)) {
+    // Ensure that all indexes are 0, meaning we're only referencing the start
+    // of the storage location
+    const GetElementPtrInst *GEP = cast<GetElementPtrInst>(Inst);
+    GetElementPtrInst::const_op_iterator it, e;
+    for(it = GEP->idx_begin(), e = GEP->idx_end(); it != e; it++) {
+      if(!isa<ConstantInt>(it->get())) return nullptr;
+      const ConstantInt *Idx = cast<ConstantInt>(it->get());
+      if(!Idx->isZero()) return nullptr;
+    }
+    return GEP->getOperand(0);
+  }
+  else return nullptr;
+}
+
+/// Get pointer typing information for a stackmap operand
+void StackMaps::getPointerInfo(const Value *Op, const DataLayout &DL,
+                               bool &isPtr, bool &isAlloca,
+                               unsigned &AllocaSize) const {
+  isPtr = false;
+  isAlloca = false;
+  AllocaSize = 0;
+  const PtrToIntInst *PTII;
+
+  if((PTII = dyn_cast<PtrToIntInst>(Op))) Op = PTII->getPointerOperand();
+
+  assert(Op != nullptr && "Invalid stackmap operand");
+  Type *Ty = Op->getType();
+  if(Ty->isPointerTy())
+  {
+    // Walk through cast operations that potentially hide allocas
+    while(!isa<AllocaInst>(Op) && (Op = getPointerCastSrc(Op)));
+    if(Op && isa<AllocaInst>(Op)) {
+      PointerType *PTy = cast<PointerType>(Ty);
+      assert(PTy->getElementType()->isSized() && "Alloca of unknown size?");
+      isPtr = PTy->getElementType()->isPointerTy();
+      isAlloca = true;
+      AllocaSize = DL.getTypeAllocSize(PTy->getElementType());
+    }
+    else isPtr = true;
+  }
+}
+
+/// Get stackmap information for register location
+void StackMaps::getRegLocation(unsigned Phys,
+                               unsigned &Dwarf,
+                               unsigned &Offset) const {
+  const TargetRegisterInfo *TRI = AP.MF->getSubtarget().getRegisterInfo();
+  assert(!TRI->isVirtualRegister(Phys) &&
+         "Virtual registers should have been rewritten by now");
+  Offset = 0;
+  Dwarf = getDwarfRegNum(Phys, TRI);
+  unsigned LLVMRegNum = TRI->getLLVMRegNum(Dwarf, false);
+  unsigned SubRegIdx = TRI->getSubRegIndex(LLVMRegNum, Phys);
+  if(SubRegIdx)
+    Offset = TRI->getSubRegIdxOffset(SubRegIdx);
+}
+
+/// Add duplicate target-specific locations for a stackmap operand
+void StackMaps::addDuplicateLocs(const CallInst *StackMap, const Value *Oper,
+                                 LocationVec &Locs, unsigned Size, bool Ptr,
+                                 bool Alloca, unsigned AllocaSize) const {
+  unsigned DwarfRegNum, Offset;
+  int FrameOff;
+
+  if(AP.MF->hasSMOpLocations(StackMap, Oper)) {
+    const MachineLiveLocs &Dups = AP.MF->getSMOpLocations(StackMap, Oper);
+    const TargetRegisterInfo *TRI = AP.MF->getSubtarget().getRegisterInfo();
+
+    for(const MachineLiveLocPtr &LL : Dups) {
+      if(LL->isReg()) {
+        const MachineLiveReg &MR = (const MachineLiveReg &)*LL;
+        getRegLocation(MR.getReg(), DwarfRegNum, Offset);
+
+        Locs.emplace_back(Location::Register, Size, DwarfRegNum, Offset,
+                          Ptr, Alloca, true, false, AllocaSize);
+      }
+      else if(LL->isStackAddr()) {
+        MachineLiveStackAddr &MLSA = (MachineLiveStackAddr &)*LL;
+        FrameOff = MLSA.calcAndGetRegOffset(AP, DwarfRegNum);
+
+        Locs.emplace_back(Location::Indirect, Size,
+          getDwarfRegNum(DwarfRegNum, TRI),
+          FrameOff, Ptr, Alloca, true, false, AllocaSize);
+      }
+      else llvm_unreachable("Unknown machine live location type");
+    }
+  }
+}
+
+MachineInstr::const_mop_iterator
+StackMaps::parsePcnOperand(MachineInstr::const_mop_iterator MOI,
+                        MachineInstr::const_mop_iterator MOE, LocationVec &Locs,
+                        LiveOutVec &LiveOuts, User::const_op_iterator &Op) const {
+  bool isPtr, isAlloca, isTemporary = false;
+  unsigned AllocaSize;
+  auto &DL = AP.MF->getDataLayout();
+  const TargetRegisterInfo *TRI = AP.MF->getSubtarget().getRegisterInfo();
+  const CallInst *IRSM = cast<CallInst>(Op->getUser());
+  const Value *IROp = Op->get();
+  int64_t TemporaryOffset = 0;
+  getPointerInfo(IROp, DL, isPtr, isAlloca, AllocaSize);
+
+  if (MOI->isImm()) {
+    // Peel off temporary value metadata
+    if (MOI->getImm() == StackMaps::TemporaryOp) {
+      isTemporary = true;
+      AllocaSize = (++MOI)->getImm();
+      TemporaryOffset = (++MOI)->getImm();
+      ++MOI;
+    }
+    switch (MOI->getImm()) {
+    default:
+      llvm_unreachable("Unrecognized operand type.");
+    case StackMaps::DirectMemRefOp: {
+      auto &DL = AP.MF->getDataLayout();
+
+      assert((isAlloca || isTemporary) &&
+             "Did not find alloca value for direct memory reference");
+      unsigned Size = DL.getPointerSizeInBits();
+      assert((Size % 8) == 0 && "Need pointer size in bytes.");
+      Size /= 8;
+      unsigned Reg = (++MOI)->getReg();
+      int64_t Imm = (++MOI)->getImm() + TemporaryOffset;
+      Locs.emplace_back(Location::Direct, Size, getDwarfRegNum(Reg, TRI), Imm,
+                        isPtr, true, false, isTemporary, AllocaSize);
+      break;
+    }
+    case StackMaps::IndirectMemRefOp: {
+      int64_t Size = (++MOI)->getImm();
+      assert(Size > 0 && "Need a valid size for indirect memory locations.");
+      Size = DL.getTypeAllocSize(IROp->getType());
+      unsigned Reg = (++MOI)->getReg();
+      int64_t Imm = (++MOI)->getImm();
+      // Note: getPointerInfo() may have found a suitable alloca for this
+      // operand, but the backend didn't actually turn it into one.
+      Locs.emplace_back(Location::Indirect, (unsigned)Size,
+                        getDwarfRegNum(Reg, TRI), Imm, isPtr, false, false,
+                        isTemporary, 0);
+      break;
+    }
+    case StackMaps::ConstantOp: {
+      ++MOI;
+      assert(MOI->isImm() && "Expected constant operand.");
+      int64_t Imm = MOI->getImm();
+      // Note: getPointerInfo() may have found a suitable alloca for this
+      // operand, but the backend didn't actually turn it into one.
+      Locs.emplace_back(Location::Constant, sizeof(int64_t), 0, Imm,
+                        isPtr, false, false, isTemporary, 0);
+      break;
+    }
+    }
+    // Note: we shouldn't have alternate locations -- constants aren't stored
+    // anywhere, and stack slots should be either allocas (which shouldn't have
+    // alternate locations) or register spill locations (handled below in the
+    // register path)
+    assert(!AP.MF->hasSMOpLocations(IRSM, IROp) &&
+           "Unhandled duplicate locations");
+    ++Op;
+    return ++MOI;
+  }
+
+  // The physical register number will ultimately be encoded as a DWARF regno.
+  // The stack map also records the size of a spill slot that can hold the
+  // register content, accurate to the actual size of the data type.
+  if (MOI->isReg()) {
+    // Skip implicit registers (this includes our scratch registers)
+    if (MOI->isImplicit())
+      return ++MOI;
+
+    assert(TargetRegisterInfo::isPhysicalRegister(MOI->getReg()) &&
+           "Virtreg operands should have been rewritten before now.");
+    assert(!MOI->getSubReg() && "Physical subreg still around.");
+
+    size_t ValSize = DL.getTypeAllocSize(IROp->getType());
+    unsigned Offset, DwarfRegNum;
+    getRegLocation(MOI->getReg(), DwarfRegNum, Offset);
+
+    // Note: getPointerInfo() may have found a suitable alloca for this
+    // operand, but the backend didn't actually turn it into one.
+    Locs.emplace_back(Location::Register, ValSize, DwarfRegNum, Offset,
+                      isPtr, false, false, isTemporary, 0);
+    addDuplicateLocs(IRSM, IROp, Locs, ValSize, isPtr, false, 0);
+    ++Op;
+    return ++MOI;
+  }
+
+  if (MOI->isRegLiveOut())
+    LiveOuts = parseRegisterLiveOutMask(MOI->getRegLiveOut());
+
+  return ++MOI;
+}
+
 MachineInstr::const_mop_iterator
 StackMaps::parseOperand(MachineInstr::const_mop_iterator MOI,
                         MachineInstr::const_mop_iterator MOE, LocationVec &Locs,
@@ -178,6 +399,7 @@ void StackMaps::print(raw_ostream &OS) {
   for (const auto &CSI : CSInfos) {
     const LocationVec &CSLocs = CSI.Locations;
     const LiveOutVec &LiveOuts = CSI.LiveOuts;
+    const ArchValues &Values = CSI.Vals;
 
     OS << WSMP << "callsite " << CSI.ID << "\n";
     OS << WSMP << "  has " << CSLocs.size() << " locations\n";
@@ -220,9 +442,17 @@ void StackMaps::print(raw_ostream &OS) {
         OS << "Constant Index " << Loc.Offset;
         break;
       }
-      OS << "\t[encoding: .byte " << Loc.Type << ", .byte 0"
+      OS << ", pointer? " << Loc.Ptr << ", alloca? " << Loc.Alloca
+         << ", duplicate? " << Loc.Duplicate
+         << ", temporary? " << Loc.Temporary;
+
+      unsigned TypeAndFlags = TYPE_AND_FLAGS(Loc.Type, Loc.Ptr, Loc.Alloca,
+                                             Loc.Duplicate, Loc.Temporary);
+
+      OS << "\t[encoding: .byte " << TypeAndFlags << ", .byte 0"
          << ", .short " << Loc.Size << ", .short " << Loc.Reg << ", .short 0"
-         << ", .int " << Loc.Offset << "]\n";
+         << ", .int " << Loc.Offset
+         << ", .uint " << Loc.AllocaSize << "]\n";
       Idx++;
     }
 
@@ -239,6 +469,87 @@ void StackMaps::print(raw_ostream &OS) {
          << LO.Size << "]\n";
       Idx++;
     }
+
+    OS << WSMP << "\thas " << Values.size() << " arch-specific live values\n";
+
+    Idx = 0;
+    for (const auto &V : Values) {
+      const Location &Loc = V.first;
+      const Operation &Op = V.second;
+
+      OS << WSMP << "\t\tArch-Val " << Idx << ": ";
+      switch(Loc.Type) {
+      case Location::Register:
+        OS << "Register ";
+        if (TRI)
+          OS << printReg(Loc.Reg, TRI);
+        else
+          OS << Loc.Reg;
+        break;
+      case Location::Indirect:
+        OS << "Indirect ";
+        if (TRI)
+          OS << printReg(Loc.Reg, TRI);
+        else
+          OS << Loc.Reg;
+        if (Loc.Offset)
+          OS << " + " << Loc.Offset;
+        break;
+      default:
+        OS << "<Unknown live value type>";
+        break;
+      }
+
+      OS << ", " << ValueGenInst::getInstName(Op.InstType) << " ";
+      switch(Op.OperandType) {
+      case Location::Register:
+        OS << "register ";
+        if (TRI)
+          OS << printReg(Op.DwarfReg, TRI);
+        else
+          OS << Op.DwarfReg;
+        break;
+      case Location::Direct:
+        OS << "value stored at register ";
+        if (TRI)
+          OS << printReg(Op.DwarfReg, TRI);
+        else
+          OS << Op.DwarfReg;
+        if (Op.Constant)
+          OS << " + " << Op.Constant;
+        break;
+      case Location::Indirect:
+        OS << "register";
+        if (TRI)
+          OS << printReg(Op.DwarfReg, TRI);
+        else
+          OS << Op.DwarfReg;
+        if (Op.Constant)
+          OS << " + " << Op.Constant;
+        break;
+      case Location::Constant:
+        if(Op.isSymbol)
+          OS << "address of " << Op.Symbol->getName();
+        else {
+          OS << "immediate ";
+          OS.write_hex(Op.Constant);
+        }
+        break;
+      default:
+        OS << "<Unknown operand type>";
+        break;
+      }
+
+      unsigned TypeAndFlags = ARCH_TYPE_AND_FLAGS(Loc.Type, Loc.Ptr);
+      unsigned OpType = ARCH_OP_TYPE(Op.InstType,
+                                     Op.isGenerated,
+                                     Op.OperandType);
+      OS << "\t[encoding: .byte " << TypeAndFlags << ", .byte " << Loc.Size
+         << ", .short " << Loc.Reg << ", .int " << Loc.Offset
+         << ", .byte " << OpType << ", .byte " << Op.Size << ", .short "
+         << Op.DwarfReg << ", .int64 " << (Op.isSymbol ? 0 : Op.Constant)
+         << "]\n";
+    }
   }
 }
 
@@ -294,6 +605,256 @@ StackMaps::parseRegisterLiveOutMask(const uint32_t *Mask) const {
   return LiveOuts;
 }
 
+/// Convert a list of instructions used to generate an architecture-specific
+/// live value into multiple individual records.
+void StackMaps::genArchValsFromInsts(ArchValues &AV,
+                                     Location &Loc,
+                                     const MachineLiveVal &MLV) {
+  assert(MLV.isGenerated() && "Invalid live value type");
+
+  unsigned PtrSize = AP.MF->getDataLayout().getPointerSizeInBits() / 8;
+  const MachineGeneratedVal &MGV = (const MachineGeneratedVal &)MLV;
+  const ValueGenInstList &I = MGV.getInstructions();
+  const TargetRegisterInfo *TRI = AP.MF->getSubtarget().getRegisterInfo();
+  const TargetRegisterClass *RC;
+  Operation Op;
+  Op.isGenerated = true;
+
+  for(auto &Inst : I) {
+    const RegInstructionBase *RI;
+    const ImmInstructionBase *II;
+    const RefInstruction *RefI;
+
+    Op.DwarfReg = 0;
+    Op.Constant = 0;
+    Op.isSymbol = false;
+    Op.Symbol = nullptr;
+
+    Op.InstType = Inst->type();
+    if (Inst->opType() != ValueGenInst::OpType::Register
+	&& Inst->opType() != ValueGenInst::OpType::Immediate
+	&& Inst->opType() != ValueGenInst::OpType::Reference)
+      llvm_unreachable("Invalid operand type");
+
+    switch(Inst->opType()) {
+    case ValueGenInst::OpType::Register:
+      RI = (const RegInstructionBase *)Inst.get();
+      assert(TRI->isPhysicalRegister(RI->getReg()) &&
+             "Virtual should have been converted to physical register");
+      RC = TRI->getMinimalPhysRegClass(RI->getReg());
+      Op.OperandType = Location::Register;
+      Op.Size = TRI->getRegSizeInBits(*RC) / 8;
+      Op.DwarfReg = getDwarfRegNum(RI->getReg(), TRI);
+      break;
+    case ValueGenInst::OpType::Immediate:
+      II = (const ImmInstructionBase *)Inst.get();
+      Op.OperandType = Location::Constant;
+      Op.Size = II->getImmSize();
+      Op.Constant = II->getImm();
+      break;
+    case ValueGenInst::OpType::Reference:
+      RefI = (const RefInstruction *)Inst.get();
+      Op.OperandType = Location::Constant;
+      Op.Size = PtrSize;
+      Op.isSymbol = true;
+      Op.Symbol = RefI->getReference(AP);
+      break;
+    }
+    AV.emplace_back(ArchValue(Loc, Op));
+  }
+}
+
+/// Add architecture-specific locations for the stackmap
+void StackMaps::addArchLiveVals(const CallInst *SM, ArchValues &AV) {
+  unsigned Offset, DwarfReg;
+  unsigned PtrSize = AP.MF->getDataLayout().getPointerSizeInBits() / 8;
+  const MachineFrameInfo *MFI = &AP.MF->getFrameInfo();
+  const TargetRegisterInfo *TRI = AP.MF->getSubtarget().getRegisterInfo();
+
+  if(AP.MF->hasSMArchSpecificLocations(SM)) {
+    const ArchLiveValues &Vals = AP.MF->getSMArchSpecificLocations(SM);
+
+    for(auto &Val : Vals) {
+      Location Loc;
+      Operation Op;
+
+      Loc.Ptr = Val.second->isPtr();
+      Loc.Alloca = false;
+      Loc.Duplicate = false;
+      Loc.AllocaSize = 0;
+
+      // Parse the location
+      if(Val.first->isReg()) {
+        const MachineLiveReg &MR = (const MachineLiveReg &)*Val.first;
+        const TargetRegisterClass *RC =
+          TRI->getMinimalPhysRegClass(MR.getReg());
+        getRegLocation(MR.getReg(), DwarfReg, Offset);
+
+        Loc.Type = Location::Register;
+        Loc.Size = TRI->getRegSizeInBits(*RC) / 8;
+        Loc.Reg = DwarfReg;
+        Loc.Offset = Offset;
+      }
+      else if(Val.first->isStackAddr()) {
+        MachineLiveStackAddr &MLSA = (MachineLiveStackAddr &)*Val.first;
+
+        Loc.Type = Location::Indirect;
+        Loc.Size = MLSA.getSize(AP);
+        Loc.Offset = MLSA.calcAndGetRegOffset(AP, DwarfReg);
+        Loc.Reg = getDwarfRegNum(DwarfReg, TRI);
+      }
+      else llvm_unreachable("Invalid architecture-specific live value");
+
+      // Parse the operation
+      Op.InstType = ValueGenInst::Set;
+      Op.isGenerated = false;
+      if(Val.second->isReference()) {
+        const MachineReference &MR = (const MachineReference &)*Val.second;
+        if(MR.isLoad()) Op.InstType = ValueGenInst::Load64;
+        Op.OperandType = Location::Constant;
+        Op.Size = PtrSize;
+        Op.isSymbol = true;
+        Op.Symbol = MR.getReference(AP);
+        AV.emplace_back(ArchValue(Loc, Op));
+      }
+      else if(Val.second->isStackObject()) {
+        const MachineStackObject &MSO = (const MachineStackObject &)*Val.second;
+        if(MSO.isLoad()) { // Loading a value from a stack slot
+          Op.OperandType = Location::Direct;
+          if(MSO.isCommonObject()) Op.Size = PtrSize;
+          else Op.Size = MFI->getObjectSize(MSO.getIndex());
+        }
+        else { // Generating a reference to a stack slot
+          Op.OperandType = Location::Indirect;
+          Op.Size = PtrSize;
+        }
+        Op.Constant = MSO.getOffsetFromReg(AP, DwarfReg);
+        Op.DwarfReg = getDwarfRegNum(DwarfReg, TRI);
+        Op.isSymbol = false;
+        AV.emplace_back(ArchValue(Loc, Op));
+      }
+      else if(Val.second->isImm()) {
+        const MachineImmediate &MI = (const MachineImmediate &)*Val.second;
+        Op.OperandType = Location::Constant;
+        Op.Size = MI.getSize();
+        Op.Constant = MI.getValue();
+        Op.isSymbol = false;
+        AV.emplace_back(ArchValue(Loc, Op));
+      }
+      else if(Val.second->isGenerated())
+        genArchValsFromInsts(AV, Loc, *Val.second);
+      else llvm_unreachable("Invalid architecture-specific live value");
+    }
+  }
+}
+
+void StackMaps::recordPcnStackMapOpers(const MachineInstr &MI, uint64_t ID,
+                                    MachineInstr::const_mop_iterator MOI,
+                                    MachineInstr::const_mop_iterator MOE,
+                                    bool recordResult) {
+  MCContext &OutContext = AP.OutStreamer->getContext();
+  MCSymbol *MILabel = OutContext.createTempSymbol();
+  AP.OutStreamer->EmitLabel(MILabel);
+  User::const_op_iterator Op = nullptr;
+
+  LocationVec Locations;
+  LiveOutVec LiveOuts;
+  ArchValues Constants;
+
+  if (recordResult) {
+    assert(PatchPointOpers(&MI).hasDef() && "Stackmap has no return value.");
+    parsePcnOperand(MI.operands_begin(), std::next(MI.operands_begin()),
+		    Locations, LiveOuts, Op);
+  }
+
+  // Find the IR stackmap instruction which corresponds to MI so we can emit
+  // type information along with the value's location
+  const BasicBlock *BB = MI.getParent()->getBasicBlock();
+  const IntrinsicInst *IRSM = nullptr;
+  const std::string SMName("llvm.experimental.pcn.stackmap");
+  for(auto BBI = BB->begin(), BBE = BB->end(); BBI != BBE; BBI++)
+  {
+    const IntrinsicInst *II;
+    if((II = dyn_cast<IntrinsicInst>(&*BBI)) &&
+       II->getCalledFunction()->getName() == SMName &&
+       cast<ConstantInt>(II->getArgOperand(0))->getZExtValue() == ID)
+    {
+      IRSM = cast<IntrinsicInst>(&*BBI);
+      break;
+    }
+  }
+  assert(IRSM && "Could not find associated stackmap instruction");
+
+  // Parse operands.
+  unsigned NumRepeat;
+  Op = std::next(IRSM->op_begin(), 2);
+  MachineInstr::const_mop_iterator MFirst = MI.operands_begin();
+  while (MOI != MOE) {
+    MOI = parsePcnOperand(MOI, MOE, Locations, LiveOuts, Op);
+    NumRepeat = AP.MF->getNumLegalizedOps(ID, MOI - MFirst) - 1;
+    for(size_t i = 0; i < NumRepeat; i++) {
+      MOI = parsePcnOperand(MOI, MOE, Locations, LiveOuts, Op);
+      --Op;
+    }
+  }
+  assert(Op == (IRSM->op_end() - 1) && "did not lower all stackmap operands");
+
+  // Add architecture-specific live values
+  addArchLiveVals(IRSM, Constants);
+
+  // Move large constants into the constant pool.
+  for (auto &Loc : Locations) {
+    // Constants are encoded as sign-extended integers.
+    // -1 is directly encoded as .long 0xFFFFFFFF with no constant pool.
+    if (Loc.Type == Location::Constant && !isInt<32>(Loc.Offset)) {
+      Loc.Type = Location::ConstantIndex;
+      // ConstPool is intentionally a MapVector of 'uint64_t's (as
+      // opposed to 'int64_t's).  We should never be in a situation
+      // where we have to insert either the tombstone or the empty
+      // keys into a map, and for a DenseMap<uint64_t, T> these are
+      // (uint64_t)0 and (uint64_t)-1.  They can be and are
+      // represented using 32 bit integers.
+      assert((uint64_t)Loc.Offset != DenseMapInfo<uint64_t>::getEmptyKey() &&
+             (uint64_t)Loc.Offset !=
+                 DenseMapInfo<uint64_t>::getTombstoneKey() &&
+             "empty and tombstone keys should fit in 32 bits!");
+      auto Result = ConstPool.insert(std::make_pair(Loc.Offset, Loc.Offset));
+      Loc.Offset = Result.first - ConstPool.begin();
+    }
+  }
+
+  // Create an expression to calculate the offset of the callsite from function
+  // entry.
+  // TODO for Popcorn, we actually want the return address of the call
+  // instruction to which this stackmap is attached.  However some backend
+  // writers, in their infinite wisdom, decided to abstract multiple assembly
+  // instructions into a single machine IR instruction (*ahem* PowerPC *ahem*).
+  // Generate an expression to correct for this "feature".
+  int RAOffset = AP.getCanonicalReturnAddr(MI.getPrevNode());
+  const MCExpr *RAFixup = MCBinaryExpr::createSub(
+      MCSymbolRefExpr::create(MILabel, OutContext),
+      MCConstantExpr::create(RAOffset, OutContext), OutContext);
+  const MCExpr *CSOffsetExpr = MCBinaryExpr::createSub(RAFixup,
+      MCSymbolRefExpr::create(AP.CurrentFnSymForSize, OutContext), OutContext);
+
+  CSInfos.emplace_back(AP.CurrentFnSym, CSOffsetExpr, ID,
+                       std::move(Locations), std::move(LiveOuts),
+                       std::move(Constants));
+
+  // Record the stack size of the current function and update callsite count.
+  const MachineFrameInfo &MFI = AP.MF->getFrameInfo();
+  const TargetRegisterInfo *RegInfo = AP.MF->getSubtarget().getRegisterInfo();
+  bool HasDynamicFrameSize =
+      MFI.hasVarSizedObjects() || RegInfo->needsStackRealignment(*(AP.MF));
+  uint64_t FrameSize = HasDynamicFrameSize ? UINT64_MAX : MFI.getStackSize();
+
+  auto CurrentIt = FnInfos.find(AP.CurrentFnSym);
+  if (CurrentIt != FnInfos.end())
+    CurrentIt->second.RecordCount++;
+  else
+    FnInfos.insert(std::make_pair(AP.CurrentFnSym, FunctionInfo(FrameSize)));
+}
+
 void StackMaps::recordStackMapOpers(const MachineInstr &MI, uint64_t ID,
                                     MachineInstr::const_mop_iterator MOI,
                                     MachineInstr::const_mop_iterator MOE,
@@ -369,6 +930,17 @@ void StackMaps::recordStackMap(const MachineInstr &MI) {
                       MI.operands_end());
 }
 
+void StackMaps::recordPcnStackMap(const MachineInstr &MI) {
+  assert(MI.getOpcode() == TargetOpcode::PCN_STACKMAP
+	 && "expected pcn_stackmap");
+
+  StackMapOpers opers(&MI);
+  const int64_t ID = MI.getOperand(PatchPointOpers::IDPos).getImm();
+  recordPcnStackMapOpers(MI, ID,
+			 std::next(MI.operands_begin(), opers.getVarIdx()),
+			 MI.operands_end());
+}
+
 void StackMaps::recordPatchPoint(const MachineInstr &MI) {
   assert(MI.getOpcode() == TargetOpcode::PATCHPOINT && "expected patchpoint");
 
@@ -448,6 +1020,38 @@ void StackMaps::emitFunctionFrameRecords(MCStreamer &OS) {
   }
 }
 
+/// Emit the function frame record for each function.
+///
+/// StkSizeRecord[NumFunctions] {
+///   uint64 : Function Address
+///   uint64 : Stack Size
+///   uint64 : Record Count
+///   uint32 : Number of Unwinding Entries
+///   uint32 : Offset into Unwinding Section
+/// }
+void StackMaps::emitPcnFunctionFrameRecords(MCStreamer &OS,
+                                         const UnwindInfo *UI) {
+  // Function Frame records.
+  LLVM_DEBUG(dbgs() << WSMP << "functions:\n");
+  for (auto const &FR : FnInfos) {
+    LLVM_DEBUG(dbgs() << WSMP << "function addr: " << FR.first
+                      << " frame size: " << FR.second.StackSize
+                      << " callsite count: " << FR.second.RecordCount << '\n');
+    OS.EmitSymbolValue(FR.first, 8);
+    OS.EmitIntValue(FR.second.StackSize, 8);
+    OS.EmitIntValue(FR.second.RecordCount, 8);
+
+    if(UI) {
+      const UnwindInfo::FuncUnwindInfo &FUI = UI->getUnwindInfo(FR.first);
+      LLVM_DEBUG(dbgs() << " unwind info start: " << FUI.SecOffset
+                   << " (" << FUI.NumUnwindRecord << " entries)\n");
+      OS.EmitIntValue(FUI.NumUnwindRecord, 4);
+      OS.EmitIntValue(FUI.SecOffset, 4);
+    }
+    else OS.EmitIntValue(0, 8);
+  }
+}
+
 /// Emit the constant pool.
 ///
 /// int64  : Constants[NumConstants]
@@ -544,6 +1148,153 @@ void StackMaps::emitCallsiteEntries(MCStreamer &OS) {
   }
 }
 
+/// Emit the Popcorn callsite info for each callsite.
+///
+/// StkMapRecord[NumRecords] {
+///   uint64 : PatchPoint ID
+///   uint32 : Index of Function Record
+///   uint32 : Instruction Offset
+///   uint16 : Reserved (record flags)
+///   uint16 : NumLocations
+///   Location[NumLocations] {
+///     uint8 (4 bits) : Register | Direct | Indirect | Constant | ConstantIndex
+///     uint8 (1 bit)  : Is it a pointer?
+///     uint8 (1 bit)  : Is it an alloca?
+///     uint8 (1 bit)  : Is it a duplicate record for the same live value?
+///     uint8 (1 bit)  : Is it a temporary value created for the stackmap?
+///     uint8          : Size in Bytes
+///     uint16         : Dwarf RegNum
+///     int32          : Offset
+///     uint32         : Size of pointed-to alloca data
+///   }
+///   uint16 : Padding
+///   uint16 : NumLiveOuts
+///   LiveOuts[NumLiveOuts] {
+///     uint16 : Dwarf RegNum
+///     uint8  : Reserved
+///     uint8  : Size in Bytes
+///   }
+///   uint16 : Padding
+///   uint16 : NumArchValues
+///   ArchValues[NumArchValues] {
+///     Location {
+///       uint8 (4 bits) : Register | Indirect
+///       uint8 (3 bits) : Padding
+///       uint8 (1 bit)  : Is it a pointer?
+///       uint8          : Size in Bytes
+///       uint16         : Dwarf RegNum
+///       int32          : Offset
+///     }
+///     Value {
+///       uint8_t (4 bits) : Instruction
+///       uint8_t (4 bits) : Register | Direct | Constant
+///       uint8_t          : Size
+///       uint16_t         : Dwarf RegNum
+///       int64_t          : Offset or Constant
+///     }
+///   }
+///   uint32 : Padding (only if required to align to 8 byte)
+/// }
+///
+/// Location Encoding, Type, Value:
+///   0x1, Register, Reg                 (value in register)
+///   0x2, Direct, Reg + Offset          (frame index)
+///   0x3, Indirect, [Reg + Offset]      (spilled value)
+///   0x4, Constant, Offset              (small constant)
+///   0x5, ConstIndex, Constants[Offset] (large constant)
+void StackMaps::emitPcnCallsiteEntries(MCStreamer &OS) {
+  LLVM_DEBUG(print(dbgs()));
+  // Callsite entries.
+  for (const auto &CSI : CSInfos) {
+    const LocationVec &CSLocs = CSI.Locations;
+    const LiveOutVec &LiveOuts = CSI.LiveOuts;
+    const ArchValues &Values = CSI.Vals;
+
+    // Determine the function index.
+    int FunctionIndex = 0;
+    for (const auto &FNI : FnInfos) {
+      if (FNI.first == CSI.Func)
+	break;
+      FunctionIndex++;
+    }
+
+    // Verify stack map entry. It's better to communicate a problem to the
+    // runtime than crash in case of in-process compilation. Currently, we do
+    // simple overflow checks, but we may eventually communicate other
+    // compilation errors this way.
+    if (CSLocs.size() > UINT16_MAX || LiveOuts.size() > UINT16_MAX ||
+        Values.size() > UINT16_MAX) {
+      OS.EmitIntValue(UINT64_MAX, 8); // Invalid ID.
+      OS.EmitIntValue(UINT32_MAX, 4); // Invalid index.
+      OS.EmitValue(CSI.CSOffsetExpr, 4);
+      OS.EmitIntValue(0, 2); // Reserved.
+      OS.EmitIntValue(0, 2); // 0 locations.
+      OS.EmitIntValue(0, 2); // padding.
+      OS.EmitIntValue(0, 2); // 0 live-out registers.
+      OS.EmitIntValue(0, 2); // padding.
+      OS.EmitIntValue(0, 2); // 0 arch-specific values.
+      OS.EmitIntValue(0, 4); // padding.
+      continue;
+    }
+
+    OS.EmitIntValue(CSI.ID, 8);
+    OS.EmitIntValue(FunctionIndex, 4);
+    OS.EmitValue(CSI.CSOffsetExpr, 4);
+
+    // Reserved for flags.
+    OS.EmitIntValue(0, 2);
+    OS.EmitIntValue(CSLocs.size(), 2);
+
+    for (const auto &Loc : CSLocs) {
+      uint8_t TypeAndFlags =
+        TYPE_AND_FLAGS(Loc.Type, Loc.Ptr, Loc.Alloca,
+                       Loc.Duplicate, Loc.Temporary);
+      OS.EmitIntValue(TypeAndFlags, 1);
+      OS.EmitIntValue(Loc.Size, 1);
+      OS.EmitIntValue(Loc.Reg, 2);
+      OS.EmitIntValue(Loc.Offset, 4);
+      OS.EmitIntValue(Loc.AllocaSize, 4);
+    }
+
+    // Num live-out registers and padding to align to 4 byte.
+    OS.EmitIntValue(0, 2);
+    OS.EmitIntValue(LiveOuts.size(), 2);
+
+    for (const auto &LO : LiveOuts) {
+      OS.EmitIntValue(LO.DwarfRegNum, 2);
+      OS.EmitIntValue(0, 1);
+      OS.EmitIntValue(LO.Size, 1);
+    }
+
+    // Num arch-specific constants and padding to align to 4 bytes.
+    OS.EmitIntValue(0, 2);
+    OS.EmitIntValue(Values.size(), 2);
+
+    for (const auto &C : Values) {
+      const Location &Loc = C.first;
+      const Operation &Op = C.second;
+
+      uint8_t TypeAndFlags = ARCH_TYPE_AND_FLAGS(Loc.Type, Loc.Ptr);
+      OS.EmitIntValue(TypeAndFlags, 1);
+      OS.EmitIntValue(Loc.Size, 1);
+      OS.EmitIntValue(Loc.Reg, 2);
+      OS.EmitIntValue(Loc.Offset, 4);
+
+      uint8_t OpType = ARCH_OP_TYPE(Op.InstType,
+                                    Op.isGenerated,
+                                    Op.OperandType);
+      OS.EmitIntValue(OpType, 1);
+      OS.EmitIntValue(Op.Size, 1);
+      OS.EmitIntValue(Op.DwarfReg, 2);
+      if(Op.isSymbol) OS.EmitSymbolValue(Op.Symbol, 8);
+      else OS.EmitIntValue(Op.Constant, 8);
+    }
+
+    // Emit alignment to 8 byte.
+    OS.EmitValueToAlignment(8);
+  }
+}
+
 /// Serialize the stackmap data.
 void StackMaps::serializeToStackMapSection() {
   (void)WSMP;
@@ -578,3 +1329,38 @@ void StackMaps::serializeToStackMapSection() {
   CSInfos.clear();
   ConstPool.clear();
 }
+
+/// Serialize the popcorn stackmap data.
+void StackMaps::serializeToPcnStackMapSection(const UnwindInfo *UI) {
+  (void)WSMP;
+  // Bail out if there's no stack map data.
+  assert((!CSInfos.empty() || ConstPool.empty()) &&
+         "Expected empty constant pool too!");
+  assert((!CSInfos.empty() || FnInfos.empty()) &&
+         "Expected empty function record too!");
+  if (CSInfos.empty())
+    return;
+
+  MCContext &OutContext = AP.OutStreamer->getContext();
+  MCStreamer &OS = *AP.OutStreamer;
+
+  // Create the section.
+  MCSection *StackMapSection =
+      OutContext.getObjectFileInfo()->getPcnStackMapSection();
+  OS.SwitchSection(StackMapSection);
+
+  // Emit a dummy symbol to force section inclusion.
+  OS.EmitLabel(OutContext.getOrCreateSymbol(Twine("__LLVM_PcnStackMaps")));
+
+  // Serialize data.
+  LLVM_DEBUG(dbgs() << "********** Stack Map Output **********\n");
+  emitStackmapHeader(OS);
+  emitPcnFunctionFrameRecords(OS, UI);
+  emitConstantPoolEntries(OS);
+  emitPcnCallsiteEntries(OS);
+  OS.AddBlankLine();
+
+  // Clean up.
+  CSInfos.clear();
+  ConstPool.clear();
+}
diff --git a/llvm/lib/CodeGen/StackSlotColoring.cpp b/llvm/lib/CodeGen/StackSlotColoring.cpp
index 99b533e10b8..bc2379888ab 100644
--- a/llvm/lib/CodeGen/StackSlotColoring.cpp
+++ b/llvm/lib/CodeGen/StackSlotColoring.cpp
@@ -324,6 +324,7 @@ bool StackSlotColoring::ColorSlots(MachineFunction &MF) {
   SmallVector<int, 16> SlotMapping(NumObjs, -1);
   SmallVector<float, 16> SlotWeights(NumObjs, 0.0);
   SmallVector<SmallVector<int, 4>, 16> RevMap(NumObjs);
+  SmallDenseMap<int, int, 16> SlotChanges;
   BitVector UsedColors(NumObjs);
 
   LLVM_DEBUG(dbgs() << "Color spill slot intervals:\n");
@@ -338,7 +339,9 @@ bool StackSlotColoring::ColorSlots(MachineFunction &MF) {
     SlotWeights[NewSS] += li->weight;
     UsedColors.set(NewSS);
     Changed |= (SS != NewSS);
+    if(SS != NewSS) SlotChanges[SS] = NewSS;
   }
+  MF.updateSMStackSlotRefs(SlotChanges);
 
   LLVM_DEBUG(dbgs() << "\nSpill slots after coloring:\n");
   for (unsigned i = 0, e = SSIntervals.size(); i != e; ++i) {
diff --git a/llvm/lib/CodeGen/StackTransformMetadata.cpp b/llvm/lib/CodeGen/StackTransformMetadata.cpp
new file mode 100644
index 00000000000..15e49ca23f3
--- /dev/null
+++ b/llvm/lib/CodeGen/StackTransformMetadata.cpp
@@ -0,0 +1,1415 @@
+//=== llvm/CodeGen/StackTransformMetadata.cpp - Stack Transformation Metadata ===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file accumulates additional data from machine functions needed to do
+// correct and complete stack transformation.
+//
+// Note: the dataflow analysis in this implementation assumes the ISA does not
+// allow memory-to-memory copies.
+//
+//===----------------------------------------------------------------------===//
+
+#include <queue>
+#include "llvm/ADT/SmallSet.h"
+#include "llvm/CodeGen/LiveIntervals.h"
+#include "llvm/CodeGen/LiveStacks.h"
+#include "llvm/CodeGen/MachineFrameInfo.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/MachineInstrBuilder.h"
+#include "llvm/CodeGen/MachineMemOperand.h"
+#include "llvm/CodeGen/MachineRegisterInfo.h"
+#include "llvm/CodeGen/Passes.h"
+#include "llvm/CodeGen/PseudoSourceValue.h"
+#include "llvm/CodeGen/StackMaps.h"
+#include "llvm/CodeGen/StackTransformTypes.h"
+#include "llvm/CodeGen/TargetInstrInfo.h"
+#include "llvm/CodeGen/VirtRegMap.h"
+#include "llvm/IR/DiagnosticInfo.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/LLVMContext.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/Target/TargetValues.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "stacktransform"
+
+static cl::opt<bool>
+NoWarnings("no-sm-warn", cl::desc("Don't issue warnings about stackmaps"),
+           cl::init(false), cl::Hidden);
+
+//===----------------------------------------------------------------------===//
+//                          StackTransformMetadata
+//===----------------------------------------------------------------------===//
+//
+// Run analyses over machine functions (before virtual register rewriting) to
+// glean additional information about live values.  This analysis finds
+// duplicate locations for live values (including backing stack slots and other
+// registers) and architecture-specific live values that must be materialized.
+//
+//===----------------------------------------------------------------------===//
+namespace {
+class StackTransformMetadata : public MachineFunctionPass {
+
+  /* Types */
+
+  /// A bundle tying together a stackmap IR instruction, the generated stackmap
+  /// machine instruction and the call machine instruction that caused the
+  /// stackmap to be emitted in the IR, respectively
+  typedef std::tuple<const CallInst *,
+                     MachineInstr *,
+                     const MachineInstr *> SMInstBundle;
+
+  /// Getters for individual elements of instruction bundles
+  static inline const
+  CallInst *getIRSM(const SMInstBundle &B) { return std::get<0>(B); }
+  static inline
+  MachineInstr *getMISM(const SMInstBundle &B) { return std::get<1>(B); }
+  static inline const
+  MachineInstr *getMICall(const SMInstBundle &B) { return std::get<2>(B); }
+
+  /// A vector of IR values.  Used when mapping from registers/stack slots to
+  /// IR values.
+  typedef SmallVector<const Value *, 4> ValueVec;
+  typedef std::shared_ptr<ValueVec> ValueVecPtr;
+
+  /// Mapping between virtual registers and IR operands
+  typedef std::pair<unsigned, ValueVecPtr> RegValsPair;
+  typedef std::map<unsigned, ValueVecPtr> RegValsMap;
+
+  /// Mapping between stackmaps and virtual registers referenced by the stackmap
+  typedef std::pair<const MachineInstr *, RegValsMap> SMRegPair;
+  typedef std::map<const MachineInstr *, RegValsMap> SMRegMap;
+
+  /// Mapping between stack slots and IR operands
+  typedef std::pair<int, ValueVecPtr> StackValsPair;
+  typedef std::map<int, ValueVecPtr> StackValsMap;
+
+  /// Mapping between stackmaps and stack slots referenced by the stackmap
+  typedef std::pair<const MachineInstr *, StackValsMap> SMStackSlotPair;
+  typedef std::map<const MachineInstr *, StackValsMap> SMStackSlotMap;
+
+  /// A value's spill location
+  class CopyLoc {
+  public:
+    enum Type { NONE, VREG, STACK_LOAD, STACK_STORE };
+    unsigned Vreg;
+    const MachineInstr *Instr;
+    CopyLoc() : Vreg(VirtRegMap::NO_PHYS_REG), Instr(nullptr) {}
+    CopyLoc(unsigned Vreg, const MachineInstr *Instr) :
+      Vreg(Vreg), Instr(Instr) {}
+    virtual CopyLoc *copy() const = 0;
+    virtual ~CopyLoc() {}
+    virtual Type getType() const = 0;
+  };
+  typedef std::shared_ptr<CopyLoc> CopyLocPtr;
+
+  /// A spill to a stack slot
+  class StackCopyLoc : public CopyLoc {
+  public:
+    int StackSlot;
+    StackCopyLoc() : StackSlot(VirtRegMap::NO_STACK_SLOT) {}
+    StackCopyLoc(unsigned Vreg, int StackSlot, const MachineInstr *Instr) :
+      CopyLoc(Vreg, Instr), StackSlot(StackSlot) {}
+    virtual CopyLoc *copy() const = 0;
+    virtual Type getType() const = 0;
+  };
+
+  /// A load from a stack slot
+  class StackLoadLoc : public StackCopyLoc {
+  public:
+    StackLoadLoc() {}
+    StackLoadLoc(unsigned Vreg, int StackSlot, const MachineInstr *Instr) :
+      StackCopyLoc(Vreg, StackSlot, Instr) {}
+    virtual CopyLoc *copy() const
+    { return new StackLoadLoc(Vreg, StackSlot, Instr); }
+    virtual Type getType() const { return CopyLoc::STACK_LOAD; }
+  };
+
+  /// A store to a stack slot
+  class StackStoreLoc : public StackCopyLoc {
+  public:
+    StackStoreLoc() {}
+    StackStoreLoc(unsigned Vreg, int StackSlot, const MachineInstr *Instr) :
+      StackCopyLoc(Vreg, StackSlot, Instr) {}
+    virtual CopyLoc *copy() const
+    { return new StackStoreLoc(Vreg, StackSlot, Instr); }
+    virtual Type getType() const { return CopyLoc::STACK_STORE; }
+  };
+
+  /// A spill to another register
+  class RegCopyLoc : public CopyLoc {
+  public:
+    unsigned SrcVreg;
+    RegCopyLoc() : SrcVreg(VirtRegMap::NO_PHYS_REG) {}
+    RegCopyLoc(unsigned DefVreg, unsigned SrcVreg, const MachineInstr *Instr) :
+      CopyLoc(DefVreg, Instr), SrcVreg(SrcVreg) {}
+    virtual CopyLoc *copy() const
+    { return new RegCopyLoc(Vreg, SrcVreg, Instr); }
+    virtual Type getType() const { return CopyLoc::VREG; }
+  };
+
+  /// Mapping between stack slots and copy locations (e.g., load from or store
+  /// to the stack slot)
+  typedef SmallVector<CopyLocPtr, 8> CopyLocVec;
+  typedef std::shared_ptr<CopyLocVec> CopyLocVecPtr;
+  typedef std::pair<int, CopyLocVecPtr> StackSlotCopyPair;
+  typedef std::map<int, CopyLocVecPtr> StackSlotCopies;
+
+  /// A work item to analyze in dataflow analysis.  Can selectively enable
+  /// traversing definitions.
+  struct WorkItem {
+    WorkItem() : Vreg(0), TraverseDefs(false) {}
+    WorkItem(unsigned Vreg, bool TraverseDefs)
+      : Vreg(Vreg), TraverseDefs(TraverseDefs) {}
+
+    unsigned Vreg;
+    bool TraverseDefs;
+  };
+
+  /* Data */
+
+  /// LLVM-provided analysis & metadata
+  MachineFunction *MF;
+  const MachineFrameInfo *MFI;
+  const MachineRegisterInfo *MRI;
+  const TargetInstrInfo *TII;
+  const TargetRegisterInfo *TRI;
+  const TargetValues *TVG;
+  LiveIntervals *LI;
+  const LiveStacks *LS;
+  const SlotIndexes *Indexes;
+  const VirtRegMap *VRM;
+
+  /// Stackmap/call instructions, mapping of virtual registers & stack slots to
+  /// IR values, stack slots used in the function, list of instructions that
+  /// copy to/from the stack
+  SmallVector<SMInstBundle, 32> SM;
+  SMRegMap SMRegs;
+  SMStackSlotMap SMStackSlots;
+  SmallSet<int, 32> UsedSS;
+  StackSlotCopies SSCopies;
+
+  /* Functions */
+
+  // Reset the analysis for a new function
+  void reset() {
+    SM.clear();
+    SMRegs.clear();
+    SMStackSlots.clear();
+    UsedSS.clear();
+    SSCopies.clear();
+  }
+
+  /// Print information about a virtual register and it's associated IR value
+  void dumpReg(unsigned Reg, const Value *IRVal) const;
+
+  /// Print information about a stack slot and it's associated IR value
+  void dumpStackSlot(int SS, const Value *IRVal) const;
+
+  /// Analyze a machine instruction to see if a value is getting copied from
+  /// another location such as a stack slot or register.
+  CopyLocPtr getCopyLocation(const MachineInstr *MI) const;
+
+  /// Gather stackmap machine instructions, the IR instructions which generated
+  /// the stackmaps, and their associated call machine instructions.  Also,
+  /// find copies to/from stack slots (since there's no other mechanism to
+  /// find/traverse them).
+  void findStackmapsAndStackSlotCopies();
+
+  /// Find all virtual register/stack slot operands in a stackmap and collect
+  /// virtual register/stack slot <-> IR value mappings
+  void mapOpsToIR(const CallInst *IRSM, const MachineInstr *MISM);
+
+  /// Extend the live range for a register to include an instruction.
+  void updateRegisterLiveInterval(MachineOperand &Src,
+                                  const MachineInstr *Inst);
+
+  /// Rather than modifying the backend machinery to prevent hoisting code
+  /// between the stackmap and call site, unwind instructions in order to get
+  /// real live value locations at the function call.
+  bool unwindToCallSite(MachineInstr *SM, const MachineInstr *Call);
+
+  /// Is a virtual register live across the machine instruction?
+  /// Note: returns false if the MI is the last instruction for which the
+  /// virtual register is alive
+  bool isVregLiveAcrossInstr(unsigned Vreg, const MachineInstr *MI) const;
+
+  /// Is a stack slot live across the machine instruction?
+  /// Note: returns false if the MI is the last instruction for which the stack
+  /// slot is alive
+  bool isSSLiveAcrossInstr(int SS, const MachineInstr *MI) const;
+
+  /// Add duplicate location information for a virtual register.  Return true
+  /// if metadata was added, or false if the virtual register is not live
+  /// across the call instruction/stackmap.
+  bool addVregMetadata(unsigned Vreg,
+                       ValueVecPtr IRVals,
+                       const SMInstBundle &SM);
+
+  /// Add duplicate location information for a stack slot.  Return true if
+  /// metadata was added, or false if the stack slot is not live across the
+  /// call instruction/stackmap.
+  bool addSSMetadata(int SS, ValueVecPtr IRVals, const SMInstBundle &SM);
+
+  /// Search stack slot copies for additional virtual registers which are live
+  /// across the stackmap.  Will check to see if the copy instructions have
+  /// already been visited, and if appropriate, will add virtual registers to
+  /// work queue.
+  void inline
+  searchStackSlotCopies(int SS,
+                        ValueVecPtr IRVals,
+                        const SMInstBundle &SM,
+                        SmallPtrSet<const MachineInstr *, 32> &Visited,
+                        std::queue<WorkItem> &work,
+                        bool TraverseDefs);
+
+  /// Find all alternate locations for virtual registers in a stackmap, and add
+  /// them to the metadata to be generated.
+  void findAlternateVregLocs(const SMInstBundle &SM);
+
+  /// Find stackmap operands that have been spilled to alternate locations
+  bool findAlternateOpLocs();
+
+  /// Ensure virtual registers used to generate architecture-specific values
+  /// are handled by the stackmap & convert to physical registers
+  void sanitizeVregs(MachineLiveValPtr &LV, const MachineInstr *SM) const;
+
+  /// Find architecture-specific live values added by the backend
+  void findArchSpecificLiveVals();
+
+  /// Warn about unhandled registers & stack slots
+  void warnUnhandled() const;
+
+public:
+  static char ID;
+  static const std::string SMName;
+
+  StackTransformMetadata() : MachineFunctionPass(ID) {}
+
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const override;
+
+  virtual bool runOnMachineFunction(MachineFunction&) override;
+
+};
+
+} // end anonymous namespace
+
+char &llvm::StackTransformMetadataID = StackTransformMetadata::ID;
+const std::string StackTransformMetadata::SMName("llvm.experimental.pcn.stackmap");
+
+INITIALIZE_PASS_BEGIN(StackTransformMetadata, "stacktransformmetadata",
+  "Gather stack transformation metadata", false, false)
+INITIALIZE_PASS_DEPENDENCY(SlotIndexes)
+INITIALIZE_PASS_DEPENDENCY(LiveIntervals)
+INITIALIZE_PASS_DEPENDENCY(LiveStacks)
+INITIALIZE_PASS_DEPENDENCY(VirtRegMap)
+INITIALIZE_PASS_END(StackTransformMetadata, "stacktransformmetadata",
+  "Gather stack transformation metadata", false, false)
+
+char StackTransformMetadata::ID = 0;
+
+void StackTransformMetadata::getAnalysisUsage(AnalysisUsage &AU) const {
+  AU.setPreservesAll();
+  AU.addRequired<LiveIntervals>();
+  AU.addRequired<LiveStacks>();
+  AU.addRequired<SlotIndexes>();
+  AU.addRequired<VirtRegMap>();
+  MachineFunctionPass::getAnalysisUsage(AU);
+}
+
+bool StackTransformMetadata::runOnMachineFunction(MachineFunction &fn) {
+  bool Changed = false;
+
+  if(fn.getFrameInfo().hasPcnStackMap()) {
+    MF = &fn;
+    MFI = &MF->getFrameInfo();
+    MRI = &MF->getRegInfo();
+    TII = MF->getSubtarget().getInstrInfo();
+    TRI = MF->getSubtarget().getRegisterInfo();
+    TVG = MF->getSubtarget().getValues();
+    Indexes = &getAnalysis<SlotIndexes>();
+    LI = &getAnalysis<LiveIntervals>();
+    LS = &getAnalysis<LiveStacks>();
+    VRM = &getAnalysis<VirtRegMap>();
+    reset();
+
+    LLVM_DEBUG(
+      dbgs() << "\n********** STACK TRANSFORMATION METADATA **********\n"
+             << "********** Function: " << MF->getName() << "\n";
+      VRM->dump();
+    );
+
+    findStackmapsAndStackSlotCopies();
+    Changed = findAlternateOpLocs();
+    findArchSpecificLiveVals();
+    if(!NoWarnings) warnUnhandled();
+  }
+
+  return Changed;
+}
+
+/// Print information about a virtual register and it's associated IR value
+void StackTransformMetadata::dumpReg(unsigned Reg, const Value *IRVal) const {
+  if(IRVal) IRVal->printAsOperand(dbgs());
+  if(TargetRegisterInfo::isPhysicalRegister(Reg))
+    dbgs() << ": in register " << printReg(Reg, TRI);
+  else {
+    assert(VRM->hasPhys(Reg) && "Invalid virtual register");
+    unsigned Phys = VRM->getPhys(Reg);
+    dbgs() << ": in register " << printReg(Phys, TRI)
+           << " (vreg " << TargetRegisterInfo::virtReg2Index(Reg) << ")";
+  }
+  dbgs() << "\n";
+}
+
+/// Print information about a stack slot and it's associated IR value
+void StackTransformMetadata::dumpStackSlot(int SS, const Value *IRVal) const {
+  assert(!MFI->isDeadObjectIndex(SS) && "Invalid stack slot");
+  if(IRVal) IRVal->printAsOperand(dbgs());
+  dbgs() << ": in stack slot " << SS << " (size: " << MFI->getObjectSize(SS)
+         << ")\n";
+}
+
+/// Analyze a machine instruction to see if a value is getting copied from
+/// another location such as a stack slot or register.
+StackTransformMetadata::CopyLocPtr
+StackTransformMetadata::getCopyLocation(const MachineInstr *MI) const {
+  unsigned SrcVreg = 0;
+  unsigned DefVreg = 0;
+  int SS;
+
+  assert(MI && "Invalid machine instruction");
+
+  // Is it a copy from another register?
+  if(MI->isCopyLike()) {
+    for(unsigned i = 0, e = MI->getNumOperands(); i != e; i++) {
+      const MachineOperand &MO = MI->getOperand(i);
+      if(MO.isReg()) {
+        if(MO.isDef()) DefVreg = MO.getReg();
+        else SrcVreg = MO.getReg();
+      }
+    }
+
+    // TODO does it have to be a virtual register or can it be a physical one?
+    // Liveness analysis seems to apply only to virtual registers.
+    if(TargetRegisterInfo::isVirtualRegister(SrcVreg) &&
+       TargetRegisterInfo::isVirtualRegister(DefVreg))
+      return CopyLocPtr(new RegCopyLoc(DefVreg, SrcVreg, MI));
+  }
+
+  // Is it a load from the stack?
+  if((DefVreg = TII->isLoadFromStackSlot(*MI, SS)) &&
+     TargetRegisterInfo::isVirtualRegister(DefVreg))
+    return CopyLocPtr(new StackLoadLoc(DefVreg, SS, MI));
+
+  // Is it a store to the stack?
+  if((SrcVreg = TII->isStoreToStackSlot(*MI, SS)) &&
+     TargetRegisterInfo::isVirtualRegister(SrcVreg))
+    return CopyLocPtr(new StackStoreLoc(SrcVreg, SS, MI));
+
+  // A non-copylike instruction
+  return CopyLocPtr(nullptr);
+}
+
+/// Gather stackmap machine instructions, the IR instructions which generated
+/// the stackmaps, and their associated call machine instructions.  Also,
+/// find copies to/from stack slots (since there's no other mechanism to
+/// find/traverse them).
+void StackTransformMetadata::findStackmapsAndStackSlotCopies() {
+  for(auto MBB = MF->begin(), MBBE = MF->end(); MBB != MBBE; MBB++) {
+    for(auto MI = MBB->instr_begin(), ME = MBB->instr_end(); MI != ME; MI++) {
+      if(MI->getOpcode() == TargetOpcode::PCN_STACKMAP) {
+        // Find the stackmap IR instruction
+        assert(MI->getOperand(0).isImm() && "Invalid stackmap ID");
+        int64_t ID = MI->getOperand(0).getImm();
+        const BasicBlock *BB = MI->getParent()->getBasicBlock();
+        const CallInst *IRSM = nullptr;
+        for(auto I = BB->begin(), IE = BB->end(); I != IE; I++)
+        {
+          const IntrinsicInst *II;
+          if((II = dyn_cast<IntrinsicInst>(&*I)) &&
+             II->getCalledFunction()->getName() == SMName &&
+             cast<ConstantInt>(II->getArgOperand(0))->getSExtValue() == ID) {
+            IRSM = cast<CallInst>(II);
+            break;
+          }
+        }
+        assert(IRSM && "Could not find stackmap IR instruction");
+
+        // Find the call instruction
+        const MachineInstr *MCI = MI->getPrevNode();
+        while(MCI != nullptr) {
+          if(MCI->isCall()) {
+            if(MCI->getOpcode() == TargetOpcode::PCN_STACKMAP)
+              MCI = nullptr;
+            break;
+          }
+          MCI = MCI->getPrevNode();
+        }
+
+        if(!MCI) {
+          LLVM_DEBUG(dbgs() << "NOTE: stackmap " << ID << " ";
+                IRSM->print(dbgs());
+                dbgs() << ": could not find associated call instruction "
+                          "(lowered to a native instruction?)\n");
+          continue;
+        }
+
+        SM.push_back(SMInstBundle(IRSM, &*MI, MCI));
+      }
+      else {
+        // Record all stack slots that are actually used.  Note that this is
+        // necessary because analysis maintained in MachineFrameInfo/LiveStacks
+        // may denote stack slots as live even though register allocation
+        // actually all references to them.
+        const PseudoSourceValue *PSV;
+        const FixedStackPseudoSourceValue *FI;
+        for(auto MemOp : MI->memoperands()) {
+          PSV = MemOp->getPseudoValue();
+          if(PSV && PSV->kind() == CallEntryPseudoSourceValue::FixedStack) {
+            FI = cast<FixedStackPseudoSourceValue>(PSV);
+            UsedSS.insert(FI->getFrameIndex());
+          }
+        }
+
+        // See if instruction copies to/from stack slot
+        StackSlotCopies::iterator it;
+        CopyLocPtr loc;
+        if(!(loc = getCopyLocation(&*MI))) continue;
+        enum CopyLoc::Type type = loc->getType();
+        if(type == CopyLoc::STACK_LOAD || type == CopyLoc::STACK_STORE) {
+          StackCopyLoc *SCL = (StackCopyLoc *)loc.get();
+          if((it = SSCopies.find(SCL->StackSlot)) == SSCopies.end())
+            it = SSCopies.emplace(SCL->StackSlot,
+                                  CopyLocVecPtr(new CopyLocVec)).first;
+          it->second->push_back(loc);
+        }
+      }
+    }
+  }
+
+  LLVM_DEBUG(
+    dbgs() << "\n*** Stack slot copies ***\n\n";
+    for(auto SC = SSCopies.begin(), SCe = SSCopies.end(); SC != SCe; SC++) {
+      dbgs() << "Stack slot " << SC->first << ":\n";
+      for(size_t i = 0, e = SC->second->size(); i < e; i++) {
+        (*SC->second)[i]->Instr->dump();
+      }
+    }
+  );
+}
+
+/// Find all virtual register/stack slot operands in a stackmap and collect
+/// virtual register/stack slot <-> IR value mappings
+void StackTransformMetadata::mapOpsToIR(const CallInst *IRSM,
+                                        const MachineInstr *MISM) {
+  RegValsMap::iterator RegIt;
+  StackValsMap::iterator SSIt;
+  MachineInstr::const_mop_iterator MOit;
+  int64_t SMID = cast<ConstantInt>(IRSM->getArgOperand(0))->getSExtValue();
+  unsigned NumMO;
+
+  // Initialize new storage location/IR map objects (i.e., for virtual
+  // registers & stack slots) for the stackmap
+  SMRegs.emplace(MISM, RegValsMap());
+  SMStackSlots.emplace(MISM, StackValsMap());
+
+  // Loop over all operands
+  MOit = std::next(MISM->operands_begin(), 2);
+  for(size_t i = 2; i < IRSM->getNumArgOperands(); i++) {
+    const Value *IRVal = IRSM->getArgOperand(i);
+    assert(IRVal && "Invalid stackmap IR operand");
+
+    // Legalization may have changed how many machine operands map to the IR
+    // value.  Loop over all relevant machine operands.
+    NumMO = MF->getNumLegalizedOps(SMID, MOit - MISM->operands_begin());
+    for(size_t j = 0; j < NumMO; j++) {
+      if(MOit->isImm()) { // Map IR values to stack slots
+        int FrameIdx = INT32_MAX;
+        switch(MOit->getImm()) {
+        case StackMaps::DirectMemRefOp:
+          MOit++;
+          assert(MOit->isFI() && "Invalid operand type");
+          FrameIdx = MOit->getIndex();
+          MOit = std::next(MOit, 2);
+          break;
+        case StackMaps::IndirectMemRefOp:
+          MOit = std::next(MOit, 2);
+          assert(MOit->isFI() && "Invalid operand type");
+          FrameIdx = MOit->getIndex();
+          MOit = std::next(MOit, 2);
+          break;
+        case StackMaps::ConstantOp: MOit = std::next(MOit, 2); continue;
+        default: llvm_unreachable("Unrecognized stackmap operand type"); break;
+        }
+
+        assert(MFI->getObjectIndexBegin() <= FrameIdx &&
+               FrameIdx <= MFI->getObjectIndexEnd() && "Invalid frame index");
+        assert(!MFI->isDeadObjectIndex(FrameIdx) && "Dead frame index");
+        LLVM_DEBUG(dumpStackSlot(FrameIdx, IRVal););
+
+        // Update the list of IR values mapped to the stack slot (multiple IR
+        // values may be mapped to a single stack slot).
+        SSIt = SMStackSlots[MISM].find(FrameIdx);
+        if(SSIt == SMStackSlots[MISM].end())
+          SSIt = SMStackSlots[MISM].emplace(FrameIdx,
+                                            ValueVecPtr(new ValueVec)).first;
+        SSIt->second->push_back(IRVal);
+      }
+      else if(MOit->isReg()) { // Map IR values to virtual registers
+        unsigned Reg = MOit->getReg();
+        MOit++;
+
+        LLVM_DEBUG(dumpReg(Reg, IRVal););
+
+        // Update the list of IR values mapped to the virtual register
+        // (multiple IR values may be mapped to a single virtual register).
+        RegIt = SMRegs[MISM].find(Reg);
+        if(RegIt == SMRegs[MISM].end())
+          RegIt = SMRegs[MISM].emplace(Reg, ValueVecPtr(new ValueVec)).first;
+        RegIt->second->push_back(IRVal);
+      } else {
+        llvm_unreachable("Unrecognized stackmap operand type.");
+      }
+    }
+  }
+}
+
+/// Extend the live range for a register to include an instruction.
+void
+StackTransformMetadata::updateRegisterLiveInterval(MachineOperand &Src,
+                                                   const MachineInstr *SM) {
+  typedef LiveInterval::Segment Segment;
+
+  assert(Src.isReg() && "Cannot update live range for non-register operand");
+
+  unsigned Vreg = Src.getReg();
+  bool hasRegUnit = false;
+  SlotIndex Slots[2] = {
+    Indexes->getInstructionIndex(*Src.getParent()).getRegSlot(),
+    Indexes->getInstructionIndex(*SM).getRegSlot()
+  };
+
+  // Find the segment ending at or containing the call instruction.  Note that
+  // we search using the insruction's base index, as the interval may end at
+  // the register index (and the end of the range is non-inclusive).
+  LiveInterval &Reg = LI->getInterval(Vreg);
+  LiveInterval::iterator Seg = Reg.find(Slots[0].getBaseIndex());
+  assert(Seg != Reg.end() && Seg->contains(Slots[0].getBaseIndex()) &&
+         "Invalid live interval");
+
+  if(Seg->end < Slots[1]) {
+    // Update the segment to include the stackmap
+    Seg = Reg.addSegment(Segment(Seg->start, Slots[1], Seg->valno));
+    LLVM_DEBUG(dbgs() << "    -> Updated register live interval: "; Seg->dump());
+
+    // We also need to update the physical register's register unit (RU) live
+    // range because LiveIntervals::addKillFlags() will use the RU's live range
+    // to avoid marking a physical register dead if two virtual registers
+    // (mapped to that physical register) have overlapping live ranges.
+    MCRegUnitIterator Outer(VRM->getPhys(Vreg), TRI);
+    for(MCRegUnitIterator Unit(Outer); Unit.isValid(); ++Unit) {
+      LiveRange &RURange = LI->getRegUnit(*Unit);
+      LiveRange::iterator RUS;
+
+      for(size_t i = 0; i < 2; i++, RUS = RURange.end()) {
+        RUS = RURange.find(Slots[i]);
+        if(RUS != RURange.end() && RUS->contains(Slots[i])) break;
+      }
+
+      if(RUS != RURange.end()) {
+        hasRegUnit = true;
+        Seg = RURange.addSegment(
+          Segment(RUS->start, Slots[1].getNextIndex(), RUS->valno));
+        LLVM_DEBUG(
+          dbgs() << "    -> Updated segment for register unit "
+                 << *Unit << ": ";
+          Seg->dump();
+        );
+        break;
+      }
+    }
+
+    // If we can't extend one of the current RU ranges, add a new range.
+    if(!hasRegUnit) {
+      LiveRange &RURange = LI->getRegUnit(*Outer);
+      VNInfo *Valno = RURange.getNextValue(Slots[0], LI->getVNInfoAllocator());
+      Seg = RURange.addSegment(
+        Segment(Slots[0], Slots[1].getNextIndex(), Valno));
+      LLVM_DEBUG(
+        dbgs() << "    -> Added segment for register unit "
+               << *Outer << ": ";
+        Seg->dump();
+      );
+    }
+  }
+}
+
+/// Rather than modifying the backend machinery to prevent hoisting code
+/// between the stackmap and call site, unwind instructions in order to get
+/// real live value locations at the function call.
+bool StackTransformMetadata::unwindToCallSite(MachineInstr *SM,
+                                              const MachineInstr *Call) {
+  bool Changed = false, Found;
+  MachineOperand *SrcOp;
+  MachineInstr *InB = SM;
+  RegValsMap::iterator VregIt, SrcIt;
+  StackValsMap::iterator SSIt;
+  CopyLocPtr C;
+  RegCopyLoc *RCL;
+  StackCopyLoc *SCL;
+  TemporaryValuePtr Tmp;
+  int lastOpNo;
+
+  // Note: anything named or related to "Src" refers to the source of the
+  // copy operation, i.e., the originating location for the value
+
+  LLVM_DEBUG(dbgs() << "\nUnwinding stackmap back to call site:\n\n");
+  while((InB = InB->getPrevNode()) && InB != Call) {
+    if((C = getCopyLocation(InB))) {
+      LLVM_DEBUG(dbgs() << "  + Copy instruction: "; InB->dump());
+
+      switch(C->getType()) {
+      default: LLVM_DEBUG(dbgs() << "    Unhandled copy type\n"); break;
+      case CopyLoc::VREG:
+        RCL = (RegCopyLoc *)C.get();
+        SrcOp = &InB->getOperand(InB->findRegisterUseOperandIdx(RCL->SrcVreg));
+
+        // Replace current vreg with source
+        Found = false;
+        for(size_t i = 2; i < SM->getNumOperands(); i++) {
+          MachineOperand &MO = SM->getOperand(i);
+          if(MO.isReg() && MO.getReg() == RCL->Vreg) {
+            MO.ChangeToRegister(RCL->SrcVreg, false, false, SrcOp->isKill(),
+                                SrcOp->isDead(), false, false);
+            InB->clearRegisterKills(RCL->SrcVreg, TRI);
+            InB->clearRegisterDeads(RCL->SrcVreg);
+            updateRegisterLiveInterval(*SrcOp, SM);
+            Found = true;
+          }
+        }
+
+        // Update operand -> IR mapping to source vreg
+        if(Found) {
+          assert(SMRegs[SM].count(RCL->Vreg) &&
+                 "Unhandled register operand in stackmap!");
+          VregIt = SMRegs[SM].find(RCL->Vreg);
+          SrcIt = SMRegs[SM].find(RCL->SrcVreg);
+          if(SrcIt != SMRegs[SM].end()) {
+            for(auto IRVal : *VregIt->second)
+              SrcIt->second->push_back(IRVal);
+          }
+          else SMRegs[SM].emplace(RCL->SrcVreg, VregIt->second);
+          SMRegs[SM].erase(RCL->Vreg);
+          Changed = true;
+        }
+
+        break;
+      case CopyLoc::STACK_LOAD:
+        SCL = (StackCopyLoc *)C.get();
+
+        // Replace current vreg with stack slot.
+        // Note: stack slots don't have liveness information to fix up
+        Found = false;
+
+	// The last stack map operand for a RISCV MachineInstr may not be an
+	// "implicit-def dead early-clobber" register.  Preserve the last
+	// register operand if necessary.
+	lastOpNo = SM->getNumOperands() - 1;
+	if (lastOpNo > 0 && SM->getOperand(lastOpNo).isReg()) {
+	  MachineOperand &MOLast = SM->getOperand(lastOpNo);
+	  lastOpNo = MOLast.isEarlyClobber() ? 1 : 0;
+	} else
+	  lastOpNo = 0;
+
+        for(size_t i = 2; i < SM->getNumOperands(); i++) {
+          MachineOperand &MO = SM->getOperand(i);
+          if(MO.isReg() && MO.getReg() == SCL->Vreg) {
+            // There's not a great way to add new operands, so trash all
+            // trailing operands up to and including the Vreg, add the spill
+            // slot, and finally add the trailing operands back.
+            SmallVector<MachineOperand, 4> TrailOps(std::next(&MO),
+                                                    SM->operands_end());
+            while(SM->getNumOperands() > (i + lastOpNo))
+	      SM->RemoveOperand(i);
+
+            MachineInstrBuilder Worker(*MF, SM);
+            Worker.addImm(StackMaps::IndirectMemRefOp);
+            Worker.addImm(MFI->getObjectSize(SCL->StackSlot));
+            Worker.addFrameIndex(SCL->StackSlot);
+            Worker.addImm(0);
+	    for(auto Trailing : TrailOps) Worker.add(Trailing);
+            Found = true;
+          }
+        }
+
+        // Update operand -> IR mapping to source stack slot
+        if(Found) {
+          assert(SMRegs[SM].count(SCL->Vreg) &&
+                 "Unhandled register operand in stackmap!");
+          SSIt = SMStackSlots[SM].find(SCL->StackSlot);
+          VregIt = SMRegs[SM].find(SCL->Vreg);
+          if(SSIt != SMStackSlots[SM].end()) {
+            for(auto IRVal : *VregIt->second)
+              SSIt->second->push_back(IRVal);
+          }
+          else SMStackSlots[SM].emplace(SCL->StackSlot, VregIt->second);
+          SMRegs[SM].erase(SCL->Vreg);
+          Changed = true;
+        }
+
+        break;
+      case CopyLoc::STACK_STORE:
+        SCL = (StackCopyLoc *)C.get();
+        SrcOp = &InB->getOperand(InB->findRegisterUseOperandIdx(SCL->Vreg));
+
+        // Replace current stack slot with vreg
+        // Note: this *must* be an indirect memory reference (spill slot)
+        // since we're copying to a register!
+        Found = false;
+        for(size_t i = 2; i < SM->getNumOperands(); i++) {
+          MachineOperand &MO = SM->getOperand(i);
+          if(MO.isFI() && MO.getIndex() == SCL->StackSlot) {
+            // TODO if the sibling register is killed/dead in the intervening
+            // instruction we probably need to propagate that to the stackmap
+            // and remove it from the other instruction.
+            unsigned StartIdx = i - 2;
+            SM->getOperand(StartIdx).ChangeToRegister(SCL->Vreg, false);
+            SM->RemoveOperand(StartIdx + 1); // Size
+            SM->RemoveOperand(StartIdx + 1); // Frame index
+            SM->RemoveOperand(StartIdx + 1); // Frame pointer offset
+            Found = true;
+          }
+        }
+
+        // Update operand -> IR mapping to source vreg
+        if(Found) {
+          assert(SMStackSlots[SM].count(SCL->StackSlot) &&
+                 "Unhandled stack slot operand in stackmap!");
+
+          // Update liveness information to include the stackmap
+          InB->clearRegisterKills(SCL->Vreg, TRI);
+          InB->clearRegisterDeads(SCL->Vreg);
+          updateRegisterLiveInterval(*SrcOp, SM);
+
+          VregIt = SMRegs[SM].find(SCL->Vreg);
+          SSIt = SMStackSlots[SM].find(SCL->StackSlot);
+          if(VregIt != SMRegs[SM].end()) {
+            for(auto IRVal : *SSIt->second)
+              VregIt->second->push_back(IRVal);
+          }
+          else SMRegs[SM].emplace(SCL->Vreg, SSIt->second);
+          SMStackSlots[SM].erase(SCL->StackSlot);
+          Changed = true;
+        }
+
+        break;
+      }
+    }
+    else if((Tmp = TVG->getTemporaryValue(InB, VRM))) {
+      LLVM_DEBUG(dbgs() << "  - Temporary for stackmap: "; InB->dump());
+      assert(Tmp->Type == TemporaryValue::StackSlotRef &&
+             "Unhandled temporary value");
+
+      // The last stack map operand for a RISCV MachineInstr may not be an
+      // "implicit-def dead early-clobber" register.  Preserve the last
+      // register operand if necessary.
+      lastOpNo = SM->getNumOperands() - 1;
+      if (lastOpNo > 0 && SM->getOperand(lastOpNo).isReg()) {
+	MachineOperand &MOLast = SM->getOperand(lastOpNo);
+	lastOpNo = MOLast.isEarlyClobber() ? 1 : 0;
+      } else
+	lastOpNo = 0;
+
+      // Replace current vreg with stack slot reference.
+      // Note: stack slots don't have liveness information to fix up
+      Found = false;
+      for(size_t i = 2; i < SM->getNumOperands(); i++) {
+        MachineOperand &MO = SM->getOperand(i);
+        if(MO.isReg() && MO.getReg() == Tmp->Vreg) {
+          // There's not a great way to add new operands, so trash all trailing
+          // operands up to and including the Vreg, add the metadata, and
+          // finally add the trailing operands back.
+          SmallVector<MachineOperand, 4> TrailOps(std::next(&MO),
+                                                  SM->operands_end());
+          while(SM->getNumOperands() > (i + lastOpNo)) SM->RemoveOperand(i);
+          MachineInstrBuilder Worker(*MF, SM);
+          Worker.addImm(StackMaps::TemporaryOp);
+          Worker.addImm(Tmp->Size);
+          Worker.addImm(Tmp->Offset);
+          Worker.addImm(StackMaps::DirectMemRefOp);
+          Worker.addFrameIndex(Tmp->StackSlot);
+          Worker.addImm(0);
+          for(auto Trailing : TrailOps) Worker.add(Trailing);
+          Found = true;
+        }
+      }
+
+      // Update operand -> IR mapping to source stack slot
+      if(Found) {
+        assert(SMRegs[SM].count(Tmp->Vreg) &&
+               "Unhandled register operand in stackmap!");
+        SSIt = SMStackSlots[SM].find(Tmp->StackSlot);
+        VregIt = SMRegs[SM].find(Tmp->Vreg);
+        if(SSIt != SMStackSlots[SM].end()) {
+          for(auto IRVal : *VregIt->second)
+            SSIt->second->push_back(IRVal);
+        }
+        else SMStackSlots[SM].emplace(Tmp->StackSlot, VregIt->second);
+        SMRegs[SM].erase(Tmp->Vreg);
+        Changed = true;
+      }
+    }
+    else LLVM_DEBUG(dbgs() << "  - Skipping "; InB->dump());
+  }
+
+  if(Changed) LLVM_DEBUG(dbgs() << "\n  Transformed stackmap: "; SM->dump());
+  return Changed;
+}
+
+/// Is a virtual register live across the machine instruction?
+/// Note: returns false if the MI is the last instruction for which the virtual
+/// register is alive
+bool
+StackTransformMetadata::isVregLiveAcrossInstr(unsigned Vreg,
+                                              const MachineInstr *MI) const {
+  assert(TRI->isVirtualRegister(Vreg) && "Invalid virtual register");
+
+  if(LI->hasInterval(Vreg)) {
+    const LiveInterval &TheLI = LI->getInterval(Vreg);
+    SlotIndex InstrIdx = Indexes->getInstructionIndex(*MI);
+    LiveInterval::const_iterator Seg = TheLI.find(InstrIdx);
+    if(Seg != TheLI.end() && Seg->contains(InstrIdx) &&
+       InstrIdx.getInstrDistance(Seg->end) != 0)
+      return true;
+  }
+  return false;
+}
+
+/// Is a stack slot live across the machine instruction?
+/// Note: returns false if the MI is the last instruction for which the stack
+/// slot is alive
+bool
+StackTransformMetadata::isSSLiveAcrossInstr(int SS,
+                                            const MachineInstr *MI) const {
+  if(LS->hasInterval(SS)) {
+    const LiveInterval &TheLI = LS->getInterval(SS);
+    SlotIndex InstrIdx = Indexes->getInstructionIndex(*MI);
+    LiveInterval::const_iterator Seg = TheLI.find(InstrIdx);
+    if(Seg != TheLI.end() && Seg->contains(InstrIdx) &&
+       InstrIdx.getInstrDistance(Seg->end) != 0)
+      return true;
+  }
+  return false;
+}
+
+/// Add duplicate location information for a virtual register.
+bool StackTransformMetadata::addVregMetadata(unsigned Vreg,
+                                             ValueVecPtr IRVals,
+                                             const SMInstBundle &SM) {
+  const CallInst *IRSM = getIRSM(SM);
+  const MachineInstr *MICall = getMICall(SM);
+  RegValsMap &Vregs = SMRegs[getMISM(SM)];
+
+  assert(TargetRegisterInfo::isVirtualRegister(Vreg) && VRM->hasPhys(Vreg) &&
+         "Cannot add virtual register metadata -- invalid virtual register");
+
+  if(Vregs.find(Vreg) == Vregs.end() && isVregLiveAcrossInstr(Vreg, MICall))
+  {
+    unsigned Phys = VRM->getPhys(Vreg);
+    for(size_t sz = 0; sz < IRVals->size(); sz++) {
+      LLVM_DEBUG(dumpReg(Vreg, (*IRVals)[sz]););
+      MF->addSMOpLocation(IRSM, (*IRVals)[sz], MachineLiveReg(Phys));
+    }
+    Vregs[Vreg] = IRVals;
+    return true;
+  }
+  else return false;
+}
+
+/// Add duplicate location information for a stack slot.
+bool StackTransformMetadata::addSSMetadata(int SS,
+                                           ValueVecPtr IRVals,
+                                           const SMInstBundle &SM) {
+  const CallInst *IRSM = getIRSM(SM);
+  const MachineInstr *MICall = getMICall(SM);
+  StackValsMap &SSlots = SMStackSlots[getMISM(SM)];
+
+  assert(!MFI->isDeadObjectIndex(SS) &&
+         "Cannot add stack slot metadata -- invalid stack slot");
+
+  if(SSlots.find(SS) == SSlots.end() && isSSLiveAcrossInstr(SS, MICall))
+  {
+    for(size_t sz = 0; sz < IRVals->size(); sz++) {
+      LLVM_DEBUG(dumpStackSlot(SS, (*IRVals)[sz]););
+      MF->addSMOpLocation(IRSM, (*IRVals)[sz], MachineLiveStackSlot(SS));
+    }
+    SSlots[SS] = IRVals;
+    return true;
+  }
+  else return false;
+}
+
+/// Search stack slot copies for additional virtual registers which are live
+/// across the stackmap.  Will check to see if the copy instructions have
+/// already been visited, and if appropriate, will add virtual registers to
+/// work queue.
+void inline
+StackTransformMetadata::searchStackSlotCopies(int SS,
+                                 ValueVecPtr IRVals,
+                                 const SMInstBundle &SM,
+                                 SmallPtrSet<const MachineInstr *, 32> &Visited,
+                                 std::queue<WorkItem> &work,
+                                 bool TraverseDefs) {
+  StackSlotCopies::const_iterator Copies;
+  CopyLocVecPtr CL;
+  CopyLocVec::const_iterator Copy, CE;
+
+  if((Copies = SSCopies.find(SS)) != SSCopies.end()) {
+    CL = Copies->second;
+    for(Copy = CL->begin(), CE = CL->end(); Copy != CE; Copy++) {
+      unsigned Vreg = (*Copy)->Vreg;
+      const MachineInstr *Instr = (*Copy)->Instr;
+
+      if(!Visited.count(Instr)) {
+        addVregMetadata(Vreg, IRVals, SM);
+        Visited.insert(Instr);
+        work.emplace(Vreg, TraverseDefs);
+      }
+    }
+  }
+}
+
+/// Find all alternate locations for virtual registers in a stackmap, and add
+/// them to the metadata to be generated.
+void
+StackTransformMetadata::findAlternateVregLocs(const SMInstBundle &SM) {
+  RegValsMap &Regs = SMRegs[getMISM(SM)];
+  std::queue<WorkItem> work;
+  SmallPtrSet<const MachineInstr *, 32> Visited;
+  StackCopyLoc *SCL;
+  RegCopyLoc *RCL;
+
+  LLVM_DEBUG(dbgs() << "\nDuplicate operand locations:\n\n";);
+
+  // Iterate over all vregs in the stackmap
+  for(RegValsMap::iterator it = Regs.begin(), end = Regs.end();
+      it != end; it++) {
+    unsigned origVreg = it->first;
+    ValueVecPtr IRVals = it->second;
+    Visited.clear();
+
+    // Follow data flow to search for all duplicate locations, including stack
+    // slots and other registers.  It's a duplicate if the following are true:
+    //
+    //   1. It's a copy-like instruction, e.g., a register move or a load
+    //      from/store to stack slot
+    //   2. The alternate location (virtual register/stack slot) is live across
+    //      the machine call instruction
+    //
+    // Note: we *must* search exhaustively (i.e., across copies from registers
+    // that are *not* live across the call) because the following can happen:
+    //
+    //   STORE vreg0, <fi#0>
+    //   ...
+    //   COPY vreg0, vreg1
+    //   ...
+    //   STACKMAP 0, 0, vreg1
+    //
+    // Here, vreg0 is *not* live across the stackmap, but <fi#0> *is*
+    work.emplace(origVreg, true);
+    while(!work.empty()) {
+      WorkItem cur;
+      unsigned vreg;
+      int ss;
+
+      // Walk over definitions
+      cur = work.front();
+      work.pop();
+      if(cur.TraverseDefs) {
+        for(auto instr = MRI->def_instr_begin(cur.Vreg),
+                 ei = MRI->def_instr_end();
+            instr != ei; instr++) {
+
+          if(Visited.count(&*instr)) continue;
+          CopyLocPtr loc = getCopyLocation(&*instr);
+          if(!loc) continue;
+
+          switch(loc->getType()) {
+          case CopyLoc::VREG:
+            RCL = (RegCopyLoc *)loc.get();
+            vreg = RCL->SrcVreg;
+            addVregMetadata(vreg, IRVals, SM);
+            Visited.insert(&*instr);
+            work.emplace(vreg, true);
+            break;
+          case CopyLoc::STACK_LOAD:
+            SCL = (StackCopyLoc *)loc.get();
+            ss = SCL->StackSlot;
+            if(addSSMetadata(ss, IRVals, SM)) {
+              Visited.insert(&*instr);
+              searchStackSlotCopies(ss, IRVals, SM, Visited, work, true);
+            }
+            break;
+          default: llvm_unreachable("Unknown/invalid location type"); break;
+          }
+        }
+      }
+
+      // Walk over uses
+      for(auto instr = MRI->use_instr_begin(cur.Vreg),
+               ei = MRI->use_instr_end();
+          instr != ei; instr++) {
+
+        if(Visited.count(&*instr)) continue;
+        CopyLocPtr loc = getCopyLocation(&*instr);
+        if(!loc) continue;
+
+        // Note: in traversing uses of the given vreg, we *don't* want to
+        // traverse definitions of sibling vregs.  Because we're in pseudo-SSA,
+        // it's possible we could be defining a register in separate dataflow
+        // paths, e.g.:
+        //
+        // BB A:
+        //   %vreg3<def> = COPY %vreg1
+        //   JMP <BB C>
+        //
+        // BB B:
+        //   %vreg3<def> = COPY %vreg2
+        //   JMP <BB C>
+        //
+        // ...
+        //
+        // If we discovered block A through vreg 1, we don't want to explore
+        // through block B in which vreg 3 is defined with a different value.
+        switch(loc->getType()) {
+        case CopyLoc::VREG:
+          RCL = (RegCopyLoc *)loc.get();
+          vreg = RCL->Vreg;
+          addVregMetadata(vreg, IRVals, SM);
+          Visited.insert(&*instr);
+          work.emplace(vreg, false);
+          break;
+        case CopyLoc::STACK_STORE:
+          SCL = (StackCopyLoc *)loc.get();
+          ss = SCL->StackSlot;
+          if(addSSMetadata(ss, IRVals, SM)) {
+            Visited.insert(&*instr);
+            searchStackSlotCopies(ss, IRVals, SM, Visited, work, false);
+          }
+          break;
+        default: llvm_unreachable("Unknown/invalid location type"); break;
+        }
+      }
+    }
+  }
+}
+
+/// Find alternate storage locations for stackmap operands
+bool StackTransformMetadata::findAlternateOpLocs() {
+  bool Changed = false;
+  RegValsMap::iterator vregIt, vregEnd;
+
+  for(auto S = SM.begin(), SE = SM.end(); S != SE; S++) {
+    const CallInst *IRSM = getIRSM(*S);
+    const MachineInstr *MICall = getMICall(*S);
+    MachineInstr *MISM = getMISM(*S);
+
+    LLVM_DEBUG(
+      dbgs() << "\nStackmap " << MISM->getOperand(0).getImm() << ":\n";
+      MISM->dump();
+      dbgs() << "\n";
+    );
+
+    // Get all virtual register/stack slot operands & their associated IR
+    // values
+    mapOpsToIR(IRSM, MISM);
+
+    // Because the CodeGen machinery is wily (and may hoist instructions above
+    // the stackmap), unwind copies until the call site.
+    Changed |= unwindToCallSite(MISM, MICall);
+
+    // Find alternate locations for vregs in stack map.  Note we don't need to
+    // find alternate stack slot locations, as allocas *should* already be in
+    // the stackmap, so the remaining stack slots are spilled registers (which
+    // are covered here).
+    findAlternateVregLocs(*S);
+  }
+
+  return Changed;
+}
+
+/// Ensure virtual registers used to generate architecture-specific values are
+/// handled by the stackmap & convert to physical registers
+void StackTransformMetadata::sanitizeVregs(MachineLiveValPtr &LV,
+                                           const MachineInstr *SM) const {
+  if(!LV) return;
+  if(LV->isGenerated()) {
+    MachineGeneratedVal *MGV = (MachineGeneratedVal *)LV.get();
+    const ValueGenInstList &Inst = MGV->getInstructions();
+    for(size_t i = 0, num = Inst.size(); i < num; i++) {
+      if(Inst[i]->opType() == ValueGenInst::OpType::Register) {
+        RegInstructionBase *RI = (RegInstructionBase *)Inst[i].get();
+        if(!TRI->isVirtualRegister(RI->getReg())) {
+          if(RI->getReg() == TRI->getFrameRegister(*MF)) continue;
+          // TODO walk through stackmap and see if physical register in
+          // instruction is contained in stackmap
+          LV.reset(nullptr);
+          return;
+        }
+        else if(!SMRegs.at(SM).count(RI->getReg())) {
+          LLVM_DEBUG(dbgs() << "WARNING: vreg "
+                       << TargetRegisterInfo::virtReg2Index(RI->getReg())
+                       << " used to generate value not handled in stackmap\n");
+          LV.reset(nullptr);
+          return;
+        }
+        else {
+          assert(VRM->hasPhys(RI->getReg()) && "Invalid virtual register");
+          RI->setReg(VRM->getPhys(RI->getReg()));
+        }
+      }
+    }
+  }
+}
+
+/// Filter out register definitions we've previously seen.
+static void
+getUnseenDefinitions(MachineRegisterInfo::def_instr_iterator DefIt,
+                     const SmallPtrSet<const MachineInstr *, 4> &Seen,
+                     SmallPtrSet<const MachineInstr *, 4> &NewDefs) {
+  NewDefs.clear();
+  do { if(!Seen.count(&*DefIt)) NewDefs.insert(&*DefIt);
+  } while((++DefIt) != MachineRegisterInfo::def_instr_end());
+}
+
+/// Try to find the best defining instruction.
+static const MachineInstr *
+tryToBreakDefMITie(const MachineInstr *MICall,
+                   const SmallPtrSet<const MachineInstr *, 4> &Definitions) {
+  // First heuristic -- find closest preceding defining instruction in the same
+  // machine basic block.
+  const MachineInstr *Cur, *BestDef = nullptr;
+  unsigned Distance, Best = UINT32_MAX;
+  SmallVector<std::pair<const MachineInstr *, unsigned>, 4> SearchDefs;
+  for(auto Def : Definitions) {
+    Cur = MICall;
+    Distance = 1;
+    while((Cur = Cur->getPrevNode())) {
+      if(Cur == Def) {
+        SearchDefs.emplace_back(Def, Distance);
+        break;
+      }
+      Distance++;
+    }
+  }
+
+  for(auto Pair : SearchDefs) {
+    if(Pair.second < Best) {
+      BestDef = Pair.first;
+      Best = Pair.second;
+    }
+  }
+
+  if(BestDef)
+    LLVM_DEBUG(dbgs() << "Choosing defining instruction"; BestDef->dump());
+  return BestDef;
+}
+
+/// Find architecture-specific live values added by the backend
+void StackTransformMetadata::findArchSpecificLiveVals() {
+  LLVM_DEBUG(dbgs() << "\n*** Finding architecture-specific live values ***\n\n";);
+
+  for(auto S = SM.begin(), SE = SM.end(); S != SE; S++)
+  {
+    const MachineInstr *MISM = getMISM(*S);
+    const MachineInstr *MICall = getMICall(*S);
+    const CallInst *IRSM = getIRSM(*S);
+    RegValsMap &CurVregs = SMRegs[MISM];
+    StackValsMap &CurSS = SMStackSlots[MISM];
+
+    LLVM_DEBUG(
+      MISM->dump();
+      dbgs() << "  -> Call instruction SlotIndex ";
+      Indexes->getInstructionIndex(*MICall).print(dbgs());
+      dbgs() << ", searching vregs 0 -> " << MRI->getNumVirtRegs()
+             << " and stack slots " << MFI->getObjectIndexBegin() << " -> "
+             << MFI->getObjectIndexEnd() << "\n";
+    );
+
+    // Include any mandatory architecture-specific live values
+    TVG->addRequiredArchLiveValues(MF, MISM, IRSM);
+
+    // Search for virtual registers not handled by the stackmap.  Registers
+    // spilled to the stack should have been converted to frame index
+    // references by now.
+    for(unsigned i = 0, numVregs = MRI->getNumVirtRegs(); i < numVregs; i++) {
+      unsigned Vreg = TargetRegisterInfo::index2VirtReg(i);
+      MachineLiveValPtr MLV;
+      MachineLiveReg MLR(0);
+
+      if(VRM->hasPhys(Vreg) && isVregLiveAcrossInstr(Vreg, MICall) &&
+         CurVregs.find(Vreg) == CurVregs.end()) {
+        LLVM_DEBUG(dbgs() << "    + vreg" << i
+                     << " is live in register but not in stackmap\n";);
+
+        // Walk the use-def chain to see if we can find a valid value.  Note we
+        // keep track of seen definitions because even though we're supposed to
+        // be in SSA form it's possible to find definition cycles.
+        const MachineInstr *DefMI;
+        unsigned ChainVreg = Vreg;
+        SmallPtrSet<const MachineInstr *, 4> SeenDefs, NewDefs;
+        do {
+          getUnseenDefinitions(MRI->def_instr_begin(ChainVreg),
+                               SeenDefs, NewDefs);
+
+          // Try to find a suitable defining instruction
+          if(NewDefs.size() == 0) {
+            LLVM_DEBUG(dbgs() << "WARNING: no unseen definition\n");
+            break;
+          }
+          else if(NewDefs.size() == 1) DefMI = *NewDefs.begin();
+          else if(!(DefMI = tryToBreakDefMITie(MICall, NewDefs))) {
+            // No suitable defining instruction, not much we can do...
+            LLVM_DEBUG(
+              dbgs() << "WARNING: multiple definitions for virtual "
+                        "register, missed in live-value analysis?\n";
+              for(auto d = MRI->def_instr_begin(ChainVreg),
+                  e = MRI->def_instr_end(); d != e; d++)
+                d->dump();
+            );
+            break;
+          }
+
+          SeenDefs.insert(DefMI);
+          MLV = TVG->getMachineValue(DefMI);
+          sanitizeVregs(MLV, MISM);
+
+          if(MLV) break; // We got a value!
+          else {
+            // Couldn't get a value, follow the use-def chain
+            CopyLocPtr Copy = getCopyLocation(DefMI);
+            if(Copy) {
+              switch(Copy->getType()) {
+              default: ChainVreg = 0; break;
+              case CopyLoc::VREG:
+                ChainVreg = ((RegCopyLoc *)Copy.get())->SrcVreg;
+                break;
+              }
+            }
+            else ChainVreg = 0;
+          }
+        } while(TargetRegisterInfo::isVirtualRegister(ChainVreg));
+
+        if(MLV) {
+          LLVM_DEBUG(dbgs() << "      Defining instruction: ";
+                MLV->getDefiningInst()->print(dbgs());
+                dbgs() << "      Value: " << MLV->toString() << "\n");
+
+          MLR.setReg(VRM->getPhys(Vreg));
+          MF->addSMArchSpecificLocation(IRSM, MLR, *MLV);
+          CurVregs.emplace(Vreg, ValueVecPtr(nullptr));
+        }
+        else {
+          LLVM_DEBUG(
+            DefMI = &*MRI->def_instr_begin(Vreg);
+            StringRef BBName = DefMI->getParent()->getName();
+            dbgs() << "      Unhandled defining instruction in basic block "
+                   << BBName << ":";
+            DefMI->print(dbgs());
+          );
+        }
+      }
+    }
+
+    // Search for stack slots not handled by the stackmap
+    for(int SS = MFI->getObjectIndexBegin(), e = MFI->getObjectIndexEnd();
+        SS < e; SS++) {
+      if(UsedSS.count(SS) && !MFI->isDeadObjectIndex(SS) &&
+         isSSLiveAcrossInstr(SS, MICall) && CurSS.find(SS) == CurSS.end()) {
+        LLVM_DEBUG(dbgs() << "    + stack slot " << SS
+                     << " is live but not in stackmap\n";);
+        // TODO add arch-specific stack slot information to machine function
+      }
+    }
+
+    LLVM_DEBUG(dbgs() << "\n";);
+  }
+}
+
+/// Find IR call instruction which generated the stackmap
+static inline const CallInst *findCalledFunc(const llvm::CallInst *IRSM) {
+  const Instruction *Func = IRSM->getPrevNode();
+  while(Func && !isa<CallInst>(Func)) Func = Func->getPrevNode();
+  return dyn_cast<CallInst>(Func);
+}
+
+/// Display a warning about unhandled values
+static inline void displayWarning(std::string &Msg,
+                                  const CallInst *CI,
+                                  const Function *F) {
+  assert(CI && "Invalid arguments");
+
+  // Note: it may be possible for us to not have a called function, for example
+  // if we call a function using a function pointer
+  const Function *CurF = CI->getParent()->getParent();
+  const std::string &Triple = CurF->getParent()->getTargetTriple();
+  Msg = "(" + Triple + ") " + Msg;
+  if(F && F->hasName()) {
+    Msg += " across call to ";
+    Msg += F->getName();
+  }
+  DiagnosticInfoOptimizationFailure DI(*CurF, CI->getDebugLoc(), Msg);
+  CurF->getContext().diagnose(DI);
+}
+
+/// Warn about unhandled registers & stack slots
+void StackTransformMetadata::warnUnhandled() const {
+  std::string Msg;
+  const CallInst *IRCall;
+  const Function *CalledFunc;
+
+  for(auto S = SM.begin(), SE = SM.end(); S != SE; S++)
+  {
+    const MachineInstr *MISM = getMISM(*S);
+    const MachineInstr *MICall = getMICall(*S);
+    const RegValsMap &CurVregs = SMRegs.at(MISM);
+    const StackValsMap &CurSS = SMStackSlots.at(MISM);
+    IRCall = findCalledFunc(getIRSM(*S));
+    CalledFunc = IRCall->getCalledFunction();
+    assert(IRCall && "No call instruction for stackmap");
+
+    // Search for virtual registers not handled by the stackmap
+    for(unsigned i = 0; i < MRI->getNumVirtRegs(); i++) {
+      unsigned Vreg = TargetRegisterInfo::index2VirtReg(i);
+
+      // Virtual register allocated to physical register
+      if(VRM->hasPhys(Vreg) && isVregLiveAcrossInstr(Vreg, MICall) &&
+         CurVregs.find(Vreg) == CurVregs.end()) {
+        Msg = "Stack transformation: unhandled register ";
+        Msg += TRI->getName(VRM->getPhys(Vreg));
+        displayWarning(Msg, IRCall, CalledFunc);
+      }
+    }
+
+    // Search for all stack slots not handled by the stackmap
+    for(int SS = MFI->getObjectIndexBegin(), e = MFI->getObjectIndexEnd();
+        SS < e; SS++) {
+      if(UsedSS.count(SS) && !MFI->isDeadObjectIndex(SS) &&
+         isSSLiveAcrossInstr(SS, MICall) && CurSS.find(SS) == CurSS.end()) {
+        Msg = "Stack transformation: unhandled stack slot ";
+        Msg += std::to_string(SS);
+        displayWarning(Msg, IRCall, CalledFunc);
+      }
+    }
+  }
+}
diff --git a/llvm/lib/CodeGen/StackTransformTypes.cpp b/llvm/lib/CodeGen/StackTransformTypes.cpp
new file mode 100644
index 00000000000..d3e5d2ca3e9
--- /dev/null
+++ b/llvm/lib/CodeGen/StackTransformTypes.cpp
@@ -0,0 +1,303 @@
+//===-- llvm/Target/TargetValueGenerator.cpp - Value Generator --*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/CodeGen/AsmPrinter.h"
+#include "llvm/CodeGen/MachineFrameInfo.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/StackTransformTypes.h"
+#include "llvm/CodeGen/TargetFrameLowering.h"
+#include "llvm/CodeGen/TargetRegisterInfo.h"
+#include "llvm/CodeGen/TargetSubtargetInfo.h"
+#include "llvm/IR/Mangler.h"
+#include "llvm/MC/MCContext.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Target/TargetMachine.h"
+
+#define DEBUG_TYPE "stacktransform"
+
+using namespace llvm;
+
+//===----------------------------------------------------------------------===//
+// Common functions
+//
+
+static MCSymbol *GetExternalSymbol(AsmPrinter &AP, StringRef Sym) {
+  SmallString<60> Name;
+  Mangler::getNameWithPrefix(Name, Sym, AP.getDataLayout());
+  return AP.OutContext.lookupSymbol(Name);
+}
+
+//===----------------------------------------------------------------------===//
+// Types for generating more complex architecture-specific live values
+//
+
+const char *ValueGenInst::InstTypeStr[] = {
+#define X(type) #type ,
+  VALUE_GEN_INST
+#undef X
+};
+
+const char *ValueGenInst::getInstName(enum InstType Type) {
+  switch(Type) {
+#define X(type) case type: 
+    VALUE_GEN_INST
+#undef X
+    return InstTypeStr[Type];
+  };
+  return "unknown";
+}
+
+std::string ValueGenInst::getInstNameStr(enum InstType Type) {
+  return std::string(getInstName(Type));
+}
+
+std::string RefInstruction::str() const {
+  std::string buf = "reference to '";
+  switch(Symbol.getType()) {
+  case MachineOperand::MO_GlobalAddress:
+    buf += Symbol.getGlobal()->getName();
+    buf += "' (global)";
+    break;
+  case MachineOperand::MO_ExternalSymbol:
+    buf += Symbol.getSymbolName();
+    buf += "' (external)";
+    break;
+  case MachineOperand::MO_MCSymbol:
+    buf += Symbol.getMCSymbol()->getName();
+    buf += "' (MC symbol)";
+    break;
+  default:
+    LLVM_DEBUG(dbgs() << "Unhandled reference type: ";
+          Symbol.print(dbgs());
+          dbgs() << "\n";);
+    buf += "n/a' (unhandled type)";
+    break;
+  }
+  return buf;
+}
+
+MCSymbol *RefInstruction::getReference(AsmPrinter &AP) const {
+//  const TargetMachine &TM = AP.TM;
+//  Mangler &Mang = TM.getObjFileLowering()->getMangler();
+
+  switch(Symbol.getType()) {
+  case MachineOperand::MO_ExternalSymbol:
+    return GetExternalSymbol(AP, Symbol.getSymbolName());
+  case MachineOperand::MO_GlobalAddress:
+    return AP.TM.getSymbol(Symbol.getGlobal());
+  case MachineOperand::MO_MCSymbol:
+    return Symbol.getMCSymbol();
+  default:
+    LLVM_DEBUG(dbgs() << "Unhandled reference type: ";
+          Symbol.print(dbgs());
+          dbgs() << "\n";);
+    return nullptr;
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// MachineSymbolRef implementation
+//
+
+bool MachineSymbolRef::operator==(const MachineLiveVal &RHS) const {
+  if(RHS.isSymbolRef()) {
+    const MachineSymbolRef &MSR = (const MachineSymbolRef &)RHS;
+    if(&MSR.Symbol == &Symbol && MSR.Load == Load) return true;
+  }
+  return false;
+}
+
+std::string MachineSymbolRef::toString() const {
+  std::string buf;
+  if(Load) buf = "dereference symbol '";
+  else buf = "reference to symbol '";
+  switch(Symbol.getType()) {
+  case MachineOperand::MO_GlobalAddress:
+    buf += Symbol.getGlobal()->getName();
+    buf += "' (global)";
+    break;
+  case MachineOperand::MO_ExternalSymbol:
+    buf += Symbol.getSymbolName();
+    buf += "' (external)";
+    break;
+  case MachineOperand::MO_MCSymbol:
+    buf += Symbol.getMCSymbol()->getName();
+    buf += "' (MC symbol)";
+    break;
+  default:
+    LLVM_DEBUG(dbgs() << "Unhandled reference type: ";
+          Symbol.print(dbgs());
+          dbgs() << "\n";);
+    buf += "n/a' (unhandled type)";
+    break;
+  }
+  return buf;
+}
+
+MCSymbol *MachineSymbolRef::getReference(AsmPrinter &AP) const {
+  switch(Symbol.getType()) {
+  case MachineOperand::MO_ExternalSymbol:
+    return GetExternalSymbol(AP, Symbol.getSymbolName());
+  case MachineOperand::MO_GlobalAddress:
+    return AP.TM.getSymbol(Symbol.getGlobal());
+  case MachineOperand::MO_MCSymbol:
+    return Symbol.getMCSymbol();
+  default:
+    LLVM_DEBUG(dbgs() << "Unhandled reference type: ";
+          Symbol.print(dbgs());
+          dbgs() << "\n";);
+    return nullptr;
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// MachineConstPoolRef implementation
+//
+
+bool MachineConstPoolRef::operator==(const MachineLiveVal &RHS) const {
+  if(RHS.isConstPoolRef()) {
+    const MachineConstPoolRef &MCPR = (const MachineConstPoolRef &)RHS;
+    if(MCPR.Index == Index) return true;
+  }
+  return false;
+}
+
+MCSymbol *MachineConstPoolRef::getReference(AsmPrinter &AP) const {
+  MCSymbol *Sym = AP.GetCPISymbol(Index);
+  assert(Sym && "Could not get constant pool reference");
+  return Sym;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineStackObject implementation
+//
+
+bool MachineStackObject::operator==(const MachineLiveVal &RHS) const {
+  if(RHS.isStackObject()) {
+    const MachineStackObject &MSO = (const MachineStackObject &)RHS;
+    if(MSO.Index == Index) return true;
+  }
+  return false;
+}
+
+std::string MachineStackObject::toString() const {
+  std::string buf;
+  if(Load) buf = "load from ";
+  else buf = "reference to ";
+  return buf + "stack slot " + std::to_string(Index);
+}
+
+int
+MachineStackObject::getOffsetFromReg(AsmPrinter &AP, unsigned &BR) const {
+  const TargetFrameLowering *TFL = AP.MF->getSubtarget().getFrameLowering();
+  return TFL->getFrameIndexReference(*AP.MF, Index, BR);
+}
+
+//===----------------------------------------------------------------------===//
+// ReturnAddress implementation
+//
+
+int ReturnAddress::getOffsetFromReg(AsmPrinter &AP, unsigned &BR) const {
+  int Off = AP.MF->getSubtarget().getRegisterInfo()->getReturnAddrLoc(*AP.MF,
+                                                                      BR);
+  if(BR == 0) llvm_unreachable("No saved return address!");
+  return Off;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineImmediate implementation
+//
+
+MachineImmediate::MachineImmediate(unsigned Size,
+                                   uint64_t Value,
+                                   const MachineInstr *DefMI,
+                                   bool Ptr)
+  : MachineLiveVal(DefMI, Ptr), Size(Size), Value(Value)
+{
+  if(Size > 8)
+    llvm_unreachable("Unsupported immediate value size of > 8 bytes");
+}
+
+bool MachineImmediate::operator==(const MachineLiveVal &RHS) const {
+  if(RHS.isImm()) {
+    const MachineImmediate &MI = (const MachineImmediate &)RHS;
+    if(MI.Size == Size && MI.Value == Value) return true;
+  }
+  return false;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineGeneratedVal implementation
+//
+
+bool MachineGeneratedVal::operator==(const MachineLiveVal &RHS) const {
+  if(!RHS.isGenerated()) return false;
+  const MachineGeneratedVal &MGV = (const MachineGeneratedVal &)RHS;
+
+  if(VG.size() != MGV.VG.size()) return false;
+  for(size_t i = 0, num = VG.size(); i < num; i++)
+    if(VG[i] != MGV.VG[i]) return false;
+  return true;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineLiveReg implementation
+//
+
+bool MachineLiveReg::operator==(const MachineLiveLoc &RHS) const {
+  if(RHS.isReg()) {
+    const MachineLiveReg &MLR = (const MachineLiveReg &)RHS;
+    if(MLR.Reg == Reg) return true;
+  }
+  return false;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineLiveStackAddr implementation
+//
+
+bool MachineLiveStackAddr::operator==(const MachineLiveLoc &RHS) const {
+  if(RHS.isStackAddr() && !RHS.isStackSlot()) {
+    const MachineLiveStackAddr &MLSA = (const MachineLiveStackAddr &)RHS;
+    if(Offset != INT32_MAX && MLSA.Offset != INT32_MAX &&
+       Offset == MLSA.Offset && Reg == MLSA.Reg && Size == MLSA.Size)
+      return true;
+  }
+  return false;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineLiveStackSlot implementation
+//
+
+bool MachineLiveStackSlot::operator==(const MachineLiveLoc &RHS) const {
+  if(RHS.isStackSlot()) {
+    const MachineLiveStackSlot &MLSS = (const MachineLiveStackSlot &)RHS;
+    if(MLSS.Index == Index) return true;
+  }
+  return false;
+}
+
+int MachineLiveStackSlot::calcAndGetRegOffset(const AsmPrinter &AP, unsigned &BP) {
+  if(Offset == INT32_MAX) {
+    const TargetFrameLowering *TFL = AP.MF->getSubtarget().getFrameLowering();
+    Offset = TFL->getFrameIndexReference(*AP.MF, Index, Reg);
+  }
+  BP = Reg;
+  return Offset;
+}
+
+unsigned MachineLiveStackSlot::getSize(const AsmPrinter &AP) {
+  if(Size == 0) {
+    const MachineFrameInfo *MFI = &AP.MF->getFrameInfo();
+    Size = MFI->getObjectSize(Index);
+  }
+  return Size;
+}
diff --git a/llvm/lib/CodeGen/TargetInstrInfo.cpp b/llvm/lib/CodeGen/TargetInstrInfo.cpp
index 868617ffe14..2f7b383212a 100644
--- a/llvm/lib/CodeGen/TargetInstrInfo.cpp
+++ b/llvm/lib/CodeGen/TargetInstrInfo.cpp
@@ -470,7 +470,8 @@ static MachineInstr *foldPatchpoint(MachineFunction &MF, MachineInstr &MI,
                                     const TargetInstrInfo &TII) {
   unsigned StartIdx = 0;
   switch (MI.getOpcode()) {
-  case TargetOpcode::STACKMAP: {
+  case TargetOpcode::STACKMAP:
+  case TargetOpcode::PCN_STACKMAP: {
     // StackMapLiveValues are foldable
     StartIdx = StackMapOpers(&MI).getVarIdx();
     break;
@@ -570,7 +571,8 @@ MachineInstr *TargetInstrInfo::foldMemoryOperand(MachineInstr &MI,
 
   if (MI.getOpcode() == TargetOpcode::STACKMAP ||
       MI.getOpcode() == TargetOpcode::PATCHPOINT ||
-      MI.getOpcode() == TargetOpcode::STATEPOINT) {
+      MI.getOpcode() == TargetOpcode::STATEPOINT ||
+      MI.getOpcode() == TargetOpcode::PCN_STACKMAP) {
     // Fold stackmap/patchpoint.
     NewMI = foldPatchpoint(MF, MI, Ops, FI, *this);
     if (NewMI)
@@ -635,7 +637,8 @@ MachineInstr *TargetInstrInfo::foldMemoryOperand(MachineInstr &MI,
 
   if ((MI.getOpcode() == TargetOpcode::STACKMAP ||
        MI.getOpcode() == TargetOpcode::PATCHPOINT ||
-       MI.getOpcode() == TargetOpcode::STATEPOINT) &&
+       MI.getOpcode() == TargetOpcode::STATEPOINT ||
+       MI.getOpcode() == TargetOpcode::PCN_STACKMAP) &&
       isLoadFromStackSlot(LoadMI, FrameIndex)) {
     // Fold stackmap/patchpoint.
     NewMI = foldPatchpoint(MF, MI, Ops, FrameIndex, *this);
diff --git a/llvm/lib/CodeGen/TargetPassConfig.cpp b/llvm/lib/CodeGen/TargetPassConfig.cpp
index 36df02692f8..8451bbcd46e 100644
--- a/llvm/lib/CodeGen/TargetPassConfig.cpp
+++ b/llvm/lib/CodeGen/TargetPassConfig.cpp
@@ -72,8 +72,11 @@ static cl::opt<bool> DisableEarlyIfConversion("disable-early-ifcvt", cl::Hidden,
     cl::desc("Disable Early If-conversion"));
 static cl::opt<bool> DisableMachineLICM("disable-machine-licm", cl::Hidden,
     cl::desc("Disable Machine LICM"));
+// FIXME: workaround so that GlobalAddress's aren't spilled to the
+// stack on RISCV.
 static cl::opt<bool> DisableMachineCSE("disable-machine-cse", cl::Hidden,
-    cl::desc("Disable Machine Common Subexpression Elimination"));
+    cl::desc("Disable Machine Common Subexpression Elimination"),
+    cl::init(true));
 static cl::opt<cl::boolOrDefault> OptimizeRegAlloc(
     "optimize-regalloc", cl::Hidden,
     cl::desc("Enable optimized register allocation compilation path."));
@@ -634,7 +637,9 @@ void TargetPassConfig::addIRPasses() {
     addPass(createVerifierPass());
 
   // Run loop strength reduction before anything else.
-  if (getOptLevel() != CodeGenOpt::None && !DisableLSR) {
+  if (getOptLevel() != CodeGenOpt::None &&
+      getArchIROptLevel() != CodeGenOpt::None &&
+      !DisableLSR) {
     addPass(createLoopStrengthReducePass());
     if (PrintLSR)
       addPass(createPrintFunctionPass(dbgs(), "\n\n*** Code after LSR ***\n"));
@@ -659,10 +664,14 @@ void TargetPassConfig::addIRPasses() {
   addPass(createUnreachableBlockEliminationPass());
 
   // Prepare expensive constants for SelectionDAG.
-  if (getOptLevel() != CodeGenOpt::None && !DisableConstantHoisting)
+  if (getOptLevel() != CodeGenOpt::None &&
+      getArchIROptLevel() != CodeGenOpt::None &&
+      !DisableConstantHoisting)
     addPass(createConstantHoistingPass());
 
-  if (getOptLevel() != CodeGenOpt::None && !DisablePartialLibcallInlining)
+  if (getOptLevel() != CodeGenOpt::None &&
+      getArchIROptLevel() != CodeGenOpt::None &&
+      !DisablePartialLibcallInlining)
     addPass(createPartiallyInlineLibCallsPass());
 
   // Instrument function entry and exit, e.g. with calls to mcount().
@@ -723,11 +732,32 @@ void TargetPassConfig::addPassesToHandleExceptions() {
 /// Add pass to prepare the LLVM IR for code generation. This should be done
 /// before exception handling preparation passes.
 void TargetPassConfig::addCodeGenPrepare() {
-  if (getOptLevel() != CodeGenOpt::None && !DisableCGP)
+  if (getOptLevel() != CodeGenOpt::None &&
+      getArchIROptLevel() != CodeGenOpt::None &&
+      !DisableCGP)
     addPass(createCodeGenPreparePass());
   addPass(createRewriteSymbolsPass());
 }
 
+void TargetPassConfig::addPopcornPasses() {
+  assert(!(AddStackMaps && AddLibcStackMaps) &&
+    "Cannot add both InsertStackMapsPass and LibcStackMapsPass");
+  assert(!(AddMigrationPoints && AddLibcStackMaps) &&
+    "Should not be instrumenting libc with extra migration points");
+
+  // Add pass to instrument IR with equivalence points, which are implemented
+  // various ways depending on other command-line arguments
+  if(AddMigrationPoints) addPass(createMigrationPointsPass());
+
+  // Add pass to instrument IR with stackmap instructions, which get lowered to
+  // metadata needed for Popcorn's stack transformation
+  if(AddStackMaps) addPass(createInsertStackMapsPass());
+
+  // Similar to creatInsertStackMaps pass, but only instruments libc thread
+  // start functions
+  if(AddLibcStackMaps) addPass(createLibcStackMapsPass());
+}
+
 /// Add common passes that perform LLVM IR to IR transforms in preparation for
 /// instruction selection.
 void TargetPassConfig::addISelPrepare() {
@@ -1103,6 +1133,10 @@ bool TargetPassConfig::addRegAssignmentOptimized() {
   // Allow targets to change the register assignments before rewriting.
   addPreRewrite();
 
+  // Gather additional stack transformation metadata before rewriting virtual
+  // registers
+  addPass(&StackTransformMetadataID);
+  
   // Finally rewrite virtual registers.
   addPass(&VirtRegRewriterID);
   // Perform stack slot coloring and post-ra machine LICM.
@@ -1176,7 +1210,8 @@ void TargetPassConfig::addOptimizedRegAlloc() {
     // Run post-ra machine LICM to hoist reloads / remats.
     //
     // FIXME: can this move into MachineLateOptimization?
-    addPass(&MachineLICMID);
+    if(getOptLevel() != CodeGenOpt::None)
+      addPass(&MachineLICMID);
   }
 }
 
diff --git a/llvm/lib/CodeGen/UnwindInfo.cpp b/llvm/lib/CodeGen/UnwindInfo.cpp
new file mode 100644
index 00000000000..b1181bc6fd4
--- /dev/null
+++ b/llvm/lib/CodeGen/UnwindInfo.cpp
@@ -0,0 +1,187 @@
+//===--------------------------- UnwindInfo.cpp ---------------------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/CodeGen/TargetFrameLowering.h"
+#include "llvm/CodeGen/TargetRegisterInfo.h"
+#include "llvm/CodeGen/TargetSubtargetInfo.h"
+#include "llvm/CodeGen/UnwindInfo.h"
+#include "llvm/MC/MCSectionELF.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/MC/MCObjectFileInfo.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "unwindinfo"
+
+static const char *UIDbg = "Unwind Info: ";
+
+void UnwindInfo::recordUnwindInfo(const MachineFunction &MF) {
+  // We *only* need this information for functions which have a stackmap, as
+  // only those function activations can be unwound during stack
+  // transformation.  This may also be a correctness criterion since we record
+  // offsets from the FBP, and not all functions may have one (stackmaps are
+  // implemented using FBPs, and thus prevent the FP-elimination optimization).
+  if(!MF.getFrameInfo().hasPcnStackMap()) return;
+
+  LLVM_DEBUG(dbgs() << "**** UnwindInfo: Analyzing " << MF.getName() << "****\n");
+
+  const MachineFrameInfo *MFI = &MF.getFrameInfo();
+  assert(MFI->isCalleeSavedInfoValid() && "No callee-saved information!");
+
+  // Get this function's saved registers
+  unsigned FrameReg;
+  const TargetFrameLowering *TFL = MF.getSubtarget().getFrameLowering();
+  const TargetRegisterInfo *TRI = MF.getSubtarget().getRegisterInfo();
+  const std::vector<CalleeSavedInfo> &CSI = MFI->getCalleeSavedInfo();
+
+  LLVM_DEBUG(dbgs() << CSI.size() << " saved registers\n");
+
+  // Get DWARF register number and FBP offset using callee saved information
+  CalleeSavedRegisters SavedRegs(CSI.size());
+  for(unsigned i = 0; i < CSI.size(); i++) {
+    SavedRegs[i].DwarfReg = TRI->getDwarfRegNum(CSI[i].getReg(), false);
+    SavedRegs[i].Offset =
+      TFL->getFrameIndexReferenceFromFP(MF, CSI[i].getFrameIdx(), FrameReg);
+
+    LLVM_DEBUG(dbgs() << "Register " << SavedRegs[i].DwarfReg << " at register "
+                 << printReg(FrameReg, TRI) << " + " << SavedRegs[i].Offset
+                 << "\n");
+    assert(FrameReg == TRI->getFrameRegister(MF) &&
+           "Invalid register used as offset for unwinding information");
+  }
+
+  // Save the information for when we emit the section
+  const MCSymbol *FuncSym = OutContext.lookupSymbol(MF.getName());
+  assert(FuncSym && "Could not find function symbol");
+  FuncCalleeSaved.insert(FuncCalleePair(FuncSym, std::move(SavedRegs)));
+}
+
+void UnwindInfo::addRegisterUnwindInfo(const MachineFunction &MF,
+                                       uint32_t MachineReg,
+                                       int32_t Offset) {
+  if(!MF.getFrameInfo().hasPcnStackMap()) return;
+
+  const MCSymbol *FuncSym = OutContext.lookupSymbol(MF.getName());
+  assert(FuncSym && "Could not find function symbol");
+  assert(FuncCalleeSaved.find(FuncSym) != FuncCalleeSaved.end() &&
+         "Cannot add register restore information -- function not found");
+  const TargetRegisterInfo *TRI = MF.getSubtarget().getRegisterInfo();
+  FuncCalleeSaved[FuncSym].push_back(
+    RegOffset(TRI->getDwarfRegNum(MachineReg, false), Offset));
+}
+
+void UnwindInfo::emitUnwindInfo(MCStreamer &OS) {
+  unsigned curIdx = 0;
+  unsigned startIdx;
+  FuncCalleeMap::const_iterator f, e;
+  for(f = FuncCalleeSaved.begin(), e = FuncCalleeSaved.end(); f != e; f++) {
+    const MCSymbol *FuncSym = f->first;
+    const CalleeSavedRegisters &CSR = f->second;
+
+    assert(FuncSym && "Invalid machine function");
+    if(CSR.size() < 2)
+      LLVM_DEBUG(dbgs() << "WARNING: should have at least 2 registers to restore "
+                               "(return address & saved FBP");
+
+    LLVM_DEBUG(dbgs() << UIDbg << "Function " << FuncSym->getName()
+                 << " (offset " << curIdx << ", "
+                 << CSR.size() << " entries):\n");
+
+    startIdx = curIdx;
+    CalleeSavedRegisters::const_iterator cs, cse;
+    for(cs = CSR.begin(), cse = CSR.end(); cs != cse; cs++) {
+      assert(cs->DwarfReg < UINT16_MAX &&
+             "Register number too large for resolution");
+      assert(INT16_MIN < cs->Offset && cs->Offset < INT16_MAX &&
+             "Register save offset too large for resolution");
+
+      LLVM_DEBUG(dbgs() << UIDbg << "  Register " << cs->DwarfReg
+                   << " saved at " << cs->Offset << "\n";);
+
+      OS.EmitIntValue(cs->DwarfReg, 2);
+      OS.EmitIntValue(cs->Offset, 2);
+      curIdx++;
+    }
+    FuncUnwindInfo FUI(startIdx, curIdx - startIdx);
+    FuncUnwindMetadata.insert(FuncUnwindPair(FuncSym, std::move(FUI)));
+  }
+}
+
+void UnwindInfo::emitAddrRangeInfo(MCStreamer &OS) {
+  FuncUnwindMap::const_iterator f, e;
+  for(f = FuncUnwindMetadata.begin(), e = FuncUnwindMetadata.end();
+      f != e;
+      f++) {
+    const MCSymbol *Func = f->first;
+    const FuncUnwindInfo &FUI = f->second;
+    OS.EmitSymbolValue(Func, 8);
+    OS.EmitIntValue(FUI.NumUnwindRecord, 4);
+    OS.EmitIntValue(FUI.SecOffset, 4);
+  }
+}
+
+/// Serialize the unwinding information.
+void UnwindInfo::serializeToUnwindInfoSection() {
+  // Bail out if there's no unwind info.
+  if(FuncCalleeSaved.empty()) return;
+
+  // Emit unwinding record information.
+  // FIXME: we only support ELF object files for now
+
+  // Switch to the unwind info section
+  MCStreamer &OS = *AP.OutStreamer;
+  assert (OutContext.getObjectFileInfo() && "ObjectFileInfo does not exist");
+  assert (OutContext.getObjectFileInfo()->getUnwindInfoSection() && "UnwindInfoSection does not exist");
+  MCSection *UnwindInfoSection =
+      OutContext.getObjectFileInfo()->getUnwindInfoSection();
+  OS.SwitchSection(UnwindInfoSection);
+
+  // Emit a dummy symbol to force section inclusion.
+  OS.EmitLabel(OutContext.getOrCreateSymbol(Twine("__StackTransform_UnwindInfo")));
+
+  // Serialize data.
+  LLVM_DEBUG(dbgs() << "********** Unwind Info Output **********\n");
+  emitUnwindInfo(OS);
+  OS.AddBlankLine();
+
+  // Switch to the unwind address range section & emit section
+  MCSection *UnwindAddrRangeSection =
+      OutContext.getObjectFileInfo()->getUnwindAddrRangeSection();
+  OS.SwitchSection(UnwindAddrRangeSection);
+  OS.EmitLabel(OutContext.getOrCreateSymbol(Twine("__StackTransform_UnwindAddrRange")));
+  emitAddrRangeInfo(OS);
+  OS.AddBlankLine();
+
+  Emitted = true;
+}
+
+const UnwindInfo::FuncUnwindInfo &
+UnwindInfo::getUnwindInfo(const MCSymbol *Func) const {
+  assert(Emitted && "Have not yet calculated per-function unwinding metadata");
+
+  FuncUnwindMap::const_iterator it = FuncUnwindMetadata.find(Func);
+  assert(it != FuncUnwindMetadata.end() && "Invalid function");
+  return it->second;
+}
+
+void UnwindInfo::print(raw_ostream &OS) {
+  OS << UIDbg << "Function unwinding information\n";
+  FuncCalleeMap::const_iterator b, e;
+  for(b = FuncCalleeSaved.begin(), e = FuncCalleeSaved.end();
+      b != e;
+      b++) {
+    OS << UIDbg << "Function - " << b->first->getName() << "\n";
+    const CalleeSavedRegisters &CSR = b->second;
+    CalleeSavedRegisters::const_iterator br, be;
+    for(br = CSR.begin(), be = CSR.end(); br != be; br++) {
+      OS << UIDbg << "Register " << br->DwarfReg
+                  << " at offset " << br->Offset << "\n";
+    }
+  }
+}
diff --git a/llvm/lib/IR/DiagnosticInfo.cpp b/llvm/lib/IR/DiagnosticInfo.cpp
index 4a8e3cca349..7c60a52dfef 100644
--- a/llvm/lib/IR/DiagnosticInfo.cpp
+++ b/llvm/lib/IR/DiagnosticInfo.cpp
@@ -62,6 +62,11 @@ DiagnosticInfoInlineAsm::DiagnosticInfoInlineAsm(const Instruction &I,
   }
 }
 
+bool DiagnosticInfoOptimizationError::isEnabled() const {
+  // Only print errors.
+  return getSeverity() == DS_Error;
+}
+
 void DiagnosticInfoInlineAsm::print(DiagnosticPrinter &DP) const {
   DP << getMsgStr();
   if (getLocCookie())
diff --git a/llvm/lib/MC/MCObjectFileInfo.cpp b/llvm/lib/MC/MCObjectFileInfo.cpp
index 9f555abe140..807388710fb 100644
--- a/llvm/lib/MC/MCObjectFileInfo.cpp
+++ b/llvm/lib/MC/MCObjectFileInfo.cpp
@@ -285,9 +285,14 @@ void MCObjectFileInfo::initMachOMCObjectFileInfo(const Triple &T) {
   DwarfTUIndexSection =
       Ctx->getMachOSection("__DWARF", "__debug_tu_index", MachO::S_ATTR_DEBUG,
                            SectionKind::getMetadata());
+
   StackMapSection = Ctx->getMachOSection("__LLVM_STACKMAPS", "__llvm_stackmaps",
                                          0, SectionKind::getMetadata());
 
+  PcnStackMapSection = Ctx->getMachOSection("__LLVM_PCN_STACKMAPS",
+					    "__llvm_pcn_stackmaps",
+					    0, SectionKind::getMetadata());
+
   FaultMapSection = Ctx->getMachOSection("__LLVM_FAULTMAPS", "__llvm_faultmaps",
                                          0, SectionKind::getMetadata());
 
@@ -469,9 +474,22 @@ void MCObjectFileInfo::initELFMCObjectFileInfo(const Triple &T, bool Large) {
   DwarfTUIndexSection =
       Ctx->getELFSection(".debug_tu_index", DebugSecType, 0);
 
+  UnwindAddrRangeSection =
+      Ctx->getELFSection(".stack_transform.unwind_arange", ELF::SHT_PROGBITS,
+                         0, sizeof(uint64_t) + sizeof(uint64_t),
+			 "");
+  UnwindInfoSection =
+      Ctx->getELFSection(".stack_transform.unwind", ELF::SHT_PROGBITS,
+			 0, sizeof(uint16_t) + sizeof(int16_t), "");
+  UnwindAddrRangeSection->setAlignment(sizeof(uint64_t));
+  UnwindInfoSection->setAlignment(sizeof(uint16_t) + sizeof(int16_t));
+
   StackMapSection =
       Ctx->getELFSection(".llvm_stackmaps", ELF::SHT_PROGBITS, ELF::SHF_ALLOC);
 
+  PcnStackMapSection =
+      Ctx->getELFSection(".llvm_pcn_stackmaps", ELF::SHT_PROGBITS, ELF::SHF_ALLOC);
+
   FaultMapSection =
       Ctx->getELFSection(".llvm_faultmaps", ELF::SHT_PROGBITS, ELF::SHF_ALLOC);
 
diff --git a/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp b/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
index 094fbd99952..627ae02e77d 100644
--- a/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
+++ b/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp
@@ -40,6 +40,7 @@
 #include "llvm/CodeGen/MachineOperand.h"
 #include "llvm/CodeGen/StackMaps.h"
 #include "llvm/CodeGen/TargetRegisterInfo.h"
+#include "llvm/CodeGen/UnwindInfo.h"
 #include "llvm/IR/DataLayout.h"
 #include "llvm/IR/DebugInfoMetadata.h"
 #include "llvm/MC/MCAsmInfo.h"
@@ -68,13 +69,14 @@ namespace {
 
 class AArch64AsmPrinter : public AsmPrinter {
   AArch64MCInstLower MCInstLowering;
-  StackMaps SM;
+  StackMaps SM, PSM;
   const AArch64Subtarget *STI;
+  UnwindInfo UI;
 
 public:
   AArch64AsmPrinter(TargetMachine &TM, std::unique_ptr<MCStreamer> Streamer)
       : AsmPrinter(TM, std::move(Streamer)), MCInstLowering(OutContext, *this),
-        SM(*this) {}
+        SM(*this), PSM(*this), UI(*this) {}
 
   StringRef getPassName() const override { return "AArch64 Assembly Printer"; }
 
@@ -92,6 +94,8 @@ public:
 
   void LowerSTACKMAP(MCStreamer &OutStreamer, StackMaps &SM,
                      const MachineInstr &MI);
+  void LowerPCN_STACKMAP(MCStreamer &OutStreamer, StackMaps &SM,
+                     const MachineInstr &MI);
   void LowerPATCHPOINT(MCStreamer &OutStreamer, StackMaps &SM,
                        const MachineInstr &MI);
 
@@ -121,6 +125,8 @@ public:
     AArch64FI = MF.getInfo<AArch64FunctionInfo>();
     STI = static_cast<const AArch64Subtarget*>(&MF.getSubtarget());
 
+    bool modified = TagCallSites(MF);
+
     SetupMachineFunction(MF);
 
     if (STI->isTargetCOFF()) {
@@ -142,8 +148,11 @@ public:
     // Emit the XRay table for this function.
     emitXRayTable();
 
+    if(MF.getFrameInfo().hasPcnStackMap())
+      UI.recordUnwindInfo(MF);
+
     // We didn't modify anything.
-    return false;
+    return modified;
   }
 
 private:
@@ -443,6 +452,9 @@ void AArch64AsmPrinter::EmitEndOfAsmFile(Module &M) {
     OutStreamer->EmitAssemblerFlag(MCAF_SubsectionsViaSymbols);
     emitStackMaps(SM);
   }
+  UI.serializeToUnwindInfoSection();
+  PSM.serializeToPcnStackMapSection(&UI);
+  UI.reset(); // Must reset after PSM serialization to clear metadata  
 }
 
 void AArch64AsmPrinter::EmitLOHs() {
@@ -787,6 +799,33 @@ void AArch64AsmPrinter::LowerSTACKMAP(MCStreamer &OutStreamer, StackMaps &SM,
     EmitToStreamer(OutStreamer, MCInstBuilder(AArch64::HINT).addImm(0));
 }
 
+void AArch64AsmPrinter::LowerPCN_STACKMAP(MCStreamer &OutStreamer,
+					  StackMaps &SM,
+					  const MachineInstr &MI) {
+  unsigned NumNOPBytes = StackMapOpers(&MI).getNumPatchBytes();
+
+  SM.recordPcnStackMap(MI);
+  assert(NumNOPBytes % 4 == 0 && "Invalid number of NOP bytes requested!");
+
+  // Scan ahead to trim the shadow.
+  const MachineBasicBlock &MBB = *MI.getParent();
+  MachineBasicBlock::const_iterator MII(MI);
+  ++MII;
+  while (NumNOPBytes > 0) {
+    if (MII == MBB.end() || MII->isCall() ||
+        MII->getOpcode() == AArch64::DBG_VALUE ||
+        MII->getOpcode() == TargetOpcode::PATCHPOINT ||
+        MII->getOpcode() == TargetOpcode::STACKMAP)
+      break;
+    ++MII;
+    NumNOPBytes -= 4;
+  }
+
+  // Emit nops.
+  for (unsigned i = 0; i < NumNOPBytes; i += 4)
+    EmitToStreamer(OutStreamer, MCInstBuilder(AArch64::HINT).addImm(0));
+}
+
 // Lower a patchpoint of the form:
 // [<def>], <id>, <numBytes>, <target>, <numArgs>
 void AArch64AsmPrinter::LowerPATCHPOINT(MCStreamer &OutStreamer, StackMaps &SM,
@@ -1068,6 +1107,9 @@ void AArch64AsmPrinter::EmitInstruction(const MachineInstr *MI) {
   case TargetOpcode::STACKMAP:
     return LowerSTACKMAP(*OutStreamer, SM, *MI);
 
+  case TargetOpcode::PCN_STACKMAP:
+    return LowerPCN_STACKMAP(*OutStreamer, PSM, *MI);
+
   case TargetOpcode::PATCHPOINT:
     return LowerPATCHPOINT(*OutStreamer, SM, *MI);
 
diff --git a/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp b/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
index 8c6e5cbd5c1..18722b7a3e7 100644
--- a/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
+++ b/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp
@@ -212,7 +212,7 @@ bool AArch64FrameLowering::hasFP(const MachineFunction &MF) const {
   if (MFI.hasCalls() && MF.getTarget().Options.DisableFramePointerElim(MF))
     return true;
   if (MFI.hasVarSizedObjects() || MFI.isFrameAddressTaken() ||
-      MFI.hasStackMap() || MFI.hasPatchPoint() ||
+      MFI.hasStackMap() || MFI.hasPcnStackMap() || MFI.hasPatchPoint() ||
       RegInfo->needsStackRealignment(MF))
     return true;
   // With large callframes around we may need to use FP to access the scavenging
@@ -1536,6 +1536,15 @@ int AArch64FrameLowering::getSEHFrameIndexOffset(const MachineFunction &MF,
              : getStackOffset(MF, ObjectOffset);
 }
 
+/// getFrameIndexReferenceFromFP - Provide a base+offset reference to an FI
+/// slot for debug info, but force base to be the frame pointer (x29).
+int
+AArch64FrameLowering::getFrameIndexReferenceFromFP(const MachineFunction &MF,
+                                                   int FI,
+                                                   unsigned &FrameReg) const {
+  return resolveFrameIndexReference(MF, FI, FrameReg, true, false);
+}
+
 int AArch64FrameLowering::resolveFrameIndexReference(const MachineFunction &MF,
                                                      int FI, unsigned &FrameReg,
                                                      bool PreferFP,
diff --git a/llvm/lib/Target/AArch64/AArch64FrameLowering.h b/llvm/lib/Target/AArch64/AArch64FrameLowering.h
index 6dbd34b2189..85639765fc1 100644
--- a/llvm/lib/Target/AArch64/AArch64FrameLowering.h
+++ b/llvm/lib/Target/AArch64/AArch64FrameLowering.h
@@ -39,6 +39,8 @@ public:
 
   int getFrameIndexReference(const MachineFunction &MF, int FI,
                              unsigned &FrameReg) const override;
+  int getFrameIndexReferenceFromFP(const MachineFunction &MF, int FI,
+                                   unsigned &FrameReg) const override;
   int resolveFrameIndexReference(const MachineFunction &MF, int FI,
                                  unsigned &FrameReg, bool PreferFP,
                                  bool ForSimm) const;
diff --git a/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp b/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
index 03923878fd5..bd88c792278 100644
--- a/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
+++ b/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
@@ -1333,6 +1333,7 @@ MachineBasicBlock *AArch64TargetLowering::EmitInstrWithCustomInserter(
     return EmitF128CSEL(MI, BB);
 
   case TargetOpcode::STACKMAP:
+  case TargetOpcode::PCN_STACKMAP:
   case TargetOpcode::PATCHPOINT:
     return emitPatchPoint(MI, BB);
 
@@ -2992,6 +2993,10 @@ SDValue AArch64TargetLowering::LowerOperation(SDValue Op,
     return LowerATOMIC_LOAD_AND(Op, DAG);
   case ISD::DYNAMIC_STACKALLOC:
     return LowerDYNAMIC_STACKALLOC(Op, DAG);
+  case (uint16_t)~TargetOpcode::STACKMAP:
+  case (uint16_t)~TargetOpcode::PCN_STACKMAP:
+    return SDValue(); // Use generic stackmap type legalizer
+
   }
 }
 
diff --git a/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp b/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
index fa9e5d808c4..66b6a3f466e 100644
--- a/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
+++ b/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp
@@ -99,6 +99,7 @@ unsigned AArch64InstrInfo::getInstSizeInBytes(const MachineInstr &MI) const {
     NumBytes = 0;
     break;
   case TargetOpcode::STACKMAP:
+  case TargetOpcode::PCN_STACKMAP:
     // The upper bound for a stackmap intrinsic is the full length of its shadow
     NumBytes = StackMapOpers(&MI).getNumPatchBytes();
     assert(NumBytes % 4 == 0 && "Invalid number of NOP bytes requested!");
diff --git a/llvm/lib/Target/AArch64/AArch64InstrInfo.td b/llvm/lib/Target/AArch64/AArch64InstrInfo.td
index 020035c7f6c..0ca808453e0 100644
--- a/llvm/lib/Target/AArch64/AArch64InstrInfo.td
+++ b/llvm/lib/Target/AArch64/AArch64InstrInfo.td
@@ -259,7 +259,7 @@ def AArch64LOADgot       : SDNode<"AArch64ISD::LOADgot", SDTIntUnaryOp>;
 def AArch64callseq_start : SDNode<"ISD::CALLSEQ_START",
                                 SDCallSeqStart<[ SDTCisVT<0, i32>,
                                                  SDTCisVT<1, i32> ]>,
-                                [SDNPHasChain, SDNPOutGlue]>;
+                                [SDNPHasChain, SDNPInGlue, SDNPOutGlue]>;
 def AArch64callseq_end   : SDNode<"ISD::CALLSEQ_END",
                                 SDCallSeqEnd<[ SDTCisVT<0, i32>,
                                                SDTCisVT<1, i32> ]>,
diff --git a/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp b/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
index 6d5a4e3d2f7..27c8bad9804 100644
--- a/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
+++ b/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp
@@ -285,6 +285,23 @@ AArch64RegisterInfo::getFrameRegister(const MachineFunction &MF) const {
   return TFI->hasFP(MF) ? AArch64::FP : AArch64::SP;
 }
 
+int AArch64RegisterInfo::getReturnAddrLoc(const MachineFunction &MF,
+                                          unsigned &BaseReg) const {
+  const TargetFrameLowering *TFL = MF.getSubtarget().getFrameLowering();
+  const MachineFrameInfo *MFI = &MF.getFrameInfo();
+  assert(MFI->isCalleeSavedInfoValid() && "No callee-saved information");
+  const std::vector<CalleeSavedInfo> &CSI = MFI->getCalleeSavedInfo();
+
+  // The return address' location is the the link register's spill slot
+  for(unsigned i = 0, e = CSI.size(); i < e; i++)
+    if(CSI[i].getReg() == AArch64::LR)
+      return TFL->getFrameIndexReference(MF, CSI[i].getFrameIdx(), BaseReg);
+
+  // We didn't find it, is it actually saved?
+  BaseReg = 0;
+  return INT32_MAX;
+}
+
 bool AArch64RegisterInfo::requiresRegisterScavenging(
     const MachineFunction &MF) const {
   return true;
@@ -451,7 +468,8 @@ void AArch64RegisterInfo::eliminateFrameIndex(MachineBasicBlock::iterator II,
 
   // Special handling of dbg_value, stackmap and patchpoint instructions.
   if (MI.isDebugValue() || MI.getOpcode() == TargetOpcode::STACKMAP ||
-      MI.getOpcode() == TargetOpcode::PATCHPOINT) {
+      MI.getOpcode() == TargetOpcode::PATCHPOINT ||
+      MI.getOpcode() == TargetOpcode::PCN_STACKMAP) {
     Offset = TFI->resolveFrameIndexReference(MF, FrameIndex, FrameReg,
                                              /*PreferFP=*/true,
                                              /*ForSimm=*/false);
diff --git a/llvm/lib/Target/AArch64/AArch64RegisterInfo.h b/llvm/lib/Target/AArch64/AArch64RegisterInfo.h
index 2c3f82c530d..19daadd8cfd 100644
--- a/llvm/lib/Target/AArch64/AArch64RegisterInfo.h
+++ b/llvm/lib/Target/AArch64/AArch64RegisterInfo.h
@@ -115,6 +115,9 @@ public:
   // Debug information queries.
   Register getFrameRegister(const MachineFunction &MF) const override;
 
+  int getReturnAddrLoc(const MachineFunction &MF,
+                       unsigned &BaseReg) const override;
+
   unsigned getRegPressureLimit(const TargetRegisterClass *RC,
                                MachineFunction &MF) const override;
 
diff --git a/llvm/lib/Target/AArch64/AArch64Subtarget.h b/llvm/lib/Target/AArch64/AArch64Subtarget.h
index 0c84cfb8329..b490f7943d1 100644
--- a/llvm/lib/Target/AArch64/AArch64Subtarget.h
+++ b/llvm/lib/Target/AArch64/AArch64Subtarget.h
@@ -18,6 +18,7 @@
 #include "AArch64InstrInfo.h"
 #include "AArch64RegisterInfo.h"
 #include "AArch64SelectionDAGInfo.h"
+#include "AArch64Values.h"
 #include "llvm/CodeGen/GlobalISel/CallLowering.h"
 #include "llvm/CodeGen/GlobalISel/InstructionSelector.h"
 #include "llvm/CodeGen/GlobalISel/LegalizerInfo.h"
@@ -209,6 +210,7 @@ protected:
   AArch64InstrInfo InstrInfo;
   AArch64SelectionDAGInfo TSInfo;
   AArch64TargetLowering TLInfo;
+  AArch64Values VGen;
 
   /// GlobalISel related APIs.
   std::unique_ptr<CallLowering> CallLoweringInfo;
@@ -246,6 +248,9 @@ public:
   const AArch64RegisterInfo *getRegisterInfo() const override {
     return &getInstrInfo()->getRegisterInfo();
   }
+  const AArch64Values *getValues() const override {
+    return &VGen;
+  }
   const CallLowering *getCallLowering() const override;
   const InstructionSelector *getInstructionSelector() const override;
   const LegalizerInfo *getLegalizerInfo() const override;
diff --git a/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp b/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
index 86546148049..69f81837880 100644
--- a/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
+++ b/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp
@@ -413,7 +413,9 @@ void AArch64PassConfig::addIRPasses() {
   // Cmpxchg instructions are often used with a subsequent comparison to
   // determine whether it succeeded. We can exploit existing control-flow in
   // ldrex/strex loops to simplify this, but it needs tidying up.
-  if (TM->getOptLevel() != CodeGenOpt::None && EnableAtomicTidy)
+  if (TM->getOptLevel() != CodeGenOpt::None &&
+      TM->getArchIROptLevel() != CodeGenOpt::None &&
+      EnableAtomicTidy)
     addPass(createCFGSimplificationPass(1, true, true, false, true));
 
   // Run LoopDataPrefetch
@@ -430,12 +432,14 @@ void AArch64PassConfig::addIRPasses() {
   TargetPassConfig::addIRPasses();
 
   // Match interleaved memory accesses to ldN/stN intrinsics.
-  if (TM->getOptLevel() != CodeGenOpt::None) {
+  if (TM->getOptLevel() != CodeGenOpt::None &&
+      TM->getArchIROptLevel() != CodeGenOpt::None) {
     addPass(createInterleavedLoadCombinePass());
     addPass(createInterleavedAccessPass());
   }
 
-  if (TM->getOptLevel() == CodeGenOpt::Aggressive && EnableGEPOpt) {
+  if (TM->getOptLevel() == CodeGenOpt::Aggressive &&
+      TM->getArchIROptLevel() != CodeGenOpt::None && EnableGEPOpt) {
     // Call SeparateConstOffsetFromGEP pass to extract constants within indices
     // and lower a GEP with multiple indices to either arithmetic operations or
     // multiple GEPs with single index.
@@ -455,12 +459,14 @@ void AArch64PassConfig::addIRPasses() {
 bool AArch64PassConfig::addPreISel() {
   // Run promote constant before global merge, so that the promoted constants
   // get a chance to be merged
-  if (TM->getOptLevel() != CodeGenOpt::None && EnablePromoteConstant)
+  if (TM->getOptLevel() != CodeGenOpt::None &&
+      TM->getArchIROptLevel() != CodeGenOpt::None && EnablePromoteConstant)
     addPass(createAArch64PromoteConstantPass());
   // FIXME: On AArch64, this depends on the type.
   // Basically, the addressable offsets are up to 4095 * Ty.getSizeInBytes().
   // and the offset has to be a multiple of the related size in bytes.
   if ((TM->getOptLevel() != CodeGenOpt::None &&
+       TM->getArchIROptLevel() != CodeGenOpt::None &&
        EnableGlobalMerge == cl::BOU_UNSET) ||
       EnableGlobalMerge == cl::BOU_TRUE) {
     bool OnlyOptimizeForSize = (TM->getOptLevel() < CodeGenOpt::Aggressive) &&
diff --git a/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp b/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
index 301bf72d523..0a00cf70876 100644
--- a/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
+++ b/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
@@ -174,6 +174,7 @@ int AArch64TTIImpl::getIntImmCost(Intrinsic::ID IID, unsigned Idx,
     }
     break;
   case Intrinsic::experimental_stackmap:
+  case Intrinsic::experimental_pcn_stackmap:
     if ((Idx < 2) || (Imm.getBitWidth() <= 64 && isInt<64>(Imm.getSExtValue())))
       return TTI::TCC_Free;
     break;
@@ -871,6 +872,9 @@ bool AArch64TTIImpl::shouldConsiderAddressTypePromotion(
   AllowPromotionWithoutCommonHeader = false;
   if (!isa<SExtInst>(&I))
     return false;
+  const TargetMachine &TM = getTLI()->getTargetMachine();
+  if (TM.getArchIROptLevel() == CodeGenOpt::None)
+    return false;
   Type *ConsideredSExtType =
       Type::getInt64Ty(I.getParent()->getParent()->getContext());
   if (I.getType() != ConsideredSExtType)
diff --git a/llvm/lib/Target/AArch64/AArch64Values.cpp b/llvm/lib/Target/AArch64/AArch64Values.cpp
new file mode 100644
index 00000000000..07d2d5f0710
--- /dev/null
+++ b/llvm/lib/Target/AArch64/AArch64Values.cpp
@@ -0,0 +1,252 @@
+//===- AArch64TargetValues.cpp - AArch64 specific value generator -===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "AArch64Values.h"
+#include "AArch64.h"
+#include "MCTargetDesc/AArch64AddressingModes.h"
+#include "llvm/CodeGen/MachineConstantPool.h"
+#include "llvm/CodeGen/TargetInstrInfo.h"
+#include "llvm/CodeGen/TargetSubtargetInfo.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+#define DEBUG_TYPE "stacktransform"
+
+using namespace llvm;
+
+static TemporaryValue *getTemporaryReference(const MachineInstr *MI,
+                                             const VirtRegMap *VRM,
+                                             unsigned Size) {
+  TemporaryValue *Val = nullptr;
+  if(MI->getOperand(0).isReg()) {
+    // Instruction format:    ADDXri  xd    xn    imm#  lsl#
+    // Stack slot reference:                <fi>  0     0
+    if(MI->getOperand(1).isFI() &&
+       MI->getOperand(2).isImm() && MI->getOperand(2).getImm() == 0 &&
+       MI->getOperand(3).isImm() && MI->getOperand(3).getImm() == 0) {
+      Val = new TemporaryValue;
+      Val->Type = TemporaryValue::StackSlotRef;
+      Val->Size = Size;
+      Val->Vreg = MI->getOperand(0).getReg();
+      Val->StackSlot = MI->getOperand(1).getIndex();
+      Val->Offset = 0;
+    }
+  }
+
+  return Val;
+}
+
+TemporaryValuePtr
+AArch64Values::getTemporaryValue(const MachineInstr *MI,
+                                 const VirtRegMap *VRM) const {
+  TemporaryValue *Val = nullptr;
+  switch(MI->getOpcode()) {
+  case AArch64::ADDXri: Val = getTemporaryReference(MI, VRM, 8); break;
+  default: break;
+  }
+  return TemporaryValuePtr(Val);
+}
+
+typedef ValueGenInst::InstType InstType;
+template <InstType T> using RegInstruction = RegInstruction<T>;
+template <InstType T> using ImmInstruction = ImmInstruction<T>;
+
+// Bitwise-conversions between floats & ints
+union IntFloat64 { double d; uint64_t i; };
+union IntFloat32 { float f; uint64_t i; };
+
+MachineLiveVal *
+AArch64Values::genADDInstructions(const MachineInstr *MI) const {
+  int Index;
+
+  switch(MI->getOpcode()) {
+  case AArch64::ADDXri:
+    if(MI->getOperand(1).isFI()) {
+      Index = MI->getOperand(1).getIndex();
+      assert(MI->getOperand(2).isImm() && MI->getOperand(2).getImm() == 0);
+      assert(MI->getOperand(3).isImm() && MI->getOperand(3).getImm() == 0);
+      return new MachineStackObject(Index, false, MI, true);
+    }
+    break;
+  default:
+    LLVM_DEBUG(dbgs() << "Unhandled ADD machine instruction");
+    break;
+  }
+  return nullptr;
+}
+
+MachineLiveVal *
+AArch64Values::genADRPInstructions(const MachineInstr *MI) const {
+  ValueGenInstList IL;
+  if(isSymbolValue(MI->getOperand(1))) {
+    IL.emplace_back(new RefInstruction(MI->getOperand(1)));
+    IL.emplace_back(new ImmInstruction<InstType::Mask>(8, ~0xfff));
+    return new MachineGeneratedVal(IL, MI, false);
+  }
+  return nullptr;
+}
+
+MachineLiveVal *
+AArch64Values::genBitfieldInstructions(const MachineInstr *MI) const {
+  int64_t R, S;
+  unsigned Size, Bits;
+  uint64_t Mask;
+  ValueGenInstList IL;
+
+  switch(MI->getOpcode()) {
+  case AArch64::UBFMXri:
+    Size = 8;
+    Bits = 64;
+    Mask = UINT64_MAX;
+
+    assert(MI->getOperand(1).isReg() &&
+           MI->getOperand(2).isImm() &&
+           MI->getOperand(3).isImm());
+
+    // TODO ensure this is correct
+    IL.emplace_back(
+      new RegInstruction<InstType::Set>(MI->getOperand(1).getReg()));
+    R = MI->getOperand(2).getImm();
+    S = MI->getOperand(3).getImm();
+    if(S >= R) {
+      IL.emplace_back(new ImmInstruction<InstType::RightShiftLog>(Size, R));
+      IL.emplace_back(
+        new ImmInstruction<InstType::Mask>(Size, ~(Mask << (S - R + 1))));
+    }
+    else {
+      IL.emplace_back(
+        new ImmInstruction<InstType::Mask>(Size, ~(Mask << (S + 1))));
+      IL.emplace_back(new ImmInstruction<InstType::LeftShift>(Size, Bits - R));
+    }
+    return new MachineGeneratedVal(IL, MI, false);
+    break;
+  default:
+    LLVM_DEBUG(dbgs() << "Unhandled bitfield instruction");
+    break;
+  }
+  return nullptr;
+}
+
+MachineLiveVal *
+AArch64Values::genLoadRegValue(const MachineInstr *MI) const {
+  switch(MI->getOpcode()) {
+  case AArch64::LDRDui:
+    if(MI->getOperand(2).isCPI()) {
+      int Idx = MI->getOperand(2).getIndex();
+      const MachineFunction *MF = MI->getParent()->getParent();
+      const MachineConstantPool *MCP = MF->getConstantPool();
+      const std::vector<MachineConstantPoolEntry> &CP = MCP->getConstants();
+      if(CP[Idx].isMachineConstantPoolEntry()) {
+        // TODO unhandled for now
+      }
+      else {
+        const Constant *Val = CP[Idx].Val.ConstVal;
+        if(isa<ConstantFP>(Val)) {
+          const ConstantFP *FPVal = cast<ConstantFP>(Val);
+          const APFloat &Flt = FPVal->getValueAPF();
+          switch(APFloat::getSizeInBits(Flt.getSemantics())) {
+          case 32: {
+            IntFloat32 I2F = { Flt.convertToFloat() };
+            return new MachineImmediate(4, I2F.i, MI, false);
+          }
+          case 64: {
+            IntFloat64 I2D = { Flt.convertToDouble() };
+            return new MachineImmediate(8, I2D.i, MI, false);
+          }
+          default: break;
+          }
+        }
+      }
+    }
+    break;
+  case AArch64::LDRXui:
+    // Note: if this is of the form %vreg, <ga:...>, then the compiler has
+    // emitted multiple instructions in order to form the full address.  We,
+    // however, don't have the instruction encoding limitations.
+    // TODO verify this note above is true, maybe using MO::getTargetFlags?
+    // Note 2: we *must* ensure the symbol is const-qualified, otherwise we
+    // risk creating a new value if the symbol's value changes between when the
+    // initial load would have occurred and the transformation, e.g.,
+    //
+    //   ldr x20, <ga:mysym>
+    //   ... (somebody changes mysym's value) ...
+    //   bl <ga:myfunc>
+    //
+    // In this situation, the transformation occurs at the call site and
+    // retrieves the updated value rather than the value that would have been
+    // loaded at the ldr instruction.
+    if(TargetValues::isSymbolValue(MI->getOperand(2)) &&
+       TargetValues::isSymbolValueConstant(MI->getOperand(2)))
+      return new MachineSymbolRef(MI->getOperand(2), true, MI);
+    break;
+  default: break;
+  }
+  return nullptr;
+}
+
+MachineLiveValPtr AArch64Values::getMachineValue(const MachineInstr *MI) const {
+  IntFloat64 Conv64;
+  MachineLiveVal* Val = nullptr;
+  const MachineOperand *MO;
+  const TargetInstrInfo *TII;
+
+  switch(MI->getOpcode()) {
+  case AArch64::ADDXri:
+    Val = genADDInstructions(MI);
+    break;
+  case AArch64::ADRP:
+    Val = genADRPInstructions(MI);
+    break;
+  case AArch64::MOVaddr:
+    MO = &MI->getOperand(1);
+    if(MO->isCPI())
+      Val = new MachineConstPoolRef(MO->getIndex(), MI);
+    else if(TargetValues::isSymbolValue(MO))
+      Val = new MachineSymbolRef(*MO, false, MI);
+    break;
+  case AArch64::COPY:
+    MO = &MI->getOperand(1);
+    if(MO->isReg() && MO->getReg() == AArch64::LR) Val = new ReturnAddress(MI);
+    break;
+  case AArch64::FMOVD0:
+    Conv64.d = 0.0;
+    Val = new MachineImmediate(8, Conv64.i, MI, false);
+    break;
+  case AArch64::FMOVDi:
+    Conv64.d = (double)AArch64_AM::getFPImmFloat(MI->getOperand(1).getImm());
+    Val = new MachineImmediate(8, Conv64.i, MI, false);
+    break;
+  case AArch64::LDRXui:
+  case AArch64::LDRDui:
+    Val = genLoadRegValue(MI);
+    break;
+  case AArch64::MOVi32imm:
+    MO = &MI->getOperand(1);
+    assert(MO->isImm() && "Invalid immediate for MOVi32imm");
+    Val = new MachineImmediate(4, MO->getImm(), MI, false);
+    break;
+  case AArch64::MOVi64imm:
+    MO = &MI->getOperand(1);
+    assert(MO->isImm() && "Invalid immediate for MOVi64imm");
+    Val = new MachineImmediate(8, MO->getImm(), MI, false);
+    break;
+  case AArch64::UBFMXri:
+    Val = genBitfieldInstructions(MI);
+    break;
+  default:
+    TII =  MI->getParent()->getParent()->getSubtarget().getInstrInfo();
+    LLVM_DEBUG(dbgs() << "Unhandled opcode: "
+                 << TII->getName(MI->getOpcode()) << "\n");
+    break;
+  }
+
+  return MachineLiveValPtr(Val);
+}
diff --git a/llvm/lib/Target/AArch64/AArch64Values.h b/llvm/lib/Target/AArch64/AArch64Values.h
new file mode 100644
index 00000000000..faf859ac338
--- /dev/null
+++ b/llvm/lib/Target/AArch64/AArch64Values.h
@@ -0,0 +1,28 @@
+//===----- AArch64TargetValues.cpp - AArch64 specific value generator -----===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Target/TargetValues.h"
+
+namespace llvm {
+
+class AArch64Values final : public TargetValues {
+public:
+  AArch64Values() {}
+  virtual TemporaryValuePtr getTemporaryValue(const MachineInstr *MI,
+                                              const VirtRegMap *VRM) const;
+  virtual MachineLiveValPtr getMachineValue(const MachineInstr *MI) const;
+
+private:
+  MachineLiveVal *genADDInstructions(const MachineInstr *MI) const;
+  MachineLiveVal *genADRPInstructions(const MachineInstr *MI) const;
+  MachineLiveVal *genBitfieldInstructions(const MachineInstr *MI) const;
+  MachineLiveVal *genLoadRegValue(const MachineInstr *MI) const;
+};
+
+}
diff --git a/llvm/lib/Target/AArch64/CMakeLists.txt b/llvm/lib/Target/AArch64/CMakeLists.txt
index 3154ed03bd4..b8abcdb532f 100644
--- a/llvm/lib/Target/AArch64/CMakeLists.txt
+++ b/llvm/lib/Target/AArch64/CMakeLists.txt
@@ -62,6 +62,7 @@ add_llvm_target(AArch64CodeGen
   AArch64TargetObjectFile.cpp
   AArch64TargetTransformInfo.cpp
   AArch64SIMDInstrOpt.cpp
+  AArch64Values.cpp
 
   DEPENDS
   intrinsics_gen
diff --git a/llvm/lib/Target/CMakeLists.txt b/llvm/lib/Target/CMakeLists.txt
index 1e6abfacb79..e6c2c6958e5 100644
--- a/llvm/lib/Target/CMakeLists.txt
+++ b/llvm/lib/Target/CMakeLists.txt
@@ -8,6 +8,7 @@ add_llvm_library(LLVMTarget
   TargetLoweringObjectFile.cpp
   TargetMachine.cpp
   TargetMachineC.cpp
+  TargetValues.cpp
 
   ADDITIONAL_HEADER_DIRS
   ${LLVM_MAIN_INCLUDE_DIR}/llvm/Target
diff --git a/llvm/lib/Target/RISCV/CMakeLists.txt b/llvm/lib/Target/RISCV/CMakeLists.txt
index 5cdaf344703..97dadd7efab 100644
--- a/llvm/lib/Target/RISCV/CMakeLists.txt
+++ b/llvm/lib/Target/RISCV/CMakeLists.txt
@@ -28,6 +28,7 @@ add_llvm_target(RISCVCodeGen
   RISCVTargetMachine.cpp
   RISCVTargetObjectFile.cpp
   RISCVTargetTransformInfo.cpp
+  RISCVValues.cpp
   )
 
 add_subdirectory(AsmParser)
diff --git a/llvm/lib/Target/RISCV/RISCVAsmPrinter.cpp b/llvm/lib/Target/RISCV/RISCVAsmPrinter.cpp
index 57631dcb511..d70926c71a1 100644
--- a/llvm/lib/Target/RISCV/RISCVAsmPrinter.cpp
+++ b/llvm/lib/Target/RISCV/RISCVAsmPrinter.cpp
@@ -21,8 +21,11 @@
 #include "llvm/CodeGen/MachineFunctionPass.h"
 #include "llvm/CodeGen/MachineInstr.h"
 #include "llvm/CodeGen/MachineModuleInfo.h"
+#include "llvm/CodeGen/StackMaps.h"
+#include "llvm/CodeGen/UnwindInfo.h"
 #include "llvm/MC/MCAsmInfo.h"
 #include "llvm/MC/MCInst.h"
+#include "llvm/MC/MCInstBuilder.h"
 #include "llvm/MC/MCStreamer.h"
 #include "llvm/MC/MCSymbol.h"
 #include "llvm/Support/TargetRegistry.h"
@@ -33,10 +36,14 @@ using namespace llvm;
 
 namespace {
 class RISCVAsmPrinter : public AsmPrinter {
+protected:
+  StackMaps SM;
+  UnwindInfo UI;
+
 public:
   explicit RISCVAsmPrinter(TargetMachine &TM,
                            std::unique_ptr<MCStreamer> Streamer)
-      : AsmPrinter(TM, std::move(Streamer)) {}
+      : AsmPrinter(TM, std::move(Streamer)), SM(*this), UI(*this) {}
 
   StringRef getPassName() const override { return "RISCV Assembly Printer"; }
 
@@ -55,6 +62,24 @@ public:
   bool lowerOperand(const MachineOperand &MO, MCOperand &MCOp) const {
     return LowerRISCVMachineOperandToMCOperand(MO, MCOp, *this);
   }
+
+  void EmitEndOfAsmFile(Module &M) override;
+  void LowerSTACKMAP(StackMaps &SM, const MachineInstr &MI);
+  void LowerPCN_STACKMAP(StackMaps &SM, const MachineInstr &MI);
+
+  bool runOnMachineFunction(MachineFunction &MF) override {
+    bool modified = TagCallSites(MF);
+    SetupMachineFunction(MF);
+    EmitFunctionBody();
+    if(MF.getFrameInfo().hasPcnStackMap())
+      UI.recordUnwindInfo(MF);
+    return modified;
+  }
+
+  bool doInitialization(Module &M) override {
+    SM.reset();
+    return AsmPrinter::doInitialization(M);
+  }
 };
 }
 
@@ -76,6 +101,11 @@ void RISCVAsmPrinter::EmitInstruction(const MachineInstr *MI) {
   if (emitPseudoExpansionLowering(*OutStreamer, MI))
     return;
 
+  if (MI->getOpcode() == TargetOpcode::STACKMAP)
+    return LowerSTACKMAP(SM, *MI);
+  else if (MI->getOpcode() == TargetOpcode::PCN_STACKMAP)
+    return LowerPCN_STACKMAP(SM, *MI);
+
   MCInst TmpInst;
   LowerRISCVMachineInstrToMCInst(MI, TmpInst, *this);
   EmitToStreamer(*OutStreamer, TmpInst);
@@ -140,6 +170,68 @@ bool RISCVAsmPrinter::PrintAsmMemoryOperand(const MachineInstr *MI,
   return AsmPrinter::PrintAsmMemoryOperand(MI, OpNo, ExtraCode, OS);
 }
 
+void RISCVAsmPrinter::EmitEndOfAsmFile(Module &M) {
+  UI.serializeToUnwindInfoSection();
+  SM.serializeToPcnStackMapSection(&UI);
+  UI.reset(); // Must reset after SM serialization to clear metadata
+}
+
+void RISCVAsmPrinter::LowerSTACKMAP(StackMaps &SM, const MachineInstr &MI) {
+  unsigned NumNOPBytes = MI.getOperand(1).getImm();
+
+  SM.recordStackMap(MI);
+  assert(NumNOPBytes % 4 == 0 && "Invalid number of NOP bytes requested!");
+
+  // Scan ahead to trim the shadow.
+  const MachineBasicBlock &MBB = *MI.getParent();
+  MachineBasicBlock::const_iterator MII(MI);
+  ++MII;
+  while (NumNOPBytes > 0) {
+    if (MII == MBB.end() || MII->isCall() ||
+        MII->getOpcode() == RISCV::DBG_VALUE ||
+        MII->getOpcode() == TargetOpcode::STACKMAP)
+      break;
+    ++MII;
+    NumNOPBytes -= 4;
+  }
+
+  // Emit nops.
+  for (unsigned i = 0; i < NumNOPBytes; i += 4)
+    EmitToStreamer(*OutStreamer,
+		  MCInstBuilder(RISCV::ADDI)
+		   .addReg(RISCV::X0)
+		   .addReg(RISCV::X0)
+		   .addImm(0));
+}
+
+void RISCVAsmPrinter::LowerPCN_STACKMAP(StackMaps &SM, const MachineInstr &MI) {
+  unsigned NumNOPBytes = MI.getOperand(1).getImm();
+
+  SM.recordPcnStackMap(MI);
+  assert(NumNOPBytes % 4 == 0 && "Invalid number of NOP bytes requested!");
+
+  // Scan ahead to trim the shadow.
+  const MachineBasicBlock &MBB = *MI.getParent();
+  MachineBasicBlock::const_iterator MII(MI);
+  ++MII;
+  while (NumNOPBytes > 0) {
+    if (MII == MBB.end() || MII->isCall() ||
+        MII->getOpcode() == RISCV::DBG_VALUE ||
+        MII->getOpcode() == TargetOpcode::STACKMAP)
+      break;
+    ++MII;
+    NumNOPBytes -= 4;
+  }
+
+  // Emit nops.
+  for (unsigned i = 0; i < NumNOPBytes; i += 4)
+    EmitToStreamer(*OutStreamer,
+		  MCInstBuilder(RISCV::ADDI)
+		   .addReg(RISCV::X0)
+		   .addReg(RISCV::X0)
+		   .addImm(0));
+}
+
 // Force static initialization.
 extern "C" void LLVMInitializeRISCVAsmPrinter() {
   RegisterAsmPrinter<RISCVAsmPrinter> X(getTheRISCV32Target());
diff --git a/llvm/lib/Target/RISCV/RISCVFrameLowering.cpp b/llvm/lib/Target/RISCV/RISCVFrameLowering.cpp
index bbaa16c0863..aedd4988409 100644
--- a/llvm/lib/Target/RISCV/RISCVFrameLowering.cpp
+++ b/llvm/lib/Target/RISCV/RISCVFrameLowering.cpp
@@ -26,9 +26,10 @@ bool RISCVFrameLowering::hasFP(const MachineFunction &MF) const {
   const TargetRegisterInfo *RegInfo = MF.getSubtarget().getRegisterInfo();
 
   const MachineFrameInfo &MFI = MF.getFrameInfo();
+
   return MF.getTarget().Options.DisableFramePointerElim(MF) ||
          RegInfo->needsStackRealignment(MF) || MFI.hasVarSizedObjects() ||
-         MFI.isFrameAddressTaken();
+         MFI.isFrameAddressTaken() || MFI.hasPcnStackMap();
 }
 
 // Determines the size of the frame and maximum call frame size.
@@ -311,6 +312,49 @@ int RISCVFrameLowering::getFrameIndexReference(const MachineFunction &MF,
   return Offset;
 }
 
+int RISCVFrameLowering::getFrameIndexReferenceFromFP(const MachineFunction &MF,
+						     int FI,
+						     unsigned &FrameReg) const {
+  const MachineFrameInfo &MFI = MF.getFrameInfo();
+  const TargetRegisterInfo *RI = MF.getSubtarget().getRegisterInfo();
+  const TargetFrameLowering *TFL = MF.getSubtarget().getFrameLowering();
+  const auto *RVFI = MF.getInfo<RISCVMachineFunctionInfo>();
+
+  // Callee-saved registers should be referenced relative to the stack
+  // pointer (positive offset), otherwise use the frame pointer (negative
+  // offset).
+  const std::vector<CalleeSavedInfo> &CSI = MFI.getCalleeSavedInfo();
+  int MinCSFI = 0;
+  int MaxCSFI = -1;
+
+  int Offset = MFI.getObjectOffset(FI) - getOffsetOfLocalArea() +
+               MFI.getOffsetAdjustment();
+
+  if (CSI.size()) {
+    MinCSFI = CSI[0].getFrameIdx();
+    MaxCSFI = CSI[CSI.size() - 1].getFrameIdx();
+  }
+
+  if (!TFL->hasFP(MF) && FI >= MinCSFI && FI <= MaxCSFI) {
+    FrameReg = RISCV::X2;
+    Offset += MF.getFrameInfo().getStackSize();
+  } else if (RI->needsStackRealignment(MF)) {
+    assert(!MFI.hasVarSizedObjects() &&
+           "Unexpected combination of stack realignment and varsized objects");
+    // If the stack was realigned, the frame pointer is set in order to allow
+    // SP to be restored, but we still access stack objects using SP.
+    FrameReg = RISCV::X2;
+    Offset += MF.getFrameInfo().getStackSize();
+  } else {
+    FrameReg = RI->getFrameRegister(MF);
+    if (hasFP(MF))
+      Offset += RVFI->getVarArgsSaveSize();
+    else
+      Offset += MF.getFrameInfo().getStackSize();
+  }
+  return Offset;
+}
+
 void RISCVFrameLowering::determineCalleeSaves(MachineFunction &MF,
                                               BitVector &SavedRegs,
                                               RegScavenger *RS) const {
diff --git a/llvm/lib/Target/RISCV/RISCVFrameLowering.h b/llvm/lib/Target/RISCV/RISCVFrameLowering.h
index 0e045c3ff85..6cf2c93ecc1 100644
--- a/llvm/lib/Target/RISCV/RISCVFrameLowering.h
+++ b/llvm/lib/Target/RISCV/RISCVFrameLowering.h
@@ -31,6 +31,8 @@ public:
 
   int getFrameIndexReference(const MachineFunction &MF, int FI,
                              unsigned &FrameReg) const override;
+  int getFrameIndexReferenceFromFP(const MachineFunction &MF, int FI,
+				   unsigned &FrameReg) const override;
 
   void determineCalleeSaves(MachineFunction &MF, BitVector &SavedRegs,
                             RegScavenger *RS) const override;
diff --git a/llvm/lib/Target/RISCV/RISCVISelLowering.cpp b/llvm/lib/Target/RISCV/RISCVISelLowering.cpp
index 2b0f64fa6db..f550498aee2 100644
--- a/llvm/lib/Target/RISCV/RISCVISelLowering.cpp
+++ b/llvm/lib/Target/RISCV/RISCVISelLowering.cpp
@@ -392,6 +392,9 @@ SDValue RISCVTargetLowering::LowerOperation(SDValue Op,
     SDValue FPConv = DAG.getNode(RISCVISD::FMV_W_X_RV64, DL, MVT::f32, NewOp0);
     return FPConv;
   }
+  case (uint16_t)~TargetOpcode::STACKMAP:
+  case (uint16_t)~TargetOpcode::PCN_STACKMAP:
+    return SDValue(); // Use generic stackmap type legalizer
   }
 }
 
@@ -1332,6 +1335,9 @@ RISCVTargetLowering::EmitInstrWithCustomInserter(MachineInstr &MI,
     return emitBuildPairF64Pseudo(MI, BB);
   case RISCV::SplitF64Pseudo:
     return emitSplitF64Pseudo(MI, BB);
+  case TargetOpcode::STACKMAP:
+  case TargetOpcode::PCN_STACKMAP:
+    return emitPatchPoint(MI, BB);
   }
 }
 
diff --git a/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp b/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp
index 99c8d2ef73d..f9936bab26c 100644
--- a/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp
+++ b/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp
@@ -20,6 +20,7 @@
 #include "llvm/CodeGen/MachineInstrBuilder.h"
 #include "llvm/CodeGen/MachineRegisterInfo.h"
 #include "llvm/CodeGen/RegisterScavenging.h"
+#include "llvm/CodeGen/StackMaps.h"
 #include "llvm/Support/ErrorHandling.h"
 #include "llvm/Support/TargetRegistry.h"
 
@@ -451,6 +452,12 @@ unsigned RISCVInstrInfo::getInstSizeInBytes(const MachineInstr &MI) const {
     return getInlineAsmLength(MI.getOperand(0).getSymbolName(),
                               *TM.getMCAsmInfo());
   }
+  case TargetOpcode::STACKMAP:
+  case TargetOpcode::PCN_STACKMAP:
+    {
+      StackMapOpers Opers(&MI);
+      return Opers.getNumPatchBytes();
+    }
   }
 }
 
diff --git a/llvm/lib/Target/RISCV/RISCVInstrInfo.td b/llvm/lib/Target/RISCV/RISCVInstrInfo.td
index 69bde15f121..1c4badc6d81 100644
--- a/llvm/lib/Target/RISCV/RISCVInstrInfo.td
+++ b/llvm/lib/Target/RISCV/RISCVInstrInfo.td
@@ -28,7 +28,7 @@ def SDT_RISCVSelectCC : SDTypeProfile<1, 5, [SDTCisSameAs<1, 2>,
 
 // Target-independent nodes, but with target-specific formats.
 def callseq_start : SDNode<"ISD::CALLSEQ_START", SDT_CallSeqStart,
-                           [SDNPHasChain, SDNPOutGlue]>;
+                           [SDNPHasChain, SDNPInGlue, SDNPOutGlue]>;
 def callseq_end   : SDNode<"ISD::CALLSEQ_END", SDT_CallSeqEnd,
                            [SDNPHasChain, SDNPOptInGlue, SDNPOutGlue]>;
 
diff --git a/llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp b/llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp
index e6a126e3e51..f2217e296ea 100644
--- a/llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp
+++ b/llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp
@@ -105,6 +105,16 @@ void RISCVRegisterInfo::eliminateFrameIndex(MachineBasicBlock::iterator II,
   MachineBasicBlock &MBB = *MI.getParent();
   bool FrameRegIsKill = false;
 
+  // Special handling of dbg_value, stackmap and patchpoint instructions.
+  if (MI.isDebugValue() || MI.getOpcode() == TargetOpcode::STACKMAP ||
+      MI.getOpcode() == TargetOpcode::PATCHPOINT ||
+      MI.getOpcode() == TargetOpcode::PCN_STACKMAP) {
+    Offset += MI.getOperand(FIOperandNum + 1).getImm();
+    MI.getOperand(FIOperandNum).ChangeToRegister(FrameReg, false /*isDef*/);
+    MI.getOperand(FIOperandNum + 1).ChangeToImmediate(Offset);
+    return;
+  }
+
   if (!isInt<12>(Offset)) {
     assert(isInt<32>(Offset) && "Int32 expected");
     // The offset won't fit in an immediate, so use a scratch register instead
@@ -129,6 +139,23 @@ Register RISCVRegisterInfo::getFrameRegister(const MachineFunction &MF) const {
   return TFI->hasFP(MF) ? RISCV::X8 : RISCV::X2;
 }
 
+int RISCVRegisterInfo::getReturnAddrLoc(const MachineFunction &MF,
+                                          unsigned &BaseReg) const {
+  const TargetFrameLowering *TFL = MF.getSubtarget().getFrameLowering();
+  const MachineFrameInfo *MFI = &MF.getFrameInfo();
+  assert(MFI->isCalleeSavedInfoValid() && "No callee-saved information");
+  const std::vector<CalleeSavedInfo> &CSI = MFI->getCalleeSavedInfo();
+
+  // The return address' location is the the link register's spill slot
+  for(unsigned i = 0, e = CSI.size(); i < e; i++)
+    if(CSI[i].getReg() == RISCV::X1)
+      return TFL->getFrameIndexReference(MF, CSI[i].getFrameIdx(), BaseReg);
+
+  // We didn't find it, is it actually saved?
+  BaseReg = 0;
+  return INT32_MAX;
+}
+
 const uint32_t *
 RISCVRegisterInfo::getCallPreservedMask(const MachineFunction & MF,
                                         CallingConv::ID /*CC*/) const {
diff --git a/llvm/lib/Target/RISCV/RISCVRegisterInfo.h b/llvm/lib/Target/RISCV/RISCVRegisterInfo.h
index 56a50fe6ddc..c628f7844b6 100644
--- a/llvm/lib/Target/RISCV/RISCVRegisterInfo.h
+++ b/llvm/lib/Target/RISCV/RISCVRegisterInfo.h
@@ -41,6 +41,9 @@ struct RISCVRegisterInfo : public RISCVGenRegisterInfo {
 
   Register getFrameRegister(const MachineFunction &MF) const override;
 
+  int getReturnAddrLoc(const MachineFunction &MF,
+		       unsigned &BaseReg) const override;
+
   bool requiresRegisterScavenging(const MachineFunction &MF) const override {
     return true;
   }
diff --git a/llvm/lib/Target/RISCV/RISCVSubtarget.h b/llvm/lib/Target/RISCV/RISCVSubtarget.h
index 106ff49f021..cf6f0002df6 100644
--- a/llvm/lib/Target/RISCV/RISCVSubtarget.h
+++ b/llvm/lib/Target/RISCV/RISCVSubtarget.h
@@ -16,6 +16,7 @@
 #include "RISCVFrameLowering.h"
 #include "RISCVISelLowering.h"
 #include "RISCVInstrInfo.h"
+#include "RISCVValues.h"
 #include "Utils/RISCVBaseInfo.h"
 #include "llvm/CodeGen/SelectionDAGTargetInfo.h"
 #include "llvm/CodeGen/TargetSubtargetInfo.h"
@@ -45,6 +46,7 @@ class RISCVSubtarget : public RISCVGenSubtargetInfo {
   RISCVInstrInfo InstrInfo;
   RISCVRegisterInfo RegInfo;
   RISCVTargetLowering TLInfo;
+  RISCVValues VGen;
   SelectionDAGTargetInfo TSInfo;
 
   /// Initializes using the passed in CPU and feature strings so that we can
@@ -75,6 +77,9 @@ public:
   const SelectionDAGTargetInfo *getSelectionDAGInfo() const override {
     return &TSInfo;
   }
+  const RISCVValues *getValues() const override {
+    return &VGen;
+  }
   bool hasStdExtM() const { return HasStdExtM; }
   bool hasStdExtA() const { return HasStdExtA; }
   bool hasStdExtF() const { return HasStdExtF; }
diff --git a/llvm/lib/Target/RISCV/RISCVTargetObjectFile.cpp b/llvm/lib/Target/RISCV/RISCVTargetObjectFile.cpp
index bbd45c970d3..df182d4c6f7 100644
--- a/llvm/lib/Target/RISCV/RISCVTargetObjectFile.cpp
+++ b/llvm/lib/Target/RISCV/RISCVTargetObjectFile.cpp
@@ -29,6 +29,9 @@ void RISCVELFTargetObjectFile::Initialize(MCContext &Ctx,
 // small section size threshold. Data in this section could be addressed by
 // using gp_rel operator.
 bool RISCVELFTargetObjectFile::isInSmallSection(uint64_t Size) const {
+  // FIXME: Use .data for everything to match other ISAs in Popcorn Linux.
+  return false;
+
   // gcc has traditionally not treated zero-sized objects as small data, so this
   // is effectively part of the ABI.
   return Size > 0 && Size <= SSThreshold;
@@ -38,6 +41,9 @@ bool RISCVELFTargetObjectFile::isInSmallSection(uint64_t Size) const {
 // section.
 bool RISCVELFTargetObjectFile::isGlobalInSmallSection(
     const GlobalObject *GO, const TargetMachine &TM) const {
+  // FIXME: Use .data for everything to match other ISAs in Popcorn Linux.
+  return false;
+  
   // Only global variables, not functions.
   const GlobalVariable *GVA = dyn_cast<GlobalVariable>(GO);
   if (!GVA)
@@ -74,11 +80,11 @@ bool RISCVELFTargetObjectFile::isGlobalInSmallSection(
 
 MCSection *RISCVELFTargetObjectFile::SelectSectionForGlobal(
     const GlobalObject *GO, SectionKind Kind, const TargetMachine &TM) const {
-  // Handle Small Section classification here.
-  if (Kind.isBSS() && isGlobalInSmallSection(GO, TM))
-    return SmallBSSSection;
-  if (Kind.isData() && isGlobalInSmallSection(GO, TM))
-    return SmallDataSection;
+//  // Handle Small Section classification here.
+//  if (Kind.isBSS() && isGlobalInSmallSection(GO, TM))
+//    return SmallBSSSection;
+//  if (Kind.isData() && isGlobalInSmallSection(GO, TM))
+//    return SmallDataSection;
 
   // Otherwise, we work the same as ELF.
   return TargetLoweringObjectFileELF::SelectSectionForGlobal(GO, Kind, TM);
@@ -100,14 +106,17 @@ void RISCVELFTargetObjectFile::getModuleMetadata(Module &M) {
 /// Return true if this constant should be placed into small data section.
 bool RISCVELFTargetObjectFile::isConstantInSmallSection(
     const DataLayout &DL, const Constant *CN) const {
+  // FIXME: Use .data for everything to match other ISAs in Popcorn Linux.
+  return false;
   return isInSmallSection(DL.getTypeAllocSize(CN->getType()));
 }
 
 MCSection *RISCVELFTargetObjectFile::getSectionForConstant(
     const DataLayout &DL, SectionKind Kind, const Constant *C,
     unsigned &Align) const {
-  if (isConstantInSmallSection(DL, C))
-    return SmallDataSection;
+  // FIXME: Use .data for everything to match other ISAs in Popcorn Linux.
+//  if (isConstantInSmallSection(DL, C))
+//    return SmallDataSection;
 
   // Otherwise, we work the same as ELF.
   return TargetLoweringObjectFileELF::getSectionForConstant(DL, Kind, C, Align);
diff --git a/llvm/lib/Target/RISCV/RISCVValues.cpp b/llvm/lib/Target/RISCV/RISCVValues.cpp
new file mode 100644
index 00000000000..a0f4ca46761
--- /dev/null
+++ b/llvm/lib/Target/RISCV/RISCVValues.cpp
@@ -0,0 +1,175 @@
+//===- RISCVTargetValues.cpp - RISCV specific value generator -===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "RISCVValues.h"
+#include "RISCV.h"
+#include "llvm/CodeGen/MachineConstantPool.h"
+#include "llvm/CodeGen/TargetInstrInfo.h"
+#include "llvm/CodeGen/TargetSubtargetInfo.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+#define DEBUG_TYPE "stacktransform"
+
+using namespace llvm;
+
+static TemporaryValue *getTemporaryReference(const MachineInstr *MI,
+                                             const VirtRegMap *VRM,
+                                             unsigned Size) {
+  TemporaryValue *Val = nullptr;
+  if(MI->getOperand(0).isReg()) {
+    // Instruction format:    ADDI  xd    xn    imm#
+    // Stack slot reference:                <fi>  0 
+    if(MI->getOperand(1).isFI() &&
+       MI->getOperand(2).isImm() && MI->getOperand(2).getImm() == 0) {
+      Val = new TemporaryValue;
+      Val->Type = TemporaryValue::StackSlotRef;
+      Val->Size = Size;
+      Val->Vreg = MI->getOperand(0).getReg();
+      Val->StackSlot = MI->getOperand(1).getIndex();
+      Val->Offset = 0;
+    }
+  }
+
+  return Val;
+}
+
+TemporaryValuePtr
+RISCVValues::getTemporaryValue(const MachineInstr *MI,
+                                 const VirtRegMap *VRM) const {
+  TemporaryValue *Val = nullptr;
+  switch(MI->getOpcode()) {
+  case RISCV::ADDI: Val = getTemporaryReference(MI, VRM, 8); break;
+  default: break;
+  }
+  return TemporaryValuePtr(Val);
+}
+
+typedef ValueGenInst::InstType InstType;
+template <InstType T> using RegInstruction = RegInstruction<T>;
+template <InstType T> using ImmInstruction = ImmInstruction<T>;
+
+// Bitwise-conversions between floats & ints
+union IntFloat64 { double d; uint64_t i; };
+union IntFloat32 { float f; uint64_t i; };
+
+MachineLiveVal *
+RISCVValues::
+genADDInstructions(const MachineInstr *MI) const {
+  int Index;
+  const MachineOperand *MO;
+
+  switch(MI->getOpcode()) {
+  case RISCV::ADDI:
+    MO = &MI->getOperand(2);
+    if(MI->getOperand(1).isFI()) {
+      Index = MI->getOperand(1).getIndex();
+      assert(MI->getOperand(2).isImm() && MI->getOperand(2).getImm() == 0);
+      return new MachineStackObject(Index, false, MI, true);
+    } else if (TargetValues::isSymbolValue(MO))
+      return new MachineSymbolRef(*MO, false, MI);
+    break;
+  default:
+    LLVM_DEBUG(dbgs() << "Unhandled ADD machine instruction");
+    break;
+  }
+  return nullptr;
+}
+
+MachineLiveVal *
+RISCVValues::genLoadRegValue(const MachineInstr *MI) const {
+  switch(MI->getOpcode()) {
+  case RISCV::LWU:
+    if(MI->getOperand(2).isCPI()) {
+      int Idx = MI->getOperand(2).getIndex();
+      const MachineFunction *MF = MI->getParent()->getParent();
+      const MachineConstantPool *MCP = MF->getConstantPool();
+      const std::vector<MachineConstantPoolEntry> &CP = MCP->getConstants();
+      if(CP[Idx].isMachineConstantPoolEntry()) {
+        // TODO unhandled for now
+      }
+      else {
+        const Constant *Val = CP[Idx].Val.ConstVal;
+        if(isa<ConstantFP>(Val)) {
+          const ConstantFP *FPVal = cast<ConstantFP>(Val);
+          const APFloat &Flt = FPVal->getValueAPF();
+          switch(APFloat::getSizeInBits(Flt.getSemantics())) {
+          case 32: {
+            IntFloat32 I2F = { Flt.convertToFloat() };
+            return new MachineImmediate(4, I2F.i, MI, false);
+          }
+          case 64: {
+            IntFloat64 I2D = { Flt.convertToDouble() };
+            return new MachineImmediate(8, I2D.i, MI, false);
+          }
+          default: break;
+          }
+        }
+      }
+    }
+    break;
+  case RISCV::LD:
+    // Note: if this is of the form %vreg, <ga:...>, then the compiler has
+    // emitted multiple instructions in order to form the full address.  We,
+    // however, don't have the instruction encoding limitations.
+    // TODO verify this note above is true, maybe using MO::getTargetFlags?
+    // Note 2: we *must* ensure the symbol is const-qualified, otherwise we
+    // risk creating a new value if the symbol's value changes between when the
+    // initial load would have occurred and the transformation, e.g.,
+    //
+    //   ldr x20, <ga:mysym>
+    //   ... (somebody changes mysym's value) ...
+    //   bl <ga:myfunc>
+    //
+    // In this situation, the transformation occurs at the call site and
+    // retrieves the updated value rather than the value that would have been
+    // loaded at the ldr instruction.
+    if(TargetValues::isSymbolValue(MI->getOperand(2)) &&
+       TargetValues::isSymbolValueConstant(MI->getOperand(2)))
+      return new MachineSymbolRef(MI->getOperand(2), true, MI);
+    break;
+  default: break;
+  }
+  return nullptr;
+}
+
+MachineLiveValPtr RISCVValues::getMachineValue(const MachineInstr *MI) const {
+  IntFloat64 Conv64;
+  MachineLiveVal* Val = nullptr;
+  const MachineOperand *MO;
+  const TargetInstrInfo *TII;
+
+  switch(MI->getOpcode()) {
+  case RISCV::ADDI:
+    Val = genADDInstructions(MI);
+    break;
+  case RISCV::PseudoLLA:
+  case RISCV::PseudoLA:
+  case RISCV::PseudoLA_TLS_IE:
+  case RISCV::PseudoLA_TLS_GD:
+    break;
+  case RISCV::COPY:
+    MO = &MI->getOperand(1);
+    if(MO->isReg() && MO->getReg() == RISCV::X1) Val = new ReturnAddress(MI);
+    break;
+  case RISCV::LWU: // load 32-bit value, zero-extend
+  case RISCV::LD: // load 64-bit value
+    Val = genLoadRegValue(MI);
+    break;
+  default:
+    TII =  MI->getParent()->getParent()->getSubtarget().getInstrInfo();
+    LLVM_DEBUG(dbgs() << "Unhandled opcode: "
+                 << TII->getName(MI->getOpcode()) << "\n");
+    break;
+  }
+
+  return MachineLiveValPtr(Val);
+}
diff --git a/llvm/lib/Target/RISCV/RISCVValues.h b/llvm/lib/Target/RISCV/RISCVValues.h
new file mode 100644
index 00000000000..1b29629ce03
--- /dev/null
+++ b/llvm/lib/Target/RISCV/RISCVValues.h
@@ -0,0 +1,28 @@
+//===----- AArch64TargetValues.cpp - AArch64 specific value generator -----===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Target/TargetValues.h"
+
+namespace llvm {
+
+class RISCVValues final : public TargetValues {
+public:
+  RISCVValues() {}
+  virtual TemporaryValuePtr getTemporaryValue(const MachineInstr *MI,
+                                              const VirtRegMap *VRM) const;
+  virtual MachineLiveValPtr getMachineValue(const MachineInstr *MI) const;
+
+private:
+  MachineLiveVal *genADDInstructions(const MachineInstr *MI) const;
+  MachineLiveVal *genADRPInstructions(const MachineInstr *MI) const;
+  MachineLiveVal *genBitfieldInstructions(const MachineInstr *MI) const;
+  MachineLiveVal *genLoadRegValue(const MachineInstr *MI) const;
+};
+
+}
diff --git a/llvm/lib/Target/TargetValues.cpp b/llvm/lib/Target/TargetValues.cpp
new file mode 100644
index 00000000000..9d6e8c73b15
--- /dev/null
+++ b/llvm/lib/Target/TargetValues.cpp
@@ -0,0 +1,40 @@
+//===--------- TargetValues.cpp - Target value generator helpers ----------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/IR/GlobalVariable.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Target/TargetValues.h"
+
+#define DEBUG_TYPE "stacktransform"
+
+using namespace llvm;
+
+bool TargetValues::isSymbolValueConstant(const MachineOperand &MO) {
+  const GlobalValue *GV;
+  const GlobalVariable *GVar;
+
+  switch(MO.getType()) {
+  case MachineOperand::MO_GlobalAddress:
+    GV = MO.getGlobal();
+    if(isa<Function>(GV)) return true;
+    else if((GVar = dyn_cast<GlobalVariable>(GV)) && GVar->isConstant())
+      return true;
+    break;
+  case MachineOperand::MO_ExternalSymbol:
+    // TODO
+    break;
+  case MachineOperand::MO_MCSymbol:
+    // TODO
+    break;
+  default:
+    LLVM_DEBUG(dbgs() << "Unhandled reference type\n");
+    break;
+  }
+  return false;
+}
diff --git a/llvm/lib/Target/X86/CMakeLists.txt b/llvm/lib/Target/X86/CMakeLists.txt
index ed34a59df4a..7d1fe41b3de 100644
--- a/llvm/lib/Target/X86/CMakeLists.txt
+++ b/llvm/lib/Target/X86/CMakeLists.txt
@@ -68,6 +68,7 @@ set(sources
   X86VZeroUpper.cpp
   X86WinAllocaExpander.cpp
   X86WinEHState.cpp
+  X86Values.cpp
   )
 
 add_llvm_target(X86CodeGen ${sources})
diff --git a/llvm/lib/Target/X86/X86AsmPrinter.cpp b/llvm/lib/Target/X86/X86AsmPrinter.cpp
index 80120722e0e..ab39b048c3d 100644
--- a/llvm/lib/Target/X86/X86AsmPrinter.cpp
+++ b/llvm/lib/Target/X86/X86AsmPrinter.cpp
@@ -24,6 +24,7 @@
 #include "llvm/CodeGen/MachineModuleInfoImpls.h"
 #include "llvm/CodeGen/TargetLoweringObjectFileImpl.h"
 #include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/DiagnosticInfo.h"
 #include "llvm/IR/InlineAsm.h"
 #include "llvm/IR/Mangler.h"
 #include "llvm/IR/Module.h"
@@ -44,7 +45,8 @@ using namespace llvm;
 
 X86AsmPrinter::X86AsmPrinter(TargetMachine &TM,
                              std::unique_ptr<MCStreamer> Streamer)
-    : AsmPrinter(TM, std::move(Streamer)), SM(*this), FM(*this) {}
+  : AsmPrinter(TM, std::move(Streamer)), SM(*this), PSM(*this), UI(*this),
+    FM(*this) {}
 
 //===----------------------------------------------------------------------===//
 // Primitive Helper Functions.
@@ -55,6 +57,8 @@ X86AsmPrinter::X86AsmPrinter(TargetMachine &TM,
 bool X86AsmPrinter::runOnMachineFunction(MachineFunction &MF) {
   Subtarget = &MF.getSubtarget<X86Subtarget>();
 
+  bool modified = TagCallSites(MF);
+
   SMShadowTracker.startFunction(MF);
   CodeEmitter.reset(TM.getTarget().createMCCodeEmitter(
       *Subtarget->getInstrInfo(), *Subtarget->getRegisterInfo(),
@@ -83,8 +87,17 @@ bool X86AsmPrinter::runOnMachineFunction(MachineFunction &MF) {
 
   EmitFPOData = false;
 
-  // We didn't modify anything.
-  return false;
+  // Add this function's register unwind info.  The x86 backend doesn't
+  // maintain the saved FBP (old RBP) and return address (RIP) as callee-saved
+  // registers, so manually add where they're saved.
+  if(MF.getFrameInfo().hasPcnStackMap()) {
+    UI.recordUnwindInfo(MF);
+    UI.addRegisterUnwindInfo(MF, X86::RIP, 8);
+    UI.addRegisterUnwindInfo(MF, X86::RBP, 0);
+  }
+
+  // We may have modified where stack map intrinsics are located.
+  return modified;
 }
 
 void X86AsmPrinter::EmitFunctionBodyStart() {
@@ -718,7 +731,10 @@ void X86AsmPrinter::EmitEndOfAsmFile(Module &M) {
     emitStackMaps(SM);
   } else if (TT.isOSBinFormatELF()) {
     emitStackMaps(SM);
+    UI.serializeToUnwindInfoSection();
+    PSM.serializeToPcnStackMapSection(&UI);
     FM.serializeToFaultMapSection();
+    UI.reset(); // Must reset after PSM serialization to clear metadata
   }
 }
 
diff --git a/llvm/lib/Target/X86/X86AsmPrinter.h b/llvm/lib/Target/X86/X86AsmPrinter.h
index a011310970b..50c675d8a6e 100644
--- a/llvm/lib/Target/X86/X86AsmPrinter.h
+++ b/llvm/lib/Target/X86/X86AsmPrinter.h
@@ -13,6 +13,7 @@
 #include "llvm/CodeGen/AsmPrinter.h"
 #include "llvm/CodeGen/FaultMaps.h"
 #include "llvm/CodeGen/StackMaps.h"
+#include "llvm/CodeGen/UnwindInfo.h"
 #include "llvm/MC/MCCodeEmitter.h"
 #include "llvm/Target/TargetMachine.h"
 
@@ -27,7 +28,8 @@ class MCSymbol;
 
 class LLVM_LIBRARY_VISIBILITY X86AsmPrinter : public AsmPrinter {
   const X86Subtarget *Subtarget;
-  StackMaps SM;
+  StackMaps SM, PSM;
+  UnwindInfo UI;
   FaultMaps FM;
   std::unique_ptr<MCCodeEmitter> CodeEmitter;
   bool EmitFPOData = false;
@@ -81,6 +83,7 @@ class LLVM_LIBRARY_VISIBILITY X86AsmPrinter : public AsmPrinter {
   // the number of NOPs used for stackmap padding.
   void EmitAndCountInstruction(MCInst &Inst);
   void LowerSTACKMAP(const MachineInstr &MI);
+  void LowerPCN_STACKMAP(const MachineInstr &MI);
   void LowerPATCHPOINT(const MachineInstr &MI, X86MCInstLower &MCIL);
   void LowerSTATEPOINT(const MachineInstr &MI, X86MCInstLower &MCIL);
   void LowerFAULTING_OP(const MachineInstr &MI, X86MCInstLower &MCIL);
diff --git a/llvm/lib/Target/X86/X86FrameLowering.cpp b/llvm/lib/Target/X86/X86FrameLowering.cpp
index 75cd92b3adc..44877614bac 100644
--- a/llvm/lib/Target/X86/X86FrameLowering.cpp
+++ b/llvm/lib/Target/X86/X86FrameLowering.cpp
@@ -88,7 +88,7 @@ bool X86FrameLowering::hasFP(const MachineFunction &MF) const {
           MFI.isFrameAddressTaken() || MFI.hasOpaqueSPAdjustment() ||
           MF.getInfo<X86MachineFunctionInfo>()->getForceFramePointer() ||
           MF.callsUnwindInit() || MF.hasEHFunclets() || MF.callsEHReturn() ||
-          MFI.hasStackMap() || MFI.hasPatchPoint() ||
+          MFI.hasStackMap() || MFI.hasPatchPoint() || MFI.hasPcnStackMap() ||
           MFI.hasCopyImplyingStackAdjustment());
 }
 
diff --git a/llvm/lib/Target/X86/X86ISelLowering.cpp b/llvm/lib/Target/X86/X86ISelLowering.cpp
index 920cdd7e625..1d19c6940e3 100644
--- a/llvm/lib/Target/X86/X86ISelLowering.cpp
+++ b/llvm/lib/Target/X86/X86ISelLowering.cpp
@@ -27301,6 +27301,9 @@ SDValue X86TargetLowering::LowerOperation(SDValue Op, SelectionDAG &DAG) const {
   case ISD::GC_TRANSITION_START:
                                 return LowerGC_TRANSITION_START(Op, DAG);
   case ISD::GC_TRANSITION_END:  return LowerGC_TRANSITION_END(Op, DAG);
+  case (uint16_t)~TargetOpcode::STACKMAP:
+  case (uint16_t)~TargetOpcode::PCN_STACKMAP:
+    return SDValue(); // Use generic stackmap type legalizer
   }
 }
 
@@ -31008,6 +31011,7 @@ X86TargetLowering::EmitInstrWithCustomInserter(MachineInstr &MI,
     return emitPatchPoint(MI, BB);
 
   case TargetOpcode::STACKMAP:
+  case TargetOpcode::PCN_STACKMAP:
   case TargetOpcode::PATCHPOINT:
     return emitPatchPoint(MI, BB);
 
diff --git a/llvm/lib/Target/X86/X86InstrInfo.td b/llvm/lib/Target/X86/X86InstrInfo.td
index 8e05dd8ec5c..1a2d95d27d4 100644
--- a/llvm/lib/Target/X86/X86InstrInfo.td
+++ b/llvm/lib/Target/X86/X86InstrInfo.td
@@ -196,7 +196,7 @@ def X86vaarg64 :
                          SDNPMemOperand]>;
 def X86callseq_start :
                  SDNode<"ISD::CALLSEQ_START", SDT_X86CallSeqStart,
-                        [SDNPHasChain, SDNPOutGlue]>;
+                        [SDNPHasChain, SDNPOptInGlue, SDNPOutGlue]>;
 def X86callseq_end :
                  SDNode<"ISD::CALLSEQ_END",   SDT_X86CallSeqEnd,
                         [SDNPHasChain, SDNPOptInGlue, SDNPOutGlue]>;
diff --git a/llvm/lib/Target/X86/X86MCInstLower.cpp b/llvm/lib/Target/X86/X86MCInstLower.cpp
index b1fefaa84be..7acab67e48d 100644
--- a/llvm/lib/Target/X86/X86MCInstLower.cpp
+++ b/llvm/lib/Target/X86/X86MCInstLower.cpp
@@ -1043,6 +1043,15 @@ void X86AsmPrinter::LowerSTACKMAP(const MachineInstr &MI) {
   SMShadowTracker.reset(NumShadowBytes);
 }
 
+// Lower a stackmap of the form:
+// <id>, <shadowBytes>, ...
+void X86AsmPrinter::LowerPCN_STACKMAP(const MachineInstr &MI) {
+  SMShadowTracker.emitShadowPadding(*OutStreamer, getSubtargetInfo());
+  PSM.recordPcnStackMap(MI);
+  unsigned NumShadowBytes = MI.getOperand(1).getImm();
+  SMShadowTracker.reset(NumShadowBytes);
+}
+
 // Lower a patchpoint of the form:
 // [<def>], <id>, <numBytes>, <target>, <numArgs>, <cc>, ...
 void X86AsmPrinter::LowerPATCHPOINT(const MachineInstr &MI,
@@ -1847,6 +1856,9 @@ void X86AsmPrinter::EmitInstruction(const MachineInstr *MI) {
   case TargetOpcode::STACKMAP:
     return LowerSTACKMAP(*MI);
 
+  case TargetOpcode::PCN_STACKMAP:
+    return LowerPCN_STACKMAP(*MI);
+
   case TargetOpcode::PATCHPOINT:
     return LowerPATCHPOINT(*MI, MCInstLowering);
 
diff --git a/llvm/lib/Target/X86/X86RegisterInfo.cpp b/llvm/lib/Target/X86/X86RegisterInfo.cpp
index c8966dfffa0..d154b96419e 100644
--- a/llvm/lib/Target/X86/X86RegisterInfo.cpp
+++ b/llvm/lib/Target/X86/X86RegisterInfo.cpp
@@ -759,7 +759,8 @@ X86RegisterInfo::eliminateFrameIndex(MachineBasicBlock::iterator II,
 
   // The frame index format for stackmaps and patchpoints is different from the
   // X86 format. It only has a FI and an offset.
-  if (Opc == TargetOpcode::STACKMAP || Opc == TargetOpcode::PATCHPOINT) {
+  if (Opc == TargetOpcode::STACKMAP || Opc == TargetOpcode::PATCHPOINT
+      || Opc == TargetOpcode::PCN_STACKMAP) {
     assert(BasePtr == FramePtr && "Expected the FP as base register");
     int64_t Offset = MI.getOperand(FIOperandNum + 1).getImm() + FIOffset;
     MI.getOperand(FIOperandNum + 1).ChangeToImmediate(Offset);
@@ -804,3 +805,10 @@ X86RegisterInfo::getPtrSizedStackRegister(const MachineFunction &MF) const {
     StackReg = getX86SubSuperRegister(StackReg, 32);
   return StackReg;
 }
+
+int X86RegisterInfo::getReturnAddrLoc(const MachineFunction &MF,
+                                      unsigned &BaseReg) const {
+  const X86MachineFunctionInfo *X86FI = MF.getInfo<X86MachineFunctionInfo>();
+  const TargetFrameLowering *TFL = MF.getSubtarget().getFrameLowering();
+  return TFL->getFrameIndexReference(MF, X86FI->getRAIndex(), BaseReg);
+}
diff --git a/llvm/lib/Target/X86/X86RegisterInfo.h b/llvm/lib/Target/X86/X86RegisterInfo.h
index b8292089806..bab19c3bac5 100644
--- a/llvm/lib/Target/X86/X86RegisterInfo.h
+++ b/llvm/lib/Target/X86/X86RegisterInfo.h
@@ -145,6 +145,9 @@ public:
   Register getFramePtr() const { return FramePtr; }
   // FIXME: Move to FrameInfok
   unsigned getSlotSize() const { return SlotSize; }
+
+  int getReturnAddrLoc(const MachineFunction &MF,
+                       unsigned &BaseReg) const override;
 };
 
 } // End llvm namespace
diff --git a/llvm/lib/Target/X86/X86Subtarget.h b/llvm/lib/Target/X86/X86Subtarget.h
index 24ccc9cb784..1bbd394e260 100644
--- a/llvm/lib/Target/X86/X86Subtarget.h
+++ b/llvm/lib/Target/X86/X86Subtarget.h
@@ -17,6 +17,7 @@
 #include "X86ISelLowering.h"
 #include "X86InstrInfo.h"
 #include "X86SelectionDAGInfo.h"
+#include "X86Values.h"
 #include "llvm/ADT/StringRef.h"
 #include "llvm/ADT/Triple.h"
 #include "llvm/CodeGen/GlobalISel/CallLowering.h"
@@ -485,6 +486,8 @@ private:
   X86TargetLowering TLInfo;
   X86FrameLowering FrameLowering;
 
+  X86Values VGen;
+
 public:
   /// This constructor initializes the data members to match that
   /// of the specified triple.
@@ -511,6 +514,7 @@ public:
   const X86RegisterInfo *getRegisterInfo() const override {
     return &getInstrInfo()->getRegisterInfo();
   }
+  const X86Values *getValues() const override { return &VGen; }
 
   /// Returns the minimum alignment known to hold of the
   /// stack frame on entry to the function and which must be maintained by every
diff --git a/llvm/lib/Target/X86/X86TargetTransformInfo.cpp b/llvm/lib/Target/X86/X86TargetTransformInfo.cpp
index 3dc59aeb263..d4ee726feab 100644
--- a/llvm/lib/Target/X86/X86TargetTransformInfo.cpp
+++ b/llvm/lib/Target/X86/X86TargetTransformInfo.cpp
@@ -2934,6 +2934,7 @@ int X86TTIImpl::getIntImmCost(Intrinsic::ID IID, unsigned Idx, const APInt &Imm,
       return TTI::TCC_Free;
     break;
   case Intrinsic::experimental_stackmap:
+  case Intrinsic::experimental_pcn_stackmap:
     if ((Idx < 2) || (Imm.getBitWidth() <= 64 && isInt<64>(Imm.getSExtValue())))
       return TTI::TCC_Free;
     break;
diff --git a/llvm/lib/Target/X86/X86Values.cpp b/llvm/lib/Target/X86/X86Values.cpp
new file mode 100644
index 00000000000..b37c5ba18ab
--- /dev/null
+++ b/llvm/lib/Target/X86/X86Values.cpp
@@ -0,0 +1,211 @@
+//===--------- X86TargetValues.cpp - X86 specific value generator ---------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "X86Values.h"
+#include "X86InstrInfo.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/CodeGen/TargetInstrInfo.h"
+#include "llvm/CodeGen/TargetSubtargetInfo.h"
+
+#define DEBUG_TYPE "stacktransform"
+
+using namespace llvm;
+
+static TemporaryValue *getTemporaryReference(const MachineInstr *MI,
+                                             const VirtRegMap *VRM,
+                                             unsigned Size) {
+  TemporaryValue *Val = nullptr;
+  if(MI->getOperand(0).isReg()) {
+    // Instruction format:  LEA64  rd     rbase  scale# ridx   disp#  rseg
+    // Stack slot reference:              <fi>   1      noreg  off    noreg
+    // TODO check for noreg in ridx & rseg?
+    if(MI->getOperand(1 + X86::AddrBaseReg).isFI() &&
+       MI->getOperand(1 + X86::AddrScaleAmt).isImm() &&
+       MI->getOperand(1 + X86::AddrScaleAmt).getImm() == 1) {
+      assert(MI->getOperand(1 + X86::AddrDisp).isImm() && "Invalid encoding");
+      Val = new TemporaryValue;
+      Val->Type = TemporaryValue::StackSlotRef;
+      Val->Size = Size;
+      Val->Vreg = MI->getOperand(0).getReg();
+      Val->StackSlot = MI->getOperand(1 + X86::AddrBaseReg).getIndex();
+      Val->Offset = MI->getOperand(1 + X86::AddrDisp).getImm();
+    }
+  }
+  return Val;
+}
+
+TemporaryValuePtr
+X86Values::getTemporaryValue(const MachineInstr *MI,
+                             const VirtRegMap *VRM) const {
+  TemporaryValue *Val = nullptr;
+  switch(MI->getOpcode()) {
+  case X86::LEA64r: Val = getTemporaryReference(MI, VRM, 8); break;
+  default: break;
+  }
+  return TemporaryValuePtr(Val);
+}
+
+typedef ValueGenInst::InstType InstType;
+template <InstType T> using RegInstruction = RegInstruction<T>;
+template <InstType T> using ImmInstruction = ImmInstruction<T>;
+
+/// Return whether the machine operand is a specific immediate value.
+static bool isImmOp(const MachineOperand &MO, int64_t Imm) {
+  if(!MO.isImm()) return false;
+  else if(MO.getImm() != Imm) return false;
+  else return true;
+}
+
+/// Return whether the machine operand is a specific register.
+static bool isRegOp(const MachineOperand &MO, unsigned Reg) {
+  if(!MO.isReg()) return false;
+  else if(MO.getReg() != Reg) return false;
+  else return true;
+}
+
+/// Return whether the machine operand is the sentinal %noreg register.
+static bool isNoRegOp(const MachineOperand &MO) { return isRegOp(MO, 0); }
+
+MachineLiveVal *X86Values::genLEAInstructions(const MachineInstr *MI) const {
+  unsigned Reg, Size;
+  int64_t Imm;
+  ValueGenInstList IL;
+
+  // TODO do we need to handle the segment register operand?
+  switch(MI->getOpcode()) {
+  case X86::LEA64r:
+    Size = 8;
+
+    if(MI->getOperand(1 + X86::AddrBaseReg).isFI()) {
+      // Stack slot address
+      if(!isImmOp(MI->getOperand(1 + X86::AddrScaleAmt), 1)) {
+        LLVM_DEBUG(dbgs() << "Unhandled scale amount for frame index\n");
+        break;
+      }
+
+      if(!isNoRegOp(MI->getOperand(1 + X86::AddrIndexReg))) {
+        LLVM_DEBUG(dbgs() <<  "Unhandled index register for frame index\n");
+        break;
+      }
+
+      if(!isImmOp(MI->getOperand(1 + X86::AddrDisp), 0)) {
+        LLVM_DEBUG(dbgs() << "Unhandled index register for frame index\n");
+        break;
+      }
+
+      return new
+        MachineStackObject(MI->getOperand(1 + X86::AddrBaseReg).getIndex(),
+                           false, MI, true);
+    }
+    else if(isRegOp(MI->getOperand(1 + X86::AddrBaseReg), X86::RIP)) {
+      // PC-relative symbol address
+      if(!isImmOp(MI->getOperand(1 + X86::AddrScaleAmt), 1)) {
+        LLVM_DEBUG(dbgs() << "Unhandled scale amount for PC-relative address\n");
+        break;
+      }
+
+      if(!isNoRegOp(MI->getOperand(1 + X86::AddrIndexReg))) {
+        LLVM_DEBUG(dbgs() << "Unhandled index register for PC-relative address\n");
+        break;
+      }
+
+      return new
+        MachineSymbolRef(MI->getOperand(1 + X86::AddrDisp), false, MI);
+    }
+    else {
+      // Raw form of LEA
+      if(!MI->getOperand(1 + X86::AddrBaseReg).isReg() ||
+         !MI->getOperand(1 + X86::AddrDisp).isImm()) {
+        LLVM_DEBUG(dbgs() << "Unhandled base register/displacement operands\n");
+        break;
+      }
+
+      // Initialize to index register * scale if indexing, or zero otherwise
+      Reg = MI->getOperand(1 + X86::AddrIndexReg).getReg();
+      if(Reg) {
+        Imm = MI->getOperand(1 + X86::AddrScaleAmt).getImm();
+        IL.emplace_back(new RegInstruction<InstType::Set>(Reg));
+        IL.emplace_back(new ImmInstruction<InstType::Multiply>(Size, Imm));
+      }
+      else IL.emplace_back(new ImmInstruction<InstType::Set>(Size, 0));
+
+      // Add the base register & displacement
+      Reg = MI->getOperand(1 + X86::AddrBaseReg).getReg();
+      Imm = MI->getOperand(1 + X86::AddrDisp).getImm();
+      IL.emplace_back(new RegInstruction<InstType::Add>(Reg));
+      IL.emplace_back(new ImmInstruction<InstType::Add>(Size, Imm));
+      return new MachineGeneratedVal(IL, MI, true);
+    }
+
+    break;
+  default:
+    LLVM_DEBUG(dbgs() << "Unhandled LEA machine instruction");
+    break;
+  }
+  return nullptr;
+}
+
+MachineLiveValPtr X86Values::getMachineValue(const MachineInstr *MI) const {
+  MachineLiveVal* Val = nullptr;
+  const MachineOperand *MO, *MO2;
+  const TargetInstrInfo *TII;
+
+  switch(MI->getOpcode()) {
+  case X86::LEA64r:
+    Val = genLEAInstructions(MI);
+    break;
+  case X86::MOV32r0:
+    Val = new MachineImmediate(4, 0, MI, false);
+    break;
+  case X86::MOV32ri:
+    MO = &MI->getOperand(1);
+    if(MO->isImm()) Val = new MachineImmediate(4, MO->getImm(), MI, false);
+    break;
+  case X86::MOV32ri64:
+    // TODO the upper 32 bits of this reference are supposed to be masked
+    MO = &MI->getOperand(1);
+    if(TargetValues::isSymbolValue(MO))
+      Val = new MachineSymbolRef(*MO, false, MI);
+    break;
+  case X86::MOV64ri:
+    MO = &MI->getOperand(1);
+    if(MO->isImm()) Val = new MachineImmediate(8, MO->getImm(), MI, false);
+    else if(TargetValues::isSymbolValue(MO))
+      Val = new MachineSymbolRef(*MO, false, MI);
+    break;
+  case X86::MOV64rm:
+    MO = &MI->getOperand(1 + X86::AddrBaseReg);
+    MO2 = &MI->getOperand(1 + X86::AddrDisp);
+    // Note: codegen'd a PC relative symbol reference
+    // Note 2: we *must* ensure the symbol is const-qualified, otherwise we
+    // risk creating a new value if the symbol's value changes between when the
+    // initial load would have occurred and the transformation, e.g.,
+    //
+    //   movq <ga:mysym>, %rax
+    //   ... (somebody changes mysym's value) ...
+    //   callq <ga:myfunc>
+    //
+    // In this situation, the transformation occurs at the call site and
+    // retrieves the updated value rather than the value that would have been
+    // loaded at the ldr instruction.
+    if(MO->isReg() && MO->getReg() == X86::RIP &&
+       TargetValues::isSymbolValue(MO2) &&
+       TargetValues::isSymbolValueConstant(MO2))
+        Val = new MachineSymbolRef(*MO2, true, MI);
+    break;
+  default:
+    TII =  MI->getParent()->getParent()->getSubtarget().getInstrInfo();
+    LLVM_DEBUG(dbgs() << "Unhandled opcode: "
+                 << TII->getName(MI->getOpcode()) << "\n");
+    break;
+  }
+
+  return MachineLiveValPtr(Val);
+}
diff --git a/llvm/lib/Target/X86/X86Values.h b/llvm/lib/Target/X86/X86Values.h
new file mode 100644
index 00000000000..8a76e00e16e
--- /dev/null
+++ b/llvm/lib/Target/X86/X86Values.h
@@ -0,0 +1,25 @@
+//===--------- X86TargetValues.cpp - X86 specific value generator ---------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Target/TargetValues.h"
+
+namespace llvm {
+
+class X86Values final : public TargetValues {
+public:
+  X86Values() {}
+  virtual TemporaryValuePtr getTemporaryValue(const MachineInstr *MI,
+                                              const VirtRegMap *VRM) const;
+  virtual MachineLiveValPtr getMachineValue(const MachineInstr *MI) const;
+
+private:
+  MachineLiveVal *genLEAInstructions(const MachineInstr *LEA) const;
+};
+
+}
diff --git a/llvm/lib/Transforms/Instrumentation/CMakeLists.txt b/llvm/lib/Transforms/Instrumentation/CMakeLists.txt
index 78b697f7f94..228029a8e71 100644
--- a/llvm/lib/Transforms/Instrumentation/CMakeLists.txt
+++ b/llvm/lib/Transforms/Instrumentation/CMakeLists.txt
@@ -4,12 +4,15 @@ add_llvm_library(LLVMInstrumentation
   CGProfile.cpp
   ControlHeightReduction.cpp
   DataFlowSanitizer.cpp
+  MigrationPoints.cpp
   GCOVProfiling.cpp
   MemorySanitizer.cpp
+  InsertStackMaps.cpp
   IndirectCallPromotion.cpp
   Instrumentation.cpp
   InstrOrderFile.cpp
   InstrProfiling.cpp
+  LibcStackMaps.cpp
   PGOInstrumentation.cpp
   PGOMemOPSizeOpt.cpp
   PoisonChecking.cpp
diff --git a/llvm/lib/Transforms/Instrumentation/InsertStackMaps.cpp b/llvm/lib/Transforms/Instrumentation/InsertStackMaps.cpp
new file mode 100644
index 00000000000..eb4c99ccd9d
--- /dev/null
+++ b/llvm/lib/Transforms/Instrumentation/InsertStackMaps.cpp
@@ -0,0 +1,358 @@
+#include <map>
+#include <set>
+#include <vector>
+#include "llvm/Pass.h"
+#include "llvm/Analysis/LiveValues.h"
+#include "llvm/Analysis/PopcornUtil.h"
+#include "llvm/IR/Dominators.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/InstIterator.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/ModuleSlotTracker.h"
+#include "llvm/IR/Type.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+#define DEBUG_TYPE "insert-stackmaps"
+
+using namespace llvm;
+
+static cl::opt<bool>
+NoLiveVals("no-live-vals",
+           cl::desc("Don't add live values to inserted stackmaps"),
+           cl::init(false),
+           cl::Hidden);
+
+namespace {
+
+/* Track slots for unnamed values */
+static ModuleSlotTracker *SlotTracker = nullptr;
+
+/* Sort values based on name */
+struct ValueComp
+{
+  bool operator ()(const Value *a, const Value *b)
+  {
+    if(a->hasName() && b->hasName())
+      return a->getName().compare(b->getName()) < 0;
+    else if(a->hasName()) return true;
+    else if(b->hasName()) return false;
+    else {
+      int slot_a = SlotTracker->getLocalSlot(a),
+          slot_b = SlotTracker->getLocalSlot(b);
+      return slot_a < slot_b;
+    }
+  }
+};
+
+/**
+ * This class instruments equivalence points in the IR with LLVM's stackmap
+ * intrinsic.  This tells the backend to record the locations of IR values
+ * after register allocation in a separate ELF section.
+ */
+class InsertStackMaps : public ModulePass
+{
+private:
+  /* Some useful typedefs */
+  typedef SmallVector<const Instruction *, 4> InstVec;
+  typedef DenseMap<const Instruction *, InstVec> InstHidingMap;
+  typedef SmallVector<const Argument *, 4> ArgVec;
+  typedef DenseMap<const Instruction *, ArgVec> ArgHidingMap;
+
+public:
+  static char ID;
+  size_t callSiteID;
+  size_t numInstrumented;
+
+  InsertStackMaps() : ModulePass(ID), callSiteID(0), numInstrumented(0) {
+    initializeInsertStackMapsPass(*PassRegistry::getPassRegistry());
+  }
+  ~InsertStackMaps() {}
+
+  /* ModulePass virtual methods */
+  virtual StringRef getPassName() const { return "Insert stackmaps"; }
+
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const
+  {
+    AU.addRequired<LiveValues>();
+    AU.addRequired<DominatorTreeWrapperPass>();
+    AU.setPreservesCFG();
+  }
+
+  /**
+   * Use liveness analysis to insert stackmap intrinsics into the IR to record
+   * live values at equivalence points.
+   *
+   * Note: currently we only insert stackmaps at function call sites.
+   */
+  virtual bool runOnModule(Module &M)
+  {
+    bool modified = false;
+
+    std::set<const Value *> *live;
+    std::set<const Value *, ValueComp> sortedLive;
+    InstHidingMap hiddenInst;
+    ArgHidingMap hiddenArgs;
+
+    LLVM_DEBUG(errs() << "\n********** Begin InsertStackMaps **********\n"
+                 << "********** Module: " << M.getName() << " **********\n\n");
+
+    this->createSMType(M);
+    if(this->addSMDeclaration(M)) modified = true;
+    SlotTracker = new ModuleSlotTracker(&M);
+
+    modified |= this->removeOldStackmaps(M);
+
+    /* Iterate over all functions/basic blocks/instructions. */
+    for(Module::iterator f = M.begin(), fe = M.end(); f != fe; f++)
+    {
+      if(f->isDeclaration()) continue;
+
+      LLVM_DEBUG(errs() << "InsertStackMaps: entering function "
+                   << f->getName() << "\n");
+
+      LiveValues &liveVals = getAnalysis<LiveValues>(*f);
+      DominatorTree &DT = getAnalysis<DominatorTreeWrapperPass>(*f).getDomTree();
+      SlotTracker->incorporateFunction(*f);
+      std::set<const Value *>::const_iterator v, ve;
+      getHiddenVals(*f, hiddenInst, hiddenArgs);
+
+      /* Find call sites in the function. */
+      for(Function::iterator b = f->begin(), be = f->end(); b != be; b++)
+      {
+        LLVM_DEBUG(
+          errs() << "InsertStackMaps: entering basic block ";
+          b->printAsOperand(errs(), false);
+          errs() << "\n"
+        );
+
+        for(BasicBlock::iterator i = b->begin(), ie = b->end(); i != ie; i++)
+        {
+          if(Popcorn::isCallSite(&*i))
+          {
+            CallSite CS(&*i);
+            if(CS.isInvoke())
+            {
+              LLVM_DEBUG(dbgs() << "WARNING: unhandled invoke:"; CS->dump());
+              continue;
+            }
+
+            IRBuilder<> builder(CS->getNextNode());
+            std::vector<Value *> args(2);
+            args[0] = ConstantInt::getSigned(Type::getInt64Ty(M.getContext()),
+                                             this->callSiteID++);
+            args[1] = ConstantInt::getSigned(Type::getInt32Ty(M.getContext()),
+                                             0);
+
+            if(NoLiveVals) {
+              builder.CreateCall(this->SMFunc, ArrayRef<Value*>(args));
+              this->numInstrumented++;
+              continue;
+            }
+
+            live = liveVals.getLiveValues(&*i);
+            for(const Value *val : *live) sortedLive.insert(val);
+            for(const auto &pair : hiddenInst) {
+              /*
+               * The two criteria for inclusion of a hidden value are:
+               *   1. The value's definition dominates the call
+               *   2. A use which hides the definition is in the stackmap
+               */
+              if(DT.dominates(pair.first, &*i) && live->count(pair.first))
+                for(auto &inst : pair.second) sortedLive.insert(inst);
+            }
+            for(const auto &pair : hiddenArgs) {
+              /*
+               * Similar criteria apply as above, except we know arguments
+               * dominate the entire function.
+               */
+              if(live->count(pair.first))
+                for(auto &inst : pair.second) sortedLive.insert(inst);
+            }
+            delete live;
+
+            LLVM_DEBUG(
+              const Function *calledFunc;
+
+              errs() << "  ";
+              if(!CS->getType()->isVoidTy()) {
+                CS->printAsOperand(errs(), false);
+                errs() << " ";
+              }
+              else errs() << "(void) ";
+
+              calledFunc = CS.getCalledFunction();
+              if(calledFunc && calledFunc->hasName())
+              {
+                StringRef name = CS.getCalledFunction()->getName();
+                errs() << name << " ";
+              }
+              errs() << "ID: " << this->callSiteID;
+
+              errs() << ", " << sortedLive.size() << " live value(s)\n   ";
+              for(const Value *val : sortedLive) {
+                errs() << " ";
+                val->printAsOperand(errs(), false);
+              }
+              errs() << "\n";
+            );
+
+            for(v = sortedLive.begin(), ve = sortedLive.end(); v != ve; v++)
+              args.push_back(const_cast<Value*>(*v));
+            builder.CreateCall(this->SMFunc, ArrayRef<Value*>(args));
+            sortedLive.clear();
+            this->numInstrumented++;
+          }
+        }
+      }
+
+      hiddenInst.clear();
+      hiddenArgs.clear();
+      this->callSiteID = 0;
+    }
+
+    LLVM_DEBUG(
+      errs() << "InsertStackMaps: finished module " << M.getName() << ", added "
+             << this->numInstrumented << " stackmaps\n\n";
+    );
+
+    if(numInstrumented > 0) modified = true;
+    delete SlotTracker;
+
+    return modified;
+  }
+
+private:
+  /* Name of stack map intrinsic */
+  static const StringRef SMName;
+
+  /* Stack map instruction creation */
+  Function *SMFunc;
+  FunctionType *SMTy; // Used for creating function declaration
+
+  /**
+   * Create the function type for the stack map intrinsic.
+   */
+  void createSMType(const Module &M)
+  {
+    std::vector<Type*> params(2);
+    params[0] = Type::getInt64Ty(M.getContext());
+    params[1] = Type::getInt32Ty(M.getContext());
+    this->SMTy = FunctionType::get(Type::getVoidTy(M.getContext()),
+                                                   ArrayRef<Type*>(params),
+                                                   true);
+  }
+
+  /**
+   * Add the stackmap intrinisic's function declaration if not already present.
+   * Return true if the declaration was added, or false if it's already there.
+   */
+  bool addSMDeclaration(Module &M)
+  {
+    if(!(this->SMFunc = M.getFunction(this->SMName)))
+    {
+      LLVM_DEBUG(errs() << "Adding stackmap function declaration to " << M.getName() << "\n");
+      this->SMFunc = Function::Create(this->SMTy, Function::ExternalLinkage, this->SMName, M);
+      this->SMFunc->setCallingConv(CallingConv::C);
+      return true;
+    }
+    else return false;
+  }
+
+  /**
+   * Iterate over all instructions, removing previously found stackmaps.
+   */
+  bool removeOldStackmaps(Module &M)
+  {
+    bool modified = false;
+    CallInst* CI;
+    const Function *F;
+
+    LLVM_DEBUG(dbgs() << "Searching for/removing old stackmaps\n";);
+
+    for(Module::iterator f = M.begin(), fe = M.end(); f != fe; f++) {
+      for(Function::iterator bb = f->begin(), bbe = f->end(); bb != bbe; bb++) {
+        for(BasicBlock::iterator i = bb->begin(), ie = bb->end(); i != ie; i++) {
+          if((CI = dyn_cast<CallInst>(&*i))) {
+            F = CI->getCalledFunction();
+            if(F && F->hasName() && F->getName() == SMName) {
+	      // FIXME: 'i' needs to point to the previous node.
+	      i = i->eraseFromParent();
+	      i = bb->begin();
+              modified = true;
+            }
+          }
+        }
+      }
+    }
+
+    LLVM_DEBUG(if(modified)
+            dbgs() << "WARNING: found previous run of Popcorn passes!\n";);
+
+    return modified;
+  }
+
+  /**
+   * Gather a list of values which may be "hidden" from live value analysis.
+   * This function collects the values used in these instructions, which are
+   * later added to the appropriate stackmaps.
+   *
+   *  - Instructions which access fields of structs or entries of arrays, like
+   *    getelementptr, can interfere with the live value analysis to hide the
+   *    backing values used in the instruction.  For example, the following IR
+   *    obscures %arr from the live value analysis:
+   *
+   *  %arr = alloca [4 x double], align 8
+   *  %arrayidx = getelementptr inbounds [4 x double], [4 x double]* %arr, i64 0, i64 0
+   *
+   *  -> Access to %arr might only happen through %arrayidx, and %arr may not
+   *     be used any more
+   *
+   */
+  void getHiddenVals(Function &F, InstHidingMap &inst, ArgHidingMap &args)
+  {
+    /* Does the instruction potentially hide values from liveness analysis? */
+    auto hidesValues = [](const Instruction *I) {
+      if(isa<ExtractElementInst>(I) || isa<InsertElementInst>(I) ||
+         isa<ExtractValueInst>(I) || isa<InsertValueInst>(I) ||
+         isa<GetElementPtrInst>(I) || isa<BitCastInst>(I))
+        return true;
+      else return false;
+    };
+
+    /* Search for instructions that obscure live values & record operands */
+    for(inst_iterator i = inst_begin(F), e = inst_end(F); i != e; ++i) {
+      InstVec &InstsHidden = inst[&*i];
+      ArgVec &ArgsHidden = args[&*i];
+
+      if(hidesValues(&*i)) {
+        for(unsigned op = 0; op < i->getNumOperands(); op++) {
+          if(isa<Instruction>(i->getOperand(op)))
+            InstsHidden.push_back(cast<Instruction>(i->getOperand(op)));
+          else if(isa<Argument>(i->getOperand(op)))
+            ArgsHidden.push_back(cast<Argument>(i->getOperand(op)));
+        }
+      }
+    }
+  }
+};
+
+} /* end anonymous namespace */
+
+char InsertStackMaps::ID = 0;
+const StringRef InsertStackMaps::SMName = "llvm.experimental.pcn.stackmap";
+
+INITIALIZE_PASS_BEGIN(InsertStackMaps, "insert-stackmaps",
+                      "Instrument equivalence points with stack maps",
+                      false, false)
+INITIALIZE_PASS_DEPENDENCY(LiveValues)
+INITIALIZE_PASS_DEPENDENCY(DominatorTreeWrapperPass)
+INITIALIZE_PASS_END(InsertStackMaps, "insert-stackmaps",
+                    "Instrument equivalence points with stack maps",
+                    false, false)
+
+namespace llvm {
+  ModulePass *createInsertStackMapsPass() { return new InsertStackMaps(); }
+}
diff --git a/llvm/lib/Transforms/Instrumentation/Instrumentation.cpp b/llvm/lib/Transforms/Instrumentation/Instrumentation.cpp
index f56a1bd91b8..c40780384b4 100644
--- a/llvm/lib/Transforms/Instrumentation/Instrumentation.cpp
+++ b/llvm/lib/Transforms/Instrumentation/Instrumentation.cpp
@@ -105,14 +105,17 @@ void llvm::initializeInstrumentation(PassRegistry &Registry) {
   initializeAddressSanitizerLegacyPassPass(Registry);
   initializeModuleAddressSanitizerLegacyPassPass(Registry);
   initializeBoundsCheckingLegacyPassPass(Registry);
+  initializeMigrationPointsPass(Registry);
   initializeControlHeightReductionLegacyPassPass(Registry);
   initializeGCOVProfilerLegacyPassPass(Registry);
+  initializeInsertStackMapsPass(Registry);
   initializePGOInstrumentationGenLegacyPassPass(Registry);
   initializePGOInstrumentationUseLegacyPassPass(Registry);
   initializePGOIndirectCallPromotionLegacyPassPass(Registry);
   initializePGOMemOPSizeOptLegacyPassPass(Registry);
   initializeInstrOrderFileLegacyPassPass(Registry);
   initializeInstrProfilingLegacyPassPass(Registry);
+  initializeLibcStackMapsPass(Registry);
   initializeMemorySanitizerLegacyPassPass(Registry);
   initializeHWAddressSanitizerLegacyPassPass(Registry);
   initializeThreadSanitizerLegacyPassPass(Registry);
diff --git a/llvm/lib/Transforms/Instrumentation/LibcStackMaps.cpp b/llvm/lib/Transforms/Instrumentation/LibcStackMaps.cpp
new file mode 100644
index 00000000000..0de6020fa14
--- /dev/null
+++ b/llvm/lib/Transforms/Instrumentation/LibcStackMaps.cpp
@@ -0,0 +1,229 @@
+#include <map>
+#include <vector>
+#include "llvm/Pass.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/Type.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/Path.h"
+#include "llvm/Support/raw_ostream.h"
+
+#define DEBUG_TYPE "libc-stackmaps"
+
+using namespace llvm;
+
+namespace {
+
+/**
+ * Instrument thread starting points with stackmaps.  These are the only
+ * functions inside of libc for which we want to generate metadata, since we
+ * disallow migration inside the public libc API.
+ */
+// TODO: only implemented for musl-libc!
+class LibcStackMaps : public ModulePass
+{
+public:
+  static char ID;
+  size_t numInstrumented;
+
+  LibcStackMaps() : ModulePass(ID), numInstrumented(0) {
+    initializeLibcStackMapsPass(*PassRegistry::getPassRegistry());
+  }
+  ~LibcStackMaps() {}
+
+  /* ModulePass virtual methods */
+  virtual StringRef getPassName() const
+  { return "Insert stackmaps in libc thread start functions"; }
+
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const
+  { AU.setPreservesCFG(); }
+
+  virtual bool runOnModule(Module &M)
+  {
+    int64_t smid;
+    bool modified = false;
+    Function *F;
+    std::map<std::string, std::vector<std::string> >::const_iterator file;
+
+    /* Is this a module (i.e., source file) we're interested in? */
+    if((file = funcs.find(sys::path::stem(M.getName()))) != funcs.end())
+    {
+      LLVM_DEBUG(dbgs() << "\n********** Begin LibcStackMaps **********\n"
+                   << "********** Module: " << file->first << " **********\n\n");
+
+      this->createSMType(M);
+      modified |= this->addSMDeclaration(M);
+
+      /* Iterate over thread starting functions in the module */
+      for(size_t f = 0, fe = file->second.size(); f < fe; f++)
+      {
+        LLVM_DEBUG(dbgs() << "LibcStackMaps: entering thread starting function "
+                     << file->second[f] << "\n");
+
+        F = M.getFunction(file->second[f]);
+        assert(F && !F->isDeclaration() && "No thread function definition");
+        modified |= this->removeOldStackmaps(F);
+        assert(smids.find(file->second[f]) != smids.end() && "No ID for function");
+        smid = smids.find(file->second[f])->second;
+
+        /*
+         * Look for & instrument a generic call instruction followed by a call
+         * to an exit function, e.g.,
+         *
+         *   %call = call i32 %main(...)
+         *   call void @exit(i32 %call)
+         */
+        for(Function::iterator bb = F->begin(), be = F->end(); bb != be; bb++)
+        {
+          bool track = false;
+          for(BasicBlock::reverse_iterator i = bb->rbegin(), ie = bb->rend();
+              i != ie; i++)
+          {
+            if(isExitCall(*i)) track = true;
+            else if(track && isa<CallInst>(*i))
+            {
+              IRBuilder<> builder(i->getNextNode());
+              std::vector<Value *> args(2);
+              args[0] = ConstantInt::getSigned(Type::getInt64Ty(M.getContext()), smid);
+              args[1] = ConstantInt::getSigned(Type::getInt32Ty(M.getContext()), 0);
+              builder.CreateCall(this->SMFunc, ArrayRef<Value*>(args));
+              this->numInstrumented++;
+              break;
+            }
+          }
+        }
+      }
+
+      LLVM_DEBUG(dbgs() << "LibcStackMaps: finished module " << M.getName()
+                   << ", added " << this->numInstrumented << " stackmaps\n\n";);
+    }
+
+    if(numInstrumented > 0) modified = true;
+    return modified;
+  }
+
+private:
+  /* Name of stack map intrinsic */
+  static const StringRef SMName;
+
+  /* Stack map instruction creation */
+  Function *SMFunc;
+  FunctionType *SMTy; // Used for creating function declaration
+
+  /* Files, functions & IDs */
+  static const std::map<std::string, std::vector<std::string> > funcs;
+  static const std::map<std::string, int64_t> smids;
+  static const std::vector<std::string> exitFuncs;
+
+  /**
+   * Create the function type for the stack map intrinsic.
+   */
+  void createSMType(const Module &M)
+  {
+    std::vector<Type*> params(2);
+    params[0] = Type::getInt64Ty(M.getContext());
+    params[1] = Type::getInt32Ty(M.getContext());
+    this->SMTy = FunctionType::get(Type::getVoidTy(M.getContext()),
+                                                   ArrayRef<Type*>(params),
+                                                   true);
+  }
+
+  /**
+   * Add the stackmap intrinisic's function declaration if not already present.
+   * Return true if the declaration was added, or false if it's already there.
+   */
+  bool addSMDeclaration(Module &M)
+  {
+    if(!(this->SMFunc = M.getFunction(this->SMName)))
+    {
+      LLVM_DEBUG(dbgs() << "Adding stackmap function declaration to " << M.getName() << "\n");
+      this->SMFunc = Function::Create(this->SMTy, Function::ExternalLinkage, this->SMName, M);
+      this->SMFunc->setCallingConv(CallingConv::C);
+      return true;
+    }
+    else return false;
+  }
+
+  /**
+   * Iterate over all instructions, removing previously found stackmaps.
+   */
+  bool removeOldStackmaps(Function *F)
+  {
+    bool modified = false;
+    CallInst* CI;
+    const Function *CurF;
+
+    LLVM_DEBUG(dbgs() << "Searching for/removing old stackmaps\n";);
+
+    for(Function::iterator bb = F->begin(), bbe = F->end(); bb != bbe; bb++) {
+      for(BasicBlock::iterator i = bb->begin(), ie = bb->end(); i != ie; i++) {
+        if((CI = dyn_cast<CallInst>(&*i))) {
+          CurF = CI->getCalledFunction();
+          if(CurF && CurF->hasName() && CurF->getName() == SMName) {
+            i->eraseFromParent();
+	    i = bb->begin();
+            modified = true;
+          }
+        }
+      }
+    }
+
+    LLVM_DEBUG(if(modified) dbgs() << "WARNING: found previous stackmaps!\n";);
+    return modified;
+  }
+
+  /**
+   * Return whether or not the instruction is a call to an exit function.
+   */
+  bool isExitCall(Instruction &I)
+  {
+    CallInst *CI;
+    Function *F;
+
+    if((CI = dyn_cast<CallInst>(&I)))
+    {
+      F = CI->getCalledFunction();
+      if(F && F->hasName())
+        for(size_t i = 0, e = exitFuncs.size(); i < e; i++)
+          if(F->getName() == exitFuncs[i]) return true;
+    }
+
+    return false;
+  }
+};
+
+} /* end anonymous namespace */
+
+char LibcStackMaps::ID = 0;
+const StringRef LibcStackMaps::SMName = "llvm.experimental.pcn.stackmap";
+
+/**
+ * Map a source code filename (minus the extension) to the names of functions
+ * inside which are to be instrumented.
+ */
+const std::map<std::string, std::vector<std::string> > LibcStackMaps::funcs = {
+  {"__libc_start_main", {"__libc_start_main"}},
+  {"pthread_create", {"start", "start_c11"}}
+};
+
+/* Map a function name to the stackmap ID representing that function. */
+const std::map<std::string, int64_t> LibcStackMaps::smids = {
+  {"__libc_start_main", UINT64_MAX},
+  {"start", UINT64_MAX - 1},
+  {"start_c11", UINT64_MAX - 2}
+};
+
+/**
+ * Thread exit function names, used to search for starting function call site
+ * to be instrumented with stackmap.
+ */
+const std::vector<std::string> LibcStackMaps::exitFuncs = {
+  "exit", "pthread_exit", "__pthread_exit"
+};
+
+INITIALIZE_PASS(LibcStackMaps, "libc-stackmaps",
+  "Instrument libc thread start functions with stack maps", false, false)
+
+namespace llvm {
+  ModulePass *createLibcStackMapsPass() { return new LibcStackMaps(); }
+}
diff --git a/llvm/lib/Transforms/Instrumentation/MigrationPoints.cpp b/llvm/lib/Transforms/Instrumentation/MigrationPoints.cpp
new file mode 100644
index 00000000000..e51cbc6b54b
--- /dev/null
+++ b/llvm/lib/Transforms/Instrumentation/MigrationPoints.cpp
@@ -0,0 +1,489 @@
+//===- MigrationPoints.cpp ------------------------------------------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Instruments the migration points selected by SelectMigrationPoints.  May
+// additionally add HTM instrumentation if selected by user & supported by the
+// architecture.
+//
+//===----------------------------------------------------------------------===//
+
+#include <fstream>
+#include <map>
+#include "llvm/Pass.h"
+#include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/Triple.h"
+#include "llvm/Analysis/PopcornUtil.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/Module.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_os_ostream.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "migration-points"
+#define MIGRATE_FLAG_NAME "__migrate_flag"
+
+/// Disable rollback-only transactions for PowerPC.
+const static cl::opt<bool>
+NoROTPPC("htm-ppc-no-rot", cl::Hidden, cl::init(false),
+  cl::desc("Disable rollback-only transactions in HTM instrumentation "
+           "(PowerPC only)"));
+
+/// Add counters to abort handlers for the specified function.  Allows in-depth
+/// profiling of which HTM sections added to the function are causing aborts.
+const static cl::opt<std::string>
+AbortCount("abort-count", cl::Hidden, cl::init(""),
+  cl::desc("Add counters for each abort handler in the specified function"),
+  cl::value_desc("function"));
+
+STATISTIC(NumMigPoints, "Number of migration points added");
+STATISTIC(NumHTMBegins, "Number of HTM begin intrinsics added");
+STATISTIC(NumHTMEnds, "Number of HTM end intrinsics added");
+
+namespace {
+
+/// MigrationPoints - insert migration points into functions, optionally adding
+/// HTM execution.
+class MigrationPoints : public FunctionPass
+{
+public:
+  static char ID;
+
+  MigrationPoints() : FunctionPass(ID) {}
+  ~MigrationPoints() {}
+
+  virtual StringRef getPassName() const
+  { return "Insert migration points"; }
+
+  /// Generate the migration library API function declaration.
+  void addMigrationIntrinsic(Module &M, bool DoHTM) {
+    LLVMContext &C = M.getContext();
+    Type *VoidTy = Type::getVoidTy(C);
+    PointerType *VoidPtrTy = Type::getInt8PtrTy(C, 0);
+    std::vector<Type *> FuncPtrArgTy = { VoidPtrTy };
+    FunctionType *FuncPtrTy = FunctionType::get(VoidTy, FuncPtrArgTy, false);
+    CallbackType = PointerType::get(FuncPtrTy, 0);
+    std::vector<Type *> ArgTy = { CallbackType, VoidPtrTy };
+    FunctionType *FuncTy = FunctionType::get(VoidTy, ArgTy, false);
+    if(DoHTM) {
+      MigrateAPI = M.getOrInsertFunction("migrate", FuncTy);
+      MigrateFlag = cast<GlobalValue>(
+        M.getOrInsertGlobal(MIGRATE_FLAG_NAME, Type::getInt32Ty(C)));
+      // TODO this needs to be thread-local storage
+      //MigrateFlag->setThreadLocal(true);
+    }
+    else {
+      MigrateAPI = M.getOrInsertFunction("check_migrate", FuncTy);
+      MigrateFlag = nullptr;
+    }
+  }
+
+  virtual bool doInitialization(Module &M) {
+    Triple TheTriple(M.getTargetTriple());
+    Arch = TheTriple.getArch();
+    Ty = Popcorn::getInstrumentationType(M);
+
+    switch(Ty) {
+    case Popcorn::HTM:
+      if(HTMBegin.find(Arch) != HTMBegin.end()) {
+        LLVM_DEBUG(dbgs() << "\n-> MigrationPoints: Adding HTM intrinsics for '"
+                     << TheTriple.getArchName() << "' <-\n");
+        HTMBeginDecl =
+          Intrinsic::getDeclaration(&M, HTMBegin.find(Arch)->second);
+        HTMEndDecl = Intrinsic::getDeclaration(&M, HTMEnd.find(Arch)->second);
+        HTMTestDecl =
+          Intrinsic::getDeclaration(&M, HTMTest.find(Arch)->second);
+        addMigrationIntrinsic(M, true);
+      }
+      else {
+        LLVM_DEBUG(
+          dbgs() << "\n-> MigrationPoints: Selected HTM instrumentation but '"
+                 << TheTriple.getArchName()
+                 << "' is not supported, falling back to call-outs <-\n"
+        );
+        Ty = Popcorn::Cycles;
+        addMigrationIntrinsic(M, false);
+      }
+      break;
+    case Popcorn::Cycles:
+      addMigrationIntrinsic(M, false);
+      break;
+    case Popcorn::None:
+      return false;
+    default: llvm_unreachable("Unknown instrumentation type"); break;
+    }
+
+    // Add abort counters if somebody requested abort profiling.
+    const Function *CounterFunc;
+    if(AbortCount != "" && (CounterFunc = M.getFunction(AbortCount)) &&
+       !CounterFunc->isDeclaration()) {
+      LLVMContext &C = M.getContext();
+      IntegerType *Unsigned = Type::getInt32Ty(C);
+      GlobalVariable *NumCtrs = cast<GlobalVariable>(
+        M.getOrInsertGlobal("__num_abort_counters", Unsigned));
+      NumCtrs->setInitializer(ConstantInt::get(Unsigned, 1024, false));
+      Type *ArrType = ArrayType::get(Type::getInt64Ty(C), 1024);
+      AbortCounters = cast<GlobalVariable>(
+        M.getOrInsertGlobal("__abort_counters", ArrType));
+      AbortCounters->setInitializer(ConstantAggregateZero::get(ArrType));
+    }
+
+    return true;
+  }
+
+  /// Insert migration points into functions
+  virtual bool runOnFunction(Function &F)
+  {
+    if(Ty == Popcorn::None) return false;
+
+    LLVM_DEBUG(dbgs() << "\n********** ADD MIGRATION POINTS **********\n"
+                 << "********** Function: " << F.getName() << "\n\n");
+
+    initializeAnalysis(F);
+
+    // Find all instrumentation points marked by previous analysis passes.
+    findInstrumentationPoints(F);
+
+    // Apply code transformations to marked instructions, including adding
+    // migration points & HTM instrumentation.
+    addMigrationPoints(F);
+
+    // Write the modified IR & close the abort handler map file if we
+    // instrumented the code to profile abort handlers.
+    if(MapFile.is_open()) {
+      MapFile.close();
+      std::fstream TheIR("htm-abort-ir.ll", std::ios::out | std::ios::trunc);
+      raw_os_ostream IRStream(TheIR);
+      F.print(IRStream);
+      TheIR.close();
+    }
+
+    return true;
+  }
+
+  /// Reset all analysis.
+  void initializeAnalysis(const Function &F) {
+    DoHTMInst = false;
+    DoAbortInstrument = false;
+    MigPointInsts.clear();
+    HTMBeginInsts.clear();
+    HTMEndInsts.clear();
+
+    if(Ty == Popcorn::HTM) {
+      // We've checked at the global scope whether HTM is enabled for the
+      // module.  Check whether the target-specific feature for HTM is enabled
+      // for the current function.
+      if(!F.hasFnAttribute("target-features")) {
+        LLVM_DEBUG(dbgs() << "-> Disabled HTM instrumentation, "
+                        "no 'target-features' attribute\n");
+        return;
+      }
+
+      Attribute TargetAttr = F.getFnAttribute("target-features");
+      assert(TargetAttr.isStringAttribute() && "Invalid target features");
+      StringRef AttrVal = TargetAttr.getValueAsString();
+      size_t pos = StringRef::npos;
+
+      switch(Arch) {
+      case Triple::ppc64le: pos = AttrVal.find("+htm"); break;
+      case Triple::x86_64: pos = AttrVal.find("+rtm"); break;
+      default: break;
+      }
+
+      DoHTMInst = (pos != StringRef::npos);
+
+      LLVM_DEBUG(if(!DoHTMInst) dbgs() << "-> Disabled HTM instrumentation, HTM "
+                                     "not listed in target-features\n");
+
+      // Enable HTM abort handler profiling if specified
+      if(DoHTMInst && AbortCount == F.getName()) {
+        DoAbortInstrument = true;
+        AbortHandlerCount = 0;
+        MapFile.open("htm-abort.map", std::ios::out | std::ios::trunc);
+        assert(MapFile.is_open() && MapFile.good() &&
+               "Could not open abort handler map file");
+      }
+    }
+
+    LLVM_DEBUG(
+      if(DoHTMInst) {
+        dbgs() << "-> Adding HTM instrumentation\n";
+        if(DoAbortInstrument) dbgs() << "  - Adding abort counters\n";
+      }
+      else dbgs() << "-> Adding call-out instrumentation\n";
+    );
+  }
+
+private:
+  //===--------------------------------------------------------------------===//
+  // Types & fields
+  //===--------------------------------------------------------------------===//
+
+  /// Type of the instrumentation to be applied to functions.
+  enum Popcorn::InstrumentType Ty;
+
+  /// The current architecture - used to access architecture-specific HTM calls
+  Triple::ArchType Arch;
+
+  /// Should we instrument code with HTM execution?  Set if HTM is enabled on
+  /// the command line and if the target is supported
+  bool DoHTMInst;
+
+  /// Should we instrument HTM abort handlers with counters for precise
+  /// profiling of which code locations cause aborts & all associated state.
+  bool DoAbortInstrument;
+  GlobalVariable *AbortCounters;
+  unsigned AbortHandlerCount;
+  std::ofstream MapFile;
+
+  /// Function declaration & migration node ID for migration library API
+  FunctionCallee MigrateAPI;
+  GlobalValue *MigrateFlag;
+  PointerType *CallbackType;
+
+  /// Function declarations for HTM intrinsics
+  Value *HTMBeginDecl;
+  Value *HTMEndDecl;
+  Value *HTMTestDecl;
+
+  /// Per-architecture LLVM intrinsic IDs for HTM begin, HTM end, and testing
+  /// if executing transactionally
+  typedef std::map<Triple::ArchType, Intrinsic::ID> IntrinsicMap;
+  const static IntrinsicMap HTMBegin;
+  const static IntrinsicMap HTMEnd;
+  const static IntrinsicMap HTMTest;
+
+  /// Code locations marked for instrumentation.
+  SmallPtrSet<Instruction *, 32> MigPointInsts;
+  SmallPtrSet<Instruction *, 32> HTMBeginInsts;
+  SmallPtrSet<Instruction *, 32> HTMEndInsts;
+
+  //===--------------------------------------------------------------------===//
+  // Instrumentation implementation
+  //===--------------------------------------------------------------------===//
+
+  /// Find instructions tagged by SelectMigrationPoints with instrumentation
+  /// metadata.
+  void findInstrumentationPoints(Function &F) {
+    for(Function::iterator BB = F.begin(), BBE = F.end(); BB != BBE; BB++) {
+      for(BasicBlock::iterator I = BB->begin(), IE = BB->end(); I != IE; I++) {
+        if(Popcorn::hasEquivalencePointMetadata(&*I)) MigPointInsts.insert(&*I);
+        if(Popcorn::isHTMBeginPoint(&*I)) HTMBeginInsts.insert(&*I);
+        if(Popcorn::isHTMEndPoint(&*I)) HTMEndInsts.insert(&*I);
+      }
+    }
+  }
+
+  /// Add a migration point directly before an instruction.
+  void addMigrationPoint(Instruction *I) {
+    LLVMContext &C = I->getContext();
+    IRBuilder<> Worker(I);
+    std::vector<Value *> Args = {
+      ConstantPointerNull::get(CallbackType),
+      ConstantPointerNull::get(Type::getInt8PtrTy(C, 0))
+    };
+    Worker.CreateCall(MigrateAPI, Args);
+  }
+
+  // Note: because we're only supporting 2 architectures for now, we're not
+  // going to abstract this out into the appropriate Target/* folders
+
+  /// Add HTM begin which avoids doing any work unless there's an abort.  In
+  /// the event of an abort, the instrumentation checks if it should migrate,
+  /// and if so, invokes the migration API.
+  void addHTMBeginInternal(Instruction *I, Value *Begin, Value *Comparison) {
+    LLVMContext &C = I->getContext();
+    BasicBlock *CurBB = I->getParent(), *NewSuccBB, *FlagCheckBB, *MigPointBB;
+
+    // Set up each of the new basic blocks
+    NewSuccBB =
+      CurBB->splitBasicBlock(I, "migpointsucc" + std::to_string(NumMigPoints));
+    MigPointBB =
+      BasicBlock::Create(C, "migpoint" + std::to_string(NumMigPoints),
+                         CurBB->getParent(), NewSuccBB);
+    FlagCheckBB =
+      BasicBlock::Create(C, "migflagcheck" + std::to_string(NumMigPoints),
+                         CurBB->getParent(), MigPointBB);
+
+    // Add check & branch based on HTM begin result Comparison.  The true
+    // target of the branch is when we've started the transaction.
+    IRBuilder<> HTMWorker(CurBB->getTerminator());
+    HTMWorker.CreateCondBr(Comparison, NewSuccBB, FlagCheckBB);
+    CurBB->getTerminator()->eraseFromParent();
+
+    // Check flag to see if we should invoke migration library API.
+    IRBuilder<> FlagCheckWorker(FlagCheckBB);
+    if(DoAbortInstrument) {
+      assert(AbortHandlerCount < 1024 && "Too abort handler many counters!");
+
+      // Write the name of the basic block to the map file so we can map abort
+      // counters to their basic blocks.
+      if(!AbortHandlerCount) MapFile << FlagCheckBB->getName().str();
+      else MapFile << " " << FlagCheckBB->getName().str();
+
+      // Add instrumentation to increment the counter's value.
+      std::string CtrNum(std::to_string(AbortHandlerCount));
+      std::vector<Value *> Idx = {
+        ConstantInt::get(Type::getInt64Ty(C), 0),
+        ConstantInt::get(Type::getInt64Ty(C), AbortHandlerCount),
+      };
+      Value *One = ConstantInt::get(Type::getInt64Ty(C), 1, false);
+      Value *GEP = FlagCheckWorker.CreateInBoundsGEP(AbortCounters, Idx,
+                                                     "ctrptr" + CtrNum);
+      Value *CtrVal = FlagCheckWorker.CreateLoad(GEP, "ctr" + CtrNum);
+      Value *Inc = FlagCheckWorker.CreateAdd(CtrVal, One);
+      FlagCheckWorker.CreateStore(Inc, GEP);
+
+      AbortHandlerCount++;
+    }
+    Value *Flag = FlagCheckWorker.CreateLoad(MigrateFlag);
+    Value *NegOne = ConstantInt::get(Type::getInt32Ty(C), -1, true);
+    Value *Cmp = FlagCheckWorker.CreateICmpEQ(Flag, NegOne);
+    FlagCheckWorker.CreateCondBr(Cmp, NewSuccBB, MigPointBB);
+
+    // Add call to migration library API.
+    IRBuilder<> MigPointWorker(MigPointBB);
+    std::vector<Value *> Args = {
+      ConstantPointerNull::get(CallbackType),
+      ConstantPointerNull::get(Type::getInt8PtrTy(C, 0))
+    };
+    MigPointWorker.CreateCall(MigrateAPI, Args);
+    MigPointWorker.CreateBr(NewSuccBB);
+  }
+
+  /// Add a transactional execution begin intrinsic for PowerPC, optionally
+  /// with rollback-only transactions.
+  void addPowerPCHTMBegin(Instruction *I) {
+    LLVMContext &C = I->getContext();
+    IRBuilder<> Worker(I);
+    std::vector<Value *> Args = { ConstantInt::get(Type::getInt32Ty(C),
+                                                   NoROTPPC ? 0 : 1,
+                                                   false) };
+    Value *HTMBeginVal = Worker.CreateCall(HTMBeginDecl, Args);
+    Value *Zero = ConstantInt::get(Type::getInt32Ty(C), 0, false);
+    Value *Cmp = Worker.CreateICmpNE(HTMBeginVal, Zero);
+    addHTMBeginInternal(I, HTMBeginVal, Cmp);
+  }
+
+  /// Add a transactional execution begin intrinsice for x86.
+  void addX86HTMBegin(Instruction *I) {
+    LLVMContext &C = I->getContext();
+    IRBuilder<> Worker(I);
+    Value *HTMBeginVal = Worker.CreateCall(HTMBeginDecl);
+    Value *Success = ConstantInt::get(Type::getInt32Ty(C), 0xffffffff, false);
+    Value *Cmp = Worker.CreateICmpEQ(HTMBeginVal, Success);
+    addHTMBeginInternal(I, HTMBeginVal, Cmp);
+  }
+
+  /// Add transactional execution end intrinsic for PowerPC.
+  void addPowerPCHTMEnd(Instruction *I) {
+    LLVMContext &C = I->getContext();
+    IRBuilder<> EndWorker(I);
+    ConstantInt *One = ConstantInt::get(IntegerType::getInt32Ty(C),
+                                        1, false);
+    EndWorker.CreateCall(HTMEndDecl, ArrayRef<Value *>(One));
+  }
+
+  /// Add transactional execution check & end intrinsics for x86.
+  void addX86HTMCheckAndEnd(Instruction *I) {
+    // Note: x86's HTM facility will cause a segfault if an xend instruction is
+    // called outside of a transaction, hence we need to check if we're in a
+    // transaction before actually trying to end it.
+    LLVMContext &C = I->getContext();
+    BasicBlock *CurBB = I->getParent(), *NewSuccBB, *HTMEndBB;
+    Function *CurF = CurBB->getParent();
+
+    // Create a new successor which contains all instructions after the HTM
+    // check & end
+    NewSuccBB = CurBB->splitBasicBlock(I,
+      ".htmendsucc" + std::to_string(NumHTMEnds));
+
+    // Create an HTM end block, which ends the transaction and jumps to the
+    // new successor
+    HTMEndBB = BasicBlock::Create(C,
+      ".htmend" + std::to_string(NumHTMEnds), CurF, NewSuccBB);
+    IRBuilder<> EndWorker(HTMEndBB);
+    EndWorker.CreateCall(HTMEndDecl);
+    EndWorker.CreateBr(NewSuccBB);
+
+    // Finally, add the HTM test & replace the unconditional branch created by
+    // splitBasicBlock() with a conditional branch to either end the
+    // transaction or continue on to the new successor
+    IRBuilder<> PredWorker(CurBB->getTerminator());
+    CallInst *HTMTestVal = PredWorker.CreateCall(HTMTestDecl);
+    ConstantInt *Zero = ConstantInt::get(IntegerType::getInt32Ty(C), 0, true);
+    Value *Cmp = PredWorker.CreateICmpNE(HTMTestVal, Zero,
+      "htmcmp" + std::to_string(NumHTMEnds));
+    PredWorker.CreateCondBr(Cmp, HTMEndBB, NewSuccBB);
+    CurBB->getTerminator()->eraseFromParent();
+  }
+
+  /// Insert migration points & HTM instrumentation for instructions.
+  void addMigrationPoints(Function &F) {
+    if(DoHTMInst) {
+      // Note: need to add the HTM ends before begins
+      for(auto I = HTMEndInsts.begin(), E = HTMEndInsts.end(); I != E; ++I) {
+        switch(Arch) {
+        case Triple::ppc64le: addPowerPCHTMEnd(*I); break;
+        case Triple::x86_64: addX86HTMCheckAndEnd(*I); break;
+        default: llvm_unreachable("HTM -- unsupported architecture");
+        }
+        NumHTMEnds++;
+      }
+
+      // The following APIs both insert HTM begins & migration points because
+      // the control flow with/without abort handlers is intertwined
+      for(auto I = HTMBeginInsts.begin(), E = HTMBeginInsts.end();
+          I != E; ++I) {
+        switch(Arch) {
+        case Triple::ppc64le: addPowerPCHTMBegin(*I); break;
+        case Triple::x86_64: addX86HTMBegin(*I); break;
+        default: llvm_unreachable("HTM -- unsupported architecture");
+        }
+        NumHTMBegins++;
+        NumMigPoints++;
+      }
+    }
+    else {
+      for(auto I = MigPointInsts.begin(), E = MigPointInsts.end();
+          I != E; ++I) {
+        addMigrationPoint(*I);
+        NumMigPoints++;
+      }
+    }
+  }
+};
+
+} /* end anonymous namespace */
+
+char MigrationPoints::ID = 0;
+
+const MigrationPoints::IntrinsicMap MigrationPoints::HTMBegin = {
+  {Triple::x86_64, Intrinsic::x86_xbegin},
+  {Triple::ppc64le, Intrinsic::ppc_tbegin}
+};
+
+const MigrationPoints::IntrinsicMap MigrationPoints::HTMEnd = {
+  {Triple::x86_64, Intrinsic::x86_xend},
+  {Triple::ppc64le, Intrinsic::ppc_tend}
+};
+
+const MigrationPoints::IntrinsicMap MigrationPoints::HTMTest = {
+  {Triple::x86_64, Intrinsic::x86_xtest},
+  {Triple::ppc64le, Intrinsic::ppc_ttest}
+};
+
+INITIALIZE_PASS(MigrationPoints, "migration-points",
+                "Insert migration points into functions", true, false)
+
+namespace llvm {
+  FunctionPass *createMigrationPointsPass()
+  { return new MigrationPoints(); }
+}
diff --git a/llvm/lib/Transforms/Utils/CMakeLists.txt b/llvm/lib/Transforms/Utils/CMakeLists.txt
index c232aa6223c..4d17f43a44d 100644
--- a/llvm/lib/Transforms/Utils/CMakeLists.txt
+++ b/llvm/lib/Transforms/Utils/CMakeLists.txt
@@ -41,6 +41,7 @@ add_llvm_library(LLVMTransformUtils
   Mem2Reg.cpp
   MetaRenamer.cpp
   ModuleUtils.cpp
+  NameStringLiterals.cpp
   NameAnonGlobals.cpp
   PredicateInfo.cpp
   PromoteMemoryToRegister.cpp
@@ -53,6 +54,7 @@ add_llvm_library(LLVMTransformUtils
   SimplifyLibCalls.cpp
   SizeOpts.cpp
   SplitModule.cpp
+  StaticVarSections.cpp
   StripNonLineTableDebugInfo.cpp
   SymbolRewriter.cpp
   UnifyFunctionExitNodes.cpp
diff --git a/llvm/lib/Transforms/Utils/NameStringLiterals.cpp b/llvm/lib/Transforms/Utils/NameStringLiterals.cpp
new file mode 100644
index 00000000000..03cbb7c9ea1
--- /dev/null
+++ b/llvm/lib/Transforms/Utils/NameStringLiterals.cpp
@@ -0,0 +1,118 @@
+#include <algorithm>
+#include <cctype>
+#include "llvm/Pass.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/GlobalVariable.h"
+#include "llvm/IR/GlobalValue.h"
+#include "llvm/IR/Module.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+#define DEBUG_TYPE "name-string-literals"
+#define CHARS_FOR_NAME 10
+
+using namespace llvm;
+
+namespace
+{
+
+/**
+ * Generate unique name for private anonymous string literals.  Uses the
+ * filename, LLVM's temporary name and (up to) the first 10 characters of the
+ * string.  Converts non-alphanumeric characters to underscores.
+ */
+std::string UniquifySymbol(const Module &M, GlobalVariable &Sym)
+{
+  std::string newName;
+  std::string::size_type loc;
+  auto filter = [](char c){ return !isalnum(c); };
+
+  newName = M.getName();
+  loc = newName.find_last_of('.');
+  newName = newName.substr(0, loc) + "_" + Sym.getName().str() + "_";
+  std::replace_if(newName.begin(), newName.end(), filter, '_');
+
+  // Check if it's a string, and if so use string content to uniquify
+  if(Sym.hasInitializer()) {
+    Constant *Initializer = Sym.getInitializer();
+    if(isa<ConstantDataSequential>(Initializer)) {
+      ConstantDataSequential *CDS = cast<ConstantDataSequential>(Initializer);
+      if(CDS->isString()) {
+        std::string data = CDS->getAsString().substr(0, CHARS_FOR_NAME);
+        std::replace_if(data.begin(), data.end(), filter, '_');
+        newName += data;
+      }
+    }
+  }
+
+  return newName;
+}
+
+/**
+ * This pass searches for anonymous read-only data for which there is no symbol
+ * and generates a symbol for the data.  This is required by the Popcorn
+ * compiler in order to align the data at link-time.
+ */
+class NameStringLiterals : public ModulePass
+{
+public:
+	static char ID;
+
+  NameStringLiterals() : ModulePass(ID) {}
+  ~NameStringLiterals() {}
+
+	/* ModulePass virtual methods */
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const { AU.setPreservesCFG(); }
+	virtual bool runOnModule(Module &M)
+  {
+    bool modified = false;
+    std::string newName;
+    Module::global_iterator gl, gle; // for global variables
+
+    LLVM_DEBUG(errs() << "\n********** Begin NameStringLiterals **********\n"
+                 << "********** Module: " << M.getName() << " **********\n\n");
+
+    // Iterate over all globals and generate symbol for anonymous string
+    // literals in each module
+    for(gl = M.global_begin(), gle = M.global_end(); gl != gle; gl++) {
+      // DONT NEED TO CHANGE NAME PER-SE just change type
+      // PrivateLinkage does NOT show up in any symbol table in the object file!
+      if(gl->getLinkage() == GlobalValue::PrivateLinkage) {
+        //change Linkage
+        //FROM private unnamed_addr constant [num x i8]
+        //TO global [num x i8]
+        gl->setLinkage(GlobalValue::ExternalLinkage);
+
+        // Make the global's name unique so we don't clash when linking with
+        // other files
+        newName = UniquifySymbol(M, *gl);
+        gl->setName(newName);
+
+        // Also REMOVE unnamed_addr value
+        if(gl->hasGlobalUnnamedAddr()) {
+          gl->setUnnamedAddr(GlobalValue::UnnamedAddr::None);
+        }
+
+        modified = true;
+
+        LLVM_DEBUG(errs() << "New anonymous string name: " << newName << "\n";);
+      } else {
+        LLVM_DEBUG(errs() << "> " <<  *gl << ", linkage: "
+                     << gl->getLinkage() << "\n");
+      }
+    }
+  
+    return modified;
+  }
+  virtual StringRef getPassName() const { return "Name string literals"; }
+};
+
+} /* end anonymous namespace */
+
+char NameStringLiterals::ID = 0;
+INITIALIZE_PASS(NameStringLiterals, "name-string-literals",
+  "Generate symbols for anonymous string literals", false, false)
+
+namespace llvm {
+  ModulePass *createNameStringLiteralsPass() { return new NameStringLiterals(); }
+}
diff --git a/llvm/lib/Transforms/Utils/StaticVarSections.cpp b/llvm/lib/Transforms/Utils/StaticVarSections.cpp
new file mode 100644
index 00000000000..1ea0d2402ca
--- /dev/null
+++ b/llvm/lib/Transforms/Utils/StaticVarSections.cpp
@@ -0,0 +1,104 @@
+#include <algorithm>
+#include "llvm/Pass.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/GlobalVariable.h"
+#include "llvm/IR/GlobalValue.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+#define DEBUG_TYPE "static-var-sections"
+
+using namespace llvm;
+
+namespace
+{
+
+std::string UniquifySymbol(const Module &M,
+                           std::string &section,
+                           GlobalVariable &Sym)
+{
+  std::string newName;
+  auto filter = [](char c){ return !isalnum(c); };
+
+  newName = M.getName().str() + "_" + Sym.getName().str();
+  std::replace_if(newName.begin(), newName.end(), filter, '_');
+
+  return section + newName;
+}
+
+/**
+ * This pass searches for static, i.e., module-private, global variables and
+ * modifies their linkage to be in their own sections similarly to other
+ * global variables with the -fdata-sections switch.  By default, LLVM doesn't
+ * apply -fdata-sections to static global variables.
+ */
+class StaticVarSections : public ModulePass
+{
+public:
+	static char ID;
+
+	StaticVarSections() : ModulePass(ID) {}
+	~StaticVarSections() {}
+
+	/* ModulePass virtual methods */
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const { AU.setPreservesCFG(); }
+	virtual bool runOnModule(Module &M)
+  {
+    bool modified = false;
+    Module::global_iterator gl, gle; // for global variables
+  
+    LLVM_DEBUG(errs() << "\n********** Beginning StaticVarSections **********\n"
+                 << "********** Module: " << M.getName() << " **********\n\n");
+  
+    // Iterate over all static globals and place them in their own section
+    for(gl = M.global_begin(), gle = M.global_end(); gl != gle; gl++) {
+      std::string secName = ".";
+      if(gl->isThreadLocal()) secName += "t";
+  
+      if(gl->hasCommonLinkage() &&
+         gl->getName().find(".cache.") != std::string::npos) {
+        gl->setLinkage(GlobalValue::InternalLinkage);
+      }
+  
+      // InternalLinkage is specifically for STATIC variables
+      if(gl->hasInternalLinkage() && !gl->hasSection()) {
+        if(gl->isConstant()) {
+          //Belongs in RODATA
+          assert(!gl->isThreadLocal() && "TLS data should not be in .rodata");
+          secName += "rodata.";
+        }
+        else if(gl->getInitializer()->isZeroValue()) {
+          //Belongs in BSS
+          secName += "bss.";
+        }
+        else {
+          //Belongs in DATA
+          secName += "data.";
+        }
+
+        secName = UniquifySymbol(M, secName, *gl);
+        gl->setSection(secName);
+        modified = true;
+
+        LLVM_DEBUG(errs() << *gl << " - new section: " << secName << "\n");
+      } else {
+        LLVM_DEBUG(errs() << "> " <<  *gl << ", linkage: "
+                     << gl->getLinkage() << "\n");
+        continue;
+      }
+    }
+    
+    return modified;
+  }
+  virtual StringRef getPassName() const { return "Static variables in separate sections"; }
+};
+
+} /* end anonymous namespace */
+
+char StaticVarSections::ID = 0;
+INITIALIZE_PASS(StaticVarSections, "static-var-sections",
+  "Put static variables into separate sections", false, false)
+
+namespace llvm {
+  ModulePass *createStaticVarSectionsPass() { return new StaticVarSections(); }
+}
diff --git a/llvm/lib/Transforms/Utils/Utils.cpp b/llvm/lib/Transforms/Utils/Utils.cpp
index 5272ab6e95d..f11df0a7311 100644
--- a/llvm/lib/Transforms/Utils/Utils.cpp
+++ b/llvm/lib/Transforms/Utils/Utils.cpp
@@ -33,7 +33,9 @@ void llvm::initializeTransformUtils(PassRegistry &Registry) {
   initializeLowerInvokeLegacyPassPass(Registry);
   initializeLowerSwitchPass(Registry);
   initializeNameAnonGlobalLegacyPassPass(Registry);
+  initializeNameStringLiteralsPass(Registry);
   initializePromoteLegacyPassPass(Registry);
+  initializeStaticVarSectionsPass(Registry);
   initializeStripNonLineTableDebugInfoPass(Registry);
   initializeUnifyFunctionExitNodesPass(Registry);
   initializeMetaRenamerPass(Registry);
diff --git a/llvm/test/CodeGen/RISCV/stackmap-nops.ll b/llvm/test/CodeGen/RISCV/stackmap-nops.ll
new file mode 100644
index 00000000000..69c0cde261b
--- /dev/null
+++ b/llvm/test/CodeGen/RISCV/stackmap-nops.ll
@@ -0,0 +1,24 @@
+; RUN: llc -verify-machineinstrs < %s -mtriple=powerpc64-unknown-gnu-linux | FileCheck %s
+
+define void @test_shadow_optimization() {
+entry:
+; Expect 12 bytes worth of nops here rather than 32: With the shadow optimization
+; in place, 20 bytes will be consumed by the frame teardown and return instr.
+; CHECK-LABEL: test_shadow_optimization:
+
+; CHECK:      nop
+; CHECK-NEXT: nop
+; CHECK-NEXT: nop
+; CHECK-NOT:  nop
+; CHECK: addi 1, 1, 64
+; CHECK: ld [[REG1:[0-9]+]], 16(1)
+; CHECK: ld 31, -8(1)
+; CHECK: mtlr [[REG1]]
+; CHECK: blr
+
+  tail call void (i64, i32, ...) @llvm.experimental.stackmap(i64  0, i32  32)
+  ret void
+}
+
+declare void @llvm.experimental.stackmap(i64, i32, ...)
+
diff --git a/llvm/test/CodeGen/RISCV/stackmap.ll b/llvm/test/CodeGen/RISCV/stackmap.ll
new file mode 100644
index 00000000000..8b2466bc060
--- /dev/null
+++ b/llvm/test/CodeGen/RISCV/stackmap.ll
@@ -0,0 +1,372 @@
+; RUN: llc -verify-machineinstrs < %s | FileCheck %s
+;
+; Note: Print verbose stackmaps using -debug-only=stackmaps.
+
+; We are not getting the correct stack alignment when cross compiling for arm64.
+; So specify a datalayout here.
+target datalayout = "E-m:e-i64:64-n32:64"
+target triple = "powerpc64-unknown-linux-gnu"
+
+; CHECK-LABEL: constantargs:
+; CHECK: {{^}}.L[[constantargs_BEGIN:.*]]:{{$}}
+
+; CHECK-LABEL: osrinline:
+; CHECK: {{^}}.L[[osrinline_BEGIN:.*]]:{{$}}
+
+; CHECK-LABEL: osrcold:
+; CHECK: {{^}}.L[[osrcold_BEGIN:.*]]:{{$}}
+
+; CHECK-LABEL: propertyRead:
+; CHECK: {{^}}.L[[propertyRead_BEGIN:.*]]:{{$}}
+
+; CHECK-LABEL: propertyWrite:
+; CHECK: {{^}}.L[[propertyWrite_BEGIN:.*]]:{{$}}
+
+; CHECK-LABEL: jsVoidCall:
+; CHECK: {{^}}.L[[jsVoidCall_BEGIN:.*]]:{{$}}
+
+; CHECK-LABEL: jsIntCall:
+; CHECK: {{^}}.L[[jsIntCall_BEGIN:.*]]:{{$}}
+
+; CHECK-LABEL: spilledValue:
+; CHECK: {{^}}.L[[spilledValue_BEGIN:.*]]:{{$}}
+
+; CHECK-LABEL: spilledStackMapValue:
+; CHECK: {{^}}.L[[spilledStackMapValue_BEGIN:.*]]:{{$}}
+
+; CHECK-LABEL: liveConstant:
+; CHECK: {{^}}.L[[liveConstant_BEGIN:.*]]:{{$}}
+
+; CHECK-LABEL: clobberLR:
+; CHECK: {{^}}.L[[clobberLR_BEGIN:.*]]:{{$}}
+
+
+; CHECK-LABEL:  .section  .llvm_stackmaps
+; CHECK-NEXT:  __LLVM_StackMaps:
+; Header
+; CHECK-NEXT:   .byte 3
+; CHECK-NEXT:   .byte 0
+; CHECK-NEXT:   .short 0
+; Num Functions
+; CHECK-NEXT:   .long 11
+; Num LargeConstants
+; CHECK-NEXT:   .long 2
+; Num Callsites
+; CHECK-NEXT:   .long 11
+
+; Functions and stack size
+; CHECK-NEXT:   .quad constantargs
+; CHECK-NEXT:   .quad 128
+; CHECK-NEXT:   .quad 1
+; CHECK-NEXT:   .quad osrinline
+; CHECK-NEXT:   .quad 144
+; CHECK-NEXT:   .quad 1
+; CHECK-NEXT:   .quad osrcold
+; CHECK-NEXT:   .quad 128
+; CHECK-NEXT:   .quad 1
+; CHECK-NEXT:   .quad propertyRead
+; CHECK-NEXT:   .quad 128
+; CHECK-NEXT:   .quad 1
+; CHECK-NEXT:   .quad propertyWrite
+; CHECK-NEXT:   .quad 128
+; CHECK-NEXT:   .quad 1
+; CHECK-NEXT:   .quad jsVoidCall
+; CHECK-NEXT:   .quad 128
+; CHECK-NEXT:   .quad 1
+; CHECK-NEXT:   .quad jsIntCall
+; CHECK-NEXT:   .quad 128
+; CHECK-NEXT:   .quad 1
+; CHECK-NEXT:   .quad spilledValue
+; CHECK-NEXT:   .quad 304
+; CHECK-NEXT:   .quad 1
+; CHECK-NEXT:   .quad spilledStackMapValue
+; CHECK-NEXT:   .quad 224
+; CHECK-NEXT:   .quad 1
+; CHECK-NEXT:   .quad liveConstant
+; CHECK-NEXT:   .quad 64
+; CHECK-NEXT:   .quad 1
+; CHECK-NEXT:   .quad clobberLR
+; CHECK-NEXT:   .quad 208
+; CHECK-NEXT:   .quad 1
+
+; Num LargeConstants
+; CHECK-NEXT:   .quad   4294967295
+; CHECK-NEXT:   .quad   4294967296
+
+; Constant arguments
+;
+; CHECK-NEXT:   .quad   1
+; CHECK-NEXT:   .long   .L{{.*}}-.L[[constantargs_BEGIN]]
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .short  4
+; SmallConstant
+; CHECK-NEXT:   .byte   4
+; CHECK-NEXT:   .byte   0
+; CHECK-NEXT:   .short  8
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .long   65535
+; SmallConstant
+; CHECK-NEXT:   .byte   4
+; CHECK-NEXT:   .byte   0
+; CHECK-NEXT:   .short  8
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .long   65536
+; SmallConstant
+; CHECK-NEXT:   .byte   5
+; CHECK-NEXT:   .byte   0
+; CHECK-NEXT:   .short  8
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .long   0
+; LargeConstant at index 0
+; CHECK-NEXT:   .byte   5
+; CHECK-NEXT:   .byte   0
+; CHECK-NEXT:   .short  8
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .long   1
+
+define void @constantargs() {
+entry:
+  %0 = inttoptr i64 244837814094590 to i8*
+  tail call void (i64, i32, i8*, i32, ...) @llvm.experimental.patchpoint.void(i64 1, i32 40, i8* %0, i32 0, i64 65535, i64 65536, i64 4294967295, i64 4294967296)
+  ret void
+}
+
+; Inline OSR Exit
+;
+; CHECK:  .long   .L{{.*}}-.L[[osrinline_BEGIN]]
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .short  2
+; CHECK-NEXT:   .byte   1
+; CHECK-NEXT:   .byte   0
+; CHECK-NEXT:   .short  8
+; CHECK-NEXT:   .short  {{[0-9]+}}
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .long   0
+; CHECK-NEXT:   .byte   1
+; CHECK-NEXT:   .byte   0
+; CHECK-NEXT:   .short  8
+; CHECK-NEXT:   .short  {{[0-9]+}}
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .long  0
+define void @osrinline(i64 %a, i64 %b) {
+entry:
+  ; Runtime void->void call.
+  call void inttoptr (i64 244837814094590 to void ()*)()
+  ; Followed by inline OSR patchpoint with 12-byte shadow and 2 live vars.
+  call void (i64, i32, ...) @llvm.experimental.stackmap(i64 3, i32 12, i64 %a, i64 %b)
+  ret void
+}
+
+; Cold OSR Exit
+;
+; 2 live variables in register.
+;
+; CHECK:  .long   .L{{.*}}-.L[[osrcold_BEGIN]]
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .short  2
+; CHECK-NEXT:   .byte   1
+; CHECK-NEXT:   .byte   0
+; CHECK-NEXT:   .short  8
+; CHECK-NEXT:   .short  {{[0-9]+}}
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .long   0
+; CHECK-NEXT:   .byte   1
+; CHECK-NEXT:   .byte   0
+; CHECK-NEXT:   .short  8
+; CHECK-NEXT:   .short  {{[0-9]+}}
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .long  0
+define void @osrcold(i64 %a, i64 %b) {
+entry:
+  %test = icmp slt i64 %a, %b
+  br i1 %test, label %ret, label %cold
+cold:
+  ; OSR patchpoint with 12-byte nop-slide and 2 live vars.
+  %thunk = inttoptr i64 244837814094590 to i8*
+  call void (i64, i32, i8*, i32, ...) @llvm.experimental.patchpoint.void(i64 4, i32 40, i8* %thunk, i32 0, i64 %a, i64 %b)
+  unreachable
+ret:
+  ret void
+}
+
+; Property Read
+; CHECK:  .long   .L{{.*}}-.L[[propertyRead_BEGIN]]
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .short  0
+;
+; FIXME: There are currently no stackmap entries. After moving to
+; AnyRegCC, we will have entries for the object and return value.
+define i64 @propertyRead(i64* %obj) {
+entry:
+  %resolveRead = inttoptr i64 244837814094590 to i8*
+  %result = call i64 (i64, i32, i8*, i32, ...) @llvm.experimental.patchpoint.i64(i64 5, i32 40, i8* %resolveRead, i32 1, i64* %obj)
+  %add = add i64 %result, 3
+  ret i64 %add
+}
+
+; Property Write
+; CHECK:  .long   .L{{.*}}-.L[[propertyWrite_BEGIN]]
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .short  2
+; CHECK-NEXT:   .byte   1
+; CHECK-NEXT:   .byte   0
+; CHECK-NEXT:   .short  8
+; CHECK-NEXT:   .short  {{[0-9]+}}
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .long   0
+; CHECK-NEXT:   .byte   1
+; CHECK-NEXT:   .byte   0
+; CHECK-NEXT:   .short  8
+; CHECK-NEXT:   .short  {{[0-9]+}}
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .long   0
+define void @propertyWrite(i64 %dummy1, i64* %obj, i64 %dummy2, i64 %a) {
+entry:
+  %resolveWrite = inttoptr i64 244837814094590 to i8*
+  call anyregcc void (i64, i32, i8*, i32, ...) @llvm.experimental.patchpoint.void(i64 6, i32 40, i8* %resolveWrite, i32 2, i64* %obj, i64 %a)
+  ret void
+}
+
+; Void JS Call
+;
+; 2 live variables in registers.
+;
+; CHECK:  .long   .L{{.*}}-.L[[jsVoidCall_BEGIN]]
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .short  2
+; CHECK-NEXT:   .byte   1
+; CHECK-NEXT:   .byte   0
+; CHECK-NEXT:   .short  8
+; CHECK-NEXT:   .short  {{[0-9]+}}
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .long   0
+; CHECK-NEXT:   .byte   1
+; CHECK-NEXT:   .byte   0
+; CHECK-NEXT:   .short  8
+; CHECK-NEXT:   .short  {{[0-9]+}}
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .long   0
+define void @jsVoidCall(i64 %dummy1, i64* %obj, i64 %arg, i64 %l1, i64 %l2) {
+entry:
+  %resolveCall = inttoptr i64 244837814094590 to i8*
+  call void (i64, i32, i8*, i32, ...) @llvm.experimental.patchpoint.void(i64 7, i32 40, i8* %resolveCall, i32 2, i64* %obj, i64 %arg, i64 %l1, i64 %l2)
+  ret void
+}
+
+; i64 JS Call
+;
+; 2 live variables in registers.
+;
+; CHECK:  .long   .L{{.*}}-.L[[jsIntCall_BEGIN]]
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .short  2
+; CHECK-NEXT:   .byte   1
+; CHECK-NEXT:   .byte   0
+; CHECK-NEXT:   .short  8
+; CHECK-NEXT:   .short  {{[0-9]+}}
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .long   0
+; CHECK-NEXT:   .byte   1
+; CHECK-NEXT:   .byte   0
+; CHECK-NEXT:   .short  8
+; CHECK-NEXT:   .short  {{[0-9]+}}
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .long   0
+define i64 @jsIntCall(i64 %dummy1, i64* %obj, i64 %arg, i64 %l1, i64 %l2) {
+entry:
+  %resolveCall = inttoptr i64 244837814094590 to i8*
+  %result = call i64 (i64, i32, i8*, i32, ...) @llvm.experimental.patchpoint.i64(i64 8, i32 40, i8* %resolveCall, i32 2, i64* %obj, i64 %arg, i64 %l1, i64 %l2)
+  %add = add i64 %result, 3
+  ret i64 %add
+}
+
+; Spilled stack map values.
+;
+; Verify 28 stack map entries.
+;
+; CHECK:  .long .L{{.*}}-.L[[spilledValue_BEGIN]]
+; CHECK-NEXT:   .short 0
+; CHECK-NEXT:   .short 28
+;
+; Check that at least one is a spilled entry from r31.
+; Location: Indirect FP + ...
+; CHECK:        .byte 3
+; CHECK-NEXT:   .byte 0
+; CHECK-NEXT:   .short 
+; CHECK-NEXT:   .short 31
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .long
+define void @spilledValue(i64 %arg0, i64 %arg1, i64 %arg2, i64 %arg3, i64 %arg4, i64 %l0, i64 %l1, i64 %l2, i64 %l3, i64 %l4, i64 %l5, i64 %l6, i64 %l7, i64 %l8, i64 %l9, i64 %l10, i64 %l11, i64 %l12, i64 %l13, i64 %l14, i64 %l15, i64 %l16, i64 %l17, i64 %l18, i64 %l19, i64 %l20, i64 %l21, i64 %l22, i64 %l23, i64 %l24, i64 %l25, i64 %l26, i64 %l27) {
+entry:
+  call void (i64, i32, i8*, i32, ...) @llvm.experimental.patchpoint.void(i64 11, i32 40, i8* null, i32 5, i64 %arg0, i64 %arg1, i64 %arg2, i64 %arg3, i64 %arg4, i64 %l0, i64 %l1, i64 %l2, i64 %l3, i64 %l4, i64 %l5, i64 %l6, i64 %l7, i64 %l8, i64 %l9, i64 %l10, i64 %l11, i64 %l12, i64 %l13, i64 %l14, i64 %l15, i64 %l16, i64 %l17, i64 %l18, i64 %l19, i64 %l20, i64 %l21, i64 %l22, i64 %l23, i64 %l24, i64 %l25, i64 %l26, i64 %l27)
+  ret void
+}
+
+; Spilled stack map values.
+;
+; Verify 30 stack map entries.
+;
+; CHECK:  .long .L{{.*}}-.L[[spilledStackMapValue_BEGIN]]
+; CHECK-NEXT:   .short 0
+; CHECK-NEXT:   .short 30
+;
+; Check that at least one is a spilled entry from r31.
+; Location: Indirect FP + ...
+; CHECK:        .byte 3
+; CHECK-NEXT:   .byte   0
+; CHECK-NEXT:   .short 
+; CHECK-NEXT:   .short 31
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .long
+define webkit_jscc void @spilledStackMapValue(i64 %l0, i64 %l1, i64 %l2, i64 %l3, i64 %l4, i64 %l5, i64 %l6, i64 %l7, i64 %l8, i64 %l9, i64 %l10, i64 %l11, i64 %l12, i64 %l13, i64 %l14, i64 %l15, i64 %l16, i64 %l17, i64 %l18, i64 %l19, i64 %l20, i64 %l21, i64 %l22, i64 %l23, i64 %l24, i64 %l25, i64 %l26, i64 %l27, i64 %l28, i64 %l29) {
+entry:
+  call void (i64, i32, ...) @llvm.experimental.stackmap(i64 12, i32 16, i64 %l0, i64 %l1, i64 %l2, i64 %l3, i64 %l4, i64 %l5, i64 %l6, i64 %l7, i64 %l8, i64 %l9, i64 %l10, i64 %l11, i64 %l12, i64 %l13, i64 %l14, i64 %l15, i64 %l16, i64 %l17, i64 %l18, i64 %l19, i64 %l20, i64 %l21, i64 %l22, i64 %l23, i64 %l24, i64 %l25, i64 %l26, i64 %l27, i64 %l28, i64 %l29)
+  ret void
+}
+
+
+; Map a constant value.
+;
+; CHECK:  .long .L{{.*}}-.L[[liveConstant_BEGIN]]
+; CHECK-NEXT:   .short 0
+; 1 location
+; CHECK-NEXT:   .short 1
+; Loc 0: SmallConstant
+; CHECK-NEXT:   .byte   4
+; CHECK-NEXT:   .byte   0
+; CHECK-NEXT:   .short  8
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .long   33
+
+define void @liveConstant() {
+  tail call void (i64, i32, ...) @llvm.experimental.stackmap(i64 15, i32 8, i32 33)
+  ret void
+}
+
+; Map a value when LR is the only free register.
+;
+; CHECK:  .long .L{{.*}}-.L[[clobberLR_BEGIN]]
+; CHECK-NEXT:   .short 0
+; 1 location
+; CHECK-NEXT:   .short 1
+; Loc 0: Indirect FP (r31) - offset
+; CHECK-NEXT:   .byte   3
+; CHECK-NEXT:   .byte   0
+; CHECK-NEXT:   .short  4
+; CHECK-NEXT:   .short  31
+; CHECK-NEXT:   .short  0
+; CHECK-NEXT:   .long   {{[0-9]+}}
+define void @clobberLR(i32 %a) {
+  tail call void asm sideeffect "nop", "~{r0},~{r3},~{r4},~{r5},~{r6},~{r7},~{r8},~{r9},~{r10},~{r11},~{r12},~{r14},~{r15},~{r16},~{r17},~{r18},~{r19},~{r20},~{r21},~{r22},~{r23},~{r24},~{r25},~{r26},~{r27},~{r28},~{r29},~{r30},~{r31}"() nounwind
+  tail call void (i64, i32, ...) @llvm.experimental.stackmap(i64 16, i32 8, i32 %a)
+  ret void
+}
+
+declare void @llvm.experimental.stackmap(i64, i32, ...)
+declare void @llvm.experimental.patchpoint.void(i64, i32, i8*, i32, ...)
+declare i64 @llvm.experimental.patchpoint.i64(i64, i32, i8*, i32, ...)
diff --git a/llvm/utils/benchmark/src/benchmark_register.h b/llvm/utils/benchmark/src/benchmark_register.h
index 0705e219f2fa..4caa5ad4da07 100644
--- a/llvm/utils/benchmark/src/benchmark_register.h
+++ b/llvm/utils/benchmark/src/benchmark_register.h
@@ -1,6 +1,7 @@
 #ifndef BENCHMARK_REGISTER_H
 #define BENCHMARK_REGISTER_H
 
+#include <limits>
 #include <vector>
 
 #include "check.h"
