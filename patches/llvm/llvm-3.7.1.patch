Index: include/llvm/ADT/SetVector.h
===================================================================
--- include/llvm/ADT/SetVector.h	(revision 320332)
+++ include/llvm/ADT/SetVector.h	(working copy)
@@ -85,6 +85,12 @@
     return vector_.end();
   }
 
+  /// \brief Return the first element of the SetVector.
+  const T &front() const {
+    assert(!empty() && "Cannot call front() on empty SetVector!");
+    return vector_.front();
+  }
+
   /// \brief Return the last element of the SetVector.
   const T &back() const {
     assert(!empty() && "Cannot call back() on empty SetVector!");
Index: include/llvm/ADT/StringSet.h
===================================================================
--- include/llvm/ADT/StringSet.h	(revision 320332)
+++ include/llvm/ADT/StringSet.h	(working copy)
@@ -23,7 +23,16 @@
   class StringSet : public llvm::StringMap<char, AllocatorTy> {
     typedef llvm::StringMap<char, AllocatorTy> base;
   public:
+    StringSet() : StringMap<char, AllocatorTy>() {}
 
+    StringSet(std::initializer_list<StringRef> List)
+      : StringMap<char, AllocatorTy>() {
+      for(auto Key : List) {
+        assert(!Key.empty());
+        base::insert(std::make_pair(Key, '\0'));
+      }
+    }
+
     std::pair<typename base::iterator, bool> insert(StringRef Key) {
       assert(!Key.empty());
       return base::insert(std::make_pair(Key, '\0'));
Index: include/llvm/ADT/ilist_node.h
===================================================================
--- include/llvm/ADT/ilist_node.h	(revision 320332)
+++ include/llvm/ADT/ilist_node.h	(working copy)
@@ -70,7 +70,7 @@
     const NodeTy *Prev = this->getPrev();
 
     // Check for sentinel.
-    if (!Prev->getNext())
+    if (!Prev || !Prev->getNext())
       return nullptr;
 
     return Prev;
@@ -81,7 +81,7 @@
     NodeTy *Next = getNext();
 
     // Check for sentinel.
-    if (!Next->getNext())
+    if (!Next || !Next->getNext())
       return nullptr;
 
     return Next;
Index: include/llvm/Analysis/LiveValues.h
===================================================================
--- include/llvm/Analysis/LiveValues.h	(nonexistent)
+++ include/llvm/Analysis/LiveValues.h	(working copy)
@@ -0,0 +1,209 @@
+/*
+ * Calculate live-value sets for functions.
+ *
+ * Liveness-analysis is based on the non-iterative dataflow algorithm for
+ * reducible graphs by Brandner et. al in:
+ *
+ * "Computing Liveness Sets for SSA-Form Programs"
+ * URL: https://hal.inria.fr/inria-00558509v1/document
+ * Accessed: 5/19/2016
+ *
+ * Author: Rob Lyerly <rlyerly@vt.edu>
+ * Date: 5/19/2016
+ */
+
+#ifndef _LIVE_VALUES_H
+#define _LIVE_VALUES_H
+
+#include <map>
+#include <set>
+#include <list>
+#include "llvm/Pass.h"
+#include "llvm/Analysis/LoopNestingTree.h"
+#include "llvm/IR/Function.h"
+#include "llvm/Support/raw_ostream.h"
+
+namespace llvm {
+
+class LiveValues : public FunctionPass
+{
+public:
+  typedef std::pair<const BasicBlock *, const BasicBlock *> Edge;
+
+  static char ID;
+
+  /**
+   * Default constructor.
+   */
+  LiveValues(void);
+
+  /**
+   * Default destructor.
+   */
+  ~LiveValues(void) {}
+
+  /**
+   * Return whether or not a given type should be included in the analysis.
+   * @return true if the type is included in liveness sets, false otherwise
+   */
+  bool includeAsm(void) const { return inlineasm; }
+  bool includeBitcasts(void) const { return bitcasts; }
+  bool includeComparisons(void) const { return comparisons; }
+  bool includeConstants(void) const { return constants; }
+  bool includeMetadata(void) const { return metadata; }
+
+  /**
+   * Set whether or not to include the specified type in the analysis (all
+   * are set to false by default by the constructor).
+   * @param include true if it should be included, false otherwise
+   */
+  void includeAsm(bool include) { inlineasm = include; }
+  void includeBitcasts(bool include) { bitcasts = include; }
+  void includeComparisons(bool include) { comparisons = include; }
+  void includeConstants(bool include) { constants = include; }
+  void includeMetadata(bool include) { metadata = include; }
+
+  /**
+   * Register which analysis passes we need.
+   * @param AU an analysis usage object
+   */
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const;
+
+  /**
+   * Calculate liveness sets for a function.
+   * @param F a function for which to calculate live values.
+   * @return false, always
+   */
+  virtual bool runOnFunction(Function &F);
+
+  /**
+   * Get the human-readable name of the pass.
+   * @return the pass name
+   */
+  virtual const char *getPassName() const { return "Live value analysis"; }
+
+  /**
+   * Print a human-readable version of the analysis.
+   * @param O an output stream
+   * @param F the function for which to print analysis
+   */
+  virtual void print(raw_ostream &O, const Function *F) const;
+
+  /**
+   * Return the live-in set for a basic block.
+   * @param BB a basic block
+   * @return a set of live-in values for the basic block; this set must be
+   *         freed by the user.
+   */
+  std::set<const Value *> *getLiveIn(const BasicBlock *BB) const;
+
+  /**
+   * Return the live-out set for a basic block.
+   * @param BB a basic block
+   * @return a set of live-out values for the basic block; this set must be
+   *         freed by the user.
+   */
+  std::set<const Value *> *getLiveOut(const BasicBlock *BB) const;
+
+  /**
+   * Get the live values across a given instruction, i.e., values live right
+   * after the invocation of the instruction (excluding the value defined by
+   * the instruction itself).
+   * @param inst an instruction
+   * @return the set of values live directly before the instruction; this set
+   *         must be freed by the user.
+   */
+  std::set<const Value *> *
+  getLiveValues(const Instruction *inst) const;
+
+private:
+  /* Should values of each type be included? */
+  bool inlineasm;
+  bool bitcasts;
+  bool comparisons;
+  bool constants;
+  bool metadata;
+
+  /* A loop nesting forest composed of 0 or more loop nesting trees. */
+  typedef std::list<LoopNestingTree> LoopNestingForest;
+
+  /* Maps live values to a basic block. */
+  typedef std::map<const BasicBlock *, std::set<const Value *> > LiveVals;
+  typedef std::pair<const BasicBlock *, std::set<const Value *> > LiveValsPair;
+
+  /* Store analysis for all functions. */
+  std::map<const Function *, LiveVals> FuncBBLiveIn;
+  std::map<const Function *, LiveVals> FuncBBLiveOut;
+
+  /**
+   * Return whether or not a value is a variable that should be tracked.
+   * @param val a value
+   * @return true if the value is a variable to be tracked, false otherwise
+   */
+  bool includeVal(const Value *val) const;
+
+  /**
+   * Insert the values used in phi-nodes at the beginning of basic block S (as
+   * values live from B) into the set uses.
+   * @param B a basic block which passes live values into phi-nodes in S
+   * @param S a basic block, successor to B
+   * @param uses set in which to add values used in phi-nodes in B
+   * @return the number of values added to the set
+   */
+  unsigned phiUses(const BasicBlock *B,
+                   const BasicBlock *S,
+                   std::set<const Value *> &uses);
+
+  /**
+   * Insert the values defined by the phi-nodes at the beginning of basic block
+   * B into the set defs.
+   * @param B a basic block
+   * @param defs set in which to add values defined by phi-nodes in B
+   * @return the number of values added to the set
+   */
+  unsigned phiDefs(const BasicBlock *B,
+                   std::set<const Value *> &defs);
+
+  /**
+   * Do a post-order traversal of the control flow graph to calculate partial
+   * liveness sets.
+   * @param F a function for which to calculate per-basic block partial
+   *          liveness sets
+   * @param liveIn per-basic block live-in values
+   * @param liveOut per-basic block live-out values
+   */
+  void dagDFS(Function &F, LiveVals &liveIn, LiveVals &liveOut);
+
+  /**
+   * Construct the loop-nesting forest for a function.
+   * @param F a function for which to calculate the loop-nesting forest.
+   * @param LNF a loop nesting forest to populate with loop nesting trees.
+   */
+  void constructLoopNestingForest(Function &F, LoopNestingForest &LNF);
+
+  /**
+   * Propagate live values throughout the loop-nesting tree.
+   * @param loopNest a loop-nesting tree
+   * @param liveIn per-basic block live-in values
+   * @param liveOut per-basic block live-out values
+   */
+  void propagateValues(const LoopNestingTree &loopNest,
+                       LiveVals &liveIn,
+                       LiveVals &liveOut);
+
+  /**
+   * Propagate live values within loops for all loop-nesting trees in the
+   * function's loop-nesting forest.
+   * @param LNF a loop nesting forest
+   * @param liveIn per-basic block live-in values
+   * @param liveOut per-basic block live-out values
+   */
+  void loopTreeDFS(LoopNestingForest &LNF,
+                   LiveVals &liveIn,
+                   LiveVals &liveOut);
+};
+
+} /* llvm namespace */
+
+#endif /* _LIVE_VALUES_H */
+
Index: include/llvm/Analysis/LoopNestingTree.h
===================================================================
--- include/llvm/Analysis/LoopNestingTree.h	(nonexistent)
+++ include/llvm/Analysis/LoopNestingTree.h	(working copy)
@@ -0,0 +1,191 @@
+/*
+ * Loop-nesting tree for a loop.  The root of a loop-nesting tree is the loop
+ * header of the outermost loop.  The children of any given node (including the
+ * root) are the basic blocks contained within the loop and the loop headers of
+ * nested loops.
+ *
+ * Note: we assume that the control-flow graphs are reducible
+ *
+ * Author: Rob Lyerly <rlyerly@vt.edu>
+ * Date: 5/23/2016
+ */
+
+#ifndef _LOOP_NESTING_TREE_H
+#define _LOOP_NESTING_TREE_H
+
+#include <list>
+#include <vector>
+#include <queue>
+#include "llvm/IR/BasicBlock.h"
+#include "llvm/Analysis/LoopInfo.h"
+#include "llvm/Support/raw_ostream.h"
+
+class LoopNestingTree {
+private:
+  /*
+   * Tree node object.
+   */
+  class Node {
+  public:
+    /**
+     * Construct a node for a basic block.
+     * @param _bb a basic block
+     * @param _parent the parent of this node, i.e. the loop header of the
+     *                containing loop
+     * @param _isLoopHeader is the basic block a loop header?
+     */
+    Node(const llvm::BasicBlock *_bb, const Node *_parent, bool _isLoopHeader)
+      : bb(_bb), parent(_parent), isLoopHeader(_isLoopHeader) {}
+
+    /**
+     * Add a child to the node.
+     * @param child a child to add to the node
+     */
+    void addChild(Node *child) { children.push_back(child); }
+
+    const llvm::BasicBlock *bb; /* Basic block encapsulated by the node. */
+    const Node *parent; /* Parent node, i.e. header of containing loop. */
+    std::list<Node *> children; /* Regular child nodes in the tree. */
+    bool isLoopHeader; /* Is the basic block a loop header? */
+  };
+
+  unsigned _size; /* Number of nodes (i.e., basic blocks) in the tree. */
+  unsigned _depth; /* Number of nested loops in the tree. */
+  Node *_root; /* Root of the tree, i.e. loop header of outermost loop. */
+
+  /**
+   * Print a node & its children.  Recurses into nested loops.
+   * @param O an output stream on which to print the tree
+   * @param node a node to print
+   * @param depth the current depth
+   */
+  void print(llvm::raw_ostream &O, Node *node, unsigned depth) const;
+
+  /**
+   * Delete the node's children & the node itself.  Recurses into nested loops.
+   * @param node the node being deleted
+   */
+  void deleteRecursive(Node *node);
+
+public:
+  /**
+   * Construct a loop-nesting tree from a strongly-connected component of the
+   * control-flow graph.
+   * @param SCC a strongly-connected component of the control-flow graph
+   * @param LI analysis from the loop info pass
+   */
+  LoopNestingTree(const std::vector<llvm::BasicBlock *> &SCC,
+                  const llvm::LoopInfo &LI);
+
+  /**
+   * Destroy a loop-nesting tree.
+   */
+  ~LoopNestingTree() { deleteRecursive(this->_root); }
+
+  /**
+   * Return the size of the loop-nesting tree, that is the number of nodes in
+   * the loop (and all nested loops).
+   * @return the number of nodes in the tree
+   */
+  unsigned size() const { return this->_size; }
+
+  /**
+   * Return the depth of the loop-nesting tree, that is the number of nested
+   * loops.  A value of one indicates that there are no nested loops.
+   * @return the number of nested loops in the tree
+   */
+  unsigned depth() const { return this->_depth; }
+
+  /**
+   * Print the tree.
+   * @param O the output stream on which to print the tree
+   */
+  void print(llvm::raw_ostream &O) const { print(O, this->_root, 0); }
+
+  /*
+   * Loop-node iterator object.  Delivers loop nodes in breadth-first order.
+   */
+  class loop_iterator {
+  public:
+    typedef loop_iterator self_type;
+    typedef const llvm::BasicBlock *value_type;
+    typedef value_type& reference;
+    typedef value_type* pointer;
+    typedef std::forward_iterator_tag iterator_category;
+
+    self_type operator++(void);
+    self_type operator++(int junk);
+    reference operator*(void) { return cur->bb; }
+    pointer operator->(void) { return &cur->bb; }
+    bool operator==(const self_type& rhs) { return cur == rhs.cur; }
+    bool operator!=(const self_type& rhs) { return cur != rhs.cur; }
+
+    friend class LoopNestingTree;
+    friend class child_iterator;
+  private:
+    Node *cur;
+    std::queue<Node *> remaining;
+
+    loop_iterator(Node *start) : cur(start) { addLoopHeaders(); }
+    void addLoopHeaders(void);
+  };
+
+  /*
+   * Child iterator object.  Traverses children of tree nodes.
+   */
+  class child_iterator {
+  public:
+    typedef child_iterator self_type;
+    typedef const llvm::BasicBlock *value_type;
+    typedef value_type& reference;
+    typedef value_type* pointer;
+    typedef std::forward_iterator_tag iterator_category;
+    enum location { BEGIN, END };
+
+    self_type operator++(void)
+      { self_type me = *this; it.operator++(); return me; }
+    self_type operator++(int junk) { it.operator++(junk); return *this; }
+    reference operator*(void) { return (*it)->bb; }
+    pointer operator->(void) { return &(*it)->bb; }
+    bool operator==(const self_type& rhs) { return it == rhs.it; }
+    bool operator!=(const self_type& rhs) { return it != rhs.it; }
+
+    friend class LoopNestingTree;
+  private:
+    std::list<Node *>::const_iterator it;
+
+    child_iterator(loop_iterator &parent, enum location loc);
+  };
+
+  /**
+   * Return an iterator for traversing all loop nodes (i.e., loop header basic
+   * blocks) in the tree.  Delivers nodes in a breadth-first ordering.
+   * @return an iterator to traverse the loop nodes in the tree
+   */
+  loop_iterator loop_begin() const { loop_iterator it(_root); return it; };
+
+  /**
+   * Return an iterator marking the end of the loop nodes in the tree.
+   * @return an iterator marking the end of the traversal
+   */
+  loop_iterator loop_end() const { loop_iterator it(nullptr); return it; };
+
+  /**
+   * Return an iterator for traversing the children of a loop node.
+   * @param an iterator associated with a loop node
+   * @return an iterator to traverse the children of a loop node
+   */
+  child_iterator children_begin(loop_iterator &parent) const
+    { child_iterator it(parent, child_iterator::BEGIN); return it; }
+
+  /**
+   * Return an iterator marking the end of the children of a loop node.
+   * @param an iterator associated with a loop node
+   * @return an iterator marking the end of the traversal
+   */
+  child_iterator children_end(loop_iterator &parent) const
+    { child_iterator it(parent, child_iterator::END); return it; }
+};
+
+#endif /* _LOOP_NESTING_TREE_H */
+
Index: include/llvm/Analysis/LoopPaths.h
===================================================================
--- include/llvm/Analysis/LoopPaths.h	(nonexistent)
+++ include/llvm/Analysis/LoopPaths.h	(working copy)
@@ -0,0 +1,286 @@
+//===- LoopPaths.h - Enumerate paths in loops -------------------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements analysis which enumerates paths in loops.  In
+// particular, this pass calculates all paths in loops which are of the
+// following form:
+//
+//  - Header to backedge, with no equivalence points on the path
+//  - Header to with equivalence point
+//  - Equivalence point to equivalence point
+//  - Equivalence point to backedge
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_ANALYSIS_LOOPPATHS_H
+#define LLVM_ANALYSIS_LOOPPATHS_H
+
+#include <set>
+#include <vector>
+#include <list>
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/ADT/SetVector.h"
+#include "llvm/Analysis/LoopInfo.h"
+#include "llvm/IR/BasicBlock.h"
+#include "llvm/Pass.h"
+#include "llvm/Support/Debug.h"
+
+namespace llvm {
+
+//===----------------------------------------------------------------------===//
+// Utilities
+//===----------------------------------------------------------------------===//
+
+/// Sort loops based on nesting depth, with deeper-nested loops coming first.
+/// If the depths are equal, sort based on pointer value so that distinct loops
+/// with equal depths are not considered equivalent during insertion.
+struct LoopNestCmp {
+  bool operator() (const Loop * const &A, const Loop * const &B) {
+    unsigned DepthA = A->getLoopDepth(), DepthB = B->getLoopDepth();
+    if(DepthA > DepthB) return true;
+    else if(DepthA < DepthB) return false;
+    else return (uint64_t)A < (uint64_t)B;
+  }
+};
+
+/// A loop nest, sorted by depth (deeper loops are first).
+typedef std::set<Loop *, LoopNestCmp> LoopNest;
+
+/// A set of basic blocks.
+typedef SmallPtrSet<const BasicBlock *, 16> BlockSet;
+
+namespace LoopPathUtilities {
+
+/// Populate a LoopNest by traversing the loop L and its children.  Does *not*
+/// traverse loops containing L (e.g., loops for which L is a child).
+void populateLoopNest(Loop *L, LoopNest &Nest);
+
+/// Get blocks contained in all sub-loops of a loop, including loops nested
+/// deeper than those in immediate sub-loops (e.g., blocks of loop depth 3
+/// inside loop depth 1).
+void getSubBlocks(Loop *L, BlockSet &SubBlocks);
+
+}
+
+//===----------------------------------------------------------------------===//
+// LoopPath helper class
+//===----------------------------------------------------------------------===//
+
+/// Nodes along a path in a loop, represented by a basic block.
+class PathNode {
+private:
+  /// The block encapsulated by the node.
+  const BasicBlock *Block;
+
+  /// Whether or not the block is an exiting block from a sub-loop inside of
+  /// the current path.
+  bool SubLoopExit;
+
+public:
+  PathNode() = delete;
+  PathNode(const BasicBlock *Block, bool SubLoopExit = false)
+    : Block(Block), SubLoopExit(SubLoopExit) {}
+
+  const BasicBlock *getBlock() const { return Block; }
+  bool isSubLoopExit() const { return SubLoopExit; }
+  bool operator<(const PathNode &RHS) const { return Block < RHS.Block; }
+};
+
+/// A path through the loop, which begins/ends either on the loop's header, the
+/// loop's backedge(s) or equivalence points.
+class LoopPath {
+private:
+  /// Nodes that comprise the path.  Iteration over the container is equivalent
+  /// to traversing the path, but container has set semantics for quick
+  /// existence checks.
+  // TODO use a DenseSet for the set template argument, which requires defining
+  // a custom comparator
+  SetVector<PathNode, std::vector<PathNode>, std::set<PathNode> > Nodes;
+
+  /// The path begins & ends on specific instructions.  Note that Start *must*
+  /// be inside the starting block and End *must* be inside the ending block.
+  const Instruction *Start, *End;
+
+  /// Does the path start at the loop header?  If not, it by definition starts
+  /// at an equivalence point.
+  bool StartsAtHeader;
+
+  /// Does the path end at a backedge?  If not, it by definition ends at an
+  /// equivalence point.
+  bool EndsAtBackedge;
+
+public:
+  LoopPath() = delete;
+  LoopPath(const std::vector<PathNode> &NodeVector,
+           const Instruction *Start, const Instruction *End,
+           bool StartsAtHeader, bool EndsAtBackedge);
+
+  bool contains(BasicBlock *BB) const { return Nodes.count(PathNode(BB)); }
+  bool contains(const BasicBlock *BB) const
+  { return Nodes.count(PathNode(BB)); }
+
+  /// Get the starting point of the path, guaranteed to be either the loop
+  /// header or an equivalence point.
+  const PathNode &startNode() const { return Nodes.front(); }
+  const Instruction *startInst() const { return Start; }
+
+  /// Get the the ending point of the path, guaranteed to be either an
+  /// equivalence point or a backedge.
+  const PathNode &endNode() const { return Nodes.back(); }
+  const Instruction *endInst() const { return End; }
+
+  /// Iterators over the path's blocks.
+  SetVector<PathNode>::iterator begin() { return Nodes.begin(); }
+  SetVector<PathNode>::iterator end() { return Nodes.end(); }
+  SetVector<PathNode>::const_iterator cbegin() const { return Nodes.begin(); }
+  SetVector<PathNode>::const_iterator cend() const { return Nodes.end(); }
+
+  /// Return whether the path starts at the loop header or equivalence point.
+  bool startsAtHeader() const { return StartsAtHeader; }
+
+  /// Return whether the path ends at a backedge block or equivalence point.
+  bool endsAtBackedge() const { return EndsAtBackedge; }
+
+  /// Return whether this is a spanning path.
+  bool isSpanningPath() const { return StartsAtHeader && EndsAtBackedge; }
+
+  /// Return whether this is an equivalence point path.
+  bool isEqPointPath() const { return !StartsAtHeader || !EndsAtBackedge; }
+
+  std::string toString() const;
+  void print(raw_ostream &O) const;
+  void dump() const { print(dbgs()); }
+};
+
+//===----------------------------------------------------------------------===//
+// Pass implementation
+//===----------------------------------------------------------------------===//
+
+/// Analyze all paths within a loop nest
+class EnumerateLoopPaths : public FunctionPass {
+private:
+  /// Loop information analysis.
+  LoopInfo *LI;
+
+  /// All calculated paths for each analyzed loop.
+  DenseMap<const Loop *, std::vector<LoopPath> > Paths;
+
+  /// Whether there are paths of each type through a basic block in a loop.
+  /// These are *only* maintained for the current loop, not any sub-loops.
+  DenseMap<const Loop *, DenseMap<const BasicBlock *, bool> >
+  HasSpPath, HasEqPointPath;
+
+  /// Information about the loop currently being analyzed
+  Loop *CurLoop;
+  SmallPtrSet<const BasicBlock *, 4> Latches;
+  BlockSet SubLoopBlocks;
+
+  /// Set if analysis had to bail out because there are too many paths through
+  /// the function.
+  bool TooManyPaths;
+
+  /// Set if analysis had to bail out because it found a non-loop cycle due to
+  /// complect control flow (usually due to goto's).
+  bool DetectedCycle;
+
+  /// Depth-first search information for the current path being explored.
+  struct LoopDFSInfo {
+  public:
+    const Instruction *Start;
+    std::vector<PathNode> PathNodes;
+    bool StartsAtHeader;
+  };
+
+  /// Empty all data structures.
+  void reset() {
+    Paths.clear();
+    HasSpPath.clear();
+    HasEqPointPath.clear();
+    CurLoop = nullptr;
+    Latches.clear();
+    SubLoopBlocks.clear();
+  }
+
+  /// Search exit blocks of the loop containing Successor.  Add the terminating
+  /// instruction of the block to either of the two vectors, depending if there
+  /// is a path of either type through the exit block.
+  inline void getSubLoopSuccessors(const BasicBlock *Successor,
+                                   std::vector<const Instruction *> &EqPoint,
+                                   std::vector<const Instruction *> &Spanning);
+
+  /// Run a depth-first search for paths in the loop starting at an instruction.
+  /// Any paths found are added to the vector of paths, and new paths to explore
+  /// are added to the search list.
+  bool loopDFS(const Instruction *I,
+               LoopDFSInfo &DFSI,
+               std::vector<LoopPath> &CurPaths,
+               std::list<const Instruction *> &NewPaths);
+
+  /// Enumerate all paths within a loop, stored in the vector argument.  Return
+  /// true if the analysis was successful or false otherwise.
+  bool analyzeLoop(Loop *L, std::vector<LoopPath> &CurPaths);
+
+public:
+  static char ID;
+  EnumerateLoopPaths() : FunctionPass(ID) {}
+
+  /// Pass interface implementation.
+  void getAnalysisUsage(AnalysisUsage &AU) const override;
+  bool runOnFunction(Function &F) override;
+
+  /// Re-run analysis to enumerate paths through a loop.  Invalidates all APIs
+  /// below which populate containers with paths (for this loop only).
+  void rerunOnLoop(Loop *L);
+
+  /// Query whether analysis failed.
+  bool analysisFailed() const { return TooManyPaths || DetectedCycle; }
+
+  /// Query whether there were too many paths to enumerate in the function.
+  bool tooManyPaths() const { return TooManyPaths; }
+
+  /// Query whether analysis detected a non-loop cycle.
+  bool detectedCycle() const { return DetectedCycle; }
+
+  bool hasPaths(const Loop *L) const { return Paths.count(L); }
+
+  /// Get all the paths through a loop.  Paths in the vector are ordered as
+  /// they were discovered in the depth-first traversal of the loop.
+  void getPaths(const Loop *L, std::vector<const LoopPath *> &P) const;
+
+  /// Get all paths through a loop that end at a backedge.
+  void getBackedgePaths(const Loop *L, std::vector<const LoopPath *> &P) const;
+  void getBackedgePaths(const Loop *L, std::set<const LoopPath *> &P) const;
+
+  /// Get all spanning paths through a loop, where a spanning path is defined
+  /// as starting at the first instruction of the header of the loop and ending
+  /// at the branch in a latch.
+  void getSpanningPaths(const Loop *L, std::vector<const LoopPath *> &P) const;
+  void getSpanningPaths(const Loop *L, std::set<const LoopPath *> &P) const;
+
+  /// Get all the paths through the loop that begin and/or end at an
+  /// equivalence point.
+  void getEqPointPaths(const Loop *L, std::vector<const LoopPath *> &P) const;
+  void getEqPointPaths(const Loop *L, std::set<const LoopPath *> &P) const;
+
+  /// Get all the paths through a loop that contain a given basic block.
+  void getPathsThroughBlock(const Loop *L, BasicBlock *BB,
+                            std::vector<const LoopPath *> &P) const;
+  void getPathsThroughBlock(const Loop *L, BasicBlock *BB,
+                            std::set<const LoopPath *> &P) const;
+
+  /// Return whether there is each type of path through a basic block.
+  bool spanningPathThroughBlock(const Loop *L, const BasicBlock *BB) const;
+  bool eqPointPathThroughBlock(const Loop *L, const BasicBlock *BB) const;
+};
+
+}
+
+#endif
+
Index: include/llvm/Analysis/Passes.h
===================================================================
--- include/llvm/Analysis/Passes.h	(revision 320332)
+++ include/llvm/Analysis/Passes.h	(working copy)
@@ -173,6 +173,33 @@
   //
   FunctionPass *createMemDerefPrinter();
 
+  //===--------------------------------------------------------------------===//
+  //
+  // createPopcornCompatibilityPass - This pass analyzes & warns users about
+  // code features not yet supported by the Popcorn compiler, runtime & OS.
+  //
+  FunctionPass *createPopcornCompatibilityPass();
+
+  //===--------------------------------------------------------------------===//
+  //
+  // createLiveValuesPass - This pass calculates live-value sets for basic
+  // blocks in a function.
+  //
+  FunctionPass *createLiveValuesPass();
+
+  //===--------------------------------------------------------------------===//
+  //
+  // createEnumerateLoopPathsPass - This pass calculates all paths between
+  // equivalence points within a loop.
+  //
+  FunctionPass *createEnumerateLoopPathsPass();
+
+  //===--------------------------------------------------------------------===//
+  //
+  // createSelectMigrationPointsPass - This pass analyzes and marks instructions
+  // inside of functions to be migration points.
+  //
+  FunctionPass *createSelectMigrationPointsPass();
 }
 
 #endif
Index: include/llvm/Analysis/PopcornUtil.h
===================================================================
--- include/llvm/Analysis/PopcornUtil.h	(nonexistent)
+++ include/llvm/Analysis/PopcornUtil.h	(working copy)
@@ -0,0 +1,188 @@
+//===- LoopPaths.h - Enumerate paths in loops -------------------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file provides Popcorn-specific utility APIs.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_ANALYSIS_POPCORNUTIL_H
+#define LLVM_ANALYSIS_POPCORNUTIL_H
+
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/IR/CallSite.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/Module.h"
+
+namespace llvm {
+namespace Popcorn {
+
+#define POPCORN_META "popcorn"
+#define POPCORN_MIGPOINT "migpoint"
+#define POPCORN_HTM_BEGIN "htmbegin"
+#define POPCORN_HTM_END "htmend"
+
+/// Add named metadata node with string operand to an instruction.
+static inline void addMetadata(Instruction *I, StringRef name, StringRef op) {
+  SmallVector<Metadata *, 2> MetaOps;
+  LLVMContext &C = I->getContext();
+  MDNode *MetaNode = I->getMetadata(name);
+
+  if(MetaNode) {
+    for(auto &Op : MetaNode->operands()) {
+      if(isa<MDString>(Op) && cast<MDString>(Op)->getString() == op) return;
+      else MetaOps.push_back(Op);
+    }
+  }
+
+  MetaOps.push_back(MDString::get(C, op));
+  MetaNode = MDNode::get(C, MetaOps);
+  I->setMetadata(name, MetaNode);
+}
+
+/// Remove string operand from named metadata node.
+static inline void
+removeMetadata(Instruction *I, StringRef name, StringRef op) {
+  SmallVector<Metadata *, 2> MetaOps;
+  MDNode *MetaNode = I->getMetadata(name);
+
+  if(MetaNode) {
+    for(auto &Op : MetaNode->operands()) {
+      if(isa<MDString>(Op) && cast<MDString>(Op)->getString() == op) continue;
+      else MetaOps.push_back(Op);
+    }
+
+    if(MetaOps.size()) {
+      MetaNode = MDNode::get(I->getContext(), MetaOps);
+      I->setMetadata(name, MetaNode);
+    }
+    else I->setMetadata(name, nullptr);
+  }
+}
+
+/// Check to see if instruction has named metadata node with string operand.
+static inline bool
+hasMetadata(const Instruction *I, StringRef name, StringRef op) {
+  const MDNode *MetaNode = I->getMetadata(name);
+  if(MetaNode)
+    for(auto &Op : MetaNode->operands())
+      if(isa<MDString>(Op) && cast<MDString>(Op)->getString() == op)
+        return true;
+  return false;
+}
+
+/// Return whether the instruction is a "true" call site, i.e., not an LLVM
+/// IR-level intrinsic or inline assembly.
+static inline bool isCallSite(const Instruction *I) {
+  if((isa<CallInst>(I) || isa<InvokeInst>(I)) && !isa<IntrinsicInst>(I)) {
+    ImmutableCallSite CS(I);
+    if(!CS.isInlineAsm()) return true;
+  }
+  return false;
+}
+
+/// Add metadata to an instruction marking it as an equivalence point.
+static inline void addEquivalencePointMetadata(Instruction *I) {
+  addMetadata(I, POPCORN_META, POPCORN_MIGPOINT);
+}
+
+/// Remove metadata from an instruction marking it as an equivalence point.
+static inline void removeEquivalencePointMetadata(Instruction *I) {
+  removeMetadata(I, POPCORN_META, POPCORN_MIGPOINT);
+}
+
+static inline bool hasEquivalencePointMetadata(Instruction *I) {
+  return hasMetadata(I , POPCORN_META, POPCORN_MIGPOINT);
+}
+
+/// Return whether an instruction is an equivalence point.  The instruction must
+/// satisfy one of the following:
+///
+/// 1. Is a function call site (not an intrinsic function call)
+/// 2. Analysis has tagged the instruction with appropriate metadata
+static inline bool isEquivalencePoint(const Instruction *I) {
+  if(isCallSite(I)) return true;
+  else return hasMetadata(I, POPCORN_META, POPCORN_MIGPOINT);
+}
+
+/// Add metadata to an instruction marking it as an HTM begin point.
+static inline void addHTMBeginMetadata(Instruction *I) {
+  addMetadata(I, POPCORN_META, POPCORN_HTM_BEGIN);
+}
+
+/// Remove metadata from an instruction marking it as an HTM begin point.
+static inline void removeHTMBeginMetadata(Instruction *I) {
+  removeMetadata(I, POPCORN_META, POPCORN_HTM_BEGIN);
+}
+
+/// Return whether an instruction is an HTM begin point.
+static inline bool isHTMBeginPoint(Instruction *I) {
+  return hasMetadata(I, POPCORN_META, POPCORN_HTM_BEGIN);
+}
+
+/// Add metadata to an instruction marking it as an HTM end point.
+static inline void addHTMEndMetadata(Instruction *I) {
+  addMetadata(I, POPCORN_META, POPCORN_HTM_END);
+}
+
+/// Remove metadata from an instruction marking it as an HTM end point.
+static inline void removeHTMEndMetadata(Instruction *I) {
+  removeMetadata(I, POPCORN_META, POPCORN_HTM_END);
+}
+
+/// Return whether an instruction is an HTM end point.
+static inline bool isHTMEndPoint(Instruction *I) {
+  return hasMetadata(I, POPCORN_META, POPCORN_HTM_END);
+}
+
+#define POPCORN_INST_KEY "popcorn-inst-ty"
+
+/// Type of instrumentation.
+enum InstrumentType {
+  HTM = 0,
+  Cycles,
+  None,
+  NumVals // Don't use!
+};
+
+/// Mark the module as having a certain instrumentation type.
+static inline void
+setInstrumentationType(Module &M, enum InstrumentType Ty) {
+  switch(Ty) {
+  case HTM:
+    M.addModuleFlag(Module::Error, POPCORN_INST_KEY, HTM);
+    break;
+  case Cycles:
+    M.addModuleFlag(Module::Error, POPCORN_INST_KEY, Cycles);
+    break;
+  case None:
+    M.addModuleFlag(Module::Error, POPCORN_INST_KEY, None);
+    break;
+  default: llvm_unreachable("Unknown instrumentation type"); break;
+  }
+}
+
+/// Get the type of instrumentation.
+static inline enum InstrumentType getInstrumentationType(Module &M) {
+  Metadata *MD = M.getModuleFlag(POPCORN_INST_KEY);
+  if(MD) {
+    ConstantAsMetadata *Val = cast<ConstantAsMetadata>(MD);
+    ConstantInt *IntVal = cast<ConstantInt>(Val->getValue());
+    uint64_t RawVal = IntVal->getZExtValue();
+    assert(RawVal < NumVals && "Invalid instrumentation type");
+    return (enum InstrumentType)RawVal;
+  }
+  else return None;
+}
+
+}
+}
+
+#endif
+
Index: include/llvm/CodeGen/AsmPrinter.h
===================================================================
--- include/llvm/CodeGen/AsmPrinter.h	(revision 320332)
+++ include/llvm/CodeGen/AsmPrinter.h	(working copy)
@@ -199,9 +199,10 @@
 
   /// Emit the specified function out to the OutStreamer.
   bool runOnMachineFunction(MachineFunction &MF) override {
+    bool modified = TagCallSites(MF);
     SetupMachineFunction(MF);
     EmitFunctionBody();
-    return false;
+    return modified;
   }
 
   //===------------------------------------------------------------------===//
@@ -329,6 +330,13 @@
   /// instructions in verbose mode.
   virtual void emitImplicitDef(const MachineInstr *MI) const;
 
+  /// Some machine instructions encapsulate a call with follow-on boilerplate
+  /// instructions, meaning labels emitted after the "instruction" do not
+  /// capture the call's true return address.  Return an offset for correcting
+  /// these labels to refer to the call's actual return address.
+  virtual int getCanonicalReturnAddr(const MachineInstr *Call) const
+  { return 0; }
+
   //===------------------------------------------------------------------===//
   // Symbol Lowering Routines.
   //===------------------------------------------------------------------===//
@@ -393,6 +401,14 @@
     EmitLabelPlusOffset(Label, 0, Size, IsSectionRelative);
   }
 
+  /// Find the stackmap intrinsic associated with a function call
+  MachineInstr *FindStackMap(MachineBasicBlock &MBB,
+                             MachineInstr *MI) const;
+
+  /// Move stackmap intrinsics directly after calls to correctly capture
+  /// return addresses
+  bool TagCallSites(MachineFunction &MF);
+
   //===------------------------------------------------------------------===//
   // Dwarf Emission Helper Routines
   //===------------------------------------------------------------------===//
Index: include/llvm/CodeGen/MachineFunction.h
===================================================================
--- include/llvm/CodeGen/MachineFunction.h	(revision 320332)
+++ include/llvm/CodeGen/MachineFunction.h	(working copy)
@@ -20,6 +20,8 @@
 
 #include "llvm/ADT/ilist.h"
 #include "llvm/CodeGen/MachineBasicBlock.h"
+#include "llvm/CodeGen/StackTransformTypes.h"
+#include "llvm/IR/Instructions.h"
 #include "llvm/IR/DebugLoc.h"
 #include "llvm/IR/Metadata.h"
 #include "llvm/Support/Allocator.h"
@@ -145,6 +147,12 @@
   /// True if the function includes any inline assembly.
   bool HasInlineAsm;
 
+  /// Duplicate live value locations for stackmap operands
+  InstToOperands SMDuplicateLocs;
+
+  /// Architecture-specific live value locations for each stackmap
+  InstToArchLiveValues SMArchSpecificLocs;
+
   MachineFunction(const MachineFunction &) = delete;
   void operator=(const MachineFunction&) = delete;
 public:
@@ -457,6 +465,9 @@
     return Mask;
   }
 
+  /// Is a register caller-saved?
+  bool isCallerSaved(unsigned Reg) const;
+
   /// allocateMemRefsArray - Allocate an array to hold MachineMemOperand
   /// pointers.  This array is owned by the MachineFunction.
   MachineInstr::mmo_iterator allocateMemRefsArray(unsigned long Num);
@@ -488,6 +499,39 @@
   /// getPICBaseSymbol - Return a function-local symbol to represent the PIC
   /// base.
   MCSymbol *getPICBaseSymbol() const;
+
+  //===--------------------------------------------------------------------===//
+  // Architecture-specific stack transformation metadata
+  //
+
+  /// Add an IR/architecture-specific location mapping for a stackmap operand
+  void addSMOpLocation(const CallInst *SM, const Value *Val,
+                       const MachineLiveLoc &MLL);
+  void addSMOpLocation(const CallInst *SM, unsigned Op,
+                       const MachineLiveLoc &MLL);
+
+  /// Add an architecture-specific live value & location for a stackmap
+  void addSMArchSpecificLocation(const CallInst *SM,
+                                 const MachineLiveLoc &MLL,
+                                 const MachineLiveVal &MLV);
+
+  /// Update stack slot references to new indexes after stack slot coloring
+  void updateSMStackSlotRefs(SmallDenseMap<int, int, 16> &Changes);
+
+  /// Are there any architecture-specific locations for operand Val in stackmap
+  /// SM?
+  bool hasSMOpLocations(const CallInst *SM, const Value *Val) const;
+
+  /// Are there any architecture-specific locations for stackmap SM?
+  bool hasSMArchSpecificLocations(const CallInst *SM) const;
+
+  /// Return the architecture-specific locations for a stackmap operand.
+  const MachineLiveLocs &getSMOpLocations(const CallInst *SM,
+                                          const Value *Val) const;
+
+  /// Return the architecture-specific locations for a stackmap that are not
+  /// associated with any operand.
+  const ArchLiveValues &getSMArchSpecificLocations(const CallInst *SM) const;
 };
 
 //===--------------------------------------------------------------------===//
Index: include/llvm/CodeGen/Passes.h
===================================================================
--- include/llvm/CodeGen/Passes.h	(revision 320332)
+++ include/llvm/CodeGen/Passes.h	(working copy)
@@ -123,6 +123,15 @@
   /// Default setting for -enable-shrink-wrap on this target.
   bool EnableShrinkWrap;
 
+  /// Add equivalence points into the application
+  bool AddMigrationPoints;
+
+  /// Add stackmaps at function call sites & equivalence points
+  bool AddStackMaps;
+
+  /// Add stackmaps describing stack state in libc thread start functions
+  bool AddLibcStackMaps;
+
 public:
   TargetPassConfig(TargetMachine *tm, PassManagerBase &pm);
   // Dummy constructor.
@@ -142,6 +151,9 @@
 
   CodeGenOpt::Level getOptLevel() const { return TM->getOptLevel(); }
 
+  CodeGenOpt::Level getArchIROptLevel() const
+  { return TM->getArchIROptLevel(); }
+
   /// Set the StartAfter, StartBefore and StopAfter passes to allow running only
   /// a portion of the normal code-gen pass sequence.
   ///
@@ -193,10 +205,33 @@
   /// Return true if shrink wrapping is enabled.
   bool getEnableShrinkWrap() const;
 
+  /// Return whether we should instrument the code with equivalence points.
+  bool addMigrationPoints() const { return AddMigrationPoints; }
+
+  /// Return whether we should emit stack transformation metadata by
+  /// instrumenting the code with IR-level StackMaps.
+  bool addStackMaps() const { return AddStackMaps; }
+
+  /// Return whether we should emit transformation metadata (via IR-level
+  /// StackMaps) for libc thread start functions.
+  bool addLibcStackMaps() const { return AddLibcStackMaps; }
+
+  /// \brief Enable/disable adding equivalence points.
+  void setAddMigrationPoints(bool Set) { AddMigrationPoints = Set; }
+
+  /// \brief Enable/disable adding StackMaps.
+  void setAddStackMaps(bool Set) { AddStackMaps = Set; }
+
+  /// \brief Enable/disable adding StackMaps to libc thread start function.
+  void setAddLibcStackMaps(bool Set) { AddLibcStackMaps = Set; }
+
   /// Return true if the default global register allocator is in use and
   /// has not be overriden on the command line with '-regalloc=...'
   bool usingDefaultRegAlloc() const;
 
+  /// Add Popcorn-specific IR passes for code generation.
+  void addPopcornPasses();
+
   /// Add common target configurable passes that perform LLVM IR to IR
   /// transforms following machine independent optimization.
   virtual void addIRPasses();
@@ -448,6 +483,10 @@
   // instruction and update the MachineFunctionInfo with that information.
   extern char &ShrinkWrapID;
 
+  /// Stack transformation metadata pass.  Gather additional stack
+  /// transformation metadata from machine functions.
+  extern char &StackTransformMetadataID;
+
   /// VirtRegRewriter pass. Rewrite virtual registers to physical registers as
   /// assigned in VirtRegMap.
   extern char &VirtRegRewriterID;
Index: include/llvm/CodeGen/StackMaps.h
===================================================================
--- include/llvm/CodeGen/StackMaps.h	(revision 320332)
+++ include/llvm/CodeGen/StackMaps.h	(working copy)
@@ -13,6 +13,8 @@
 #include "llvm/ADT/MapVector.h"
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/CodeGen/MachineInstr.h"
+#include "llvm/CodeGen/StackTransformTypes.h"
+#include "llvm/IR/Instructions.h"
 #include "llvm/Support/Debug.h"
 #include <map>
 #include <vector>
@@ -22,6 +24,7 @@
 class AsmPrinter;
 class MCExpr;
 class MCStreamer;
+class UnwindInfo;
 
 /// \brief MI-level patchpoint operands.
 ///
@@ -142,9 +145,20 @@
     unsigned Size;
     unsigned Reg;
     int64_t Offset;
-    Location() : Type(Unprocessed), Size(0), Reg(0), Offset(0) {}
-    Location(LocationType Type, unsigned Size, unsigned Reg, int64_t Offset)
-        : Type(Type), Size(Size), Reg(Reg), Offset(Offset) {}
+    bool Ptr;
+    bool Alloca;
+    bool Duplicate;
+    bool Temporary;
+    unsigned AllocaSize;
+    Location() : Type(Unprocessed), Size(0), Reg(0), Offset(0),
+                 Ptr(false), Alloca(false), Duplicate(false),
+                 Temporary(false), AllocaSize(0) {}
+    Location(LocationType Type, unsigned Size, unsigned Reg, int64_t Offset,
+             bool Ptr, bool Alloca, bool Duplicate, bool Temporary,
+             unsigned AllocaSize)
+        : Type(Type), Size(Size), Reg(Reg), Offset(Offset), Ptr(Ptr),
+          Alloca(Alloca), Duplicate(Duplicate), Temporary(Temporary),
+          AllocaSize(AllocaSize) {}
   };
 
   struct LiveOutReg {
@@ -158,10 +172,29 @@
         : Reg(Reg), DwarfRegNum(DwarfRegNum), Size(Size) {}
   };
 
+  struct Operation {
+    ValueGenInst::InstType InstType;
+    Location::LocationType OperandType;
+    unsigned Size;
+    unsigned DwarfReg;
+    int64_t Constant;
+    bool isGenerated;
+    bool isSymbol;
+    const MCSymbol *Symbol;
+    Operation()
+      : Size(0), DwarfReg(0), Constant(0), isGenerated(false),
+        isSymbol(false), Symbol(nullptr) {}
+  };
+
   // OpTypes are used to encode information about the following logical
   // operand (which may consist of several MachineOperands) for the
   // OpParser.
-  typedef enum { DirectMemRefOp, IndirectMemRefOp, ConstantOp } OpType;
+  typedef enum {
+    DirectMemRefOp,
+    IndirectMemRefOp,
+    ConstantOp,
+    TemporaryOp
+  } OpType;
 
   StackMaps(AsmPrinter &AP);
 
@@ -185,7 +218,7 @@
   /// If there is any stack map data, create a stack map section and serialize
   /// the map info into it. This clears the stack map data structures
   /// afterwards.
-  void serializeToStackMapSection();
+  void serializeToStackMapSection(const UnwindInfo *UI = nullptr);
 
 private:
   static const char *WSMP;
@@ -193,17 +226,23 @@
   typedef SmallVector<LiveOutReg, 8> LiveOutVec;
   typedef MapVector<uint64_t, uint64_t> ConstantPool;
   typedef MapVector<const MCSymbol *, uint64_t> FnStackSizeMap;
+  typedef std::pair<Location, Operation> ArchValue;
+  typedef SmallVector<ArchValue, 8> ArchValues;
 
   struct CallsiteInfo {
+    const MCSymbol *Func;
     const MCExpr *CSOffsetExpr;
     uint64_t ID;
     LocationVec Locations;
     LiveOutVec LiveOuts;
-    CallsiteInfo() : CSOffsetExpr(nullptr), ID(0) {}
-    CallsiteInfo(const MCExpr *CSOffsetExpr, uint64_t ID,
-                 LocationVec &&Locations, LiveOutVec &&LiveOuts)
-        : CSOffsetExpr(CSOffsetExpr), ID(ID), Locations(std::move(Locations)),
-          LiveOuts(std::move(LiveOuts)) {}
+    ArchValues Vals;
+    CallsiteInfo() : Func(nullptr), CSOffsetExpr(nullptr), ID(0) {}
+    CallsiteInfo(const MCSymbol *Func, const MCExpr *CSOffsetExpr,
+                 uint64_t ID, LocationVec &&Locations,
+                 LiveOutVec &&LiveOuts, ArchValues &&Vals)
+        : Func(Func), CSOffsetExpr(CSOffsetExpr), ID(ID),
+          Locations(std::move(Locations)), LiveOuts(std::move(LiveOuts)),
+          Vals(std::move(Vals)) {}
   };
 
   typedef std::vector<CallsiteInfo> CallsiteInfoList;
@@ -213,10 +252,22 @@
   ConstantPool ConstPool;
   FnStackSizeMap FnStackSize;
 
+  /// Get stackmap information for register location
+  void getRegLocation(unsigned Phys, unsigned &Dwarf, unsigned &Offset) const;
+
+  /// Get pointer typing information for stackmap operand
+  void getPointerInfo(const Value *Op, const DataLayout &DL, bool &isPtr,
+                      bool &isAlloca, unsigned &AllocaSize) const;
+
+  /// Add duplicate target-specific locations for a stackmap operand
+  void addDuplicateLocs(const CallInst *StackMap, const Value *Oper,
+                        LocationVec &Locs, unsigned Size, bool Ptr,
+                        bool Alloca, unsigned AllocaSize) const;
+
   MachineInstr::const_mop_iterator
   parseOperand(MachineInstr::const_mop_iterator MOI,
                MachineInstr::const_mop_iterator MOE, LocationVec &Locs,
-               LiveOutVec &LiveOuts) const;
+               LiveOutVec &LiveOuts, User::const_op_iterator &Op) const;
 
   /// \brief Create a live-out register record for the given register @p Reg.
   LiveOutReg createLiveOutReg(unsigned Reg,
@@ -226,6 +277,15 @@
   /// registers that need to be recorded in the stackmap.
   LiveOutVec parseRegisterLiveOutMask(const uint32_t *Mask) const;
 
+  /// Convert a list of instructions used to generate an architecture-specific
+  /// live value into multiple individual records.
+  void genArchValsFromInsts(ArchValues &AV,
+                            Location &Loc,
+                            const MachineLiveVal &MLV);
+
+  /// Add architecture-specific locations for the stackmap.
+  void addArchLiveVals(const CallInst *SM, ArchValues &AV);
+
   /// This should be called by the MC lowering code _immediately_ before
   /// lowering the MI to an MCInst. It records where the operands for the
   /// instruction are stored, and outputs a label to record the offset of
@@ -240,7 +300,7 @@
   void emitStackmapHeader(MCStreamer &OS);
 
   /// \brief Emit the function frame record for each function.
-  void emitFunctionFrameRecords(MCStreamer &OS);
+  void emitFunctionFrameRecords(MCStreamer &OS, const UnwindInfo *UI);
 
   /// \brief Emit the constant pool.
   void emitConstantPoolEntries(MCStreamer &OS);
Index: include/llvm/CodeGen/StackTransformTypes.def
===================================================================
--- include/llvm/CodeGen/StackTransformTypes.def	(nonexistent)
+++ include/llvm/CodeGen/StackTransformTypes.def	(working copy)
@@ -0,0 +1,36 @@
+//===-- llvm/Target/StackTransformTypes.def - Generator Opcodes -*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Macros which define the set of available instructions for the ISA-agnostic
+// value generator.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_CODEGEN_STACKTRANSFORMTYPES_DEF
+#define LLVM_CODEGEN_STACKTRANSFORMTYPES_DEF
+
+// TODO generate using TableGen rather than X macros
+
+/// Each instruction is defined by a mnemonic and an operand (represented using
+/// the various ValueGenInst types).
+#define VALUE_GEN_INST \
+  X(Set)             /* Set the destination to another value */ \
+  X(Add)             /* Add a value to the destination */ \
+  X(Subtract)        /* Subtract a value from the destination */ \
+  X(Multiply)        /* Multiply the destination by another value */ \
+  X(Divide)          /* Divide the destination by another value */ \
+  X(LeftShift)       /* Left-shift the destination */ \
+  X(RightShiftLog)   /* Right-shift (logical) the destination */ \
+  X(RightShiftArith) /* Right-shift (arithmetic) the destination */ \
+  X(Mask)            /* Apply bit mask to the destination */ \
+  X(Load32)          /* Load 32 bits from memory */ \
+  X(Load64)          /* Load 64 bits from memory */
+
+#endif
+
Index: include/llvm/CodeGen/StackTransformTypes.h
===================================================================
--- include/llvm/CodeGen/StackTransformTypes.h	(nonexistent)
+++ include/llvm/CodeGen/StackTransformTypes.h	(working copy)
@@ -0,0 +1,577 @@
+//===------- StackTransformTypes.h - Stack Transform Types ------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_CODEGEN_STACKTRANFORMTYPES_H
+#define LLVM_CODEGEN_STACKTRANFORMTYPES_H
+
+#include <cstdint>
+#include <string>
+#include <vector>
+#include <map>
+#include <memory>
+#include "llvm/CodeGen/MachineOperand.h"
+#include "llvm/CodeGen/StackTransformTypes.def"
+#include "llvm/ADT/SmallVector.h"
+
+namespace llvm {
+
+class AsmPrinter;
+class Instruction;
+class MachineInstr;
+class MCSymbol;
+class Value;
+
+//===----------------------------------------------------------------------===//
+// Types for generating more complex architecture-specific live values
+//
+
+#define INV_INST_TYPE "Invalid instruction type"
+
+/// ValueGenInst - an instruction for the transformation runtime which generates
+/// a live value.  These instructions specify a simple operation (e.g., add) and
+/// an operand.  These instructions, coupled together with a destination
+/// location (i.e., a register or stack slot), allow the runtime to construct
+/// more complex live values like bit-shifts or pointers into arrays of structs.
+class ValueGenInst {
+public:
+  virtual ~ValueGenInst() {}
+
+  /// Instruction types
+  enum InstType {
+#define X(type) type,
+    VALUE_GEN_INST
+#undef X
+  };
+  virtual InstType type() const = 0;
+
+  /// Operand types
+  enum OpType { Register, Immediate, Reference };
+  virtual OpType opType() const = 0;
+
+  /// Equivalence checking.  Depends on both instruction & operand type, and
+  /// any operand-specific information.
+  virtual bool operator==(const ValueGenInst &RHS) const = 0;
+
+  /// Get a human-readable name for the instruction type
+  static const char *getInstName(enum InstType Type);
+  static std::string getInstNameStr(enum InstType Type);
+
+  /// Get a human-readable description of the instruction & operand
+  virtual std::string str() const = 0;
+private:
+  static const char *InstTypeStr[];
+};
+
+/// RegInstructionBase - base class for register operand instructions.  The
+/// register is stored as an architecture-specific physical register.
+class RegInstructionBase : public ValueGenInst {
+public:
+  virtual OpType opType() const { return Register; }
+  unsigned getReg() const { return Reg; }
+  void setReg(unsigned Reg) { this->Reg = Reg; }
+
+protected:
+  /// The register used in the instruction
+  // Note: will be converted to DWARF during metadata emission
+  unsigned Reg;
+
+  RegInstructionBase(unsigned Reg) : Reg(Reg) {}
+};
+
+/// RegInstruction<T> - register-based instructions.  Instructions are
+/// specified via template argument.
+template<ValueGenInst::InstType Type>
+class RegInstruction : public RegInstructionBase {
+  static_assert(Type == Set || Type == Add || Type == Subtract ||
+                Type == Multiply || Type == Divide,
+                INV_INST_TYPE " for register instruction");
+public:
+  RegInstruction(unsigned Reg) : RegInstructionBase(Reg) {}
+
+  virtual InstType type() const { return Type; }
+
+  virtual bool operator==(const ValueGenInst &RHS) const {
+    if(RHS.type() == Type && RHS.opType() == Register) {
+      const RegInstruction<Type> &RI = (const RegInstruction<Type> &)RHS;
+      if(RI.Reg == Reg) return true;
+    }
+    return false;
+  }
+
+  virtual std::string str() const
+  { return getInstNameStr(Type) + " register " + std::to_string(Reg); }
+};
+
+/// ImmInstructionBase - base class for immediate operand instructions.
+class ImmInstructionBase : public ValueGenInst {
+public:
+  virtual OpType opType() const { return Immediate; }
+  unsigned getImmSize() const { return Size; }
+  int64_t getImm() const { return Imm; }
+  void setImm(unsigned Size, int64_t Imm)
+  { this->Size = Size; this->Imm = Imm; }
+
+protected:
+  unsigned Size; // in bytes
+  int64_t Imm;
+
+  ImmInstructionBase(unsigned Size, int64_t Imm) : Size(Size), Imm(Imm) {}
+};
+
+/// ImmInstruction<T> - immediate-based instructions.  Instructions are
+/// specified via template argument.
+template<ValueGenInst::InstType Type>
+class ImmInstruction : public ImmInstructionBase {
+public:
+  ImmInstruction(unsigned Size, int64_t Imm) : ImmInstructionBase(Size, Imm) {}
+
+  virtual InstType type() const { return Type; }
+
+  virtual bool operator==(const ValueGenInst &RHS) const {
+    if(RHS.type() == Type && RHS.opType() == Immediate) {
+      const ImmInstruction<Type> &II = (const ImmInstruction<Type> &)RHS;
+      if(II.Imm == Imm && II.Size == Size) return true;
+    }
+    return false;
+  }
+
+  virtual std::string str() const
+  { return getInstNameStr(Type) + " immediate " + std::to_string(Imm); }
+};
+
+/// ReferenceInstruction - references to symbols.  Only supports set
+/// instructions.
+class RefInstruction : public ValueGenInst {
+public:
+  RefInstruction(const MachineOperand &Symbol) : Symbol(Symbol) {}
+
+  virtual InstType type() const { return Set; }
+  virtual OpType opType() const { return Reference; }
+  MCSymbol *getReference(AsmPrinter &AP) const;
+  virtual bool operator==(const ValueGenInst &RHS) const {
+    if(RHS.type() == Set && RHS.opType() == Reference) {
+      const RefInstruction &RI = (const RefInstruction &)RHS;
+      if(&RI.Symbol == &Symbol) return true;
+    }
+    return false;
+  }
+  virtual std::string str() const;
+
+private:
+  // MCSymbols may not exist yet, so instead store the operand to look up the
+  // MCSymbol at metadata emission time.
+  // Note: store hard-copy (not reference) because optimizations may convert
+  // symbol reference to a different type, e.g., register
+  const MachineOperand Symbol;
+};
+
+/// Wrap raw pointers to ValueGenInst in smart pointers.  Use shared_ptr so we
+/// can use copy constructors for containers of these instructions.
+typedef std::shared_ptr<ValueGenInst> ValueGenInstPtr;
+
+/// A list of instructions used to generate a value
+typedef std::vector<ValueGenInstPtr> ValueGenInstList;
+
+#undef INV_INST_TYPE
+
+//===----------------------------------------------------------------------===//
+// Machine-specific live values
+//
+// These are the live values used to populate an architecture-specific location,
+// e.g., a reference to a global symbol or an immediate value
+
+/// MachineLiveVal - A machine-specific live value
+class MachineLiveVal {
+public:
+  /// Constructors & destructors.
+  // Note: create child class objects rather than objects of this class.
+  virtual ~MachineLiveVal() {}
+  virtual MachineLiveVal *copy() const = 0;
+
+  /// Possible live value types
+  enum Type { SymbolRef, ConstPoolRef, StackObject, Immediate, Generated };
+
+  /// Determine the live value's type
+  virtual enum Type getType() const = 0;
+  virtual bool isReference() const { return false; }
+  virtual bool isSymbolRef() const { return false; }
+  virtual bool isConstPoolRef() const { return false; }
+  virtual bool isStackObject() const { return false; }
+  virtual bool isImm() const { return false; }
+  virtual bool isGenerated() const { return false; }
+
+  /// Equivalence checking
+  virtual bool operator==(const MachineLiveVal &RHS) const = 0;
+
+  /// Generate a human-readable string describing the value
+  virtual std::string toString() const = 0;
+
+  /// Get the machine instruction which defines the live value
+  const MachineInstr *getDefiningInst() const { return DefMI; }
+
+  /// Return whether this value is a pointer
+  // Note: if the value *could* be a pointer, this should be set so the runtime
+  // can do pointer-to-stack checks
+  bool isPtr() const { return Ptr; }
+
+protected:
+  /// Defining instruction for live value
+  const MachineInstr *DefMI;
+
+  /// Is this a pointer?
+  // Note: if the value *could* be a pointer, this should be set so the runtime
+  // can do pointer-to-stack checks
+  bool Ptr;
+
+  MachineLiveVal(const MachineInstr *DefMI, bool Ptr)
+    : DefMI(DefMI), Ptr(Ptr) {}
+  MachineLiveVal(const MachineLiveVal &C) : DefMI(C.DefMI), Ptr(C.Ptr) {}
+};
+
+/// MachineReference - a reference to some binary object outside of thread
+/// local storage
+class MachineReference : public MachineLiveVal {
+public:
+  virtual bool isReference() const { return true; }
+
+  /// Get a symbol reference for label generation
+  virtual MCSymbol *getReference(AsmPrinter &AP) const = 0;
+
+  /// Return whether we are to load the reference's value
+  virtual bool isLoad() const { return false; }
+
+protected:
+  MachineReference(const MachineInstr *DefMI)
+    : MachineLiveVal(DefMI, true) {}
+  MachineReference(const MachineReference &C) : MachineLiveVal(C) {}
+};
+
+/// MachineSymbolRef - a reference to a global symbol
+class MachineSymbolRef : public MachineReference {
+public:
+  MachineSymbolRef(const MachineOperand &Symbol,
+                   bool Load,
+                   const MachineInstr *DefMI)
+    : MachineReference(DefMI), Symbol(Symbol), Load(Load) {}
+  MachineSymbolRef(const MachineSymbolRef &C)
+    : MachineReference(C), Symbol(C.Symbol), Load(C.Load) {}
+  virtual MachineLiveVal *copy() const { return new MachineSymbolRef(*this); }
+
+  virtual enum Type getType() const { return SymbolRef; }
+  virtual bool isSymbolRef() const { return true; }
+
+  virtual bool operator==(const MachineLiveVal &RHS) const;
+  virtual std::string toString() const;
+  virtual MCSymbol *getReference(AsmPrinter &AP) const;
+  virtual bool isLoad() const { return Load; }
+
+private:
+  // MCSymbols may not exist yet, so instead store the operand to look up the
+  // MCSymbol at metadata emission time.
+  // Note: store hard-copy (not reference) because optimizations may convert
+  // symbol reference to a different type, e.g., register
+  const MachineOperand Symbol;
+
+  // Should we load the reference's value?
+  bool Load;
+};
+
+/// MachineConstPoolRef - a reference to a constant pool entry
+class MachineConstPoolRef : public MachineReference {
+public:
+  MachineConstPoolRef(int Index, const MachineInstr *DefMI)
+    : MachineReference(DefMI), Index(Index) {}
+  MachineConstPoolRef(const MachineConstPoolRef &C)
+    : MachineReference(C), Index(C.Index) {}
+  virtual MachineLiveVal *copy() const
+  { return new MachineConstPoolRef(*this); }
+
+  virtual enum Type getType() const { return ConstPoolRef; }
+  virtual bool isConstPoolRef() const { return true; }
+
+  virtual bool operator==(const MachineLiveVal &RHS) const;
+
+  virtual std::string toString() const
+  { return "reference to constant pool index " + std::to_string(Index); }
+
+  virtual MCSymbol *getReference(AsmPrinter &AP) const;
+
+private:
+  int Index;
+};
+
+/// MachineStackObject - an object on the stack
+class MachineStackObject : public MachineLiveVal {
+public:
+  MachineStackObject(int Index,
+                     bool Load,
+                     const MachineInstr *DefMI,
+                     bool Ptr = false)
+    : MachineLiveVal(DefMI, Ptr), Index(Index), Load(Load) {}
+  MachineStackObject(const MachineStackObject &C)
+    : MachineLiveVal(C), Index(C.Index), Load(C.Load) {}
+  virtual MachineLiveVal *copy() const
+  { return new MachineStackObject(*this); }
+
+  /// Objects common across stack frames for all supported architectures
+  enum Common { None, ReturnAddr = INT32_MAX };
+
+  virtual enum Type getType() const { return StackObject; }
+  virtual bool isStackObject() const { return true; }
+  virtual enum Common getCommonObjectType() const { return None; }
+  virtual bool isCommonObject() const { return false; }
+
+  virtual bool operator==(const MachineLiveVal &RHS) const;
+
+  virtual std::string toString() const;
+
+  /// Return the object's offset from a base register (returned in BR)
+  virtual int getOffsetFromReg(AsmPrinter &AP, unsigned &BR) const;
+
+  int getIndex() const { return Index; }
+  void setIndex(int Index) { this->Index = Index; }
+  bool isLoad() const { return Load; }
+  void setLoad(bool Load) { this->Load = Load; }
+
+private:
+  /// The stack slot index of a stack object
+  int Index;
+
+  /// Are we generating a reference to a stack object or loading a value from
+  /// the stack slot?
+  bool Load;
+};
+
+/// ReturnAddress - the return address stored on the stack
+class ReturnAddress : public MachineStackObject {
+public:
+  ReturnAddress(const MachineInstr *DefMI)
+    : MachineStackObject(ReturnAddr, true, DefMI, false) {}
+  ReturnAddress(const ReturnAddress &C) : MachineStackObject(C) {}
+  virtual MachineLiveVal *copy() const { return new ReturnAddress(*this); }
+
+  virtual enum Common getCommonObjectType() const { return ReturnAddr; }
+  virtual bool isCommonObject() const { return true; }
+
+  virtual std::string toString() const
+  { return "function return address"; }
+
+  virtual int getOffsetFromReg(AsmPrinter &AP, unsigned &BR) const;
+};
+
+/// MachineImmediate - an immediate value
+class MachineImmediate : public MachineLiveVal {
+public:
+  MachineImmediate(unsigned Size,
+                   uint64_t Value,
+                   const MachineInstr *DefMI,
+                   bool Ptr = false);
+  MachineImmediate(const MachineImmediate &C)
+    : MachineLiveVal(C), Size(C.Size), Value(C.Value) {}
+  virtual MachineLiveVal *copy() const { return new MachineImmediate(*this); }
+
+  virtual enum Type getType() const { return Immediate; }
+  virtual bool isImm() const { return true; }
+
+  virtual bool operator==(const MachineLiveVal &RHS) const;
+
+  virtual std::string toString() const
+  { return "immediate value: " + std::to_string(Value); }
+
+  unsigned getSize() const { return Size; }
+  uint64_t getValue() const { return Value; }
+
+private:
+  unsigned Size; // in bytes
+  uint64_t Value;
+};
+
+/// MachineGeneratedVal - a value generated through a set of small operations
+class MachineGeneratedVal : public MachineLiveVal {
+public:
+  MachineGeneratedVal(const ValueGenInstList &VG,
+                      const MachineInstr *DefMI,
+                      bool Ptr)
+    : MachineLiveVal(DefMI, Ptr), VG(VG) {}
+  virtual MachineLiveVal *copy() const
+  { return new MachineGeneratedVal(*this); }
+
+  enum Type getType() const { return Generated; }
+  bool isGenerated() const { return true; }
+
+  virtual bool operator==(const MachineLiveVal &RHS) const;
+
+  virtual std::string toString() const
+  { return "generated value, " + std::to_string(VG.size()) + " instruction(s)"; }
+
+  const ValueGenInstList &getInstructions() const { return VG; }
+
+private:
+  ValueGenInstList VG;
+};
+
+// TODO add API to generate "Operation"
+
+//===----------------------------------------------------------------------===//
+// Machine-specific locations
+//
+// These are locations to be populated with the live values, e.g., a register or
+// stack slot.
+//
+// Note: these represent a live value's *destination*, not the live value
+// itself.  For example, don't confuse MachineStackObject above (a live value
+// to be copied from a stack slot) versus MachineLiveStackSlot below (the
+// location where a live value will be stored).
+
+/// MachineLiveLoc - an architecture-specific location for a live value
+class MachineLiveLoc {
+public:
+  /// Constructors & destructors.
+  // Note: create child class objects rather than objects of this class.
+  virtual ~MachineLiveLoc() {}
+  virtual MachineLiveLoc *copy() const = 0;
+  virtual bool operator==(const MachineLiveLoc &R) const = 0;
+
+  /// Determine the live value location type
+  virtual bool isReg() const { return false; }
+  virtual bool isStackAddr() const { return false; }
+  virtual bool isStackSlot() const { return false; }
+
+  virtual std::string toString() const = 0;
+};
+
+/// MachineLiveReg - a live value stored in a register.  Stores the register
+/// number as an architecture-specific physical register.
+class MachineLiveReg : public MachineLiveLoc {
+public:
+  MachineLiveReg(unsigned Reg) : Reg(Reg) {}
+  MachineLiveReg(const MachineLiveReg &C) : Reg(C.Reg) {}
+  virtual MachineLiveLoc *copy() const { return new MachineLiveReg(*this); }
+
+  virtual bool isReg() const { return true; }
+
+  virtual bool operator==(const MachineLiveLoc &RHS) const;
+
+  unsigned getReg() const { return Reg; }
+  void setReg(unsigned Reg) { this->Reg = Reg; }
+
+  virtual std::string toString() const
+  { return "live value in register " + std::to_string(Reg); }
+
+private:
+  unsigned Reg;
+};
+
+/// MachineLiveStackAddr - a live value stored at a known stack address.  Can
+/// be used for stack objects at hard-coded offsets, e.g., the TOC pointer save
+/// location for PowerPC/ELFv2 ABI.
+class MachineLiveStackAddr : public MachineLiveLoc {
+public:
+  MachineLiveStackAddr() : Offset(INT32_MAX), Reg(UINT32_MAX), Size(0) {}
+  MachineLiveStackAddr(int Offset, unsigned Reg, unsigned Size)
+    : Offset(Offset), Reg(Reg), Size(Size) {}
+  MachineLiveStackAddr(const MachineLiveStackAddr &C)
+    : Offset(C.Offset), Reg(C.Reg), Size(C.Size) {}
+  virtual MachineLiveLoc *copy() const
+  { return new MachineLiveStackAddr(*this); }
+
+  virtual bool isStackAddr() const { return true; }
+
+  virtual bool operator==(const MachineLiveLoc &RHS) const;
+
+  int getOffset() const { return Offset; }
+  void setOffset(int Offset) { this->Offset = Offset; }
+  unsigned getReg() const { return Reg; }
+  void setReg(unsigned Reg) { this->Reg = Reg; }
+  void setSize(unsigned Size) { this->Size = Size; }
+
+  // Calculate the final position of the stack object.  Return the object's
+  // location as an offset from a base pointer register.
+  virtual int calcAndGetRegOffset(const AsmPrinter &AP, unsigned &BP)
+  { BP = Reg; return Offset; }
+
+  // The size of a stack object may need to be determined by code emission
+  // metadata in child classes, hence the AsmPrinter argument
+  virtual unsigned getSize(const AsmPrinter &AP) { return Size; }
+
+  virtual std::string toString() const
+  {
+    return "live value at register " + std::to_string(Reg) +
+           " + " + std::to_string(Offset);
+  }
+
+protected:
+  // The object is referenced by an offset from a (physical) register's value.
+  int Offset;
+  unsigned Reg, Size;
+};
+
+/// MachineLiveStackSlot - a live value stored in a stack slot.  A more
+/// abstract version of MachineLiveStackAddr, where the value is in a virtual
+/// stack slot whose address won't be determined until instruction emission.
+class MachineLiveStackSlot : public MachineLiveStackAddr {
+public:
+  MachineLiveStackSlot(int Index) : Index(Index) {}
+  MachineLiveStackSlot(const MachineLiveStackSlot &C)
+    : MachineLiveStackAddr(C), Index(C.Index) {}
+  virtual MachineLiveLoc *copy() const
+  { return new MachineLiveStackSlot(*this); }
+
+  virtual bool isStackSlot() const { return true; }
+
+  virtual bool operator==(const MachineLiveLoc &RHS) const;
+
+  unsigned getStackSlot() const { return Index; }
+  void setStackSlot(int Index) { this->Index = Index; }
+  virtual int calcAndGetRegOffset(const AsmPrinter &AP, unsigned &BP);
+  virtual unsigned getSize(const AsmPrinter &AP);
+
+  virtual std::string toString() const
+  { return "live value in stack slot " + std::to_string(Index); }
+
+private:
+  int Index;
+};
+
+/// Useful typedefs for data structures needed to store additional stack
+/// transformation metadata not captured in the stackmap instructions.
+
+/// Tidy up objects defined above into smart pointers
+typedef std::unique_ptr<MachineLiveVal> MachineLiveValPtr;
+typedef std::unique_ptr<MachineLiveLoc> MachineLiveLocPtr;
+
+/// A vector of architecture-specific live value locations
+// Note: we could use a set instead (because we want unique live values), but
+// because we're using MachineLiveLoc pointers the set would only uniquify
+// based on the pointer, not the pointed-to value.
+typedef SmallVector<MachineLiveLocPtr, 4> MachineLiveLocs;
+
+/// Map IR value to a list of architecture-specific live value locations.
+/// Usually used to store duplicate locations for an IR value.
+typedef std::map<const Value *, MachineLiveLocs> IRToMachineLocs;
+typedef std::pair<const Value *, MachineLiveLocs> IRMachineLocPair;
+
+/// Map an IR instruction to the metadata about its IR operands (and their
+/// associated architecture-specific live values locations).
+typedef std::map<const Instruction *, IRToMachineLocs> InstToOperands;
+typedef std::pair<const Instruction *, IRToMachineLocs> InstOperandPair;
+
+/// A pair to couple an architecture-specific location to the value used to
+/// populate it, and a vector for storing several of them.
+typedef std::pair<MachineLiveLocPtr, MachineLiveValPtr> ArchLiveValue;
+typedef SmallVector<ArchLiveValue, 8> ArchLiveValues;
+
+// Map an IR instruction to architecture-specific live values
+typedef std::map<const Instruction *, ArchLiveValues> InstToArchLiveValues;
+typedef std::pair<const Instruction *, ArchLiveValues> InstArchLiveValuePair;
+
+}
+
+#endif
+
Index: include/llvm/CodeGen/UnwindInfo.h
===================================================================
--- include/llvm/CodeGen/UnwindInfo.h	(nonexistent)
+++ include/llvm/CodeGen/UnwindInfo.h	(working copy)
@@ -0,0 +1,112 @@
+//===----------------- UnwindInfo.h - UnwindInfo ---------------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Generate unwinding information for stack transformation runtime.  Note that
+// this is implemented assuming the function uses a frame base pointer (FBP).
+// This requirement is guaranteed to be satisfied if the function has a
+// stackmap, which are the only functions for which we want to generate
+// unwinding information.
+//
+//===---------------------------------------------------------------------===//
+
+#ifndef LLVM_CODEGEN_UNWINDINFO_H
+#define LLVM_CODEGEN_UNWINDINFO_H
+
+#include "llvm/CodeGen/AsmPrinter.h"
+#include "llvm/CodeGen/MachineFrameInfo.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/MC/MCContext.h"
+#include "llvm/MC/MCStreamer.h"
+#include "llvm/Support/Debug.h"
+#include <map>
+
+namespace llvm {
+
+class UnwindInfo {
+public:
+  /// Per-function unwinding metadata classes & typedefs
+  class FuncUnwindInfo {
+  public:
+    uint32_t SecOffset; // Offset into unwinding record section
+    uint32_t NumUnwindRecord; // Number of unwinding records
+
+    FuncUnwindInfo() : SecOffset(UINT32_MAX), NumUnwindRecord(0) {}
+    FuncUnwindInfo(uint32_t SecOffset, uint32_t NumUnwindRecord)
+      : SecOffset(SecOffset), NumUnwindRecord(NumUnwindRecord) {}
+  };
+
+  typedef std::pair<const MCSymbol *, FuncUnwindInfo> FuncUnwindPair;
+  typedef std::map<const MCSymbol *, FuncUnwindInfo> FuncUnwindMap;
+
+  /// Unwinding record classes & typedefs
+  class RegOffset {
+  public:
+    uint32_t DwarfReg;
+    int32_t Offset;
+
+    RegOffset() : DwarfReg(0), Offset(0) {}
+    RegOffset(uint32_t DwarfReg, int32_t Offset) :
+      DwarfReg(DwarfReg), Offset(Offset) {}
+  };
+
+  typedef SmallVector<RegOffset, 32> CalleeSavedRegisters;
+  typedef std::pair<const MCSymbol *, CalleeSavedRegisters> FuncCalleePair;
+  typedef std::map<const MCSymbol *, CalleeSavedRegisters> FuncCalleeMap;
+
+  /// \brief Constructors
+  UnwindInfo() = delete;
+  UnwindInfo(AsmPrinter &AP)
+    : AP(AP), OutContext(AP.OutStreamer->getContext()), Emitted(false) {};
+
+  /// \bried Clear all saved unwinding information
+  void reset() {
+    Emitted = false;
+    FuncCalleeSaved.clear();
+    FuncUnwindMetadata.clear();
+  }
+
+  /// \brief Store unwinding information for a function
+  void recordUnwindInfo(const MachineFunction &MF);
+
+  /// \brief Add a register restore offset for a function.  MachineReg will get
+  /// converted to a DWARF register internally.
+  void addRegisterUnwindInfo(const MachineFunction &MF,
+                             uint32_t MachineReg,
+                             int32_t Offset);
+
+  /// Create an unwinding information section and serialize the map info into
+  /// it.
+  ///
+  /// Note: unlike StackMaps::serializeToStackMapSection, this function *does
+  /// not* clear out the data structures.  This is so that the stack map
+  /// machinery can access per-function unwinding information.
+  void serializeToUnwindInfoSection();
+
+  /// Get unwinding section metadata for a function
+  const FuncUnwindInfo &getUnwindInfo(const MCSymbol *Func) const;
+
+private:
+  AsmPrinter &AP;
+  MCContext &OutContext;
+  FuncCalleeMap FuncCalleeSaved;
+  FuncUnwindMap FuncUnwindMetadata;
+  bool Emitted;
+
+  /// \brief Emit the unwind info for each function.
+  void emitUnwindInfo(MCStreamer &OS);
+
+  /// \brief Emit the address range info for each function.
+  void emitAddrRangeInfo(MCStreamer &OS);
+
+  void print(raw_ostream &OS);
+  void debug() { print(dbgs()); }
+};
+}
+
+#endif
Index: include/llvm/IR/DiagnosticInfo.h
===================================================================
--- include/llvm/IR/DiagnosticInfo.h	(revision 320332)
+++ include/llvm/IR/DiagnosticInfo.h	(working copy)
@@ -57,6 +57,7 @@
   DK_OptimizationRemarkMissed,
   DK_OptimizationRemarkAnalysis,
   DK_OptimizationFailure,
+  DK_OptimizationError,
   DK_MIRParser,
   DK_FirstPluginKind
 };
@@ -461,6 +462,29 @@
   bool isEnabled() const override;
 };
 
+/// Diagnostic information for show-stopping failures.
+class DiagnosticInfoOptimizationError
+    : public DiagnosticInfoOptimizationBase {
+public:
+  /// \p Fn is the function where the diagnostic is being emitted. \p DLoc is
+  /// the location information to use in the diagnostic. If line table
+  /// information is available, the diagnostic will include the source code
+  /// location. \p Msg is the message to show. Note that this class does not
+  /// copy this message, so this reference must be valid for the whole life time
+  /// of the diagnostic.
+  DiagnosticInfoOptimizationError(const Function &Fn, const DebugLoc &DLoc,
+                                  const Twine &Msg)
+      : DiagnosticInfoOptimizationBase(DK_OptimizationError, DS_Error,
+                                       nullptr, Fn, DLoc, Msg) {}
+
+  static bool classof(const DiagnosticInfo *DI) {
+    return DI->getKind() == DK_OptimizationError;
+  }
+
+  /// \see DiagnosticInfoOptimizationBase::isEnabled.
+  bool isEnabled() const override;
+};
+
 /// Emit a warning when loop vectorization is specified but fails. \p Fn is the
 /// function triggering the warning, \p DLoc is the debug location where the
 /// diagnostic is generated. \p Msg is the message string to use.
Index: include/llvm/IR/ModuleSlotTracker.h
===================================================================
--- include/llvm/IR/ModuleSlotTracker.h	(revision 320332)
+++ include/llvm/IR/ModuleSlotTracker.h	(working copy)
@@ -61,6 +61,9 @@
   /// Purge the currently incorporated function and incorporate \c F.  If \c F
   /// is currently incorporated, this is a no-op.
   void incorporateFunction(const Function &F);
+
+  /// Get a local value's slot.
+  int getLocalSlot(const Value *);
 };
 
 } // end namespace llvm
Index: include/llvm/InitializePasses.h
===================================================================
--- include/llvm/InitializePasses.h	(revision 320332)
+++ include/llvm/InitializePasses.h	(working copy)
@@ -119,6 +119,7 @@
 void initializeDominatorTreeWrapperPassPass(PassRegistry&);
 void initializeEarlyIfConverterPass(PassRegistry&);
 void initializeEdgeBundlesPass(PassRegistry&);
+void initializeEnumerateLoopPathsPass(PassRegistry&);
 void initializeExpandPostRAPass(PassRegistry&);
 void initializeGCOVProfilerPass(PassRegistry&);
 void initializeInstrProfilingPass(PassRegistry&);
@@ -131,6 +132,7 @@
 void initializeScalarizerPass(PassRegistry&);
 void initializeEarlyCSELegacyPassPass(PassRegistry &);
 void initializeEliminateAvailableExternallyPass(PassRegistry&);
+void initializeMigrationPointsPass(PassRegistry&);
 void initializeExpandISelPseudosPass(PassRegistry&);
 void initializeFunctionAttrsPass(PassRegistry&);
 void initializeGCMachineCodeAnalysisPass(PassRegistry&);
@@ -146,6 +148,7 @@
 void initializeInductiveRangeCheckEliminationPass(PassRegistry&);
 void initializeIndVarSimplifyPass(PassRegistry&);
 void initializeInlineCostAnalysisPass(PassRegistry&);
+void initializeInsertStackMapsPass(PassRegistry&);
 void initializeInstructionCombiningPassPass(PassRegistry&);
 void initializeInstCountPass(PassRegistry&);
 void initializeInstNamerPass(PassRegistry&);
@@ -153,6 +156,7 @@
 void initializeIntervalPartitionPass(PassRegistry&);
 void initializeJumpThreadingPass(PassRegistry&);
 void initializeLCSSAPass(PassRegistry&);
+void initializeLibcStackMapsPass(PassRegistry&);
 void initializeLICMPass(PassRegistry&);
 void initializeLazyValueInfoPass(PassRegistry&);
 void initializeLibCallAliasAnalysisPass(PassRegistry&);
@@ -162,6 +166,7 @@
 void initializeLiveRegMatrixPass(PassRegistry&);
 void initializeLiveStacksPass(PassRegistry&);
 void initializeLiveVariablesPass(PassRegistry&);
+void initializeLiveValuesPass(PassRegistry&);
 void initializeLoaderPassPass(PassRegistry&);
 void initializeLocalStackSlotPassPass(PassRegistry&);
 void initializeLoopDeletionPass(PassRegistry&);
@@ -208,6 +213,7 @@
 void initializeMetaRenamerPass(PassRegistry&);
 void initializeMergeFunctionsPass(PassRegistry&);
 void initializeModuleDebugInfoPrinterPass(PassRegistry&);
+void initializeNameStringLiteralsPass(PassRegistry&);
 void initializeNaryReassociatePass(PassRegistry&);
 void initializeNoAAPass(PassRegistry&);
 void initializeObjCARCAliasAnalysisPass(PassRegistry&);
@@ -222,6 +228,7 @@
 void initializePHIEliminationPass(PassRegistry&);
 void initializePartialInlinerPass(PassRegistry&);
 void initializePeepholeOptimizerPass(PassRegistry&);
+void initializePopcornCompatibilityPass(PassRegistry&);
 void initializePostDomOnlyPrinterPass(PassRegistry&);
 void initializePostDomOnlyViewerPass(PassRegistry&);
 void initializePostDomPrinterPass(PassRegistry&);
@@ -250,9 +257,11 @@
 void initializeSROA_SSAUpPass(PassRegistry&);
 void initializeScalarEvolutionAliasAnalysisPass(PassRegistry&);
 void initializeScalarEvolutionPass(PassRegistry&);
+void initializeSelectMigrationPointsPass(PassRegistry&);
 void initializeShrinkWrapPass(PassRegistry &);
 void initializeSimpleInlinerPass(PassRegistry&);
-void initializeShadowStackGCLoweringPass(PassRegistry&);  
+void initializeShadowStackGCLoweringPass(PassRegistry&);
+void initializeStaticVarSectionsPass(PassRegistry&);
 void initializeRegisterCoalescerPass(PassRegistry&);
 void initializeSingleLoopExtractorPass(PassRegistry&);
 void initializeSinkingPass(PassRegistry&);
@@ -263,6 +272,7 @@
 void initializeStackProtectorPass(PassRegistry&);
 void initializeStackColoringPass(PassRegistry&);
 void initializeStackSlotColoringPass(PassRegistry&);
+void initializeStackTransformMetadataPass(PassRegistry&);
 void initializeStraightLineStrengthReducePass(PassRegistry &);
 void initializeStripDeadDebugInfoPass(PassRegistry&);
 void initializeStripDeadPrototypesPassPass(PassRegistry&);
Index: include/llvm/LinkAllPasses.h
===================================================================
--- include/llvm/LinkAllPasses.h	(revision 320332)
+++ include/llvm/LinkAllPasses.h	(working copy)
@@ -33,6 +33,7 @@
 #include "llvm/Transforms/Instrumentation.h"
 #include "llvm/Transforms/ObjCARC.h"
 #include "llvm/Transforms/Scalar.h"
+#include "llvm/Transforms/Utils.h"
 #include "llvm/Transforms/Utils/SymbolRewriter.h"
 #include "llvm/Transforms/Utils/UnifyFunctionExitNodes.h"
 #include "llvm/Transforms/Vectorize.h"
@@ -81,8 +82,10 @@
       (void) llvm::createDomPrinterPass();
       (void) llvm::createDomOnlyViewerPass();
       (void) llvm::createDomViewerPass();
+      (void) llvm::createMigrationPointsPass();
       (void) llvm::createGCOVProfilerPass();
       (void) llvm::createInstrProfilingPass();
+      (void) llvm::createEnumerateLoopPathsPass();
       (void) llvm::createFunctionInliningPass();
       (void) llvm::createAlwaysInlinerPass();
       (void) llvm::createGlobalDCEPass();
@@ -92,11 +95,14 @@
       (void) llvm::createIPSCCPPass();
       (void) llvm::createInductiveRangeCheckEliminationPass();
       (void) llvm::createIndVarSimplifyPass();
+      (void) llvm::createInsertStackMapsPass();
       (void) llvm::createInstructionCombiningPass();
       (void) llvm::createInternalizePass();
       (void) llvm::createLCSSAPass();
+      (void) llvm::createLibcStackMapsPass();
       (void) llvm::createLICMPass();
       (void) llvm::createLazyValueInfoPass();
+      (void) llvm::createLiveValuesPass();
       (void) llvm::createLoopExtractorPass();
       (void) llvm::createLoopInterchangePass();
       (void) llvm::createLoopSimplifyPass();
@@ -110,6 +116,7 @@
       (void) llvm::createLowerInvokePass();
       (void) llvm::createLowerSwitchPass();
       (void) llvm::createNaryReassociatePass();
+      (void) llvm::createNameStringLiteralsPass();
       (void) llvm::createNoAAPass();
       (void) llvm::createObjCARCAliasAnalysisPass();
       (void) llvm::createObjCARCAPElimPass();
@@ -120,6 +127,7 @@
       (void) llvm::createPromoteMemoryToRegisterPass();
       (void) llvm::createDemoteRegisterToMemoryPass();
       (void) llvm::createPruneEHPass();
+      (void) llvm::createPopcornCompatibilityPass();
       (void) llvm::createPostDomOnlyPrinterPass();
       (void) llvm::createPostDomPrinterPass();
       (void) llvm::createPostDomOnlyViewerPass();
@@ -134,6 +142,7 @@
       (void) llvm::createSafeStackPass();
       (void) llvm::createScalarReplAggregatesPass();
       (void) llvm::createSingleLoopExtractorPass();
+      (void) llvm::createStaticVarSectionsPass();
       (void) llvm::createStripSymbolsPass();
       (void) llvm::createStripNonDebugSymbolsPass();
       (void) llvm::createStripDeadDebugInfoPass();
@@ -170,6 +179,7 @@
       (void) llvm::createBBVectorizePass();
       (void) llvm::createPartiallyInlineLibCallsPass();
       (void) llvm::createScalarizerPass();
+      (void) llvm::createSelectMigrationPointsPass();
       (void) llvm::createSeparateConstOffsetFromGEPPass();
       (void) llvm::createSpeculativeExecutionPass();
       (void) llvm::createRewriteSymbolsPass();
Index: include/llvm/MC/MCCodeGenInfo.h
===================================================================
--- include/llvm/MC/MCCodeGenInfo.h	(revision 320332)
+++ include/llvm/MC/MCCodeGenInfo.h	(working copy)
@@ -32,6 +32,11 @@
   ///
   CodeGenOpt::Level OptLevel;
 
+  /// ArchIROptLevel - Optimization level (architecture-specific IR
+  /// optimizations only).  Defaults to OptLevel.
+  ///
+  CodeGenOpt::Level ArchIROptLevel;
+
 public:
   void initMCCodeGenInfo(Reloc::Model RM = Reloc::Default,
                          CodeModel::Model CM = CodeModel::Default,
@@ -43,8 +48,12 @@
 
   CodeGenOpt::Level getOptLevel() const { return OptLevel; }
 
+  CodeGenOpt::Level getArchIROptLevel() const { return ArchIROptLevel; }
+
   // Allow overriding OptLevel on a per-function basis.
   void setOptLevel(CodeGenOpt::Level Level) { OptLevel = Level; }
+
+  void setArchIROptLevel(CodeGenOpt::Level Level) { ArchIROptLevel = Level; }
 };
 } // namespace llvm
 
Index: include/llvm/MC/MCObjectFileInfo.h
===================================================================
--- include/llvm/MC/MCObjectFileInfo.h	(revision 320332)
+++ include/llvm/MC/MCObjectFileInfo.h	(working copy)
@@ -135,6 +135,10 @@
   /// Null if this target doesn't support a BSS section. ELF and MachO only.
   MCSection *TLSBSSSection; // Defaults to ".tbss".
 
+  /// Unwinding address ranges & register location sections.
+  MCSection *UnwindAddrRangeSection;
+  MCSection *UnwindInfoSection;
+
   /// StackMap section.
   MCSection *StackMapSection;
 
@@ -267,6 +271,8 @@
   const MCSection *getTLSDataSection() const { return TLSDataSection; }
   MCSection *getTLSBSSSection() const { return TLSBSSSection; }
 
+  MCSection *getUnwindInfoSection() const { return UnwindInfoSection; }
+  MCSection *getUnwindAddrRangeSection() const { return UnwindAddrRangeSection; }
   MCSection *getStackMapSection() const { return StackMapSection; }
   MCSection *getFaultMapSection() const { return FaultMapSection; }
 
Index: include/llvm/Target/TargetFrameLowering.h
===================================================================
--- include/llvm/Target/TargetFrameLowering.h	(revision 320332)
+++ include/llvm/Target/TargetFrameLowering.h	(working copy)
@@ -227,6 +227,14 @@
     return 0;
   }
 
+  /// Same as above, except that the 'frame register' will always be the ISA's
+  /// frame pointer (which can be different from the variable 'frame register'
+  /// which may be the stack pointer, frame pointer, etc.)
+  virtual int getFrameIndexReferenceFromFP(const MachineFunction &MF, int FI,
+                                           unsigned &FrameReg) const {
+    return getFrameIndexReference(MF, FI, FrameReg);
+  }
+
   /// This method determines which of the registers reported by
   /// TargetRegisterInfo::getCalleeSavedRegs() should actually get saved.
   /// The default implementation checks populates the \p SavedRegs bitset with
Index: include/llvm/Target/TargetMachine.h
===================================================================
--- include/llvm/Target/TargetMachine.h	(revision 320332)
+++ include/llvm/Target/TargetMachine.h	(working copy)
@@ -171,6 +171,13 @@
   /// \brief Overrides the optimization level.
   void setOptLevel(CodeGenOpt::Level Level) const;
 
+  /// Returns the architecture-specific IR optimization level: None, Less,
+  /// Default or Aggressive.
+  CodeGenOpt::Level getArchIROptLevel() const;
+
+  /// \brief Overrides the architecture-specific IR optimization level.
+  void setArchIROptLevel(CodeGenOpt::Level Level) const;
+
   void setFastISel(bool Enable) { Options.EnableFastISel = Enable; }
 
   bool shouldPrintMachineCode() const { return Options.PrintMachineCode; }
Index: include/llvm/Target/TargetRegisterInfo.h
===================================================================
--- include/llvm/Target/TargetRegisterInfo.h	(revision 320332)
+++ include/llvm/Target/TargetRegisterInfo.h	(working copy)
@@ -869,6 +869,17 @@
   /// getFrameRegister - This method should return the register used as a base
   /// for values allocated in the current stack frame.
   virtual unsigned getFrameRegister(const MachineFunction &MF) const = 0;
+
+  /// getReturnAddrLoc - This method should return the location of the saved
+  /// return address on the stack, expressed as a base register (returned via
+  /// BaseReg) and an offset
+  virtual int getReturnAddrLoc(const MachineFunction &MF,
+                               unsigned &BaseReg) const
+  {
+    llvm_unreachable("Not implemented for target!");
+    BaseReg = 0;
+    return INT32_MAX;
+  }
 };
 
 
Index: include/llvm/Target/TargetSubtargetInfo.h
===================================================================
--- include/llvm/Target/TargetSubtargetInfo.h	(revision 320332)
+++ include/llvm/Target/TargetSubtargetInfo.h	(working copy)
@@ -32,6 +32,7 @@
 class TargetRegisterInfo;
 class TargetSchedModel;
 class TargetSelectionDAGInfo;
+class TargetValues;
 struct MachineSchedPolicy;
 template <typename T> class SmallVectorImpl;
 
@@ -95,6 +96,13 @@
     return nullptr;
   }
 
+  /// getValues - Returns the value generator object for the target or specific
+  /// subtarget
+  ///
+  virtual const TargetValues *getValues() const {
+    return nullptr;
+  };
+
   /// Resolve a SchedClass at runtime, where SchedClass identifies an
   /// MCSchedClassDesc with the isVariant property. This may return the ID of
   /// another variant SchedClass, but repeated invocation must quickly terminate
Index: include/llvm/Target/TargetValues.h
===================================================================
--- include/llvm/Target/TargetValues.h	(nonexistent)
+++ include/llvm/Target/TargetValues.h	(working copy)
@@ -0,0 +1,93 @@
+//===----- llvm/Target/TargetValues.h - Value Properties ----*- C++ -----*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file provides an API for detecting properties of architecture-specific
+// values & for generating a series of simple metadata instructions for
+// reconstituting a value.  This is used by the stack transformation runtime to
+// set up architecture-specific live values.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_TARGET_TARGETVAL_H
+#define LLVM_TARGET_TARGETVAL_H
+
+#include <memory>
+#include "llvm/CodeGen/StackTransformTypes.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/VirtRegMap.h"
+#include "llvm/IR/Instructions.h"
+
+namespace llvm {
+
+/// A value created by the backend to satisfy the stackmap.  Note that this is
+/// not necessarily an error -- some backends may create and save the value
+/// whereas other backends may materialize the value as needed.
+struct TemporaryValue {
+  enum type {
+    StackSlotRef
+  };
+
+  enum type Type;
+  unsigned Size;
+
+  /// The virtual register used if the temporary is stored in a register
+  unsigned Vreg;
+
+  /// If the value is of type StackSlotRef, the stack slot and offset into the
+  /// stack slot being referenced
+  int StackSlot;
+  int64_t Offset;
+};
+
+typedef std::unique_ptr<TemporaryValue> TemporaryValuePtr;
+
+//===----------------------------------------------------------------------===//
+// Superclass for ISA-specific values
+//
+
+class TargetValues {
+public:
+  TargetValues(const TargetValues &) = delete;
+  void operator=(const TargetValues &) = delete;
+  virtual ~TargetValues() {};
+
+  /// The code generator may have materialized a temporary value solely to
+  /// satisfy the stackmap if the value is materialized as-needed elsewhere.
+  /// Return metadata describing the temporary value in this situation.
+  virtual TemporaryValuePtr getTemporaryValue(const MachineInstr *MI,
+                                              const VirtRegMap *VRM) const
+  { return nullptr; }
+
+  /// Return a machine-specific value generated by a machine instruction.
+  virtual MachineLiveValPtr getMachineValue(const MachineInstr *MI) const = 0;
+
+  /// Add any required architecture-specific live values, e.g., the TOC pointer
+  /// on PowerPC.
+  virtual void addRequiredArchLiveValues(MachineFunction *MF,
+                                         const MachineInstr *MIStackMap,
+                                         const CallInst *IRStackMap) const
+  { return; }
+
+protected:
+  TargetValues() {}
+
+  /// Return whether or not the operand is some type of symbol reference.
+  static bool isSymbolValue(const MachineOperand &MO)
+  { return MO.isGlobal() || MO.isSymbol() || MO.isMCSymbol(); }
+  static bool isSymbolValue(const MachineOperand *MO)
+  { return isSymbolValue(*MO); }
+  static bool isSymbolValueConstant(const MachineOperand &MO);
+  static bool isSymbolValueConstant(const MachineOperand *MO)
+  { return isSymbolValueConstant(*MO); }
+};
+
+} // End llvm namespace
+
+#endif
+
Index: include/llvm/Transforms/Instrumentation.h
===================================================================
--- include/llvm/Transforms/Instrumentation.h	(revision 320332)
+++ include/llvm/Transforms/Instrumentation.h	(working copy)
@@ -136,6 +136,17 @@
 /// protect against stack-based overflow vulnerabilities.
 FunctionPass *createSafeStackPass();
 
+/// \brief This pass inserts equivalence points into functions.
+FunctionPass *createMigrationPointsPass();
+
+/// \brief This pass inserts stack map intrinsics at equivalence points in
+/// order to record live value locations
+ModulePass *createInsertStackMapsPass();
+
+/// \brief This pass inserts stack map intrinsics similarly to InsertStackMaps,
+/// but only in thread start functions inside of libc
+ModulePass *createLibcStackMapsPass();
+
 } // End llvm namespace
 
 #endif
Index: include/llvm/Transforms/Utils.h
===================================================================
--- include/llvm/Transforms/Utils.h	(nonexistent)
+++ include/llvm/Transforms/Utils.h	(working copy)
@@ -0,0 +1,16 @@
+namespace llvm {
+
+//===----------------------------------------------------------------------===//
+//
+// NameStringLiterals - Give symbol names to anonymous string literals so they
+// can be aligned at link-time
+//
+ModulePass *createNameStringLiteralsPass();
+
+//===----------------------------------------------------------------------===//
+//
+// StaticVarSections - Put static global variables into their own sections
+//
+ModulePass *createStaticVarSectionsPass();
+
+}
Index: lib/Analysis/Analysis.cpp
===================================================================
--- lib/Analysis/Analysis.cpp	(revision 320332)
+++ lib/Analysis/Analysis.cpp	(working copy)
@@ -42,8 +42,10 @@
   initializeDomViewerPass(Registry);
   initializeDomPrinterPass(Registry);
   initializeDomOnlyViewerPass(Registry);
+  initializeEnumerateLoopPathsPass(Registry);
   initializePostDomViewerPass(Registry);
   initializeDomOnlyPrinterPass(Registry);
+  initializePopcornCompatibilityPass(Registry);
   initializePostDomPrinterPass(Registry);
   initializePostDomOnlyViewerPass(Registry);
   initializePostDomOnlyPrinterPass(Registry);
@@ -53,6 +55,7 @@
   initializeLazyValueInfoPass(Registry);
   initializeLibCallAliasAnalysisPass(Registry);
   initializeLintPass(Registry);
+  initializeLiveValuesPass(Registry);
   initializeLoopInfoWrapperPassPass(Registry);
   initializeMemDepPrinterPass(Registry);
   initializeMemDerefPrinterPass(Registry);
@@ -66,6 +69,7 @@
   initializeRegionOnlyPrinterPass(Registry);
   initializeScalarEvolutionPass(Registry);
   initializeScalarEvolutionAliasAnalysisPass(Registry);
+  initializeSelectMigrationPointsPass(Registry);
   initializeTargetTransformInfoWrapperPassPass(Registry);
   initializeTypeBasedAliasAnalysisPass(Registry);
   initializeScopedNoAliasAAPass(Registry);
Index: lib/Analysis/CMakeLists.txt
===================================================================
--- lib/Analysis/CMakeLists.txt	(revision 320332)
+++ lib/Analysis/CMakeLists.txt	(working copy)
@@ -34,10 +34,13 @@
   LibCallAliasAnalysis.cpp
   LibCallSemantics.cpp
   Lint.cpp
+  LiveValues.cpp
   Loads.cpp
   LoopAccessAnalysis.cpp
   LoopInfo.cpp
+  LoopNestingTree.cpp
   LoopPass.cpp
+  LoopPaths.cpp
   MemDepPrinter.cpp
   MemDerefPrinter.cpp
   MemoryBuiltins.cpp
@@ -46,6 +49,7 @@
   ModuleDebugInfoPrinter.cpp
   NoAliasAnalysis.cpp
   PHITransAddr.cpp
+  PopcornCompatibility.cpp
   PostDominators.cpp
   PtrUseVisitor.cpp
   RegionInfo.cpp
@@ -55,6 +59,7 @@
   ScalarEvolutionAliasAnalysis.cpp
   ScalarEvolutionExpander.cpp
   ScalarEvolutionNormalization.cpp
+  SelectMigrationPoints.cpp
   SparsePropagation.cpp
   TargetLibraryInfo.cpp
   TargetTransformInfo.cpp
Index: lib/Analysis/LiveValues.cpp
===================================================================
--- lib/Analysis/LiveValues.cpp	(nonexistent)
+++ lib/Analysis/LiveValues.cpp	(working copy)
@@ -0,0 +1,386 @@
+#include "llvm/Analysis/LiveValues.h"
+#include "llvm/IR/Metadata.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/CFG.h"
+#include "llvm/Analysis/CFG.h"
+#include "llvm/Analysis/LoopInfo.h"
+#include "llvm/ADT/PostOrderIterator.h"
+#include "llvm/ADT/SCCIterator.h"
+#include "llvm/Support/Debug.h"
+
+#define DEBUG_TYPE "live-values"
+
+using namespace llvm;
+
+char LiveValues::ID = 0;
+INITIALIZE_PASS_BEGIN(LiveValues, "live-values", 
+                    "Live-value set calculation", true, true)
+INITIALIZE_PASS_DEPENDENCY(LoopInfoWrapperPass)
+INITIALIZE_PASS_END(LiveValues, "live-values", 
+                    "Live-value set calculation", true, true)
+
+namespace llvm {
+  FunctionPass *createLiveValuesPass() { return new LiveValues(); }
+}
+
+///////////////////////////////////////////////////////////////////////////////
+// Public API
+///////////////////////////////////////////////////////////////////////////////
+
+LiveValues::LiveValues(void)
+  : FunctionPass(ID), inlineasm(false), bitcasts(true), comparisons(true),
+    constants(false), metadata(false) {}
+
+void LiveValues::getAnalysisUsage(AnalysisUsage &AU) const
+{
+  AU.addRequired<LoopInfoWrapperPass>();
+  AU.setPreservesAll();
+}
+
+bool LiveValues::runOnFunction(Function &F)
+{
+  if(FuncBBLiveIn.count(&F))
+  {
+    DEBUG(
+      errs() << "\nFound previous analysis for " << F.getName() << "\n\n";
+      print(errs(), &F);
+    );
+  }
+  else
+  {
+    DEBUG(errs() << "\n********** Beginning LiveValues **********\n"
+                 << "********** Function: " << F.getName() << " **********\n\n"
+                    "LiveValues: performing bottom-up dataflow analysis\n");
+
+    LoopNestingForest LNF;
+    FuncBBLiveIn.emplace(&F, LiveVals());
+    FuncBBLiveOut.emplace(&F, LiveVals());
+
+    /* 1. Compute partial liveness sets using a postorder traversal. */
+    dagDFS(F, FuncBBLiveIn[&F], FuncBBLiveOut[&F]);
+
+    DEBUG(errs() << "LiveValues: constructing loop-nesting forest\n");
+
+    /* 2. Construct loop-nesting forest. */
+    constructLoopNestingForest(F, LNF);
+
+    DEBUG(errs() << "LiveValues: propagating values within loop-nests\n");
+
+    /* 3. Propagate live variables within loop bodies. */
+    loopTreeDFS(LNF, FuncBBLiveIn[&F], FuncBBLiveOut[&F]);
+
+    DEBUG(
+      print(errs(), &F);
+      errs() << "LiveValues: finished analysis\n"
+    );
+  }
+
+  return false;
+}
+
+void
+LiveValues::print(raw_ostream &O, const Function *F) const
+{
+  LiveVals::const_iterator bbIt;
+  std::set<const Value *>::const_iterator valIt;
+  const Module *M = F->getParent();
+
+  O << "LiveValues: results of live-value analysis\n";
+
+  if(!FuncBBLiveIn.count(F) || !FuncBBLiveOut.count(F))
+  {
+    if(F->hasName())
+      O << "No liveness information for function " << F->getName() << "\n";
+    else
+      O << "No liveness information for requested function\n";
+  }
+  else
+  {
+    for(bbIt = FuncBBLiveIn.at(F).cbegin();
+        bbIt != FuncBBLiveIn.at(F).cend();
+        bbIt++)
+    {
+      const BasicBlock *bb = bbIt->first;
+      const std::set<const Value *> &liveInVals = bbIt->second;
+      const std::set<const Value *> &liveOutVals = FuncBBLiveOut.at(F).at(bb);
+
+      bb->printAsOperand(O, false, M);
+      O << "\n  Live-in:\n    ";
+      for(valIt = liveInVals.cbegin(); valIt != liveInVals.cend(); valIt++)
+      {
+        (*valIt)->printAsOperand(O, false, M);
+        O << " ";
+      }
+
+      O << "\n  Live-out:\n    ";
+      for(valIt = liveOutVals.cbegin(); valIt != liveOutVals.cend(); valIt++)
+      {
+        (*valIt)->printAsOperand(O, false, M);
+        O << " ";
+      }
+
+      O << "\n";
+    }
+  }
+}
+
+std::set<const Value *> *LiveValues::getLiveIn(const BasicBlock *BB) const
+{
+  const Function *F = BB->getParent();
+  return new std::set<const Value *>(FuncBBLiveIn.at(F).at(BB));
+}
+
+std::set<const Value *> *LiveValues::getLiveOut(const BasicBlock *BB) const
+{
+  const Function *F = BB->getParent();
+  return new std::set<const Value *>(FuncBBLiveOut.at(F).at(BB));
+}
+
+std::set<const Value *>
+*LiveValues::getLiveValues(const Instruction *inst) const
+{
+  const BasicBlock *BB = inst->getParent();
+  const Function *F = BB->getParent();
+  BasicBlock::const_reverse_iterator ri, rie;
+  std::set<const Value *> *live = nullptr;
+
+  // Note: some functions have unreachable basic blocks (e.g., functions that
+  // call exit and then return a value).  If we don't have analysis for the
+  // block, return an empty set.
+  const LiveVals &Blocks = FuncBBLiveOut.at(F);
+  if(Blocks.count(BB)) live = new std::set<const Value *>(Blocks.at(BB));
+  else return new std::set<const Value *>;
+
+  for(ri = BB->rbegin(), rie = BB->rend(); ri != rie; ri++)
+  {
+    if(&*ri == inst) break;
+
+    live->erase(&*ri);
+    for(User::const_op_iterator op = ri->op_begin();
+        op != ri->op_end();
+        op++)
+      if(includeVal(*op))
+        live->insert(*op);
+  }
+
+  live->erase(&*ri);
+  for(User::const_op_iterator op = ri->op_begin();
+      op != ri->op_end();
+      op++)
+    if(includeVal(*op))
+      live->insert(*op);
+
+  return live;
+}
+
+///////////////////////////////////////////////////////////////////////////////
+// Private API
+///////////////////////////////////////////////////////////////////////////////
+
+bool LiveValues::includeVal(const llvm::Value *val) const
+{
+  bool include = true;
+
+  // TODO other values that should be filtered out?
+  if(isa<BasicBlock>(val))
+    include = false;
+  else if(isa<InlineAsm>(val) && !inlineasm)
+    include = false;
+  else if(isa<BitCastInst>(val) && !bitcasts)
+    include = false;
+  else if(isa<CmpInst>(val) && !comparisons)
+    include = false;
+  else if(isa<Constant>(val) && !constants)
+    include = false;
+  else if(isa<MetadataAsValue>(val) && !metadata)
+    include = false;
+
+  return include;
+}
+
+unsigned LiveValues::phiUses(const BasicBlock *B,
+                             const BasicBlock *S,
+                             std::set<const Value *> &uses)
+{
+  const PHINode *phi;
+  unsigned added = 0;
+
+  for(BasicBlock::const_iterator it = S->begin(); it != S->end(); it++)
+  {
+    if((phi = dyn_cast<PHINode>(&*it))) {
+      for(unsigned i = 0; i < phi->getNumIncomingValues(); i++)
+        if(phi->getIncomingBlock(i) == B &&
+           includeVal(phi->getIncomingValue(i)))
+          if(uses.insert(phi->getIncomingValue(i)).second)
+            added++;
+    }
+    else break; // phi-nodes are always at the start of the basic block
+  }
+
+  return added;
+}
+
+unsigned LiveValues::phiDefs(const BasicBlock *B,
+                             std::set<const Value *> &uses)
+{
+  const PHINode *phi;
+  unsigned added = 0;
+
+  for(BasicBlock::const_iterator it = B->begin(); it != B->end(); it++)
+  {
+    if((phi = dyn_cast<PHINode>(&*it))) {
+      if(includeVal(phi))
+        if(uses.insert(&*it).second)
+          added++;
+    }
+    else break; // phi-nodes are always at the start of the basic block
+  }
+
+  return added;
+}
+
+void LiveValues::dagDFS(Function &F, LiveVals &liveIn, LiveVals &liveOut)
+{
+  std::set<const Value *> live, phiDefined;
+  std::set<Edge> loopEdges;
+  SmallVector<Edge, 16> loopEdgeVec;
+
+  /* Find loop edges & convert to set for existence checking. */
+  FindFunctionBackedges(F, loopEdgeVec);
+  for(SmallVectorImpl<Edge>::const_iterator eit = loopEdgeVec.begin();
+      eit != loopEdgeVec.end();
+      eit++)
+    loopEdges.insert(*eit);
+
+  /* Calculate partial liveness sets for CFG nodes. */
+  for(auto B = po_iterator<const BasicBlock *>::begin(&F.getEntryBlock());
+      B != po_iterator<const BasicBlock *>::end(&F.getEntryBlock());
+      B++)
+  {
+    /* Calculate live-out set (lines 4-7 of Algorithm 2). */
+    for(succ_const_iterator S = succ_begin(*B); S != succ_end(*B); S++)
+    {
+      // Note: skip self-loop-edges, as adding values from phi-uses of this
+      // block causes use-def violations, and LLVM will complain.  This
+      // shouldn't matter, as phi-defs will cover this case.
+      if(*S == *B) continue;
+
+      phiUses(*B, *S, live);
+      if(!loopEdges.count(Edge(*B, *S)))
+      {
+        phiDefs(*S, phiDefined);
+        for(std::set<const Value *>::const_iterator vi = liveIn[*S].begin();
+            vi != liveIn[*S].end();
+            vi++)
+          if(!phiDefined.count(*vi) && includeVal(*vi)) live.insert(*vi);
+        phiDefined.clear();
+      }
+    }
+    liveOut.insert(LiveValsPair(*B, std::set<const Value *>(live)));
+
+    /* Calculate live-in set (lines 8-11 of Algorithm 2). */
+    for(BasicBlock::const_reverse_iterator inst = (*B)->rbegin();
+        inst != (*B)->rend();
+        inst++)
+    {
+      if(isa<PHINode>(&*inst)) break;
+
+      live.erase(&*inst);
+      for(User::const_op_iterator op = inst->op_begin();
+          op != inst->op_end();
+          op++)
+        if(includeVal(*op)) live.insert(*op);
+    }
+    phiDefs(*B, live);
+    liveIn.insert(LiveValsPair(*B, std::set<const Value *>(live)));
+
+    live.clear();
+
+    DEBUG(
+      errs() << "  ";
+      (*B)->printAsOperand(errs(), false);
+      errs() << ":\n";
+      errs() << "    Live-in:\n      ";
+      std::set<const Value *>::const_iterator it;
+      for(it = liveIn[*B].begin(); it != liveIn[*B].end(); it++)
+      {
+        (*it)->printAsOperand(errs(), false);
+        errs() << " ";
+      }
+      errs() << "\n    Live-out:\n      ";
+      for(it = liveOut[*B].begin(); it != liveOut[*B].end(); it++)
+      {
+        (*it)->printAsOperand(errs(), false);
+        errs() << " ";
+      }
+      errs() << "\n";
+    );
+  }
+}
+
+void LiveValues::constructLoopNestingForest(Function &F,
+                                            LoopNestingForest &LNF)
+{
+  LoopInfo &LI = getAnalysis<LoopInfoWrapperPass>().getLoopInfo();
+
+  for(scc_iterator<Function *> scc = scc_begin(&F);
+      scc != scc_end(&F);
+      ++scc)
+  {
+    const std::vector<BasicBlock *> &SCC = *scc;
+    LNF.emplace_back(SCC, LI);
+
+    DEBUG(
+      errs() << "Loop nesting tree: "
+             << LNF.back().size() << " node(s), loop-nesting depth: "
+             << LNF.back().depth() << "\n";
+      LNF.back().print(errs());
+      errs() << "\n"
+    );
+  }
+}
+
+void LiveValues::propagateValues(const LoopNestingTree &loopNest,
+                                 LiveVals &liveIn,
+                                 LiveVals &liveOut)
+{
+  std::set<const Value *> liveLoop, phiDefined;
+
+  /* Iterate over all loop nodes. */
+  for(LoopNestingTree::loop_iterator loop = loopNest.loop_begin();
+      loop != loopNest.loop_end();
+      loop++)
+  {
+    /* Calculate LiveLoop (lines 3 & 4 of Algorithm 3). */
+    phiDefs(*loop, phiDefined);
+    for(std::set<const Value *>::const_iterator it = liveIn[*loop].begin();
+        it != liveIn[*loop].end();
+        it++)
+      if(!phiDefined.count(*it) && includeVal(*it))
+        liveLoop.insert(*it);
+
+    /* Propagate values to children (lines 5-8 of Algorithm 3). */
+    for(LoopNestingTree::child_iterator child = loopNest.children_begin(loop);
+        child != loopNest.children_end(loop);
+        child++) {
+      for(std::set<const Value *>::const_iterator it = liveLoop.begin();
+          it != liveLoop.end();
+          it++) {
+        liveIn[*child].insert(*it);
+        liveOut[*child].insert(*it);
+      }
+    }
+
+    liveLoop.clear();
+  }
+}
+
+void LiveValues::loopTreeDFS(LoopNestingForest &LNF,
+                             LiveVals &liveIn,
+                             LiveVals &liveOut)
+{
+  LoopNestingForest::const_iterator it;
+  for(it = LNF.begin(); it != LNF.end(); it++)
+    propagateValues(*it, liveIn, liveOut);
+}
+
Index: lib/Analysis/LoopNestingTree.cpp
===================================================================
--- lib/Analysis/LoopNestingTree.cpp	(nonexistent)
+++ lib/Analysis/LoopNestingTree.cpp	(working copy)
@@ -0,0 +1,148 @@
+#include "llvm/Analysis/LoopNestingTree.h"
+#include "llvm/Support/raw_ostream.h"
+
+using namespace llvm;
+
+///////////////////////////////////////////////////////////////////////////////
+// Public API
+///////////////////////////////////////////////////////////////////////////////
+
+LoopNestingTree::LoopNestingTree(const std::vector<BasicBlock *> &SCC,
+                                 const LoopInfo &LI)
+  : _size(1), _depth(1), _root(nullptr)
+{
+  unsigned depth = 1, nodeDepth;
+  const Loop *loop = nullptr;
+  Node *loopHeader = nullptr, *newHeader = nullptr;
+  std::list<Node *> work;
+
+  /* Bootstrap by grabbing the loop of the first basic block encountered. */
+  loop = LI[SCC.front()];
+  if(!loop) // Is the SCC actually a loop?
+  {
+    _root = new Node(SCC.front(), nullptr, true);
+    return;
+  }
+
+  /* Get header of outermost loop, the tree's root. */
+  while(loop->getLoopDepth() > 1)
+    loop = loop->getParentLoop();
+  _root = new Node(loop->getHeader(), nullptr, true);
+  work.push_back(_root);
+
+  /* Parse the loop-headers of the SCC into the tree. */
+  while(work.size())
+  {
+    loopHeader = work.front();
+    work.pop_front();
+    loop = LI[loopHeader->bb];
+    depth = LI.getLoopDepth(loopHeader->bb);
+    _depth = (depth > _depth ? depth : _depth);
+
+    /* Add children of the loop header. */
+    for(auto bbi = loop->block_begin()++; bbi != loop->block_end(); bbi++)
+    {
+      nodeDepth = LI.getLoopDepth(*bbi);
+      if(nodeDepth == depth) // Regular child node
+      {
+        loopHeader->addChild(new Node(*bbi, loopHeader, false));
+        _size++;
+      }
+      else if(nodeDepth == (depth + 1) && // Header of nested loop
+              LI.isLoopHeader(*bbi))
+      {
+        newHeader = new Node(*bbi, loopHeader, true);
+        loopHeader->addChild(newHeader);
+        work.push_back(newHeader);
+        _size++;
+      }
+    }
+  }
+}
+
+LoopNestingTree::loop_iterator LoopNestingTree::loop_iterator::operator++()
+{
+  loop_iterator me = *this;
+  if(remaining.size())
+  {
+    cur = remaining.front();
+    remaining.pop();
+    addLoopHeaders();
+  }
+  else cur = nullptr;
+  return me;
+}
+
+LoopNestingTree::loop_iterator
+LoopNestingTree::loop_iterator::operator++(int junk)
+{
+  if(remaining.size())
+  {
+    cur = remaining.front();
+    remaining.pop();
+    addLoopHeaders();
+  }
+  else cur = nullptr;
+  return *this;
+}
+
+LoopNestingTree::child_iterator::child_iterator(loop_iterator &parent,
+                                                enum location loc)
+{
+  if(loc == BEGIN) it = parent.cur->children.begin();
+  else it = parent.cur->children.end();
+}
+
+///////////////////////////////////////////////////////////////////////////////
+// Private API
+///////////////////////////////////////////////////////////////////////////////
+
+void LoopNestingTree::print(raw_ostream &O, Node *node, unsigned depth) const
+{
+  for(unsigned i = 0; i < depth; i++) O << " ";
+  node->bb->printAsOperand(O, false);
+  O << "\n";
+  if(node->children.size())
+  {
+    for(unsigned i = 0; i < depth; i++) O << " ";
+    O << "\\\n";
+
+    for(std::list<Node *>::const_iterator it = node->children.begin();
+        it != node->children.end();
+        it++)
+    {
+      if((*it)->isLoopHeader) print(O, (*it), depth + 1);
+      else
+      {
+        for(unsigned i = 0; i < depth + 1; i++) O << " ";
+        (*it)->bb->printAsOperand(O, false);
+        O << "\n";
+      }
+    }
+  }
+}
+
+void LoopNestingTree::deleteRecursive(Node *node)
+{
+  for(std::list<Node *>::iterator it = node->children.begin();
+      it != node->children.end();
+      it++)
+  {
+    if((*it)->isLoopHeader) deleteRecursive(*it);
+    else delete *it;
+  }
+  delete node;
+}
+
+void LoopNestingTree::loop_iterator::addLoopHeaders()
+{
+  if(cur != nullptr)
+  {
+    for(std::list<Node *>::const_iterator it = cur->children.begin();
+        it != cur->children.end();
+        it++)
+      if((*it)->isLoopHeader)
+        remaining.push(*it);
+  }
+}
+
Index: lib/Analysis/LoopPaths.cpp
===================================================================
--- lib/Analysis/LoopPaths.cpp	(nonexistent)
+++ lib/Analysis/LoopPaths.cpp	(working copy)
@@ -0,0 +1,550 @@
+//===- LoopPaths.h - Enumerate paths in loops -------------------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements analysis which enumerates paths in loops.  In
+// particular, this pass calculates all paths in loops which are of the
+// following form:
+//
+//  - Header to backedge block, with no equivalence points on the path
+//  - Header to block with equivalence point
+//  - Block with equivalence point to block with equivalence point
+//  - Block with equivalence point to backedge block
+//
+// Note that backedge blocks may or may not also be exit blocks.
+//
+//===----------------------------------------------------------------------===//
+
+#include <queue>
+#include "llvm/Analysis/LoopPaths.h"
+#include "llvm/Analysis/PopcornUtil.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "looppaths"
+
+const static cl::opt<unsigned>
+MaxNumPaths("max-num-paths", cl::Hidden, cl::init(10000),
+  cl::desc("Max number of paths to analyze"));
+
+void LoopPathUtilities::populateLoopNest(Loop *L, LoopNest &Nest) {
+  std::queue<Loop *> ToVisit;
+  Nest.clear();
+  Nest.insert(L);
+  ToVisit.push(L);
+
+  while(ToVisit.size()) {
+    const Loop *Sub = ToVisit.front();
+    ToVisit.pop();
+    for(auto L : Sub->getSubLoops()) {
+      Nest.insert(L);
+      ToVisit.push(L);
+    }
+  }
+}
+
+void LoopPathUtilities::getSubBlocks(Loop *L, BlockSet &SubBlocks) {
+  SubBlocks.clear();
+  LoopNest Nest;
+
+  for(auto Sub : L->getSubLoops()) {
+    populateLoopNest(Sub, Nest);
+    for(auto Nested : Nest)
+      for(auto BB : Nested->getBlocks())
+        SubBlocks.insert(BB);
+  }
+}
+
+LoopPath::LoopPath(const std::vector<PathNode> &NodeVector,
+                   const Instruction *Start, const Instruction *End,
+                   bool StartsAtHeader, bool EndsAtBackedge)
+  : Start(Start), End(End), StartsAtHeader(StartsAtHeader),
+    EndsAtBackedge(EndsAtBackedge) {
+  assert(NodeVector.size() && "Trivial path");
+  assert(Start && Start->getParent() == NodeVector.front().getBlock() &&
+         "Invalid starting instruction");
+  assert(End && End->getParent() == NodeVector.back().getBlock() &&
+         "Invalid ending instruction");
+
+  for(auto Node : NodeVector) Nodes.insert(Node);
+}
+
+std::string LoopPath::toString() const {
+  std::string buf = "Path with " + std::to_string(Nodes.size()) + " nodes(s)\n";
+
+  buf += "  Start: ";
+  if(Start->hasName()) buf += Start->getName();
+  else buf += "<unnamed instruction>";
+  buf += "\n";
+
+  buf += "  End: ";
+  if(End->hasName()) buf += End->getName();
+  else buf += "<unnamed instruction>";
+  buf += "\n";
+
+  buf += "  Nodes:\n";
+  for(auto Node : Nodes) {
+    buf += "    ";
+    const BasicBlock *BB = Node.getBlock();
+    if(BB->hasName()) buf += BB->getName();
+    else buf += "<unnamed block>";
+    if(Node.isSubLoopExit()) buf += " (sub-loop exit)";
+    buf += "\n";
+  }
+
+  return buf;
+}
+
+void LoopPath::print(raw_ostream &O) const {
+  O << "    Path with " << std::to_string(Nodes.size()) << " nodes(s)\n";
+  O << "    Start:"; Start->print(O); O << "\n";
+  O << "    End:"; End->print(O); O << "\n";
+  O << "    Nodes:\n";
+  for(auto Node : Nodes) {
+    const BasicBlock *BB = Node.getBlock();
+    if(BB->hasName()) O << "      " << BB->getName();
+    else O << "      <unnamed block>";
+    if(Node.isSubLoopExit()) O << " (sub-loop exit)";
+    O << "\n";
+  }
+}
+
+void EnumerateLoopPaths::getAnalysisUsage(AnalysisUsage &AU) const {
+  AU.addRequired<LoopInfoWrapperPass>();
+  AU.setPreservesAll();
+}
+
+/// Search over the instructions in a basic block (starting at I) for
+/// equivalence points.  Return an equivalence point if found, or null
+/// otherwise.
+static const Instruction *hasEquivalencePoint(const Instruction *I) {
+  if(!I) return nullptr;
+  for(BasicBlock::const_iterator it(I), e = I->getParent()->end();
+      it != e; ++it)
+    if(Popcorn::isEquivalencePoint(it)) return it;
+  return nullptr;
+}
+
+/// Add a value to a list if it's not already contained in the list.  This is
+/// used to unique new instructions at which to start searches, as multiple
+/// paths may end at the same equivalence point (but we don't need to search
+/// it multiple times).
+static inline void pushIfNotPresent(const Instruction *I,
+                                    std::list<const Instruction *> &List) {
+  if(std::find(List.begin(), List.end(), I) == List.end()) List.push_back(I);
+}
+
+/// Return whether the current path contains a basic block.
+static inline bool pathContains(const std::vector<PathNode> &Path,
+                                const BasicBlock *BB) {
+  for(auto &Node : Path)
+    if(Node.getBlock() == BB) return true;
+  return false;
+}
+
+// TODO this needs to be converted to iteration rather than recursion
+
+void
+EnumerateLoopPaths::getSubLoopSuccessors(const BasicBlock *Successor,
+                                   std::vector<const Instruction *> &EqPoint,
+                                   std::vector<const Instruction *> &Spanning) {
+  const Instruction *Term;
+  const Loop *SubLoop;
+  SmallVector<BasicBlock *, 4> ExitBlocks;
+
+  EqPoint.clear();
+  Spanning.clear();
+  assert(CurLoop->contains(Successor) && SubLoopBlocks.count(Successor) &&
+         "Invalid sub-loop block");
+
+  SubLoop = LI->getLoopFor(Successor);
+  SubLoop->getExitingBlocks(ExitBlocks);
+  for(auto Exit : ExitBlocks) {
+    Term = Exit->getTerminator();
+    if(HasSpPath[SubLoop][Exit]) Spanning.push_back(Term);
+    if(HasEqPointPath[SubLoop][Exit]) EqPoint.push_back(Term);
+  }
+}
+
+static inline void printNewPath(raw_ostream &O, const LoopPath &Path) {
+  O << "Found path that start at ";
+  if(Path.startsAtHeader()) O << "the header";
+  else O << "an equivalence point";
+  O << " and ends at ";
+  if(Path.endsAtBackedge()) O << "a loop backedge";
+  else O << "an equivalence point";
+  Path.print(O);
+}
+
+bool EnumerateLoopPaths::loopDFS(const Instruction *I,
+                                 LoopDFSInfo &DFSI,
+                                 std::vector<LoopPath> &CurPaths,
+                                 std::list<const Instruction *> &NewPaths) {
+  const Instruction *EqPoint;
+  const BasicBlock *BB = I->getParent(), *PathBlock;
+  std::vector<const Instruction *> EqPointInsts, SpanningInsts;
+
+  if(!SubLoopBlocks.count(BB)) {
+    if(pathContains(DFSI.PathNodes, BB)) {
+      DetectedCycle = true;
+      return false;
+    }
+
+    DFSI.PathNodes.emplace_back(BB, false);
+
+    if((EqPoint = hasEquivalencePoint(I))) {
+      CurPaths.emplace_back(DFSI.PathNodes, DFSI.Start, EqPoint,
+                            DFSI.StartsAtHeader, false);
+
+      if(CurPaths.size() > MaxNumPaths) {
+        TooManyPaths = true;
+        return false;
+      }
+
+      for(auto Node : DFSI.PathNodes) {
+        PathBlock = Node.getBlock();
+        if(!SubLoopBlocks.count(PathBlock))
+          HasEqPointPath[CurLoop][PathBlock] = true;
+      }
+
+      // Add instruction after equivalence point (or at start of successor
+      // basic blocks if EqPoint is the last instruction in its block) as start
+      // of new equivalence point path to be searched.
+      if(!EqPoint->isTerminator())
+        pushIfNotPresent(EqPoint->getNextNode(), NewPaths);
+      else {
+        for(auto Succ : successors(BB)) {
+          if(!CurLoop->contains(Succ) || // Skip exit blocks & latches
+             Succ == CurLoop->getHeader()) continue;
+          else if(!SubLoopBlocks.count(Succ)) // Successor is in same outer loop
+            pushIfNotPresent(&Succ->front(), NewPaths);
+          else { // Successor is in sub-loop
+            getSubLoopSuccessors(Succ, EqPointInsts, SpanningInsts);
+            for(auto SLE : EqPointInsts) pushIfNotPresent(SLE, NewPaths);
+            for(auto SLE : SpanningInsts)
+              if(!loopDFS(SLE, DFSI, CurPaths, NewPaths)) return false;
+          }
+        }
+      }
+
+      DEBUG(printNewPath(dbgs(), CurPaths.back()));
+    }
+    else if(Latches.count(BB)) {
+      CurPaths.emplace_back(DFSI.PathNodes, DFSI.Start, BB->getTerminator(),
+                            DFSI.StartsAtHeader, true);
+
+      if(CurPaths.size() > MaxNumPaths) {
+        TooManyPaths = true;
+        return false;
+      }
+
+      if(DFSI.StartsAtHeader) {
+        for(auto Node : DFSI.PathNodes) {
+          PathBlock = Node.getBlock();
+          if(!SubLoopBlocks.count(PathBlock))
+            HasSpPath[CurLoop][PathBlock] = true;
+        }
+      }
+      else {
+        for(auto Node : DFSI.PathNodes) {
+          PathBlock = Node.getBlock();
+          if(!SubLoopBlocks.count(PathBlock))
+            HasEqPointPath[CurLoop][PathBlock] = true;
+        }
+      }
+
+      DEBUG(printNewPath(dbgs(), CurPaths.back()));
+    }
+    else {
+      for(auto Succ : successors(BB)) {
+        if(!CurLoop->contains(Succ)) continue;
+        else if(!SubLoopBlocks.count(Succ)) {
+          if(!loopDFS(&Succ->front(), DFSI, CurPaths, NewPaths))
+            return false;
+        }
+        else {
+          getSubLoopSuccessors(Succ, EqPointInsts, SpanningInsts);
+          for(auto SLE : EqPointInsts) {
+            // Rather than stopping the path at the equivalence point inside
+            // of a sub-loop, stop it at the end of the current block
+            // TODO this can create duplicates for a path that reaches a
+            // sub-loop with multiple exiting blocks, but the analysis in
+            // MigrationPoints doesn't care about paths that don't end at a
+            // backedge anyway
+            CurPaths.emplace_back(DFSI.PathNodes, DFSI.Start,
+                                  BB->getTerminator(),
+                                  DFSI.StartsAtHeader, false);
+
+            if(CurPaths.size() > MaxNumPaths) {
+              TooManyPaths = true;
+              return false;
+            }
+
+            pushIfNotPresent(SLE, NewPaths);
+            DEBUG(printNewPath(dbgs(), CurPaths.back()));
+          }
+          for(auto SLE : SpanningInsts)
+            if(!loopDFS(SLE, DFSI, CurPaths, NewPaths)) return false;
+        }
+      }
+    }
+    DFSI.PathNodes.pop_back();
+  }
+  else {
+    if(pathContains(DFSI.PathNodes, BB)) {
+      DetectedCycle = true;
+      return false;
+    }
+
+    // This is a sub-loop block; we only want to explore successors who are not
+    // contained in this sub-loop but are still contained in the current loop.
+    DFSI.PathNodes.emplace_back(BB, true);
+
+    const Loop *WeedOutLoop = LI->getLoopFor(BB);
+    for(auto Succ : successors(BB)) {
+      if(WeedOutLoop->contains(Succ) || !CurLoop->contains(Succ)) continue;
+      else if(!SubLoopBlocks.count(Succ)) {
+        if(!loopDFS(&Succ->front(), DFSI, CurPaths, NewPaths))
+          return false;
+      }
+      else {
+        getSubLoopSuccessors(Succ, EqPointInsts, SpanningInsts);
+        for(auto SLE : EqPointInsts) {
+          // TODO this can create duplicates for a path that reaches a sub-loop
+          // with multiple exiting blocks, but the analysis in MigrationPoints
+          // doesn't care about paths that don't end at a backedge anyway
+          CurPaths.emplace_back(DFSI.PathNodes, DFSI.Start,
+                                BB->getTerminator(),
+                                DFSI.StartsAtHeader, false);
+
+          if(CurPaths.size() > MaxNumPaths) {
+            TooManyPaths = true;
+            return false;
+          }
+
+          DEBUG(printNewPath(dbgs(), CurPaths.back()));
+          pushIfNotPresent(SLE, NewPaths);
+        }
+        for(auto SLE: SpanningInsts)
+          if(!loopDFS(SLE, DFSI, CurPaths, NewPaths)) return false;
+      }
+    }
+    DFSI.PathNodes.pop_back();
+  }
+
+  return true;
+}
+
+bool EnumerateLoopPaths::analyzeLoop(Loop *L, std::vector<LoopPath> &CurPaths) {
+  std::list<const Instruction *> NewPaths;
+  SmallVector<BasicBlock *, 4> LatchVec;
+  LoopDFSInfo DFSI;
+
+  CurPaths.clear();
+  HasSpPath[L].clear();
+  HasEqPointPath[L].clear();
+
+  DEBUG(
+    DebugLoc DL(L->getStartLoc());
+    dbgs() << "Enumerating paths";
+    if(DL) {
+      dbgs() << " for loop at ";
+      DL.print(dbgs());
+    }
+    dbgs() << ": "; L->dump();
+  );
+
+  // Store information about the current loop, it's backedges, and sub-loops
+  CurLoop = L;
+  Latches.clear();
+  L->getLoopLatches(LatchVec);
+  for(auto L : LatchVec) Latches.insert(L);
+  LoopPathUtilities::getSubBlocks(L, SubLoopBlocks);
+
+  assert(Latches.size() && "No backedges, not a loop?");
+  assert(!SubLoopBlocks.count(L->getHeader()) && "Header is in sub-loop?");
+
+  DFSI.Start = &L->getHeader()->front();
+  DFSI.StartsAtHeader = true;
+  if(!loopDFS(DFSI.Start, DFSI, CurPaths, NewPaths)) return false;
+  assert(DFSI.PathNodes.size() == 0 && "Invalid traversal");
+
+  DFSI.StartsAtHeader = false;
+  while(!NewPaths.empty()) {
+    DFSI.Start = NewPaths.front();
+    NewPaths.pop_front();
+    if(!loopDFS(DFSI.Start, DFSI, CurPaths, NewPaths)) return false;
+    assert(DFSI.PathNodes.size() == 0 && "Invalid traversal");
+  }
+
+  return true;
+}
+
+bool EnumerateLoopPaths::runOnFunction(Function &F) {
+  DEBUG(dbgs() << "\n********** ENUMERATE LOOP PATHS **********\n"
+               << "********** Function: " << F.getName() << "\n\n");
+
+  reset();
+  TooManyPaths = DetectedCycle = false;
+  LI = &getAnalysis<LoopInfoWrapperPass>().getLoopInfo();
+  std::vector<LoopNest> Nests;
+
+  // Discover all loop nests.
+  for(LoopInfo::iterator I = LI->begin(), E = LI->end(); I != E; ++I) {
+    if((*I)->getLoopDepth() != 1) continue;
+    Nests.push_back(LoopNest());
+    LoopPathUtilities::populateLoopNest(*I, Nests.back());
+  }
+
+  // Search all loops within all loop nests.
+  for(auto Nest : Nests) {
+    DEBUG(dbgs() << "Analyzing nest with " << std::to_string(Nest.size())
+                 << " loops\n");
+
+    for(auto L : Nest) {
+      std::vector<LoopPath> &CurPaths = Paths[L];
+      assert(CurPaths.size() == 0 && "Re-processing loop?");
+      if(!analyzeLoop(L, CurPaths)) break;
+    }
+
+    if(analysisFailed()) break;
+  }
+
+  if(TooManyPaths) {
+    DEBUG(dbgs() << "WARNING: too many paths, bailing on analysis\n");
+    reset();
+  }
+
+  if(DetectedCycle) {
+    DEBUG(dbgs() << "WARNING: detected a cycle, bailing on analysis\n");
+    reset();
+  }
+
+  return false;
+}
+
+void EnumerateLoopPaths::rerunOnLoop(Loop *L) {
+  // We *should* be analyzing a loop for the 2+ time
+  std::vector<LoopPath> &CurPaths = Paths[L];
+  DEBUG(if(!CurPaths.size()) dbgs() << "  -> No previous analysis?\n");
+  if(!analyzeLoop(L, CurPaths)) reset();
+}
+
+void EnumerateLoopPaths::getPaths(const Loop *L,
+                                  std::vector<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second) P.push_back(&Path);
+}
+
+void
+EnumerateLoopPaths::getBackedgePaths(const Loop *L,
+                                     std::vector<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.endsAtBackedge()) P.push_back(&Path);
+}
+
+void
+EnumerateLoopPaths::getBackedgePaths(const Loop *L,
+                                     std::set<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.endsAtBackedge()) P.insert(&Path);
+}
+
+void
+EnumerateLoopPaths::getSpanningPaths(const Loop *L,
+                                     std::vector<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.isSpanningPath()) P.push_back(&Path);
+}
+
+void
+EnumerateLoopPaths::getSpanningPaths(const Loop *L,
+                                     std::set<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.isSpanningPath()) P.insert(&Path);
+}
+
+void
+EnumerateLoopPaths::getEqPointPaths(const Loop *L,
+                                    std::vector<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.isEqPointPath()) P.push_back(&Path);
+}
+
+void
+EnumerateLoopPaths::getEqPointPaths(const Loop *L,
+                                    std::set<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.isEqPointPath()) P.insert(&Path);
+}
+
+void
+EnumerateLoopPaths::getPathsThroughBlock(const Loop *L, BasicBlock *BB,
+                                         std::vector<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  assert(L->contains(BB) && "Loop does not contain basic block");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.contains(BB)) P.push_back(&Path);
+}
+
+void
+EnumerateLoopPaths::getPathsThroughBlock(const Loop *L, BasicBlock *BB,
+                                         std::set<const LoopPath *> &P) const {
+  assert(hasPaths(L) && "No paths for loop");
+  assert(L->contains(BB) && "Loop does not contain basic block");
+  P.clear();
+  for(const LoopPath &Path : Paths.find(L)->second)
+    if(Path.contains(BB)) P.insert(&Path);
+}
+
+bool EnumerateLoopPaths::spanningPathThroughBlock(const Loop *L,
+                                                  const BasicBlock *BB) const {
+  assert(hasPaths(L) && "No paths for loop");
+  assert(L->contains(BB) && "Loop does not contain basic block");
+  return HasSpPath.find(L)->second.find(BB)->second;
+}
+
+bool EnumerateLoopPaths::eqPointPathThroughBlock(const Loop *L,
+                                                 const BasicBlock *BB) const {
+  assert(hasPaths(L) && "No paths for loop");
+  assert(L->contains(BB) && "Loop does not contain basic block");
+  return HasEqPointPath.find(L)->second.find(BB)->second;
+}
+
+char EnumerateLoopPaths::ID = 0;
+INITIALIZE_PASS_BEGIN(EnumerateLoopPaths, "looppaths",
+                      "Enumerate paths in loops",
+                      false, true)
+INITIALIZE_PASS_DEPENDENCY(LoopInfoWrapperPass)
+INITIALIZE_PASS_END(EnumerateLoopPaths, "looppaths",
+                    "Enumerate paths in loops",
+                    false, true)
+
+
+namespace llvm {
+  FunctionPass *createEnumerateLoopPathsPass()
+  { return new EnumerateLoopPaths(); }
+}
+
Index: lib/Analysis/PopcornCompatibility.cpp
===================================================================
--- lib/Analysis/PopcornCompatibility.cpp	(nonexistent)
+++ lib/Analysis/PopcornCompatibility.cpp	(working copy)
@@ -0,0 +1,142 @@
+//===- PopcornCompatibility.cpp -------------------------------------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Looks for code features which are not currently handled by the Popcorn
+// compiler/stack transformation process.  These code features either *might*
+// cause issues during stack transformation (and hence the compiler will issue
+// a warning), or are guaranteed to not be handled correctly and will cause
+// compilation to abort.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Pass.h"
+#include "llvm/IR/CallSite.h"
+#include "llvm/IR/DiagnosticInfo.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/LLVMContext.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "popcorn-compat"
+
+namespace {
+
+class PopcornCompatibility : public FunctionPass
+{
+public:
+  static char ID;
+
+  PopcornCompatibility() : FunctionPass(ID) {
+    initializePopcornCompatibilityPass(*PassRegistry::getPassRegistry());
+  }
+  ~PopcornCompatibility() {}
+
+  virtual const char *getPassName() const
+  { return "Popcorn compatibility checking"; }
+
+  //===--------------------------------------------------------------------===//
+  // Warning & error printing
+  //===--------------------------------------------------------------------===//
+
+  /// Emit a warning message for a given location, denoted by an instruction.
+  static void warn(const Instruction *I, const std::string &Msg) {
+    const Function *F = I->getParent()->getParent();
+    std::string Warning("Popcorn compatibility");
+    if(F->hasName()) {
+      Warning += " in function '";
+      Warning += F->getName();
+      Warning += "'";
+    }
+    Warning += ": " + Msg;
+    DiagnosticInfoOptimizationFailure DI(*F, I->getDebugLoc(), Warning);
+    I->getContext().diagnose(DI);
+  }
+
+  /// Emit a warning message for a function.
+  static void warn(const Function &F, const std::string &Msg)
+  { warn(F.getEntryBlock().begin(), Msg); }
+
+  /// Emit an error message for a given location, denoted by an instruction.
+  static void error(const Instruction *I, const std::string &Msg) {
+    const Function *F = I->getParent()->getParent();
+    std::string Error("Popcorn compatibility");
+    if(F->hasName()) {
+      Error += " in function '";
+      Error += F->getName();
+      Error += "'";
+    }
+    Error += ": " + Msg;
+    DiagnosticInfoOptimizationError DI(*F, I->getDebugLoc(), Error);
+    I->getContext().diagnose(DI);
+  }
+
+  //===--------------------------------------------------------------------===//
+  // Properties of instructions
+  //===--------------------------------------------------------------------===//
+
+  /// Return whether the alloca is dynamically-sized.
+  static bool isVariableSizedAlloca(const Instruction &I) {
+    const AllocaInst *AI;
+    if((AI = dyn_cast<AllocaInst>(&I)) && !AI->isStaticAlloca()) return true;
+    else return false;
+  }
+
+  static bool isInlineAsm(const Instruction &I) {
+    if((isa<CallInst>(I) || isa<InvokeInst>(I)) && !isa<IntrinsicInst>(I)) {
+      ImmutableCallSite CS(&I);
+      if(CS.isInlineAsm()) return true;
+    }
+    return false;
+  }
+
+  //===--------------------------------------------------------------------===//
+  // The main show
+  //===--------------------------------------------------------------------===//
+
+  virtual bool runOnFunction(Function &F) {
+    std::string Msg;
+
+    if(!F.isDeclaration() && !F.isIntrinsic()) {
+      if(F.isVarArg()) warn(F, "function takes a variable number of arguments");
+      for(auto &BB : F) {
+        for(auto &I : BB) {
+          if(isVariableSizedAlloca(I)) {
+            Msg = "stack variable '";
+            Msg += I.getName();
+            Msg += "' is dynamically sized (will cause "
+                   "issues during code generation)";
+            error(&I, Msg);
+          }
+
+          if(isInlineAsm(I))
+            warn(&I, "inline assembly may have unanalyzable side-effects");
+
+          if(isa<VAArgInst>(I) || isa<VACopyInst>(I) || isa<VAEndInst>(I))
+            warn(&I, "va_arg not transformable across architectures");
+        }
+      }
+    }
+    return false;
+  }
+
+private:
+};
+
+} /* end anonymous namespace */
+
+char PopcornCompatibility::ID = 0;
+
+INITIALIZE_PASS(PopcornCompatibility, "popcorn-compat",
+                "Analyze code for compatibility issues", false, true)
+
+namespace llvm {
+  FunctionPass *createPopcornCompatibilityPass()
+  { return new PopcornCompatibility(); }
+}
+
Index: lib/Analysis/SelectMigrationPoints.cpp
===================================================================
--- lib/Analysis/SelectMigrationPoints.cpp	(nonexistent)
+++ lib/Analysis/SelectMigrationPoints.cpp	(working copy)
@@ -0,0 +1,1837 @@
+//===- SelectMigrationPoints.cpp ------------------------------------------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Select code locations to instrument with migration points, which are
+// locations where threads make calls to invoke the migration process in
+// addition to any other instrumentation (e.g., hardware transactional memory,
+// HTM, stops & starts).  Migration points only occur at equivalence points, or
+// locations in the program code where there is a direct mapping between
+// architecture-specific execution state, like registers and stack, across
+// different ISAs.  In our implementation, every function call site is an
+// equivalence point; hence, calls inserted to invoke the migration by
+// definition create equivalence points at the migration point.  Thus, all
+// migration points are equivalence points, but not all equivalence points are
+// migration points.
+//
+// By default, the pass only inserts migration points at the beginning and end
+// of a function.  More advanced analyses can be used to instrument function
+// bodies (in particular, loops) with more migration points and HTM execution.
+//
+// More details about equivalence points can be found in the paper "A Unified
+// Model of Pointwise Migration of Procedural Computations" by von Bank et. al
+// (http://dl.acm.org/citation.cfm?id=197402).
+//
+//===----------------------------------------------------------------------===//
+
+#include <cmath>
+#include <map>
+#include <memory>
+#include "llvm/Pass.h"
+#include "llvm/ADT/PostOrderIterator.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/StringSet.h"
+#include "llvm/ADT/SCCIterator.h"
+#include "llvm/Analysis/LoopInfo.h"
+#include "llvm/Analysis/LoopIterator.h"
+#include "llvm/Analysis/LoopPaths.h"
+#include "llvm/Analysis/PopcornUtil.h"
+#include "llvm/Analysis/ScalarEvolution.h"
+#include "llvm/Analysis/ScalarEvolutionExpressions.h"
+#include "llvm/IR/CallSite.h"
+#include "llvm/IR/DiagnosticInfo.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/Intrinsics.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/Module.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_os_ostream.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "migration-points"
+
+/// Insert more migration points into the body of a function.  Analyze
+/// execution behavior & attempt to instrument the code to reduce the time
+/// until the thread reaches a migration point.
+const static cl::opt<bool>
+MoreMigPoints("more-mig-points", cl::Hidden, cl::init(false),
+  cl::desc("Add additional migration points into the body of functions"));
+
+/// By default we assume that loops will execute "enough iterations as to
+/// require instrumentation".  That's not necessarily true, so contrain N in
+/// hitting migration point every N iterations.  If analysis determines that
+/// we need to hit analysis for some number larger than N, don't instrument
+/// the loop.
+const static cl::opt<unsigned>
+MaxItersPerMigPoint("max-iters-per-migpoint", cl::Hidden, cl::init(UINT32_MAX),
+  cl::desc("Max iterations per migration point"));
+
+/// Percent of capacity (determined by analysis type, e.g., HTM buffer size) at
+/// which point weight objects will request a new migration point be inserted.
+const static cl::opt<unsigned>
+CapacityThreshold("cap-threshold", cl::Hidden, cl::init(80),
+  cl::desc("Percent of capacity at which point a new migration point should "
+           "be inserted (only applies to -more-mig-points)"));
+
+/// Per-function capacity threshold.
+const static cl::list<std::string>
+FuncCapThreshold("func-cap", cl::Hidden, cl::ZeroOrMore,
+  cl::desc("Function-specific capacity threshold in function,value pairs"));
+
+/// Normally we instrument function entry points with migration points.  If
+/// we're below some percent of capacity at all exit points & we haven't added
+/// instrumentation into the body (i.e., nothing depends on a clean slate to
+/// start), skip this instrumentation.
+const static cl::opt<unsigned>
+StartThreshold("start-threshold", cl::Hidden, cl::init(5),
+  cl::desc("Don't instrument function entry points under a percent of "
+           "capacity (only applies to -more-mig-points), used for "
+           "small functions"));
+
+/// Per-function starting threshold.
+const static cl::list<std::string>
+FuncStartThreshold("func-start", cl::Hidden, cl::ZeroOrMore,
+  cl::desc("Function-specific start threshold in function,value pairs"));
+
+/// Normally we instrument function exit points with migration points.  If
+/// we're below some percent of capacity, skip this instrumentation (useful for
+/// very small/short-lived functions).
+const static cl::opt<unsigned>
+RetThreshold("ret-threshold", cl::Hidden, cl::init(5),
+  cl::desc("Don't instrument function exit points under a percent of "
+           "capacity (only applies to -more-mig-points)"));
+
+/// Per-function return threshold.
+const static cl::list<std::string>
+FuncRetThreshold("func-ret", cl::Hidden, cl::ZeroOrMore,
+  cl::desc("Function-specific return threshold in function,value pairs"));
+
+/// Don't instrument a specific function with extra migration points.
+const static cl::list<std::string>
+FuncNoInst("func-no-inst", cl::Hidden, cl::ZeroOrMore,
+  cl::desc("Don't instrument a particular function with migration points"));
+
+/// Target cycles between migration points when instrumenting applications with
+/// more migration points (but without HTM).  Allows tuning trade off between
+/// migration point response time and overhead.
+const static cl::opt<unsigned>
+MillionCyclesBetweenMigPoints("migpoint-cycles", cl::Hidden, cl::init(50),
+  cl::desc("Cycles between migration points, in millions of cycles"));
+
+/// Cover the application in transactional execution by inserting HTM
+/// stop/start instructions at migration points.  Tailors the analysis to
+/// reduce capacity aborts by estimating memory access behavior.
+const static cl::opt<bool>
+HTMExec("htm-execution", cl::NotHidden, cl::init(false),
+  cl::desc("Instrument migration points with HTM execution "
+           "(only supported on PowerPC 64-bit & x86-64)"));
+
+/// Disable wrapping mem<set, copy, move> instructions for which we don't know
+/// the size.
+const static cl::opt<bool>
+NoWrapUnknownMem("htm-no-wrap-unknown-mem", cl::Hidden, cl::init(false),
+  cl::desc("Disable wrapping mem<set, copy, move> of unknown size with HTM"));
+
+/// Disable wrapping libc functions which are likely to cause HTM aborts with
+/// HTM stop/start intrinsics.  Wrapping happens by default with HTM execution.
+const static cl::opt<bool>
+NoWrapLibc("htm-no-wrap-libc", cl::Hidden, cl::init(false),
+  cl::desc("Disable wrapping libc functions with HTM stop/start"));
+
+/// HTM memory read buffer size for tuning analysis when inserting additional
+/// migration points.
+const static cl::opt<unsigned>
+HTMReadBufSizeArg("htm-buf-read", cl::Hidden, cl::init(32),
+  cl::desc("HTM analysis tuning - HTM read buffer size, in kilobytes"),
+  cl::value_desc("size"));
+
+/// HTM memory write buffer size for tuning analysis when inserting additional
+/// migration points.
+const static cl::opt<unsigned>
+HTMWriteBufSizeArg("htm-buf-write", cl::Hidden, cl::init(8),
+  cl::desc("HTM analysis tuning - HTM write buffer size, in kilobytes"),
+  cl::value_desc("size"));
+
+#define KB 1024
+#define HTMReadBufSize (HTMReadBufSizeArg * KB)
+#define HTMWriteBufSize (HTMWriteBufSizeArg * KB)
+
+#define MILLION 1000000
+#define CyclesBetweenMigPoints \
+  ((unsigned long)MillionCyclesBetweenMigPoints * MILLION)
+#define MEM_WEIGHT 40
+
+STATISTIC(LoopsTransformed, "Number of loops transformed");
+STATISTIC(NumIVsAdded, "Number of induction variables added");
+
+namespace {
+
+/// Get the integer size of a value, if statically known.
+static int64_t getValueSize(const Value *V) {
+  if(isa<ConstantInt>(V)) return cast<ConstantInt>(V)->getSExtValue();
+  return -1;
+}
+
+/// Return a percentage of a value.
+static inline size_t getValuePercent(size_t V, unsigned P) {
+  assert(P <= 100 && "Invalid percentage");
+  return floor(((double)V) * (((double)P) / 100.0));
+}
+
+/// Return the number of cache lines accessed for a given number of
+/// (assumed contiguous) bytes.
+static inline size_t getNumCacheLines(size_t Bytes, unsigned LineSize) {
+  return ceil((double)Bytes / (double)LineSize);
+}
+
+/// Abstract weight metric.  Child classes implement for analyzing different
+/// resource capacities, e.g., HTM buffer sizes.
+class Weight {
+protected:
+  /// Number of times the weight was reset.
+  size_t Resets;
+
+  Weight() : Resets(0) {}
+  Weight(const Weight &C) : Resets(C.Resets) {}
+
+public:
+  virtual ~Weight() {};
+  virtual Weight *copy() const = 0;
+
+  /// Expose types of child implementations.
+  virtual bool isCycleWeight() const { return false; }
+  virtual bool isHTMWeight() const { return false; }
+
+  /// Analyze an instruction & update accounting.
+  virtual void analyze(const Instruction *I, const DataLayout *DL) = 0;
+
+  /// Return whether or not we should add a migration point.  This is tuned
+  /// based on the resource capacity and percentage threshold options.
+  virtual bool shouldAddMigPoint(unsigned percent) const {
+    return !underPercentOfThreshold(percent);
+  }
+
+  /// Reset the weight.
+  virtual void reset() { Resets++; }
+
+  /// Update this weight with the max of this weight and another.
+  virtual void max(const Weight *RHS) = 0;
+  virtual void max(const std::unique_ptr<Weight> &RHS) { max(RHS.get()); }
+
+  /// Multiply the weight by a factor, e.g., a number of loop iterations.
+  virtual void multiply(size_t factor) = 0;
+
+  /// Add another weight to this weight.
+  virtual void add(const Weight *RHS) = 0;
+  virtual void add(const std::unique_ptr<Weight> &RHS) { add(RHS.get()); }
+
+  /// Number of times this weight "fits" into the resource capacity before we
+  /// need to place a migration point.  This is used for calculating how many
+  /// iterations of a loop can be executed between migration points.
+  virtual size_t numIters(unsigned percent) const = 0;
+
+  /// Return whether or not the weight is within some percent (0-100) of the
+  /// resource capacity for a type of weight.
+  virtual bool underPercentOfThreshold(unsigned percent) const = 0;
+
+  /// Return a human-readable string describing weight information.
+  virtual std::string toString() const = 0;
+};
+
+typedef std::unique_ptr<Weight> WeightPtr;
+
+/// Weight metrics for HTM analysis, which basically depend on the number
+/// of bytes loaded & stored.
+class HTMWeight : public Weight {
+private:
+  // The number of bytes loaded & stored, respectively.
+  size_t LoadBytes, StoreBytes;
+
+  // Statistics about when the weight was reset (i.e., at HTM stop/starts).
+  size_t ResetLoad, ResetStore;
+
+public:
+  HTMWeight(size_t LoadBytes = 0, size_t StoreBytes = 0)
+    : LoadBytes(LoadBytes), StoreBytes(StoreBytes), ResetLoad(0),
+      ResetStore(0) {}
+  HTMWeight(const HTMWeight &C)
+    : Weight(C), LoadBytes(C.LoadBytes), StoreBytes(C.StoreBytes),
+      ResetLoad(C.ResetLoad), ResetStore(C.ResetStore) {}
+  virtual Weight *copy() const { return new HTMWeight(*this); }
+
+  virtual bool isHTMWeight() const { return true; }
+
+  /// Analyze an instruction for memory operations.
+  virtual void analyze(const Instruction *I, const DataLayout *DL) {
+    Type *Ty;
+
+    // TODO do extractelement, insertelement, shufflevector, extractvalue, or
+    // insertvalue read/write memory?
+    // TODO Need to handle the following instructions/instrinsics (also see
+    // Instruction::mayLoad() / Instruction::mayStore()):
+    //   llvm.masked.load
+    //   llvm.masked.store
+    //   llvm.masked.gather
+    //   llvm.masked.store
+    switch(I->getOpcode()) {
+    default: break;
+    case Instruction::Load: {
+      const LoadInst *LI = cast<LoadInst>(I);
+      Ty = LI->getPointerOperand()->getType()->getPointerElementType();
+      LoadBytes += DL->getTypeStoreSize(Ty);
+      break;
+    }
+
+    case Instruction::Store: {
+      const StoreInst *SI = cast<StoreInst>(I);
+      Ty = SI->getValueOperand()->getType();
+      StoreBytes += DL->getTypeStoreSize(Ty);
+      break;
+    }
+
+    case Instruction::AtomicCmpXchg: {
+      const AtomicCmpXchgInst *Cmp = cast<AtomicCmpXchgInst>(I);
+      Ty = Cmp->getPointerOperand()->getType()->getPointerElementType();
+      LoadBytes += DL->getTypeStoreSize(Ty);
+      StoreBytes += DL->getTypeStoreSize(Ty);
+    }
+
+    case Instruction::AtomicRMW: {
+      const AtomicRMWInst *RMW = cast<AtomicRMWInst>(I);
+      Ty = RMW->getPointerOperand()->getType()->getPointerElementType();
+      LoadBytes += DL->getTypeStoreSize(Ty);
+      StoreBytes += DL->getTypeStoreSize(Ty);
+    }
+
+    case Instruction::Call: {
+      const IntrinsicInst *II = dyn_cast<IntrinsicInst>(I);
+      bool Loads = false, Stores = false;
+      int64_t Size = 0;
+
+      if(!II) break;
+      switch(II->getIntrinsicID()) {
+      default: break;
+      case Intrinsic::memcpy:
+      case Intrinsic::memmove:
+        // Arguments: i8* dest, i8* src, i<x> len, i32 align, i1 isvolatile
+        Loads = Stores = true;
+        Size = getValueSize(II->getArgOperand(2));
+        break;
+      case Intrinsic::memset:
+        // Arguments: i8* dest, i8 val, i<x> len, i32 align, i1 isvolatile
+        Stores = true;
+        Size = getValueSize(II->getArgOperand(2));
+        break;
+      }
+
+      // Size > 0: we know the size statically
+      // Size < 0: we can't determine the size statically
+      // Size == 0: some intrinsic we don't care about
+      if(Size > 0) {
+        if(Loads) LoadBytes += Size;
+        if(Stores) StoreBytes += Size;
+      }
+      else if(Size < 0) {
+        // Assume we're doing heavy reading & writing -- may need to revise if
+        // transaction begin/ends are too expensive.
+        if(Loads) LoadBytes += HTMReadBufSize;
+        if(Stores) StoreBytes += HTMWriteBufSize;
+      }
+
+      break;
+    }
+    }
+  }
+
+  virtual void reset() {
+    Weight::reset();
+    ResetLoad += LoadBytes;
+    ResetStore += StoreBytes;
+    LoadBytes = StoreBytes = 0;
+  }
+
+  /// The max value for HTM weights is the max of the two weights' LoadBytes
+  /// and StoreBytes (maintained separately).
+  virtual void max(const Weight *RHS) {
+    assert(RHS->isHTMWeight() && "Cannot mix weight types");
+    const HTMWeight *W = (const HTMWeight *)RHS;
+    if(W->LoadBytes > LoadBytes) LoadBytes = W->LoadBytes;
+    if(W->StoreBytes > StoreBytes) StoreBytes = W->StoreBytes;
+  }
+
+  virtual void multiply(size_t factor) {
+    LoadBytes *= factor;
+    StoreBytes *= factor;
+  }
+
+  virtual void add(const Weight *RHS) {
+    assert(RHS->isHTMWeight() && "Cannot mix weight types");
+    const HTMWeight *W = (const HTMWeight *)RHS;
+    LoadBytes += W->LoadBytes;
+    StoreBytes += W->StoreBytes;
+  }
+
+  /// The number of times this weight's load & stores could be executed without
+  /// overflowing the capacity threshold of the HTM buffers.
+  virtual size_t numIters(unsigned percent) const {
+    size_t NumLoadIters = UINT64_MAX, NumStoreIters = UINT64_MAX,
+      FPHtmReadSize = getValuePercent(HTMReadBufSize, percent),
+      FPHtmWriteSize = getValuePercent(HTMWriteBufSize, percent);
+
+    if(!LoadBytes && !StoreBytes) return 1024; // Return a safe value
+    else {
+      if(LoadBytes) NumLoadIters = FPHtmReadSize / LoadBytes;
+      if(StoreBytes) NumStoreIters = FPHtmWriteSize / StoreBytes;
+
+      if(!NumLoadIters && !NumStoreIters) return 1;
+      else return NumLoadIters < NumStoreIters ? NumLoadIters : NumStoreIters;
+    }
+  }
+
+  virtual bool underPercentOfThreshold(unsigned percent) const {
+    if(LoadBytes <= getValuePercent(HTMReadBufSize, percent) &&
+       StoreBytes <= getValuePercent(HTMWriteBufSize, percent))
+      return true;
+    else return false;
+  }
+
+  virtual std::string toString() const {
+    return std::to_string(LoadBytes) + " byte(s) loaded, " +
+           std::to_string(StoreBytes) + " byte(s) stored";
+  }
+};
+
+/// Weight metric for temporally-spaced migration points.
+class CycleWeight : public Weight {
+private:
+  // An estimate of the number of cycles since the last migration point.
+  size_t Cycles;
+
+  // Statistics about when the weight was reset (i.e., at migration points).
+  size_t ResetCycles;
+
+public:
+  CycleWeight(size_t Cycles = 0) : Cycles(Cycles), ResetCycles(0) {}
+  CycleWeight(const CycleWeight &C)
+    : Weight(C), Cycles(C.Cycles), ResetCycles(C.ResetCycles) {}
+  virtual CycleWeight *copy() const { return new CycleWeight(*this); }
+
+  virtual bool isCycleWeight() const { return true; }
+
+  virtual void analyze(const Instruction *I, const DataLayout *DL) {
+    Type *Ty;
+
+    // Cycles are estimated using Agner Fog's instruction latency guide at
+    // http://www.agner.org/optimize/instruction_tables.pdf for "Broadwell".
+    switch(I->getOpcode()) {
+    default: break;
+    // Terminator instructions
+    // TODO Ret, Invoke, Resume
+    case Instruction::Br: Cycles += 2; break;
+    case Instruction::Switch: Cycles += 2; break;
+    case Instruction::IndirectBr: Cycles += 2; break;
+
+    // Binary instructions
+    case Instruction::Add: Cycles++; break;
+    case Instruction::FAdd: Cycles += 3; break;
+    case Instruction::Sub: Cycles++; break;
+    case Instruction::FSub: Cycles += 3; break;
+    case Instruction::Mul: Cycles += 2; break;
+    case Instruction::FMul: Cycles += 3; break;
+    case Instruction::UDiv: Cycles += 73; break;
+    case Instruction::SDiv: Cycles += 81; break;
+    case Instruction::FDiv: Cycles += 14; break;
+    case Instruction::URem: Cycles += 73; break;
+    case Instruction::SRem: Cycles += 81; break;
+    case Instruction::FRem: Cycles += 14; break;
+
+    // Logical operators
+    case Instruction::Shl: Cycles += 2; break;
+    case Instruction::LShr: Cycles += 2; break;
+    case Instruction::AShr: Cycles += 2; break;
+    case Instruction::And: Cycles += 1; break;
+    case Instruction::Or: Cycles += 1; break;
+    case Instruction::Xor: Cycles += 1; break;
+
+    // Memory instructions
+    case Instruction::Load: {
+      const LoadInst *LI = cast<LoadInst>(I);
+      Ty = LI->getPointerOperand()->getType()->getPointerElementType();
+      Cycles += getNumCacheLines(DL->getTypeStoreSize(Ty), 64) * MEM_WEIGHT;
+      break;
+    }
+    case Instruction::Store: {
+      const StoreInst *SI = cast<StoreInst>(I);
+      Ty = SI->getValueOperand()->getType();
+      Cycles += getNumCacheLines(DL->getTypeStoreSize(Ty), 64) * MEM_WEIGHT;
+      break;
+    }
+    case Instruction::GetElementPtr: Cycles++; break;
+    case Instruction::Fence: Cycles += 33; break;
+    case Instruction::AtomicCmpXchg: Cycles += 21; break;
+    case Instruction::AtomicRMW: Cycles += 21; break;
+
+    // Cast instructions
+    case Instruction::Trunc: Cycles++; break;
+    case Instruction::ZExt: Cycles++; break;
+    case Instruction::SExt: Cycles++; break;
+    case Instruction::FPToUI: Cycles += 4; break;
+    case Instruction::FPToSI: Cycles += 4; break;
+    case Instruction::UIToFP: Cycles += 5; break;
+    case Instruction::SIToFP: Cycles += 5; break;
+    case Instruction::FPTrunc: Cycles += 4; break;
+    case Instruction::FPExt: Cycles += 2; break;
+
+    // Other instructions
+    // TODO VAArg, ExtractElement, InsertElement, ShuffleVector, ExtractValue,
+    // InsertValue, LandingPad
+    case Instruction::ICmp: Cycles++; break;
+    case Instruction::FCmp: Cycles += 3; break;
+    case Instruction::Call: {
+      const IntrinsicInst *II = dyn_cast<IntrinsicInst>(I);
+      int64_t Size = 0;
+
+      if(!II) Cycles += 3;
+      else {
+        switch(II->getIntrinsicID()) {
+        default: break;
+        case Intrinsic::memcpy:
+        case Intrinsic::memmove:
+        case Intrinsic::memset:
+          // Arguments: i8* dest, i8* src, i<x> len, i32 align, i1 isvolatile
+          Size = getValueSize(II->getArgOperand(2));
+          break;
+        }
+
+        if(Size > 0) Cycles += getNumCacheLines(Size, 64) * MEM_WEIGHT;
+      }
+      break;
+    }
+    case Instruction::Select: Cycles += 3; break;
+    }
+  }
+
+  virtual void reset() {
+    Weight::reset();
+    ResetCycles += Cycles;
+    Cycles = 0;
+  }
+
+  virtual void max(const Weight *RHS) {
+    assert(RHS->isCycleWeight() && "Cannot mix weight types");
+    const CycleWeight *W = (const CycleWeight *)RHS;
+    if(W->Cycles > Cycles) Cycles = W->Cycles;
+  }
+
+  virtual void multiply(size_t factor) { Cycles *= factor; }
+  virtual void add(const Weight *RHS) {
+    assert(RHS->isCycleWeight() && "Cannot mix weight types");
+    const CycleWeight *W = (const CycleWeight *)RHS;
+    Cycles += W->Cycles;
+  }
+
+  virtual size_t numIters(unsigned percent) const {
+    if(!Cycles) return 1048576; // Return a safe value
+    else {
+      size_t FPCycleCap = getValuePercent(CyclesBetweenMigPoints, percent);
+      size_t Iters = FPCycleCap / Cycles;
+      return Iters ? Iters : 1;
+    }
+  }
+
+  virtual bool underPercentOfThreshold(unsigned percent) const {
+    if(Cycles <= getValuePercent(CyclesBetweenMigPoints, percent)) return true;
+    else return false;
+  }
+
+  virtual std::string toString() const {
+    return std::to_string(Cycles) + " cycles";
+  }
+};
+
+/// Get a weight object with zero-initialized weight based on the type of
+/// analysis being used to instrument the application.
+///
+/// Note: returns a dynamically allocated object to be managed by the caller
+static Weight *getZeroWeight() {
+  if(HTMExec) return new HTMWeight();
+  else return new CycleWeight();
+}
+
+/// SelectMigrationPoints - select locations at which to insert migration
+/// points into functions.
+class SelectMigrationPoints : public FunctionPass
+{
+public:
+  static char ID;
+
+  SelectMigrationPoints() : FunctionPass(ID) {
+    initializeSelectMigrationPointsPass(*PassRegistry::getPassRegistry());
+  }
+  ~SelectMigrationPoints() {}
+
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const {
+    AU.addRequired<LoopInfoWrapperPass>();
+    AU.addRequired<EnumerateLoopPaths>();
+    AU.addRequired<ScalarEvolution>();
+  }
+
+  virtual const char *getPassName() const
+  { return "Select migration point locations"; }
+
+  static inline unsigned splitFuncValPair(const std::string &Pair,
+                                          std::string &Func) {
+    unsigned Val;
+    size_t Comma = Pair.rfind(',');
+    Func = Pair.substr(0, Comma);
+    Val = stoul(Pair.substr(Comma + 1));
+    assert(Val <= 100 && "Invalid percentage");
+    return Val;
+  }
+
+  /// Parse per-function threshold values from the command line.
+  void parsePerFuncThresholds() {
+    unsigned Val;
+    std::string Name;
+
+    FuncCapList.clear();
+    FuncStartList.clear();
+    FuncRetList.clear();
+    NoInstFuncs.clear();
+
+    for(auto Pair : FuncCapThreshold) {
+      Val = splitFuncValPair(Pair, Name);
+      FuncCapList[Name] = Val;
+    }
+    for(auto Pair : FuncStartThreshold) {
+      Val = splitFuncValPair(Pair, Name);
+      FuncStartList[Name] = Val;
+    }
+    for(auto Pair : FuncRetThreshold) {
+      Val = splitFuncValPair(Pair, Name);
+      FuncRetList[Name] = Val;
+    }
+    for(auto Func : FuncNoInst) NoInstFuncs.insert(Func);
+  }
+
+  virtual bool doInitialization(Module &M) {
+    DL = &M.getDataLayout();
+    addPopcornFnAttributes(M);
+    if(MoreMigPoints) parsePerFuncThresholds();
+    if(HTMExec) Popcorn::setInstrumentationType(M, Popcorn::HTM);
+    else Popcorn::setInstrumentationType(M, Popcorn::Cycles);
+    return false;
+  }
+
+  /// Select where to insert migration points into functions.
+  virtual bool runOnFunction(Function &F)
+  {
+    DEBUG(dbgs() << "\n********** SELECT MIGRATION POINTS **********\n"
+                 << "********** Function: " << F.getName() << "\n\n");
+
+    if(F.hasFnAttribute("popcorn-noinstr") ||
+       NoInstFuncs.find(F.getName()) != NoInstFuncs.end()) return false;
+
+    initializeAnalysis(F);
+
+    // Some operations (e.g., big memory copies, I/O) will cause aborts.
+    // Instrument these operations to stop & resume transactions afterwards.
+    if(HTMExec) {
+      bool AddedMigPoint = wrapWithHTM(F, isBigMemoryOp,
+        "memory operations that overflow HTM buffers");
+      if(!NoWrapLibc)
+        AddedMigPoint |= wrapWithHTM(F, isLibcIO, "I/O functions");
+      if(AddedMigPoint) LP->runOnFunction(F);
+    }
+
+    if(MoreMigPoints && !LP->analysisFailed()) {
+      StringRef FuncName = F.getName();
+      StringMap<unsigned>::const_iterator It;
+      if((It = FuncCapList.find(FuncName)) != FuncCapList.end())
+        CurCapThresh = It->second;
+      else CurCapThresh = CapacityThreshold;
+      if((It = FuncStartList.find(FuncName)) != FuncStartList.end())
+        CurStartThresh = It->second;
+      else CurStartThresh = StartThreshold;
+      if((It = FuncRetList.find(FuncName)) != FuncRetList.end())
+        CurRetThresh = It->second;
+      else CurRetThresh = RetThreshold;
+
+      DEBUG(
+        dbgs() << "\n-> Analyzing function body to add migration points <-\n"
+               << "\nCapacity threshold: " << std::to_string(CurCapThresh)
+               << "\nStart threshold: " << std::to_string(CurStartThresh)
+               << "\nReturn threshold: " << std::to_string(CurRetThresh)
+               << "\nMaximum iterations/migration point: "
+               << std::to_string(MaxItersPerMigPoint);
+
+        if(HTMExec)
+          dbgs() << "\nAnalyzing for HTM Instrumentation"
+                 << "\n  HTM read buffer size: "
+                 << std::to_string(HTMReadBufSizeArg) << "kb"
+                 << "\n  HTM write buffer size: "
+                 << std::to_string(HTMWriteBufSizeArg) << "kb\n";
+        else
+          dbgs() << "\nAnalyzing for migration call out instrumentation"
+                 << "\n  Target millions of cycles between migration points: "
+                 << std::to_string(MillionCyclesBetweenMigPoints) << "\n";
+      );
+
+      // We by default mark the function start as a migration point, but if we
+      // don't add any instrumentation & the function's exit weights are
+      // sufficiently small avoid instrumentation altogether.
+      bool MarkStart = false;
+      if(!analyzeFunctionBody(F)) {
+        for(Function::iterator BB = F.begin(), E = F.end(); BB != E; BB++)
+          if(isa<ReturnInst>(BB->getTerminator()) &&
+             !BBWeights[BB].BlockWeight->underPercentOfThreshold(CurStartThresh))
+            MarkStart = true;
+      }
+      else MarkStart = true;
+
+      if(MarkStart) {
+        DEBUG(dbgs() << "-> Marking function entry as a migration point <-\n");
+        markAsMigPoint(F.getEntryBlock().getFirstInsertionPt(), true, true);
+      }
+      else { DEBUG(dbgs() << "-> Eliding instrumenting function entry <-\n"); }
+    }
+    else {
+      if(MoreMigPoints) {
+        std::string Msg = "too many paths to instrument function with more "
+                          "migration points -- falling back to instrumenting "
+                          "function entry/exit";
+        DiagnosticInfoOptimizationFailure DI(F, nullptr, Msg);
+        F.getContext().diagnose(DI);
+      }
+
+      DEBUG(dbgs() << "-> Marking function entry as a migration point <-\n");
+      markAsMigPoint(F.getEntryBlock().getFirstInsertionPt(), true, true);
+
+      // Instrument function exit point(s)
+      DEBUG(dbgs() << "-> Marking function exit(s) as a migration point <-\n");
+      for(Function::iterator BB = F.begin(), E = F.end(); BB != E; BB++)
+        if(isa<ReturnInst>(BB->getTerminator()))
+          markAsMigPoint(BB->getTerminator(), true, true);
+    }
+
+    // Finally, apply transformations to loops headers according to analysis.
+    transformLoopHeaders(F);
+
+    return true;
+  }
+
+  /// Reset all analysis.
+  void initializeAnalysis(const Function &F) {
+    SE = &getAnalysis<ScalarEvolution>();
+    LI = &getAnalysis<LoopInfoWrapperPass>().getLoopInfo();
+    LP = &getAnalysis<EnumerateLoopPaths>();
+    BBWeights.clear();
+    LoopWeights.clear();
+    TransformLoops.clear();
+    MigPointInsts.clear();
+    HTMBeginInsts.clear();
+    HTMEndInsts.clear();
+  }
+
+private:
+  //===--------------------------------------------------------------------===//
+  // Types & fields
+  //===--------------------------------------------------------------------===//
+
+  /// Configuration for the function currently being analyzed.
+  unsigned CurCapThresh;
+  unsigned CurStartThresh;
+  unsigned CurRetThresh;
+
+  /// The current architecture - used to access architecture-specific HTM calls
+  const DataLayout *DL;
+
+  /// Parsed per-function thresholds.
+  StringMap<unsigned> FuncCapList;
+  StringMap<unsigned> FuncStartList;
+  StringMap<unsigned> FuncRetList;
+  StringSet<> NoInstFuncs;
+
+  /// Analyses on which we depend
+  ScalarEvolution *SE;
+  LoopInfo *LI;
+  EnumerateLoopPaths *LP;
+
+  /// libc functions which are likely to cause an HTM abort through a syscall
+  const static StringSet<> LibcIO;
+
+  /// Weight information for basic blocks.
+  class BasicBlockWeightInfo {
+  public:
+    /// Weight of the basic block at the end of its execution.  If the block has
+    /// a migration point, the weight *only* captures the instructions following
+    /// the migration point (migration points "reset" the weight).
+    WeightPtr BlockWeight;
+
+    BasicBlockWeightInfo() : BlockWeight(nullptr) {}
+    BasicBlockWeightInfo(const Weight *BlockWeight)
+      : BlockWeight(BlockWeight->copy()) {}
+    BasicBlockWeightInfo(const WeightPtr &BlockWeight)
+      : BlockWeight(BlockWeight->copy()) {}
+
+    std::string toString() const {
+      if(BlockWeight) return BlockWeight->toString();
+      else return "<uninitialized basic block weight info>";
+    }
+  };
+
+  /// Weight information for loops.  Maintains weights at loop exit points as
+  /// well as path-specific weight information for the loop & exit blocks.
+  class LoopWeightInfo {
+  private:
+    /// The weight of the loop upon entry.  Zero in the default case, but may
+    /// be set if analysis elides instrumentation in and around the loop.
+    WeightPtr EntryWeight;
+
+    /// The maximum weight when exiting the loop at each of its exit blocks.
+    /// Automatically recalculated when any of its ingredients are changed.
+    DenseMap<const BasicBlock *, WeightPtr> ExitWeights;
+
+    /// Whether the loop has either of the two types of paths, and if so the
+    /// maximum weight of each type.  Note that the spanning path weight is
+    /// *not* scaled by the number of iterations, ItersPerMigPoint.
+    bool LoopHasSpanningPath, LoopHasEqPointPath;
+    WeightPtr LoopSpanningPathWeight, LoopEqPointPathWeight;
+
+    /// Number of iterations between migration points if the loop has one or
+    /// more spanning paths, or zero otherwise.
+    size_t ItersPerMigPoint;
+
+    /// Whether there are either of the two types of paths through each exit
+    /// block, and if so the maximum weight of each type.
+    DenseMap<const BasicBlock *, bool> ExitHasSpanningPath, ExitHasEqPointPath;
+    DenseMap<const BasicBlock *, WeightPtr> ExitSpanningPathWeights,
+                                            ExitEqPointPathWeights;
+
+    /// Calculate the exit block's maximum weight, which is the max of both the
+    /// spanning path exit weight and equivalence point path exit weight.
+    void computeExitWeight(const BasicBlock *BB) {
+      // Note: these operations are in a specific order -- change with care!
+
+      // Calculate the loop weight up until the current iteration
+      WeightPtr BBWeight(getZeroWeight());
+      if(LoopHasSpanningPath) BBWeight->max(getLoopSpanningPathWeight(true));
+      if(LoopHasEqPointPath) BBWeight->max(LoopEqPointPathWeight);
+
+      // Calculate the maximum possible value of the current iteration:
+      //   - Spanning path: loop weight + current path weight
+      //   - Equivalence point path: current weight path
+      if(ExitHasSpanningPath[BB]) BBWeight->add(ExitSpanningPathWeights[BB]);
+      if(ExitHasEqPointPath[BB]) BBWeight->max(ExitEqPointPathWeights[BB]);
+
+      ExitWeights[BB] = std::move(BBWeight);
+    }
+
+    void computeAllExitWeights() {
+      for(auto I = ExitWeights.begin(), E = ExitWeights.end(); I != E; I++)
+        computeExitWeight(I->first);
+    }
+
+  public:
+    LoopWeightInfo() = delete;
+    LoopWeightInfo(const Loop *L)
+      : EntryWeight(getZeroWeight()), LoopHasSpanningPath(false),
+        LoopHasEqPointPath(false), ItersPerMigPoint(0) {
+      SmallVector<BasicBlock *, 4> ExitBlocks;
+      L->getExitingBlocks(ExitBlocks);
+      for(auto Block : ExitBlocks) {
+        ExitHasSpanningPath[Block] = false;
+        ExitHasEqPointPath[Block] = false;
+      }
+    }
+
+    /// Set the weight upon entering the loop & recompute all exit weights.
+    void setEntryWeight(const WeightPtr &W) {
+      EntryWeight.reset(W->copy());
+      computeAllExitWeights();
+    }
+
+    /// Get the number of iterations between migration points, or zero if there
+    /// are no spanning paths through the loop.
+    size_t getItersPerMigPoint() const {
+      return ItersPerMigPoint;
+    }
+
+    /// Get the loop's spanning path weight, scaled based on the number of
+    /// iterations.  Also includes loop entry weight if requested.
+    WeightPtr getLoopSpanningPathWeight(bool AddEntry) const {
+      assert(LoopHasSpanningPath && "No spanning path weight for loop");
+      WeightPtr Ret(LoopSpanningPathWeight->copy());
+      Ret->multiply(ItersPerMigPoint - 1);
+      if(AddEntry) Ret->add(EntryWeight);
+      return Ret;
+    }
+
+    /// Set the loop's spanning path weight & recompute all exit weights.
+    ///  - W: the maximum weight of a single spanning path iteration
+    ///  - I: the number of iterations per migration point
+    void setLoopSpanningPathWeight(const WeightPtr &W, size_t I) {
+      LoopHasSpanningPath = true;
+      LoopSpanningPathWeight.reset(W->copy());
+      ItersPerMigPoint = I;
+      computeAllExitWeights();
+    }
+
+    /// Get the loop's equivalence point path weight.
+    WeightPtr getLoopEqPointPathWeight() const
+    {
+      assert(LoopHasEqPointPath && "No equivalence point path weight for loop");
+      return WeightPtr(LoopEqPointPathWeight->copy());
+    }
+
+    /// Set the loop's equivalence point path weight & recompute all exit
+    /// weights.
+    void setLoopEqPointPathWeight(const WeightPtr &W) {
+      LoopHasEqPointPath = true;
+      LoopEqPointPathWeight.reset(W->copy());
+      computeAllExitWeights();
+    }
+
+    /// Get an exit block's spanning path weight.  This is the raw weight for
+    /// a single iteration of paths through this exiting block, it does *not*
+    /// incorporate loop weights.
+    WeightPtr getExitSpanningPathWeight(const BasicBlock *BB) const
+    {
+      assert(ExitHasSpanningPath.find(BB)->second &&
+             "No spanning path weight for exit block");
+      return WeightPtr(ExitSpanningPathWeights.find(BB)->second->copy());
+    }
+
+    /// Set the exit block's spanning path weight & recompute the exit block's
+    /// overall maximum weight.
+    void setExitSpanningPathWeight(const BasicBlock *BB, const WeightPtr &W)
+    {
+      ExitHasSpanningPath[BB] = true;
+      ExitSpanningPathWeights[BB].reset(W->copy());
+      computeExitWeight(BB);
+    }
+
+    /// Get an exit block's equivalence point path weight.  This is the raw
+    /// weight for a single iteration of paths through this exiting block, it
+    /// does *not* incorporate loop weights.
+    WeightPtr getExitEqPointPathWeight(const BasicBlock *BB) const
+    {
+      assert(ExitHasEqPointPath.find(BB)->second &&
+             "No equivalence point path weight for exit block");
+      return WeightPtr(ExitEqPointPathWeights.find(BB)->second->copy());
+    }
+
+    /// Set the equivalence point path exit block weight & recompute the exit
+    /// block's overall maximum weight.
+    void setExitEqPointPathWeight(const BasicBlock *BB, const WeightPtr &W)
+    {
+      ExitHasEqPointPath[BB] = true;
+      ExitEqPointPathWeights[BB].reset(W->copy());
+      computeExitWeight(BB);
+    }
+
+    /// Return whether the loop/exit block has spanning and equivalence point
+    /// paths through it.
+    bool loopHasSpanningPath() const { return LoopHasSpanningPath; }
+    bool loopHasEqPointPath() const { return LoopHasEqPointPath; }
+    bool exitHasSpanningPath(const BasicBlock *BB) const
+    { return ExitHasSpanningPath.find(BB)->second; }
+    bool exitHasEqPointPath(const BasicBlock *BB) const
+    { return ExitHasEqPointPath.find(BB)->second; }
+
+    /// Return the weight of a given exiting basic block.
+    const WeightPtr &getExitWeight(const BasicBlock *BB) const {
+      assert(ExitWeights.count(BB) && "Invalid exit basic block");
+      return ExitWeights.find(BB)->second;
+    }
+
+    const WeightPtr &operator[](const BasicBlock *BB) const
+    { return getExitWeight(BB); }
+
+    std::string toString() const {
+      if(!ExitWeights.size()) return "<uninitialized loop weight info>";
+      else {
+        std::string buf = "Exit block weights:\n";
+        for(auto It = ExitWeights.begin(), E = ExitWeights.end();
+            It != E; ++It) {
+          buf += "    ";
+          if(It->first->hasName()) {
+            buf += It->first->getName();
+            buf += ": ";
+          }
+          buf += It->second->toString() + "\n";
+        }
+        return buf;
+      }
+    }
+  };
+
+  /// Weight information gathered by analyses for basic blocks & loops
+  typedef std::map<const BasicBlock *, BasicBlockWeightInfo> BlockWeightMap;
+  typedef std::map<const Loop *, LoopWeightInfo> LoopWeightMap;
+  BlockWeightMap BBWeights;
+  LoopWeightMap LoopWeights;
+
+  /// Code locations marked for instrumentation.
+  SmallPtrSet<Loop *, 16> TransformLoops;
+  SmallPtrSet<Instruction *, 32> MigPointInsts;
+  SmallPtrSet<Instruction *, 32> HTMBeginInsts;
+  SmallPtrSet<Instruction *, 32> HTMEndInsts;
+
+  //===--------------------------------------------------------------------===//
+  // Analysis implementation
+  //===--------------------------------------------------------------------===//
+
+  /// Add Popcorn-related function attributes where appropriate.
+  void addPopcornFnAttributes(Module &M) const {
+    auto GlobalAnnos = M.getNamedGlobal("llvm.global.annotations");
+    if(GlobalAnnos) {
+      auto a = cast<ConstantArray>(GlobalAnnos->getOperand(0));
+      for(unsigned int i = 0; i < a->getNumOperands(); i++) {
+        auto e = cast<ConstantStruct>(a->getOperand(i));
+        if(auto fn = dyn_cast<Function>(e->getOperand(0)->getOperand(0))) {
+          auto Anno = cast<ConstantDataArray>(
+                        cast<GlobalVariable>(
+                          e->getOperand(1)->getOperand(0)
+                        )->getOperand(0)
+                      )->getAsCString();
+          fn->addFnAttr(Anno);
+        }
+      }
+    }
+  }
+
+  /// Return whether the instruction requires HTM begin instrumentation.
+  bool shouldAddHTMBegin(Instruction *I) const {
+    if(Popcorn::isHTMBeginPoint(I)) return true;
+    else return HTMBeginInsts.count(I);
+  }
+
+  /// Return whether the instruction requires HTM end instrumentation.
+  bool shouldAddHTMEnd(Instruction *I) const {
+    if(Popcorn::isHTMEndPoint(I)) return true;
+    else return HTMEndInsts.count(I);
+  }
+
+  /// Return whether the instruction is a migration point.  We assume that all
+  /// called functions have migration points internally.
+  bool isMigrationPoint(Instruction *I) const {
+    if(Popcorn::isEquivalencePoint(I)) return true;
+    else return MigPointInsts.count(I);
+  }
+
+  /// Return whether the instruction is marked for any instrumentation.
+  bool isMarkedForInstrumentation(Instruction *I) const {
+    return isMigrationPoint(I) || shouldAddHTMBegin(I) || shouldAddHTMEnd(I);
+  }
+
+  /// Mark an instruction to be instrumented with an HTM begin, directly before
+  /// the instruction
+  bool markAsHTMBegin(Instruction *I) {
+    if(!HTMExec) return false;
+    DEBUG(dbgs() << "  + Marking"; I->print(dbgs());
+          dbgs() << " as HTM begin\n");
+    HTMBeginInsts.insert(I);
+    Popcorn::addHTMBeginMetadata(I);
+    return true;
+  }
+
+  /// Mark an instruction to be instrumented with an HTM end, directly before
+  /// the instruction
+  bool markAsHTMEnd(Instruction *I) {
+    if(!HTMExec) return false;
+    DEBUG(dbgs() << "  + Marking"; I->print(dbgs());
+          dbgs() << " as HTM end\n");
+    HTMEndInsts.insert(I);
+    Popcorn::addHTMEndMetadata(I);
+    return true;
+  }
+
+  /// Mark an instruction to be instrumented with a migration point, directly
+  /// before the instruction.  Optionally mark instruction as needing HTM
+  /// start/stop intrinsics.
+  bool markAsMigPoint(Instruction *I, bool AddHTMBegin, bool AddHTMEnd) {
+    // Don't clobber any existing instrumentation
+    if(isMarkedForInstrumentation(I)) return false;
+    DEBUG(dbgs() << "  + Marking"; I->print(dbgs());
+          dbgs() << " as a migration point\n");
+    MigPointInsts.insert(I);
+    Popcorn::addEquivalencePointMetadata(I);
+    if(AddHTMBegin) markAsHTMBegin(I);
+    if(AddHTMEnd) markAsHTMEnd(I);
+    return true;
+  }
+
+  /// Instruction matching function type.
+  typedef bool (*InstMatch)(const Instruction *, unsigned Thresh);
+
+  /// Return whether the instruction is a memory operation that will overflow
+  /// HTM buffers.
+  static bool isBigMemoryOp(const Instruction *I, unsigned Thresh) {
+    if(!I || !isa<IntrinsicInst>(I)) return false;
+    const IntrinsicInst *II = cast<IntrinsicInst>(I);
+    int64_t Size = 0;
+    switch(II->getIntrinsicID()) {
+    default: return false;
+    case Intrinsic::memcpy: case Intrinsic::memmove: case Intrinsic::memset:
+      // Arguments: i8* dest, i8* src, i<x> len, i32 align, i1 isvolatile
+      Size = getValueSize(II->getArgOperand(2));
+      break;
+    }
+
+    if(Size >= 0) { // We know the size
+      size_t USize = (size_t)Size;
+      return USize >= getValuePercent(HTMReadBufSize, Thresh) ||
+             USize >= getValuePercent(HTMWriteBufSize, Thresh);
+    }
+    else return !NoWrapUnknownMem;
+  }
+
+  /// Return whether the instruction is a libc I/O call.
+  static bool isLibcIO(const Instruction *I, unsigned Thresh) {
+    if(!I || !Popcorn::isCallSite(I)) return false;
+    const ImmutableCallSite CS(I);
+    const Function *CalledFunc = CS.getCalledFunction();
+    if(CalledFunc && CalledFunc->hasName())
+      return LibcIO.find(CalledFunc->getName()) != LibcIO.end();
+    return false;
+  }
+
+  /// Search for & wrap operations that match a certain criteria.
+  bool wrapWithHTM(Function &F, InstMatch Matcher, const char *Desc) {
+    bool AddedMigPoint = false;
+
+    DEBUG(dbgs() << "\n-> Wrapping " << Desc << " with HTM stop/start <-\n");
+    for(Function::iterator BB = F.begin(), BE = F.end(); BB != BE; BB++) {
+      if(LI->getLoopFor(BB)) continue; // Don't do this in loops!
+      for(BasicBlock::iterator I = BB->begin(), E = BB->end(); I != E; I++) {
+        if(Matcher(I, CurCapThresh)) {
+          markAsHTMEnd(I);
+
+          // Search subsequent instructions for other libc calls to prevent
+          // pathological transaction stop/starts.
+          const static size_t searchSpan = 10;
+          BasicBlock::iterator NextI(I->getNextNode());
+          for(size_t rem = searchSpan; rem > 0 && NextI != E; rem--, NextI++) {
+            if(Matcher(NextI, CurCapThresh)) {
+              DEBUG(dbgs() << "  - Found another match:"; NextI->dump());
+              I = NextI;
+              rem = searchSpan;
+            }
+          }
+
+          // TODO analyze successor blocks as well
+
+          AddedMigPoint |= markAsMigPoint(I->getNextNode(), true, false);
+        }
+      }
+    }
+
+    return AddedMigPoint;
+  }
+
+  /// Get the starting weight for a basic block based on the max weights of its
+  /// predecessors.
+  ///
+  /// Note: returns a dynamically allocated object to be managed by the caller
+  Weight *getInitialWeight(const BasicBlock *BB) const {
+    Weight *PredWeight = getZeroWeight();
+    const Loop *L = LI->getLoopFor(BB);
+    bool BBIsHeader = L && (BB == L->getHeader());
+    unsigned LDepth = L ? L->getLoopDepth() : 0;
+
+    for(auto Pred : predecessors(BB)) {
+      const Loop *PredLoop = LI->getLoopFor(Pred);
+
+      // We *only* gather header initial weights when analyzing whether to
+      // instrument loop entry, which doesn't depend on latches.
+      if(BBIsHeader && PredLoop == L) continue;
+
+      // Determine if the predecessor is an exit block from another loop:
+      //
+      //   1. The predecessor is in a loop
+      //   2. The predecessor's loop is not BB's loop
+      //   3. The nesting depth of the predecessor's loop is >= BB's loop*
+      //
+      // If it's an exit block, use the loop weight info to get the exit
+      // weight.  Otherwise, use the basic block weight info.
+      //
+      // *Note: if the predecessor's nesting depth is < BB's, then BB is in a
+      // child loop inside the predecessor's loop, and the predecessor is NOT a
+      // loop exiting block.
+      if(PredLoop && PredLoop != L && PredLoop->getLoopDepth() >= LDepth) {
+        assert(LoopWeights.count(PredLoop) &&
+               "Invalid reverse post-order traversal");
+        PredWeight->max(LoopWeights.at(PredLoop)[Pred]);
+      }
+      else {
+        assert(BBWeights.count(Pred) && "Invalid reverse post-order traversal");
+        PredWeight->max(BBWeights.at(Pred).BlockWeight);
+      }
+    }
+
+    return PredWeight;
+  }
+
+  /// Analyze a single basic block with an initial starting weight and update
+  /// it with the block's ending weight.  Return whether or not a migration
+  /// point was added.
+  bool traverseBlock(BasicBlock *BB, Weight *CurWeight) {
+    bool AddedMigPoint = false;
+
+    DEBUG(
+      dbgs() << "      Analyzing basic block";
+      if(BB->hasName()) dbgs() << " '" << BB->getName() << "'";
+      dbgs() << "\n";
+    );
+
+    // TODO this doesn't respect spans identified by wrapWithHTM()!
+
+    for(BasicBlock::iterator I = BB->begin(), E = BB->end(); I != E; I++) {
+      if(isa<PHINode>(I)) continue;
+
+      // Check if there is or there should be a migration point before the
+      // instruction, and if so, reset the weight.  Note: markAsMigPoint()
+      // internally avoids tampering with existing instrumentation.
+      if(isMigrationPoint(I)) CurWeight->reset();
+      else if(CurWeight->shouldAddMigPoint(CurCapThresh)) {
+        AddedMigPoint |= markAsMigPoint(I, true, true);
+        CurWeight->reset();
+      }
+
+      CurWeight->analyze(I, DL);
+    }
+
+    DEBUG(dbgs() << "       - Weight: " << CurWeight->toString() << "\n");
+
+    return AddedMigPoint;
+  }
+
+  bool traverseBlock(BasicBlock *BB, WeightPtr &Initial)
+  { return traverseBlock(BB, Initial.get()); }
+
+  /// Mark loop predecessors, i.e., all branches into the loop header, as
+  /// migration points.  Return whether or not a migration point was added.
+  bool markLoopPredecessors(const Loop *L) {
+    bool AddedMigPoint = false;
+    BasicBlock *Header = L->getHeader();
+    for(auto Pred : predecessors(Header)) {
+      // Weed out latches
+      if(!L->contains(Pred)) {
+        // Avoid adding migration points in bodies of predecessor loops when
+        // exiting from one loop directly into the header of another, e.g.,
+        //
+        //   for.body:  ;Body of first loop
+        //     ...
+        //     br i1 %cmp, for.body, for.body.2
+        //
+        //   for.body.2: ;Body of second, completely independent loop
+        //     ...
+        const Loop *PredL = LI->getLoopFor(Pred);
+        if(PredL == nullptr || (PredL->getLoopDepth() < L->getLoopDepth()))
+          AddedMigPoint |= markAsMigPoint(Pred->getTerminator(), true, true);
+      }
+    }
+    return AddedMigPoint;
+  }
+
+  /// Analyze & mark loop entry with migration points.  Avoid instrumenting if
+  /// we can execute the entire loop & any entry code without overflowing our
+  /// resource capacity.
+  bool traverseLoopEntry(Loop *L) {
+    // We don't need to instrument around the loop if we're instrumenting the
+    // header, as we'll hit a migration point at the beginning of the loop.
+    if(TransformLoops.count(L)) return false;
+
+    assert(LoopWeights.count(L) && "Invalid reverse post-order traversal");
+    LoopWeightInfo &LWI = LoopWeights.at(L);
+
+    // If the loop only has equivalence point paths, assume that we'll hit an
+    // equivalence point before we abort -- may need to revise if there are too
+    // many capacity aborts.
+    if(!LWI.loopHasSpanningPath()) {
+      DEBUG(dbgs() << "       - Loop only has equivalence point paths, "
+                      "can elide instrumenting loop entry points\n");
+      return false;
+    }
+
+    // TODO what if it's an irreducible loop, i.e., > 1 header?
+    BasicBlock *Header = L->getHeader();
+    WeightPtr HeaderWeight(getInitialWeight(Header));
+
+    DEBUG(dbgs() << "       + Analyzing loop entry points to "
+                 << Header->getName() << ", header weight: "
+                 << HeaderWeight->toString() << "\n");
+
+    // See if any of the exit spanning path weights are too heavy to include
+    // the entry point weight (entry point weights don't affect equivalence
+    // point paths).
+    bool InstrumentLoopEntry = false;
+    SmallVector<BasicBlock *, 4> ExitBlocks;
+    L->getExitingBlocks(ExitBlocks);
+    for(auto Exit : ExitBlocks) {
+      if(LWI.exitHasSpanningPath(Exit)) {
+        WeightPtr SpExitWeight(LWI.getLoopSpanningPathWeight(false));
+        SpExitWeight->add(LWI.getExitSpanningPathWeight(Exit));
+        SpExitWeight->add(HeaderWeight);
+        if(SpExitWeight->shouldAddMigPoint(CurCapThresh))
+          InstrumentLoopEntry = true;
+      }
+    }
+
+    if(InstrumentLoopEntry) {
+      DEBUG(dbgs() << "       - One or more spanning path(s) were too heavy, "
+                      "instrumenting loop entry points\n");
+      return markLoopPredecessors(L);
+    }
+    else {
+      DEBUG(dbgs() << "       + Can elide instrumenting loop entry points\n");
+      LWI.setEntryWeight(HeaderWeight);
+      return false;
+    }
+  }
+
+  /// Traverse a loop and instrument with migration points on paths that are
+  /// too "heavy".  Return whether or not a migration point was added.
+  bool traverseLoop(Loop *L) {
+    bool AddedMigPoint = false;
+    LoopBlocksDFS DFS(L); DFS.perform(LI);
+    LoopBlocksDFS::RPOIterator Block = DFS.beginRPO(), E = DFS.endRPO();
+    SmallPtrSet<const Loop *, 4> MarkedLoops;
+    Loop *BlockLoop;
+
+    assert(Block != E && "Loop with no basic blocks");
+
+    DEBUG(
+      dbgs() << "  + Analyzing "; L->dump();
+      dbgs() << "    - At "; L->getStartLoc().dump();
+    );
+
+    // TODO what if it's an irreducible loop, i.e., > 1 header?
+    BasicBlock *CurBB = *Block;
+    WeightPtr HdrWeight(getZeroWeight());
+    AddedMigPoint |= traverseBlock(CurBB, HdrWeight);
+    BBWeights[CurBB] = std::move(HdrWeight);
+
+    for(++Block; Block != E; ++Block) {
+      CurBB = *Block;
+      BlockLoop = LI->getLoopFor(CurBB);
+      if(BlockLoop == L) { // Block is in same loop & nesting depth
+        WeightPtr PredWeight(getInitialWeight(CurBB));
+        AddedMigPoint |= traverseBlock(CurBB, PredWeight);
+        BBWeights[CurBB] = std::move(PredWeight);
+      }
+      else if(!MarkedLoops.count(BlockLoop)) {
+        // Block is in a sub-loop, analyze & mark sub-loop's entry.  Only
+        // analyze direct sub-loops, as deeper-nested (2+) loops will have
+        // already been analyzed by their parents.
+        if(BlockLoop->getLoopDepth() - L->getLoopDepth() == 1)
+          AddedMigPoint |= traverseLoopEntry(BlockLoop);
+        MarkedLoops.insert(BlockLoop);
+      }
+    }
+
+    DEBUG(dbgs() << "    Finished analyzing loop\n");
+
+    return AddedMigPoint;
+  }
+
+  /// Analyze a path in a loop up until a particular end instruction and return
+  /// its weight.  Doesn't do any marking.
+  ///
+  /// Note: returns a dynamically allocated object to be managed by the caller
+  Weight *traversePathInternal(const LoopPath *LP,
+                               const Instruction *PathEnd,
+                               bool &ActuallyEqPoint) const {
+    assert(LP->cbegin() != LP->cend() && "Trivial loop path, no blocks");
+    assert(LP->contains(PathEnd->getParent()) && "Invalid end instruction");
+    ActuallyEqPoint = false;
+
+    Loop *SubLoop;
+    Weight *PathWeight = getZeroWeight();
+    SetVector<PathNode>::const_iterator Node = LP->cbegin(),
+                                        EndNode = LP->cend();
+    const BasicBlock *NodeBlock = Node->getBlock(),
+                     *EndBlock = PathEnd->getParent();
+    BasicBlock::const_iterator Inst, EndInst, PathEndInst(PathEnd);
+
+    if(Node->isSubLoopExit()) {
+      // Since the sub-loop exit block is the start of the path, it's by
+      // definition exiting from an equivalence point path.
+      SubLoop = LI->getLoopFor(NodeBlock);
+      assert(LoopWeights.count(SubLoop) && "Invalid traversal");
+      const LoopWeightInfo &LWI = LoopWeights.at(SubLoop);
+      PathWeight->add(LWI.getExitEqPointPathWeight(NodeBlock));
+    }
+    else {
+      for(Inst = LP->startInst(), EndInst = NodeBlock->end();
+          Inst != EndInst && Inst != PathEndInst; Inst++)
+        PathWeight->analyze(Inst, DL);
+    }
+
+    if(NodeBlock == EndBlock) {
+      PathWeight->analyze(PathEndInst, DL);
+      return PathWeight;
+    }
+
+    for(Node++; Node != EndNode; Node++) {
+      NodeBlock = Node->getBlock();
+      if(Node->isSubLoopExit()) {
+        // Since the sub-loop exit block is in the middle of the path, it's by
+        // definition exiting from a spanning path.  EnumerateLoopPaths doesn't
+        // know about loops we've marked for transformation, however, so reset
+        // the path weight for loops that'll have a migration point added to
+        // their header.
+        SubLoop = LI->getLoopFor(NodeBlock);
+        assert(LoopWeights.count(SubLoop) && "Invalid traversal");
+        const LoopWeightInfo &LWI = LoopWeights.at(SubLoop);
+        if(TransformLoops.count(SubLoop)) {
+          ActuallyEqPoint = true;
+          PathWeight->reset();
+        }
+
+        // TODO we need to ultimately deal with the following situation more
+        // gracefully:
+        //
+        //   loop 1: all spanning paths, contains loop 2
+        //     loop 2: all spanning paths, contains loop 3
+        //       loop 3: all spanning paths, to be instrumented
+        //
+        // Analysis determines loop 3 needs to be instrumented.  If all paths
+        // in loop 2 go through loop 3, then loop 2 no longer has spanning
+        // paths but only equivalence point paths.  The previous if statement
+        // detects this, and reports it to calculateLoopExitWeights().  However
+        // when analyzing paths through loop 1, we can't detect that loop 2
+        // only has equivalence points paths.
+
+        if(LWI.loopHasSpanningPath()) {
+          PathWeight->add(LWI.getLoopSpanningPathWeight(false));
+          PathWeight->add(LWI.getExitSpanningPathWeight(NodeBlock));
+        }
+        else {
+          ActuallyEqPoint = true;
+          PathWeight->reset();
+          PathWeight->add(LWI[NodeBlock]);
+        }
+      }
+      else {
+        for(Inst = NodeBlock->begin(), EndInst = NodeBlock->end();
+            Inst != EndInst && Inst != PathEndInst; Inst++)
+          PathWeight->analyze(Inst, DL);
+      }
+
+      if(NodeBlock == EndBlock) break;
+    }
+    PathWeight->analyze(PathEndInst, DL);
+
+    return PathWeight;
+  }
+
+  /// Analyze a path in a loop and return its weight.  Doesn't do any marking.
+  ///
+  /// Note: returns a dynamically allocated object to be managed by the caller
+  Weight *traversePath(const LoopPath *LP, bool &ActuallyEqPoint) const {
+    DEBUG(dbgs() << "  + Analyzing loop path: "; LP->dump());
+    return traversePathInternal(LP, LP->endInst(), ActuallyEqPoint);
+  }
+
+  /// Analyze a path until a given exit block & return path's weight up until
+  /// the exit point.
+  ///
+  /// Note: returns a dynamically allocated object to be managed by the caller
+  Weight *traversePathUntilExit(const LoopPath *LP,
+                                BasicBlock *Exit,
+                                bool &ActuallyEqPoint) const
+  { return traversePathInternal(LP, Exit->getTerminator(), ActuallyEqPoint); }
+
+  /// Get the loop trip count if available and less than UINT32_MAX, or 0
+  /// otherwise.
+  ///
+  /// Note: ported from ScalarEvolution::getSmallConstantMaxTripCount() in
+  /// later LLVM releases.
+  unsigned getTripCount(const Loop *L) const {
+    const SCEVConstant *MaxExitCount =
+      dyn_cast<SCEVConstant>(SE->getMaxBackedgeTakenCount(L));
+    if(!MaxExitCount) return 0;
+    ConstantInt *ExitConst = MaxExitCount->getValue();
+    if(ExitConst->getValue().getActiveBits() > 32) return 0;
+    else return ((unsigned)ExitConst->getZExtValue()) + 1;
+  }
+
+  /// Calculate the exit weights of a loop at all exit points.
+  void calculateLoopExitWeights(Loop *L) {
+    assert(!LoopWeights.count(L) && "Previously analyzed loop?");
+
+    bool HasSpPath = false, HasEqPointPath = false, ActuallyEqPoint;
+    std::vector<const LoopPath *> Paths;
+    LoopWeights.emplace(L, LoopWeightInfo(L));
+    LoopWeightInfo &LWI = LoopWeights.at(L);
+    SmallVector<BasicBlock *, 4> ExitBlocks;
+    WeightPtr SpanningWeight(getZeroWeight()),
+              EqPointWeight(getZeroWeight());
+    LP->getBackedgePaths(L, Paths);
+
+    DEBUG(dbgs() << "\n    Calculating loop path weights: "
+                 << std::to_string(Paths.size()) << " backedge path(s)\n");
+
+    // Analyze weights of individual paths through the loop that end at a
+    // backedge, as these will dictate the loop's weight.
+    for(auto Path : Paths) {
+      WeightPtr PathWeight(traversePath(Path, ActuallyEqPoint));
+      DEBUG(dbgs() << "    Path weight: " << PathWeight->toString() << " ");
+      if(Path->isSpanningPath() && !ActuallyEqPoint) {
+        HasSpPath = true;
+        SpanningWeight->max(PathWeight);
+        DEBUG(dbgs() << "(spanning path)\n");
+      }
+      else {
+        HasEqPointPath = true;
+        EqPointWeight->max(PathWeight);
+        DEBUG(dbgs() << "(equivalence point path)\n");
+      }
+    }
+
+    // Calculate/store the loop's spanning and equivalence point path weights.
+    if(HasSpPath) {
+      // Optimization: if the loop trip count is smaller than the number of
+      // iterations between migration points, elide loop instrumentation.
+      size_t NumIters = SpanningWeight->numIters(CurCapThresh);
+      unsigned TripCount = getTripCount(L);
+      assert(NumIters > 0 && "Should have added a migration point");
+      if(TripCount && TripCount < NumIters) {
+        DEBUG(dbgs() << "  Eliding loop instrumentation, loop trip count: "
+                     << std::to_string(TripCount) << "\n");
+        NumIters = TripCount;
+      }
+      else if(L->getLoopDepth() > 1 &&
+              NumIters > (size_t)MaxItersPerMigPoint) {
+        DEBUG(dbgs() << "  Eliding loop instrumentation (exceeded maximum "
+                        " iterations per migration point), loop trip count: "
+                     << std::to_string(MaxItersPerMigPoint) << "\n");
+        NumIters = MaxItersPerMigPoint;
+      }
+      // TODO mark first insertion point in loop header as migration point,
+      // propagate whether we added a migration point as return value
+      else TransformLoops.insert(L);
+      LWI.setLoopSpanningPathWeight(SpanningWeight, NumIters);
+
+      DEBUG(
+        dbgs() << "  Loop spanning path weight: " << SpanningWeight->toString()
+               << ", " << std::to_string(NumIters)
+               << " iteration(s)/migration point\n";
+      );
+    }
+    if(HasEqPointPath) {
+      LWI.setLoopEqPointPathWeight(EqPointWeight);
+
+      DEBUG(dbgs() << "  Loop equivalence point path weight: "
+                   << EqPointWeight->toString() << "\n");
+    }
+
+    DEBUG(dbgs() << "\n    Calculating loop exit weights");
+
+    // Calculate the weight of the loop at every exit point.  Maintain separate
+    // spanning & equivalence point path exit weights so that if we avoid
+    // instrumenting loop boundaries in traverseLoopEntry() we can update the
+    // exit weights.
+    L->getExitingBlocks(ExitBlocks);
+    for(auto Exit : ExitBlocks) {
+      HasSpPath = HasEqPointPath = false;
+      SpanningWeight.reset(getZeroWeight());
+      EqPointWeight.reset(getZeroWeight());
+
+      LP->getPathsThroughBlock(L, Exit, Paths);
+      for(auto Path : Paths) {
+        WeightPtr PathWeight(traversePathUntilExit(Path, Exit, ActuallyEqPoint));
+        if(Path->isSpanningPath() && !ActuallyEqPoint) {
+          HasSpPath = true;
+          SpanningWeight->max(PathWeight);
+        }
+        else {
+          HasEqPointPath = true;
+          EqPointWeight->max(PathWeight);
+        }
+      }
+
+      if(HasSpPath) LWI.setExitSpanningPathWeight(Exit, SpanningWeight);
+      if(HasEqPointPath) LWI.setExitEqPointPathWeight(Exit, EqPointWeight);
+    }
+  }
+
+  /// Analyze loop nests & mark locations for migration points.  Return whether
+  /// or not a migration point was added.
+  bool traverseLoopNest(const std::vector<BasicBlock *> &SCC) {
+    bool AddedMigPoint = false;
+    Loop *L;
+    LoopNest Nest;
+
+    // Get outermost loop in loop nest & enumerate the rest of the nest
+    assert(LI->getLoopFor(SCC.front()) && "No loop in SCC");
+    L = LI->getLoopFor(SCC.front());
+    while(L->getLoopDepth() != 1) L = L->getParentLoop();
+    LoopPathUtilities::populateLoopNest(L, Nest);
+
+    DEBUG(
+      dbgs() << " + Analyzing loop nest at "; L->getStartLoc().print(dbgs());
+      dbgs() << " with " << std::to_string(Nest.size()) << " loop(s)\n\n";
+    );
+
+    for(auto CurLoop : Nest) {
+      // Note: if migration points were added to any sub-loo(s) then we need to
+      // re-run the LoopPaths analysis on the outer loop.
+      // TODO this is a little overzealous, sibling loops (e.g., 2 sub-loops at
+      // the same depth and contained in the same outer loop) can cause
+      // unnecessary re-enumerations.
+      if(traverseLoop(CurLoop) || AddedMigPoint) {
+        AddedMigPoint = true;
+        LP->rerunOnLoop(CurLoop);
+      }
+
+      // TODO if we are instrumenting the loop header, re-enumerate paths
+      calculateLoopExitWeights(CurLoop);
+
+      DEBUG(dbgs() << "\n  Loop analysis: "
+                   << LoopWeights.at(CurLoop).toString() << "\n");
+    }
+
+    DEBUG(dbgs() << " - Finished loop nest\n");
+
+    return AddedMigPoint;
+  }
+
+  /// Analyze the function's body to add migration points.  Return whether or
+  /// not a migration point was added.
+  bool analyzeFunctionBody(Function &F) {
+    std::set<const Loop *> MarkedLoops;
+    bool AddedMigPoint = false;
+    Loop *BlockLoop;
+
+    // Analyze & mark paths through loop nests
+    DEBUG(dbgs() << "\n-> Analyzing loop nests <-\n");
+    for(scc_iterator<Function *> SCC = scc_begin(&F), E = scc_end(&F);
+        SCC != E; ++SCC)
+      if(SCC.hasLoop()) AddedMigPoint |= traverseLoopNest(*SCC);
+
+    // Analyze the rest of the function body
+    DEBUG(dbgs() << "\n-> Analyzing the rest of the function body <-\n");
+    ReversePostOrderTraversal<Function *> RPOT(&F);
+    for(auto BB = RPOT.begin(), BE = RPOT.end(); BB != BE; ++BB) {
+      BlockLoop = LI->getLoopFor(*BB);
+      if(!BlockLoop) {
+        WeightPtr PredWeight(getInitialWeight(*BB));
+        AddedMigPoint |= traverseBlock(*BB, PredWeight);
+        BBWeights[*BB] = std::move(PredWeight);
+      }
+      else if(!MarkedLoops.count(BlockLoop)) {
+        // Block is in a loop, analyze & mark loop's boundaries
+        AddedMigPoint |= traverseLoopEntry(BlockLoop);
+        MarkedLoops.insert(BlockLoop);
+      }
+    }
+
+    // Finally, determine if we should add a migration point at exit block(s).
+    for(Function::iterator BB = F.begin(), E = F.end(); BB != E; BB++) {
+      if(isa<ReturnInst>(BB->getTerminator())) {
+        assert(!LI->getLoopFor(BB) && "Returning inside a loop");
+        assert(BBWeights.count(BB) && "Missing block weight");
+        const BasicBlockWeightInfo &BBWI = BBWeights[BB].BlockWeight;
+        if(!BBWI.BlockWeight->underPercentOfThreshold(CurRetThresh)) {
+          DEBUG(dbgs() << " - Not under weight threshold, marking return\n");
+          markAsMigPoint(BB->getTerminator(), true, true);
+        }
+      }
+    }
+
+    return AddedMigPoint;
+  }
+
+  //===--------------------------------------------------------------------===//
+  // Instrumentation implementation
+  //===--------------------------------------------------------------------===//
+
+  /// Either find an existing induction variable (and its stride), or create
+  /// one for a loop.
+  Instruction *getInductionVariable(Loop *L, size_t &Stride) {
+    BasicBlock *H = L->getHeader();
+    const SCEVAddRecExpr *Induct;
+    const SCEVConstant *StrideExpr;
+    Type *IVTy;
+
+    // Search for the induction variable & it's stride
+    for(BasicBlock::iterator I = H->begin(), E = H->end(); I != E; ++I) {
+      if(!isa<PHINode>(*I)) break;
+      IVTy = I->getType();
+      if(IVTy->isPointerTy() || !SE->isSCEVable(IVTy)) continue;
+      Induct = dyn_cast<SCEVAddRecExpr>(SE->getSCEV(I));
+      if(Induct && isa<SCEVConstant>(Induct->getStepRecurrence(*SE))) {
+        StrideExpr = cast<SCEVConstant>(Induct->getStepRecurrence(*SE));
+        Stride = std::abs(StrideExpr->getValue()->getSExtValue());
+
+        // TODO if stride != 1, it's hard to ensure we're hitting a migration
+        // point every n iterations unless we know the *exact* number at which
+        // it starts.  For example, if stride = 4 but we start at 1, the
+        // migration point checking logic has to add checks for 1, 5, 9, etc.
+        // It's easier to just create our own induction variable.
+        if(Stride != 1) continue;
+
+        DEBUG(dbgs() << "Found induction variable with loop stride of "
+                     << std::to_string(Stride) << ":"; I->print(dbgs());
+              dbgs() << "\n");
+
+        return I;
+      }
+    }
+
+    DEBUG(dbgs() << "No induction variable, adding'migpoint.iv."
+                 << std::to_string(NumIVsAdded) << "' to the loop\n");
+
+    LLVMContext &C = H->getContext();
+    Type *Int32Ty = Type::getInt32Ty(C);
+    IRBuilder<> PhiBuilder(H->getFirstInsertionPt());
+    PHINode *IV = PhiBuilder.CreatePHI(Int32Ty, 0,
+      "migpoint.iv." + std::to_string(NumIVsAdded++));
+    Constant *One = ConstantInt::get(Int32Ty, 1, 0),
+             *Zero = ConstantInt::get(Int32Ty, 0, 0);
+    for(auto Pred : predecessors(H)) {
+      IRBuilder<> AddRecBuilder(Pred->getTerminator());
+      if(L->contains(Pred)) { // Backedge
+        Value *RecVal = AddRecBuilder.CreateAdd(IV, One);
+        IV->addIncoming(RecVal, Pred);
+      }
+      else IV->addIncoming(Zero, Pred);
+    }
+
+    Stride = 1;
+    return IV;
+  }
+
+  /// Round a value down to the nearest power of 2.  Stolen/modified from
+  /// https://graphics.stanford.edu/~seander/bithacks.html#RoundUpPowerOf2
+  unsigned roundDownPowerOf2(unsigned Count) {
+    unsigned Starting = Count;
+    Count--;
+    Count |= Count >> 1;
+    Count |= Count >> 2;
+    Count |= Count >> 4;
+    Count |= Count >> 8;
+    Count |= Count >> 16;
+    Count++;
+
+    // If we're already a power of 2, then the above math returns the same
+    // value.  Otherwise, we've rounded *up* to the nearest power of 2 and need
+    // to divide by 2 to round *down*.
+    if(Count != Starting) Count >>= 1;
+    return Count;
+  }
+
+  /// Transform a loop header so that migration points (and any concomitant
+  /// costs) are only experienced every nth iteration, based on weight metrics
+  void transformLoopHeader(Loop *L) {
+    BasicBlock *Header = L->getHeader();
+    size_t ItersPerMigPoint, Stride = 0, InstrStride;
+
+    // If the first instruction has already been marked due to heuristics that
+    // bookend libc I/O & big memory operations, then there's nothing to do.
+    Instruction *First = Header->getFirstInsertionPt();
+    if(isMarkedForInstrumentation(First)) return;
+
+    DEBUG(dbgs() << "+ Instrumenting "; L->dump());
+
+    assert(LoopWeights.count(L) && "No loop analysis");
+    ItersPerMigPoint = LoopWeights.at(L).getItersPerMigPoint();
+
+    if(ItersPerMigPoint > 1) {
+      BasicBlock *NewSuccBB, *MigPointBB;
+      Instruction *IV = getInductionVariable(L, Stride);
+
+      IntegerType *IVType = cast<IntegerType>(IV->getType());
+      Function *CurF = Header->getParent();
+      LLVMContext &C = Header->getContext();
+
+      // Create new successor for all instructions after migration point
+      NewSuccBB = Header->splitBasicBlock(Header->getFirstInsertionPt(),
+        "l.postmigpoint" + std::to_string(LoopsTransformed));
+
+      // Create new block for migration point
+      MigPointBB = BasicBlock::Create(C,
+        "l.migpoint" + std::to_string(LoopsTransformed), CurF, NewSuccBB);
+      IRBuilder<> MigPointWorker(MigPointBB);
+      Instruction *Br = cast<Instruction>(MigPointWorker.CreateBr(NewSuccBB));
+      markAsMigPoint(Br, true, true);
+
+      // Add check and branch to migration point only every nth iteration.
+      // Round down to nearest power-of-2, which allows us to use a simple
+      // bitmask for migration point check (URem instructions can cause
+      // non-negligible overhead in tight-loops).
+      IRBuilder<> Worker(Header->getTerminator());
+      InstrStride = roundDownPowerOf2(ItersPerMigPoint * Stride) - 1;
+      assert(InstrStride > 0 && "Invalid migration point stride");
+      Constant *N = ConstantInt::get(IVType, InstrStride, IVType->getSignBit()),
+               *Zero = ConstantInt::get(IVType, 0, IVType->getSignBit());
+      Value *Rem = Worker.CreateAnd(IV, N);
+      Value *Cmp = Worker.CreateICmpEQ(Rem, Zero);
+      Worker.CreateCondBr(Cmp, MigPointBB, NewSuccBB);
+      Header->getTerminator()->eraseFromParent();
+
+      DEBUG(dbgs() << "Instrumenting to hit migration point every "
+                   << std::to_string(InstrStride + 1) << " iterations\n");
+    }
+    else {
+      DEBUG(dbgs() << "Instrumenting to hit migration point every iteration\n");
+      markAsMigPoint(Header->getFirstInsertionPt(), true, true);
+    }
+  }
+
+  /// Insert migration points & HTM instrumentation for instructions.
+  void transformLoopHeaders(Function &F) {
+    DEBUG(dbgs() << "\n-> Transforming loop headers <-\n");
+    for(auto Loop : TransformLoops) {
+      transformLoopHeader(Loop);
+      LoopsTransformed++;
+    }
+  }
+};
+
+} /* end anonymous namespace */
+
+char SelectMigrationPoints::ID = 0;
+
+INITIALIZE_PASS_BEGIN(SelectMigrationPoints, "select-migration-points",
+                      "Select migration points locations", true, false)
+INITIALIZE_PASS_DEPENDENCY(LoopInfoWrapperPass)
+INITIALIZE_PASS_DEPENDENCY(EnumerateLoopPaths)
+INITIALIZE_PASS_DEPENDENCY(ScalarEvolution)
+INITIALIZE_PASS_END(SelectMigrationPoints, "select-migration-points",
+                    "Select migration points locations", true, false)
+
+const StringSet<> SelectMigrationPoints::LibcIO = {
+  "fopen", "freopen", "fclose", "fflush", "fwide",
+  "setbuf", "setvbuf", "fread", "fwrite",
+  "fgetc", "getc", "fgets", "fputc", "putc", "fputs",
+  "getchar", "gets", "putchar", "puts", "ungetc",
+  "fgetwc", "getwc", "fgetws", "fputwc", "putwc", "fputws",
+  "getwchar", "putwchar", "ungetwc",
+  "scanf", "fscanf", "vscanf", "vfscanf",
+  "printf", "fprintf", "vprintf", "vfprintf",
+  "wscanf", "fwscanf", "vwscanf", "vfwscanf",
+  "wprintf", "fwprintf", "vwprintf", "vfwprintf",
+  "ftell", "fgetpos", "fseek", "fsetpos", "rewind",
+  "clearerr", "feof", "ferror", "perror",
+  "remove", "rename", "tmpfile", "tmpnam",
+  "__isoc99_fscanf", "exit"
+};
+
+namespace llvm {
+  FunctionPass *createSelectMigrationPointsPass()
+  { return new SelectMigrationPoints(); }
+}
+
Index: lib/CodeGen/AsmPrinter/AsmPrinter.cpp
===================================================================
--- lib/CodeGen/AsmPrinter/AsmPrinter.cpp	(revision 320332)
+++ lib/CodeGen/AsmPrinter/AsmPrinter.cpp	(working copy)
@@ -1160,6 +1160,41 @@
   return CurExceptionSym;
 }
 
+MachineInstr *AsmPrinter::FindStackMap(MachineBasicBlock &MBB,
+                                       MachineInstr *MI) const {
+  MachineBasicBlock::instr_iterator i, ie;
+  for(i = MI->getNextNode(), ie = MBB.instr_end();
+      i != ie;
+      i = i->getNextNode()) {
+    if(i->getOpcode() == TargetOpcode::STACKMAP)
+      return &*i;
+    else if(i->isCall())
+      break;
+  }
+
+  // Call site without a stackmap implies that either the call was generated by
+  // the backend or the LLVM bitcode was never instrumented by the StackInfo
+  // pass.  This is not necessarily an error!
+  return nullptr;
+}
+
+bool AsmPrinter::TagCallSites(MachineFunction &MF) {
+  bool tagged = false;
+  for(auto MBB = MF.begin(), MBBE = MF.end(); MBB != MBBE; MBB++) {
+    for(auto MI = MBB->instr_begin(), MIE = MBB->instr_end(); MI != MIE; MI++) {
+      if(MI->isCall() && !MI->isPseudo()) {
+        MachineInstr *SMI = FindStackMap(*MBB, &*MI);
+        if(SMI != nullptr) {
+          MBB->remove(SMI);
+          MI = MBB->insert(++MI, SMI);
+          tagged = true;
+        }
+      }
+    }
+  }
+  return tagged;
+}
+
 void AsmPrinter::SetupMachineFunction(MachineFunction &MF) {
   this->MF = &MF;
   // Get the function symbol.
Index: lib/CodeGen/CMakeLists.txt
===================================================================
--- lib/CodeGen/CMakeLists.txt	(revision 320332)
+++ lib/CodeGen/CMakeLists.txt	(working copy)
@@ -111,6 +111,8 @@
   StackSlotColoring.cpp
   StackMapLivenessAnalysis.cpp
   StackMaps.cpp
+  StackTransformMetadata.cpp
+  StackTransformTypes.cpp
   StatepointExampleGC.cpp
   TailDuplication.cpp
   TargetFrameLoweringImpl.cpp
@@ -122,6 +124,7 @@
   TargetSchedule.cpp
   TwoAddressInstructionPass.cpp
   UnreachableBlockElim.cpp
+  UnwindInfo.cpp
   VirtRegMap.cpp
   WinEHPrepare.cpp
 
Index: lib/CodeGen/CodeGen.cpp
===================================================================
--- lib/CodeGen/CodeGen.cpp	(revision 320332)
+++ lib/CodeGen/CodeGen.cpp	(working copy)
@@ -68,6 +68,7 @@
   initializeStackMapLivenessPass(Registry);
   initializeStackProtectorPass(Registry);
   initializeStackSlotColoringPass(Registry);
+  initializeStackTransformMetadataPass(Registry);
   initializeTailDuplicatePassPass(Registry);
   initializeTargetPassConfigPass(Registry);
   initializeTwoAddressInstructionPassPass(Registry);
Index: lib/CodeGen/LLVMTargetMachine.cpp
===================================================================
--- lib/CodeGen/LLVMTargetMachine.cpp	(revision 320332)
+++ lib/CodeGen/LLVMTargetMachine.cpp	(working copy)
@@ -42,6 +42,28 @@
 EnableFastISelOption("fast-isel", cl::Hidden,
   cl::desc("Enable the \"fast\" instruction selector"));
 
+// Popcorn-specific IR-level instrumentation
+enum PopcornInstrumentation {
+  none, // No instrumentation
+  metadata, // Only generate migration metadata, don't insert migration points
+  migpoints, // Only add migration points, don't generate rewriting metadata
+  migration, // Add migration points & generate rewriting metadata for migration
+  libc // Generate rewriting metadata for libc thread start functions
+};
+
+static cl::opt<PopcornInstrumentation> PopcornInstrument("popcorn-instrument",
+  cl::desc("Add Popcorn-specific instrumentation to applications"),
+  cl::init(none),
+  cl::values(
+    clEnumVal(none, "No instrumentation (default)"),
+    clEnumVal(metadata, "Only generate migration metadata (no migration points"),
+    clEnumVal(migpoints, "Only add migration points (no migration metadata)"),
+    clEnumVal(migration, "Add migration points & generate migration metadata"),
+    clEnumVal(libc, "Instrument libc thread start functions for migration"),
+    NULL
+  )
+);
+
 void LLVMTargetMachine::initAsmInfo() {
   MRI = TheTarget.createMCRegInfo(getTargetTriple().str());
   MII = TheTarget.createMCInstrInfo();
@@ -105,8 +127,36 @@
   // Set PassConfig options provided by TargetMachine.
   PassConfig->setDisableVerify(DisableVerify);
 
+  /// Popcorn compiler - multi-ISA binary configurations.  Requires that IR
+  /// passed to backends is identical, save for certain architecture-specific
+  /// quirks like atomic operations or intrinsics
+  if(PopcornInstrument != PopcornInstrumentation::none) {
+    TM->setArchIROptLevel(CodeGenOpt::None);
+
+    switch(PopcornInstrument) {
+    case PopcornInstrumentation::metadata:
+      PassConfig->setAddStackMaps(true);
+      break;
+    case PopcornInstrumentation::migpoints:
+      PassConfig->setAddMigrationPoints(true);
+      break;
+    case PopcornInstrumentation::migration:
+      PassConfig->setAddMigrationPoints(true);
+      PassConfig->setAddStackMaps(true);
+      break;
+    case PopcornInstrumentation::libc:
+      PassConfig->setAddLibcStackMaps(true);
+      break;
+    default:
+      llvm_unreachable("Invalid instrumentation type");
+      break;
+    }
+  }
+
   PM.add(PassConfig);
 
+  PassConfig->addPopcornPasses();
+
   PassConfig->addIRPasses();
 
   PassConfig->addCodeGenPrepare();
Index: lib/CodeGen/MachineFunction.cpp
===================================================================
--- lib/CodeGen/MachineFunction.cpp	(revision 320332)
+++ lib/CodeGen/MachineFunction.cpp	(working copy)
@@ -253,6 +253,15 @@
                                MMO->getBaseAlignment());
 }
 
+/// Is a register caller-saved?
+bool MachineFunction::isCallerSaved(unsigned Reg) const {
+  assert(TargetRegisterInfo::isPhysicalRegister(Reg) && "Invalid register");
+  CallingConv::ID CC = Fn->getCallingConv();
+  const uint32_t *Mask =
+    RegInfo->getTargetRegisterInfo()->getCallPreservedMask(*this, CC);
+  return !((Mask[Reg / 32] >> Reg % 32) & 1);
+}
+
 MachineInstr::mmo_iterator
 MachineFunction::allocateMemRefsArray(unsigned long Num) {
   return Allocator.Allocate<MachineMemOperand *>(Num);
@@ -482,6 +491,164 @@
                                Twine(getFunctionNumber()) + "$pb");
 }
 
+/// Get the value for key K in Map M, or create a new one if it doesn't exist.
+template<typename Key, typename Val, typename Map>
+static Val &getOrCreateMapping(const Key *K, Map &M) {
+  typename Map::iterator It;
+  if((It = M.find(K)) == M.end())
+    It = M.insert(std::pair<const Key *, Val>(K, Val())).first;
+  return It->second;
+}
+
+/// Add an IR/architecture-specific location mapping for a stackmap operand
+void MachineFunction::addSMOpLocation(const CallInst *SM,
+                                      const Value *Val,
+                                      const MachineLiveLoc &MLL) {
+  auto containsLoc = [](const MachineLiveLocs &Vals,
+                        const MachineLiveLoc &Cur) -> bool {
+    for(auto &LV : Vals) if(Cur == *LV) return true;
+    return false;
+  };
+
+  assert(SM && "Invalid stackmap");
+  assert(Val && "Invalid stackmap operand");
+
+  IRToMachineLocs &IRMap =
+    getOrCreateMapping<Instruction,
+                       IRToMachineLocs,
+                       InstToOperands>(SM, SMDuplicateLocs);
+  MachineLiveLocs &Vals =
+    getOrCreateMapping<Value,
+                       MachineLiveLocs,
+                       IRToMachineLocs>(Val, IRMap);
+  if(!containsLoc(Vals, MLL))
+    Vals.push_back(MachineLiveLocPtr(MLL.copy()));
+}
+
+/// Add an IR/architecture-specific location mapping for a stackmap operand
+void MachineFunction::addSMOpLocation(const CallInst *SM,
+                                      unsigned Op,
+                                      const MachineLiveLoc &MLL) {
+  assert(SM && "Invalid stackmap");
+  assert(Op < SM->getNumArgOperands() && "Invalid operand number");
+  addSMOpLocation(SM, SM->getArgOperand(Op), MLL);
+}
+
+
+/// Add an architecture-specific live value & location for a stackmap
+void MachineFunction::addSMArchSpecificLocation(const CallInst *SM,
+                                                const MachineLiveLoc &MLL,
+                                                const MachineLiveVal &MLV) {
+  auto containsLoc = [](const ArchLiveValues &Vals,
+                        const MachineLiveLoc &Cur) -> bool {
+    for(auto &LV : Vals) if(Cur == *LV.first) return true;
+    return false;
+  };
+
+  assert(SM && "Invalid stackmap");
+  ArchLiveValues &Vals =
+    getOrCreateMapping<Instruction,
+                       ArchLiveValues,
+                       InstToArchLiveValues>(SM, SMArchSpecificLocs);
+  if(!containsLoc(Vals, MLL))
+    Vals.push_back(ArchLiveValue(MachineLiveLocPtr(MLL.copy()),
+                                 MachineLiveValPtr(MLV.copy())));
+}
+
+/// Update stack slot references to new indexes after stack slot coloring
+void
+MachineFunction::updateSMStackSlotRefs(SmallDenseMap<int, int, 16> &Changes) {
+
+  if(Changes.size()) {
+    DEBUG(dbgs() << "Updating stackmap stack slot references\n";);
+
+    int SS;
+    SmallDenseMap<int, int, 16>::iterator Change;
+
+    // Iterate over all operand duplicate locations
+    for(auto &InstIt : SMDuplicateLocs) {
+      for(auto &IRIt : InstIt.second) {
+        for(auto &MLL : IRIt.second) {
+          if(MLL->isStackSlot()) {
+            MachineLiveStackSlot &LSS = (MachineLiveStackSlot &)*MLL;
+            SS = LSS.getStackSlot();
+            Change = Changes.find(SS);
+            if(Change != Changes.end())
+              LSS.setStackSlot(Change->second);
+          }
+        }
+      }
+    }
+
+    // Iterate over all architecture-specific locations.
+    // Note: both the destination & source location can be stack slots
+    for(auto &InstIt : SMArchSpecificLocs) {
+      for(auto &MLL : InstIt.second) {
+        if(MLL.first->isStackSlot()) {
+          MachineLiveStackSlot &LSS = (MachineLiveStackSlot &)*MLL.first;
+          SS = LSS.getStackSlot();
+          Change = Changes.find(SS);
+          if(Change != Changes.end())
+            LSS.setStackSlot(Change->second);
+        }
+
+        if(MLL.second->isStackObject()) {
+          MachineStackObject &MSO = (MachineStackObject &)*MLL.second;
+          SS = MSO.getIndex();
+          Change = Changes.find(SS);
+          if(Change != Changes.end())
+            MSO.setIndex(Change->second);
+        }
+      }
+    }
+  }
+}
+
+/// Are there any architecture-specific locations for operand Val in stackmap
+/// SM?
+bool
+MachineFunction::hasSMOpLocations(const CallInst *SM, const Value *Val) const {
+  assert(SM && "Invalid stackmap");
+  assert(Val && "Invalid stackmap operand");
+  InstToOperands::const_iterator InstIt;
+  if((InstIt = SMDuplicateLocs.find(SM)) != SMDuplicateLocs.end())
+    return InstIt->second.find(Val) != InstIt->second.end();
+  return false;
+}
+
+/// Are there any architecture-specific locations for stackmap SM?
+bool
+MachineFunction::hasSMArchSpecificLocations(const llvm::CallInst *SM) const {
+  assert(SM && "Invalid stackmap");
+  return SMArchSpecificLocs.find(SM) != SMArchSpecificLocs.end();
+}
+
+/// Return the architecture-specific locations for a stackmap operand.
+const MachineLiveLocs &
+MachineFunction::getSMOpLocations(const CallInst *SM,
+                                  const Value *Val ) const {
+  assert(SM && "Invalid stackmap");
+  assert(Val && "Invalid stackmap operand");
+  InstToOperands::const_iterator InstIt = SMDuplicateLocs.find(SM);
+  assert(InstIt != SMDuplicateLocs.end() &&
+         "No duplicate locations for stackmap");
+  IRToMachineLocs::const_iterator IRIt = InstIt->second.find(Val);
+  assert(IRIt != InstIt->second.end() &&
+         "No duplicate locations for stackmap operand");
+  return IRIt->second;
+}
+
+/// Return the architecture-specific locations for a stackmap that are not
+/// associated with any operand.
+const ArchLiveValues &
+MachineFunction::getSMArchSpecificLocations(const CallInst *SM) const {
+  assert(SM && "Invalid stackmap");
+  InstToArchLiveValues::const_iterator InstIt = SMArchSpecificLocs.find(SM);
+  assert(InstIt != SMArchSpecificLocs.end() &&
+         "No architecture-specific locations for stackmap");
+  return InstIt->second;
+}
+
 //===----------------------------------------------------------------------===//
 //  MachineFrameInfo implementation
 //===----------------------------------------------------------------------===//
Index: lib/CodeGen/Passes.cpp
===================================================================
--- lib/CodeGen/Passes.cpp	(revision 320332)
+++ lib/CodeGen/Passes.cpp	(working copy)
@@ -217,7 +217,9 @@
     : ImmutablePass(ID), PM(&pm), StartBefore(nullptr), StartAfter(nullptr),
       StopAfter(nullptr), Started(true), Stopped(false),
       AddingMachinePasses(false), TM(tm), Impl(nullptr), Initialized(false),
-      DisableVerify(false), EnableTailMerge(true), EnableShrinkWrap(false) {
+      DisableVerify(false), EnableTailMerge(true), EnableShrinkWrap(false),
+      AddMigrationPoints(false), AddStackMaps(false),
+      AddLibcStackMaps(false) {
 
   Impl = new PassConfigImpl();
 
@@ -391,7 +393,9 @@
     addPass(createVerifierPass());
 
   // Run loop strength reduction before anything else.
-  if (getOptLevel() != CodeGenOpt::None && !DisableLSR) {
+  if (getOptLevel() != CodeGenOpt::None &&
+      getArchIROptLevel() != CodeGenOpt::None &&
+      !DisableLSR) {
     addPass(createLoopStrengthReducePass());
     if (PrintLSR)
       addPass(createPrintFunctionPass(dbgs(), "\n\n*** Code after LSR ***\n"));
@@ -406,10 +410,14 @@
   addPass(createUnreachableBlockEliminationPass());
 
   // Prepare expensive constants for SelectionDAG.
-  if (getOptLevel() != CodeGenOpt::None && !DisableConstantHoisting)
+  if (getOptLevel() != CodeGenOpt::None &&
+      getArchIROptLevel() != CodeGenOpt::None &&
+      !DisableConstantHoisting)
     addPass(createConstantHoistingPass());
 
-  if (getOptLevel() != CodeGenOpt::None && !DisablePartialLibcallInlining)
+  if (getOptLevel() != CodeGenOpt::None &&
+      getArchIROptLevel() != CodeGenOpt::None &&
+      !DisablePartialLibcallInlining)
     addPass(createPartiallyInlineLibCallsPass());
 }
 
@@ -449,11 +457,32 @@
 /// Add pass to prepare the LLVM IR for code generation. This should be done
 /// before exception handling preparation passes.
 void TargetPassConfig::addCodeGenPrepare() {
-  if (getOptLevel() != CodeGenOpt::None && !DisableCGP)
+  if (getOptLevel() != CodeGenOpt::None &&
+      getArchIROptLevel() != CodeGenOpt::None &&
+      !DisableCGP)
     addPass(createCodeGenPreparePass(TM));
   addPass(createRewriteSymbolsPass());
 }
 
+void TargetPassConfig::addPopcornPasses() {
+  assert(!(AddStackMaps && AddLibcStackMaps) &&
+    "Cannot add both InsertStackMapsPass and LibcStackMapsPass");
+  assert(!(AddMigrationPoints && AddLibcStackMaps) &&
+    "Should not be instrumenting libc with extra migration points");
+
+  // Add pass to instrument IR with equivalence points, which are implemented
+  // various ways depending on other command-line arguments
+  if(AddMigrationPoints) addPass(createMigrationPointsPass());
+
+  // Add pass to instrument IR with stackmap instructions, which get lowered to
+  // metadata needed for Popcorn's stack transformation
+  if(AddStackMaps) addPass(createInsertStackMapsPass());
+
+  // Similar to creatInsertStackMaps pass, but only instruments libc thread
+  // start functions
+  if(AddLibcStackMaps) addPass(createLibcStackMapsPass());
+}
+
 /// Add common passes that perform LLVM IR to IR transforms in preparation for
 /// instruction selection.
 void TargetPassConfig::addISelPrepare() {
@@ -754,6 +783,10 @@
   // Allow targets to change the register assignments before rewriting.
   addPreRewrite();
 
+  // Gather additional stack transformation metadata before rewriting virtual
+  // registers
+  addPass(&StackTransformMetadataID);
+
   // Finally rewrite virtual registers.
   addPass(&VirtRegRewriterID);
 
@@ -766,7 +799,8 @@
   // Run post-ra machine LICM to hoist reloads / remats.
   //
   // FIXME: can this move into MachineLateOptimization?
-  addPass(&PostRAMachineLICMID);
+  if(getOptLevel() != CodeGenOpt::None)
+    addPass(&PostRAMachineLICMID);
 }
 
 //===---------------------------------------------------------------------===//
Index: lib/CodeGen/RegAllocFast.cpp
===================================================================
--- lib/CodeGen/RegAllocFast.cpp	(revision 320332)
+++ lib/CodeGen/RegAllocFast.cpp	(working copy)
@@ -1078,6 +1078,13 @@
 /// runOnMachineFunction - Register allocate the whole function
 ///
 bool RAFast::runOnMachineFunction(MachineFunction &Fn) {
+  // TODO the fast register allocator behaves poorly for stackmaps with lots
+  // of operands, and since it doesn't use the VirtRegRewriter pass we can't
+  // capture correct stackmap operand locations
+  if(Fn.getFrameInfo()->hasStackMap())
+    llvm_unreachable("Fast register allocator not supported for stack"
+                     "transformation");
+
   DEBUG(dbgs() << "********** FAST REGISTER ALLOCATION **********\n"
                << "********** Function: " << Fn.getName() << '\n');
   MF = &Fn;
Index: lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp	(revision 320332)
+++ lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp	(working copy)
@@ -886,6 +886,9 @@
   case ISD::SRL:
   case ISD::ROTL:
   case ISD::ROTR: Res = PromoteIntOp_Shift(N); break;
+
+  case (uint16_t)~TargetOpcode::STACKMAP:
+    Res = PromoteIntOp_STACKMAP(N, OpNo); break;
   }
 
   // If the result is null, the sub-method took care of registering results etc.
@@ -1131,6 +1134,27 @@
                                 SExtPromotedInteger(N->getOperand(0))), 0);
 }
 
+SDValue DAGTypeLegalizer::PromoteIntOp_STACKMAP(SDNode *N, unsigned OpNo) {
+  std::vector<SDValue> Ops(N->getNumOperands());
+  SDLoc dl(N);
+
+  for(unsigned i = 0; i < N->getNumOperands(); i++) {
+    if(i == OpNo) {
+      if(N->getOperand(i).getValueType() == MVT::i1 ||
+         N->getOperand(i).getValueType() == MVT::i8 ||
+         N->getOperand(i).getValueType() == MVT::i16) {
+        if(N->getOperand(i).getOpcode() == ISD::TRUNCATE)
+          Ops[i] = N->getOperand(i)->getOperand(0);
+        else
+          Ops[i] = DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i32, N->getOperand(i));
+      }
+    }
+    else Ops[i] = N->getOperand(i);
+  }
+
+  return SDValue(DAG.UpdateNodeOperands(N, Ops), 0);
+}
+
 SDValue DAGTypeLegalizer::PromoteIntOp_STORE(StoreSDNode *N, unsigned OpNo){
   assert(ISD::isUNINDEXEDStore(N) && "Indexed store during type legalization!");
   SDValue Ch = N->getChain(), Ptr = N->getBasePtr();
Index: lib/CodeGen/SelectionDAG/LegalizeTypes.h
===================================================================
--- lib/CodeGen/SelectionDAG/LegalizeTypes.h	(revision 320332)
+++ lib/CodeGen/SelectionDAG/LegalizeTypes.h	(working copy)
@@ -288,6 +288,7 @@
   SDValue PromoteIntOp_Shift(SDNode *N);
   SDValue PromoteIntOp_SIGN_EXTEND(SDNode *N);
   SDValue PromoteIntOp_SINT_TO_FP(SDNode *N);
+  SDValue PromoteIntOp_STACKMAP(SDNode *N, unsigned OpNo);
   SDValue PromoteIntOp_STORE(StoreSDNode *N, unsigned OpNo);
   SDValue PromoteIntOp_TRUNCATE(SDNode *N);
   SDValue PromoteIntOp_UINT_TO_FP(SDNode *N);
Index: lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp	(revision 320332)
+++ lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp	(working copy)
@@ -6542,6 +6542,43 @@
   Callee = getValue(CI.getCalledValue());
   NullPtr = DAG.getIntPtrConstant(0, DL, true);
 
+  // Popcorn: we need to insert the stackmap directly after the call
+  // instruction, so grab the chain from the CALLSEQ_END node.  Note that
+  // although we're moving the stackmap between the call & return value
+  // copy-out, the stackmap doesn't generate code so we're not clobbering
+  // return values.
+  Chain = getRoot();
+  SDNode *CSE = Chain.getNode(), *RetCopyOut = nullptr;
+  while(CSE && CSE->getOpcode() != ISD::CALLSEQ_END) {
+    RetCopyOut = CSE;
+    CSE = CSE->getGluedNode();
+  }
+
+  // It's possible the function call that induced this stackmap to be generated
+  // got lowered directly to a machine instruction, which can lead to weird
+  // cases like trying to glue this stackmap to another.  This causes the
+  // instruction scheduler to choke, so avoid that situation.
+  // TODO why does gluing 2 stackmaps cause it to die?  It complains about
+  // NumLiveRegs already being zero for releasing call dependencies in
+  // ScheduleDAGRRList.cpp:759
+  if(CSE && CSE->getOpcode() == ISD::CALLSEQ_END) {
+    SDNode *Check = CSE;
+    do {
+      if(Check->isMachineOpcode() &&
+         Check->getMachineOpcode() == TargetOpcode::STACKMAP) {
+        CSE = nullptr;
+        break;
+      }
+    } while((Check = Check->getGluedNode()));
+  }
+
+  if(CSE && CSE->getOpcode() == ISD::CALLSEQ_END) Chain = SDValue(CSE, 0);
+  else {
+    DEBUG(dbgs() << "WARNING: no call node for: "; Chain->dump());
+    RetCopyOut = nullptr;
+  }
+  assert(Chain.getSimpleValueType() == MVT::Other && "Invalid chain");
+
   // The stackmap intrinsic only records the live variables (the arguemnts
   // passed to it) and emits NOPS (if requested). Unlike the patchpoint
   // intrinsic, this won't be lowered to a function call. This means we don't
@@ -6552,9 +6589,26 @@
   // chain, flag = STACKMAP(id, nbytes, ..., chain, flag)
   // chain, flag = CALLSEQ_END(chain, 0, 0, flag)
   //
-  Chain = DAG.getCALLSEQ_START(getRoot(), NullPtr, DL);
+  Chain = DAG.getCALLSEQ_START(Chain, NullPtr, DL);
   InFlag = Chain.getValue(1);
 
+  // Popcorn: add glue operand to CALLSEQ_START to tie the stackmap to the
+  // call sequence (note that it already produces a glue value).
+  if(CSE) {
+    SmallVector<EVT, 2> VTs(Chain->value_begin(), Chain->value_end());
+    SDVTList VTList = DAG.getVTList(VTs);
+    for(auto &Op : Chain->ops()) Ops.push_back(Op);
+    assert(Ops.back().getSimpleValueType() != MVT::Glue &&
+           "Already have glue");
+    Ops.push_back(SDValue(CSE, 1));
+    assert(Ops.back().getSimpleValueType() == MVT::Glue &&
+           "Invalid glue operand");
+    Chain.setNode(DAG.MorphNodeTo(Chain.getNode(), Chain->getOpcode(),
+                                  VTList, Ops));
+    InFlag = Chain.getValue(1);
+    Ops.clear();
+  }
+
   // Add the <id> and <numBytes> constants.
   SDValue IDVal = getValue(CI.getOperand(PatchPointOpers::IDPos));
   Ops.push_back(DAG.getTargetConstant(
@@ -6581,9 +6635,30 @@
   InFlag = Chain.getValue(1);
 
   Chain = DAG.getCALLSEQ_END(Chain, NullPtr, NullPtr, InFlag, DL);
+  InFlag = Chain.getValue(1);
 
   // Stackmaps don't generate values, so nothing goes into the NodeMap.
 
+  // Popcorn: fix-up the glue so the return-value copy outs happen after
+  // the stackmap's CALLSEQ_END.  Note that this is *required*, otherwise
+  // the backend won't correctly track physical register outputs from call.
+  if(RetCopyOut) {
+    unsigned NOpts = RetCopyOut->getNumOperands();
+    Ops.clear();
+    for(size_t i = 0; i < NOpts; i++) {
+      if(RetCopyOut->getOperand(i).getSimpleValueType() == MVT::Other)
+        Ops.push_back(Chain);
+      else if(RetCopyOut->getOperand(i).getSimpleValueType() == MVT::Glue)
+        Ops.push_back(InFlag);
+      else Ops.push_back(RetCopyOut->getOperand(i));
+    }
+    RetCopyOut = DAG.UpdateNodeOperands(RetCopyOut, Ops);
+
+    while(RetCopyOut->getGluedUser()) RetCopyOut = RetCopyOut->getGluedUser();
+    Chain = SDValue(RetCopyOut, NOpts - 2);
+    assert(Chain.getSimpleValueType() == MVT::Other && "Invalid chain");
+  }
+
   // Set the root to the target-lowered call chain.
   DAG.setRoot(Chain);
 
Index: lib/CodeGen/StackColoring.cpp
===================================================================
--- lib/CodeGen/StackColoring.cpp	(revision 320332)
+++ lib/CodeGen/StackColoring.cpp	(working copy)
@@ -713,7 +713,7 @@
 
   // This is a simple greedy algorithm for merging allocas. First, sort the
   // slots, placing the largest slots first. Next, perform an n^2 scan and look
-  // for disjoint slots. When you find disjoint slots, merge the samller one
+  // for disjoint slots. When you find disjoint slots, merge the smaller one
   // into the bigger one and update the live interval. Remove the small alloca
   // and continue.
 
Index: lib/CodeGen/StackMaps.cpp
===================================================================
--- lib/CodeGen/StackMaps.cpp	(revision 320332)
+++ lib/CodeGen/StackMaps.cpp	(working copy)
@@ -12,13 +12,18 @@
 #include "llvm/CodeGen/MachineFrameInfo.h"
 #include "llvm/CodeGen/MachineFunction.h"
 #include "llvm/CodeGen/MachineInstr.h"
+#include "llvm/CodeGen/UnwindInfo.h"
 #include "llvm/IR/DataLayout.h"
+#include "llvm/IR/DiagnosticInfo.h"
+#include "llvm/IR/IntrinsicInst.h"
 #include "llvm/MC/MCContext.h"
 #include "llvm/MC/MCExpr.h"
 #include "llvm/MC/MCObjectFileInfo.h"
 #include "llvm/MC/MCSectionMachO.h"
 #include "llvm/MC/MCStreamer.h"
+#include "llvm/MC/MCSymbol.h"
 #include "llvm/Support/CommandLine.h"
+#include "llvm/Target/TargetFrameLowering.h"
 #include "llvm/Target/TargetMachine.h"
 #include "llvm/Target/TargetOpcodes.h"
 #include "llvm/Target/TargetRegisterInfo.h"
@@ -29,6 +34,16 @@
 
 #define DEBUG_TYPE "stackmaps"
 
+#define TYPE_AND_FLAGS(type, ptr, alloca, dup, temp) \
+  ((uint8_t)type) << 4 | ((uint8_t)ptr) << 3 | \
+  ((uint8_t)alloca) << 2 | ((uint8_t)dup << 1) | \
+  ((uint8_t)temp)
+
+#define ARCH_TYPE_AND_FLAGS(type, ptr) ((uint8_t)type) << 4 | ((uint8_t)ptr)
+
+#define ARCH_OP_TYPE(inst, gen, op) \
+  ((uint8_t)inst) << 4 | ((uint8_t)gen) << 3 | (uint8_t)op
+
 static cl::opt<int> StackMapVersion(
     "stackmap-version", cl::init(1),
     cl::desc("Specify the stackmap encoding version (default = 1)"));
@@ -84,32 +99,154 @@
   return (unsigned)RegNum;
 }
 
+/// If the instruction is simply casting a pointer to another type, return
+/// the value used as the source of the cast.  This is required because allocas
+/// may be cast to different pointer types (which appear as non-allocas) but
+/// may still be represented by a direct memory reference in the stackmap.
+static inline const Value *getPointerCastSrc(const Value *Inst) {
+  assert(Inst->getType()->isPointerTy() && "Not a pointer type");
+
+  if(isa<BitCastInst>(Inst)) {
+    // Ensure that we're casting to another pointer type
+    const BitCastInst *BC = cast<BitCastInst>(Inst);
+    if(!BC->getSrcTy()->isPointerTy()) return nullptr;
+    else return BC->getOperand(0);
+  }
+  else if(isa<GetElementPtrInst>(Inst)) {
+    // Ensure that all indexes are 0, meaning we're only referencing the start
+    // of the storage location
+    const GetElementPtrInst *GEP = cast<GetElementPtrInst>(Inst);
+    GetElementPtrInst::const_op_iterator it, e;
+    for(it = GEP->idx_begin(), e = GEP->idx_end(); it != e; it++) {
+      if(!isa<ConstantInt>(it->get())) return nullptr;
+      const ConstantInt *Idx = cast<ConstantInt>(it->get());
+      if(!Idx->isZero()) return nullptr;
+    }
+    return GEP->getOperand(0);
+  }
+  else return nullptr;
+}
+
+/// Get pointer typing information for a stackmap operand
+void StackMaps::getPointerInfo(const Value *Op, const DataLayout &DL,
+                               bool &isPtr, bool &isAlloca,
+                               unsigned &AllocaSize) const {
+  isPtr = false;
+  isAlloca = false;
+  AllocaSize = 0;
+
+  assert(Op != nullptr && "Invalid stackmap operand");
+  Type *Ty = Op->getType();
+  if(Ty->isPointerTy())
+  {
+    // Walk through cast operations that potentially hide allocas
+    while(!isa<AllocaInst>(Op) && (Op = getPointerCastSrc(Op)));
+    if(Op && isa<AllocaInst>(Op)) {
+      PointerType *PTy = cast<PointerType>(Ty);
+      assert(PTy->getElementType()->isSized() && "Alloca of unknown size?");
+      isPtr = PTy->getElementType()->isPointerTy();
+      isAlloca = true;
+      AllocaSize = DL.getTypeAllocSize(PTy->getElementType());
+    }
+    else isPtr = true;
+  }
+}
+
+/// Get stackmap information for register location
+void StackMaps::getRegLocation(unsigned Phys,
+                               unsigned &Dwarf,
+                               unsigned &Offset) const {
+  const TargetRegisterInfo *TRI = AP.MF->getSubtarget().getRegisterInfo();
+  assert(!TRI->isVirtualRegister(Phys) &&
+         "Virtual registers should have been rewritten by now");
+  Offset = 0;
+  Dwarf = getDwarfRegNum(Phys, TRI);
+  unsigned LLVMRegNum = TRI->getLLVMRegNum(Dwarf, false);
+  unsigned SubRegIdx = TRI->getSubRegIndex(LLVMRegNum, Phys);
+  if(SubRegIdx)
+    Offset = TRI->getSubRegIdxOffset(SubRegIdx);
+}
+
+/// Add duplicate target-specific locations for a stackmap operand
+void StackMaps::addDuplicateLocs(const CallInst *StackMap, const Value *Oper,
+                                 LocationVec &Locs, unsigned Size, bool Ptr,
+                                 bool Alloca, unsigned AllocaSize) const {
+  unsigned DwarfRegNum, Offset;
+  int FrameOff;
+
+  if(AP.MF->hasSMOpLocations(StackMap, Oper)) {
+    const MachineLiveLocs &Dups = AP.MF->getSMOpLocations(StackMap, Oper);
+    const TargetRegisterInfo *TRI = AP.MF->getSubtarget().getRegisterInfo();
+
+    for(const MachineLiveLocPtr &LL : Dups) {
+      if(LL->isReg()) {
+        const MachineLiveReg &MR = (const MachineLiveReg &)*LL;
+        getRegLocation(MR.getReg(), DwarfRegNum, Offset);
+
+        Locs.emplace_back(Location::Register, Size, DwarfRegNum, Offset,
+                          Ptr, Alloca, true, false, AllocaSize);
+      }
+      else if(LL->isStackAddr()) {
+        MachineLiveStackAddr &MLSA = (MachineLiveStackAddr &)*LL;
+        FrameOff = MLSA.calcAndGetRegOffset(AP, DwarfRegNum);
+
+        Locs.emplace_back(Location::Indirect, Size,
+          getDwarfRegNum(DwarfRegNum, TRI),
+          FrameOff, Ptr, Alloca, true, false, AllocaSize);
+      }
+      else llvm_unreachable("Unknown machine live location type");
+    }
+  }
+}
+
 MachineInstr::const_mop_iterator
 StackMaps::parseOperand(MachineInstr::const_mop_iterator MOI,
                         MachineInstr::const_mop_iterator MOE, LocationVec &Locs,
-                        LiveOutVec &LiveOuts) const {
+                        LiveOutVec &LiveOuts, User::const_op_iterator &Op) const {
+  bool isPtr, isAlloca, isTemporary = false;
+  unsigned AllocaSize;
+  auto &DL = AP.MF->getDataLayout();
   const TargetRegisterInfo *TRI = AP.MF->getSubtarget().getRegisterInfo();
+  const CallInst *IRSM = cast<CallInst>(Op->getUser());
+  const Value *IROp = Op->get();
+  int64_t TemporaryOffset = 0;
+  getPointerInfo(IROp, DL, isPtr, isAlloca, AllocaSize);
+
   if (MOI->isImm()) {
+    // Peel off temporary value metadata
+    if (MOI->getImm() == StackMaps::TemporaryOp) {
+      isTemporary = true;
+      AllocaSize = (++MOI)->getImm();
+      TemporaryOffset = (++MOI)->getImm();
+      ++MOI;
+    }
+
     switch (MOI->getImm()) {
     default:
       llvm_unreachable("Unrecognized operand type.");
     case StackMaps::DirectMemRefOp: {
-      unsigned Size = AP.TM.getDataLayout()->getPointerSizeInBits();
+      assert((isAlloca || isTemporary) &&
+             "Did not find alloca value for direct memory reference");
+      unsigned Size = DL.getPointerSizeInBits();
       assert((Size % 8) == 0 && "Need pointer size in bytes.");
       Size /= 8;
       unsigned Reg = (++MOI)->getReg();
-      int64_t Imm = (++MOI)->getImm();
-      Locs.emplace_back(StackMaps::Location::Direct, Size,
-                        getDwarfRegNum(Reg, TRI), Imm);
+      int64_t Imm = (++MOI)->getImm() + TemporaryOffset;
+      Locs.emplace_back(Location::Direct, Size, getDwarfRegNum(Reg, TRI), Imm,
+                        isPtr, true, false, isTemporary, AllocaSize);
       break;
     }
     case StackMaps::IndirectMemRefOp: {
       int64_t Size = (++MOI)->getImm();
       assert(Size > 0 && "Need a valid size for indirect memory locations.");
+      Size = DL.getTypeAllocSize(IROp->getType());
       unsigned Reg = (++MOI)->getReg();
       int64_t Imm = (++MOI)->getImm();
-      Locs.emplace_back(StackMaps::Location::Indirect, Size,
-                        getDwarfRegNum(Reg, TRI), Imm);
+      // Note: getPointerInfo() may have found a suitable alloca for this
+      // operand, but the backend didn't actually turn it into one.
+      Locs.emplace_back(Location::Indirect, (unsigned)Size,
+                        getDwarfRegNum(Reg, TRI), Imm, isPtr, false, false,
+                        isTemporary, 0);
       break;
     }
     case StackMaps::ConstantOp: {
@@ -116,17 +253,26 @@
       ++MOI;
       assert(MOI->isImm() && "Expected constant operand.");
       int64_t Imm = MOI->getImm();
-      Locs.emplace_back(Location::Constant, sizeof(int64_t), 0, Imm);
+      // Note: getPointerInfo() may have found a suitable alloca for this
+      // operand, but the backend didn't actually turn it into one.
+      Locs.emplace_back(Location::Constant, sizeof(int64_t), 0, Imm,
+                        isPtr, false, false, isTemporary, 0);
       break;
     }
     }
+    // Note: we shouldn't have alternate locations -- constants aren't stored
+    // anywhere, and stack slots should be either allocas (which shouldn't have
+    // alternate locations) or register spill locations (handled below in the
+    // register path)
+    assert(!AP.MF->hasSMOpLocations(IRSM, IROp) &&
+           "Unhandled duplicate locations");
+    ++Op;
     return ++MOI;
   }
 
   // The physical register number will ultimately be encoded as a DWARF regno.
   // The stack map also records the size of a spill slot that can hold the
-  // register content. (The runtime can track the actual size of the data type
-  // if it needs to.)
+  // register content, accurate to the actual size of the data type.
   if (MOI->isReg()) {
     // Skip implicit registers (this includes our scratch registers)
     if (MOI->isImplicit())
@@ -134,17 +280,18 @@
 
     assert(TargetRegisterInfo::isPhysicalRegister(MOI->getReg()) &&
            "Virtreg operands should have been rewritten before now.");
-    const TargetRegisterClass *RC = TRI->getMinimalPhysRegClass(MOI->getReg());
     assert(!MOI->getSubReg() && "Physical subreg still around.");
 
-    unsigned Offset = 0;
-    unsigned DwarfRegNum = getDwarfRegNum(MOI->getReg(), TRI);
-    unsigned LLVMRegNum = TRI->getLLVMRegNum(DwarfRegNum, false);
-    unsigned SubRegIdx = TRI->getSubRegIndex(LLVMRegNum, MOI->getReg());
-    if (SubRegIdx)
-      Offset = TRI->getSubRegIdxOffset(SubRegIdx);
+    size_t ValSize = DL.getTypeAllocSize(IROp->getType());
+    unsigned Offset, DwarfRegNum;
+    getRegLocation(MOI->getReg(), DwarfRegNum, Offset);
 
-    Locs.emplace_back(Location::Register, RC->getSize(), DwarfRegNum, Offset);
+    // Note: getPointerInfo() may have found a suitable alloca for this
+    // operand, but the backend didn't actually turn it into one.
+    Locs.emplace_back(Location::Register, ValSize, DwarfRegNum, Offset,
+                      isPtr, false, false, isTemporary, 0);
+    addDuplicateLocs(IRSM, IROp, Locs, ValSize, isPtr, false, 0);
+    ++Op;
     return ++MOI;
   }
 
@@ -161,6 +308,7 @@
   for (const auto &CSI : CSInfos) {
     const LocationVec &CSLocs = CSI.Locations;
     const LiveOutVec &LiveOuts = CSI.LiveOuts;
+    const ArchValues &Values = CSI.Vals;
 
     OS << WSMP << "callsite " << CSI.ID << "\n";
     OS << WSMP << "  has " << CSLocs.size() << " locations\n";
@@ -194,7 +342,7 @@
           OS << TRI->getName(Loc.Reg);
         else
           OS << Loc.Reg;
-        OS << "+" << Loc.Offset;
+        OS << " + " << Loc.Offset;
         break;
       case Location::Constant:
         OS << "Constant " << Loc.Offset;
@@ -203,8 +351,16 @@
         OS << "Constant Index " << Loc.Offset;
         break;
       }
-      OS << "\t[encoding: .byte " << Loc.Type << ", .byte " << Loc.Size
-         << ", .short " << Loc.Reg << ", .int " << Loc.Offset << "]\n";
+      OS << ", pointer? " << Loc.Ptr << ", alloca? " << Loc.Alloca
+         << ", duplicate? " << Loc.Duplicate
+         << ", temporary? " << Loc.Temporary;
+
+      unsigned TypeAndFlags = TYPE_AND_FLAGS(Loc.Type, Loc.Ptr, Loc.Alloca,
+                                             Loc.Duplicate, Loc.Temporary);
+
+      OS << "\t[encoding: .byte " << TypeAndFlags << ", .byte " << Loc.Size
+         << ", .short " << Loc.Reg << ", .int " << Loc.Offset
+         << ", .uint " << Loc.AllocaSize << "]\n";
       Idx++;
     }
 
@@ -221,6 +377,87 @@
          << LO.Size << "]\n";
       Idx++;
     }
+
+    OS << WSMP << "\thas " << Values.size() << " arch-specific live values\n";
+
+    Idx = 0;
+    for (const auto &V : Values) {
+      const Location &Loc = V.first;
+      const Operation &Op = V.second;
+
+      OS << WSMP << "\t\tArch-Val " << Idx << ": ";
+      switch(Loc.Type) {
+      case Location::Register:
+        OS << "Register ";
+        if (TRI)
+          OS << TRI->getName(Loc.Reg);
+        else
+          OS << Loc.Reg;
+        break;
+      case Location::Indirect:
+        OS << "Indirect ";
+        if (TRI)
+          OS << TRI->getName(Loc.Reg);
+        else
+          OS << Loc.Reg;
+        if (Loc.Offset)
+          OS << " + " << Loc.Offset;
+        break;
+      default:
+        OS << "<Unknown live value type>";
+        break;
+      }
+
+      OS << ", " << ValueGenInst::getInstName(Op.InstType) << " ";
+      switch(Op.OperandType) {
+      case Location::Register:
+        OS << "register ";
+        if (TRI)
+          OS << TRI->getName(Op.DwarfReg);
+        else
+          OS << Op.DwarfReg;
+        break;
+      case Location::Direct:
+        OS << "value stored at register ";
+        if (TRI)
+          OS << TRI->getName(Op.DwarfReg);
+        else
+          OS << Op.DwarfReg;
+        if (Op.Constant)
+          OS << " + " << Op.Constant;
+        break;
+      case Location::Indirect:
+        OS << "register";
+        if (TRI)
+          OS << TRI->getName(Op.DwarfReg);
+        else
+          OS << Op.DwarfReg;
+        if (Op.Constant)
+          OS << " + " << Op.Constant;
+        break;
+      case Location::Constant:
+        if(Op.isSymbol)
+          OS << "address of " << Op.Symbol->getName();
+        else {
+          OS << "immediate ";
+          OS.write_hex(Op.Constant);
+        }
+        break;
+      default:
+        OS << "<Unknown operand type>";
+        break;
+      }
+
+      unsigned TypeAndFlags = ARCH_TYPE_AND_FLAGS(Loc.Type, Loc.Ptr);
+      unsigned OpType = ARCH_OP_TYPE(Op.InstType,
+                                     Op.isGenerated,
+                                     Op.OperandType);
+      OS << "\t[encoding: .byte " << TypeAndFlags << ", .byte " << Loc.Size
+         << ", .short " << Loc.Reg << ", .int " << Loc.Offset
+         << ", .byte " << OpType << ", .byte " << Op.Size << ", .short "
+         << Op.DwarfReg << ", .int64 " << (Op.isSymbol ? 0 : Op.Constant)
+         << "]\n";
+    }
   }
 }
 
@@ -277,6 +514,145 @@
   return LiveOuts;
 }
 
+/// Convert a list of instructions used to generate an architecture-specific
+/// live value into multiple individual records.
+void StackMaps::genArchValsFromInsts(ArchValues &AV,
+                                     Location &Loc,
+                                     const MachineLiveVal &MLV) {
+  assert(MLV.isGenerated() && "Invalid live value type");
+
+  unsigned PtrSize = AP.MF->getDataLayout().getPointerSizeInBits() / 8;
+  const MachineGeneratedVal &MGV = (const MachineGeneratedVal &)MLV;
+  const ValueGenInstList &I = MGV.getInstructions();
+  const TargetRegisterInfo *TRI = AP.MF->getSubtarget().getRegisterInfo();
+  const TargetRegisterClass *RC;
+  Operation Op;
+  Op.isGenerated = true;
+
+  for(auto &Inst : I) {
+    const RegInstructionBase *RI;
+    const ImmInstructionBase *II;
+    const RefInstruction *RefI;
+
+    Op.DwarfReg = 0;
+    Op.Constant = 0;
+    Op.isSymbol = false;
+    Op.Symbol = nullptr;
+
+    Op.InstType = Inst->type();
+    switch(Inst->opType()) {
+    case ValueGenInst::OpType::Register:
+      RI = (const RegInstructionBase *)Inst.get();
+      assert(TRI->isPhysicalRegister(RI->getReg()) &&
+             "Virtual should have been converted to physical register");
+      RC = TRI->getMinimalPhysRegClass(RI->getReg());
+      Op.OperandType = Location::Register;
+      Op.Size = RC->getSize();
+      Op.DwarfReg = getDwarfRegNum(RI->getReg(), TRI);
+      break;
+    case ValueGenInst::OpType::Immediate:
+      II = (const ImmInstructionBase *)Inst.get();
+      Op.OperandType = Location::Constant;
+      Op.Size = II->getImmSize();
+      Op.Constant = II->getImm();
+      break;
+    case ValueGenInst::OpType::Reference:
+      RefI = (const RefInstruction *)Inst.get();
+      Op.OperandType = Location::Constant;
+      Op.Size = PtrSize;
+      Op.isSymbol = true;
+      Op.Symbol = RefI->getReference(AP);
+      break;
+    default: llvm_unreachable("Invalid operand type"); break;
+    }
+    AV.emplace_back(ArchValue(Loc, Op));
+  }
+}
+
+/// Add architecture-specific locations for the stackmap
+void StackMaps::addArchLiveVals(const CallInst *SM, ArchValues &AV) {
+  unsigned Offset, DwarfReg;
+  unsigned PtrSize = AP.MF->getDataLayout().getPointerSizeInBits() / 8;
+  const MachineFrameInfo *MFI = AP.MF->getFrameInfo();
+  const TargetRegisterInfo *TRI = AP.MF->getSubtarget().getRegisterInfo();
+
+  if(AP.MF->hasSMArchSpecificLocations(SM)) {
+    const ArchLiveValues &Vals = AP.MF->getSMArchSpecificLocations(SM);
+
+    for(auto &Val : Vals) {
+      Location Loc;
+      Operation Op;
+
+      Loc.Ptr = Val.second->isPtr();
+      Loc.Alloca = false;
+      Loc.Duplicate = false;
+      Loc.AllocaSize = 0;
+
+      // Parse the location
+      if(Val.first->isReg()) {
+        const MachineLiveReg &MR = (const MachineLiveReg &)*Val.first;
+        const TargetRegisterClass *RC =
+          TRI->getMinimalPhysRegClass(MR.getReg());
+        getRegLocation(MR.getReg(), DwarfReg, Offset);
+
+        Loc.Type = Location::Register;
+        Loc.Size = RC->getSize();
+        Loc.Reg = DwarfReg;
+        Loc.Offset = Offset;
+      }
+      else if(Val.first->isStackAddr()) {
+        MachineLiveStackAddr &MLSA = (MachineLiveStackAddr &)*Val.first;
+
+        Loc.Type = Location::Indirect;
+        Loc.Size = MLSA.getSize(AP);
+        Loc.Offset = MLSA.calcAndGetRegOffset(AP, DwarfReg);
+        Loc.Reg = getDwarfRegNum(DwarfReg, TRI);
+      }
+      else llvm_unreachable("Invalid architecture-specific live value");
+
+      // Parse the operation
+      Op.InstType = ValueGenInst::Set;
+      Op.isGenerated = false;
+      if(Val.second->isReference()) {
+        const MachineReference &MR = (const MachineReference &)*Val.second;
+        if(MR.isLoad()) Op.InstType = ValueGenInst::Load64;
+        Op.OperandType = Location::Constant;
+        Op.Size = PtrSize;
+        Op.isSymbol = true;
+        Op.Symbol = MR.getReference(AP);
+        AV.emplace_back(ArchValue(Loc, Op));
+      }
+      else if(Val.second->isStackObject()) {
+        const MachineStackObject &MSO = (const MachineStackObject &)*Val.second;
+        if(MSO.isLoad()) { // Loading a value from a stack slot
+          Op.OperandType = Location::Direct;
+          if(MSO.isCommonObject()) Op.Size = PtrSize;
+          else Op.Size = MFI->getObjectSize(MSO.getIndex());
+        }
+        else { // Generating a reference to a stack slot
+          Op.OperandType = Location::Indirect;
+          Op.Size = PtrSize;
+        }
+        Op.Constant = MSO.getOffsetFromReg(AP, DwarfReg);
+        Op.DwarfReg = getDwarfRegNum(DwarfReg, TRI);
+        Op.isSymbol = false;
+        AV.emplace_back(ArchValue(Loc, Op));
+      }
+      else if(Val.second->isImm()) {
+        const MachineImmediate &MI = (const MachineImmediate &)*Val.second;
+        Op.OperandType = Location::Constant;
+        Op.Size = MI.getSize();
+        Op.Constant = MI.getValue();
+        Op.isSymbol = false;
+        AV.emplace_back(ArchValue(Loc, Op));
+      }
+      else if(Val.second->isGenerated())
+        genArchValsFromInsts(AV, Loc, *Val.second);
+      else llvm_unreachable("Invalid architecture-specific live value");
+    }
+  }
+}
+
 void StackMaps::recordStackMapOpers(const MachineInstr &MI, uint64_t ID,
                                     MachineInstr::const_mop_iterator MOI,
                                     MachineInstr::const_mop_iterator MOE,
@@ -285,21 +661,46 @@
   MCContext &OutContext = AP.OutStreamer->getContext();
   MCSymbol *MILabel = OutContext.createTempSymbol();
   AP.OutStreamer->EmitLabel(MILabel);
+  User::const_op_iterator Op = nullptr;
 
   LocationVec Locations;
   LiveOutVec LiveOuts;
+  ArchValues Constants;
 
   if (recordResult) {
     assert(PatchPointOpers(&MI).hasDef() && "Stackmap has no return value.");
     parseOperand(MI.operands_begin(), std::next(MI.operands_begin()), Locations,
-                 LiveOuts);
+                 LiveOuts, Op);
   }
 
+  // Find the IR stackmap instruction which corresponds to MI so we can emit
+  // type information along with the value's location
+  const BasicBlock *BB = MI.getParent()->getBasicBlock();
+  const IntrinsicInst *IRSM = nullptr;
+  const std::string SMName("llvm.experimental.stackmap");
+  for(auto BBI = BB->begin(), BBE = BB->end(); BBI != BBE; BBI++)
+  {
+    const IntrinsicInst *II;
+    if((II = dyn_cast<IntrinsicInst>(&*BBI)) &&
+       II->getCalledFunction()->getName() == SMName &&
+       cast<ConstantInt>(II->getArgOperand(0))->getZExtValue() == ID)
+    {
+      IRSM = cast<IntrinsicInst>(&*BBI);
+      break;
+    }
+  }
+  assert(IRSM && "Could not find associated stackmap instruction");
+
   // Parse operands.
+  Op = std::next(IRSM->op_begin(), 2);
   while (MOI != MOE) {
-    MOI = parseOperand(MOI, MOE, Locations, LiveOuts);
+    MOI = parseOperand(MOI, MOE, Locations, LiveOuts, Op);
   }
+  assert(Op == (IRSM->op_end() - 1) && "did not lower all stackmap operands");
 
+  // Add architecture-specific live values
+  addArchLiveVals(IRSM, Constants);
+
   // Move large constants into the constant pool.
   for (auto &Loc : Locations) {
     // Constants are encoded as sign-extended integers.
@@ -323,12 +724,21 @@
 
   // Create an expression to calculate the offset of the callsite from function
   // entry.
-  const MCExpr *CSOffsetExpr = MCBinaryExpr::createSub(
+  // TODO for Popcorn, we actually want the return address of the call
+  // instruction to which this stackmap is attached.  However some backend
+  // writers, in their infinite wisdom, decided to abstract multiple assembly
+  // instructions into a single machine IR instruction (*ahem* PowerPC *ahem*).
+  // Generate an expression to correct for this "feature".
+  int RAOffset = AP.getCanonicalReturnAddr(MI.getPrevNode());
+  const MCExpr *RAFixup = MCBinaryExpr::createSub(
       MCSymbolRefExpr::create(MILabel, OutContext),
+      MCConstantExpr::create(RAOffset, OutContext), OutContext);
+  const MCExpr *CSOffsetExpr = MCBinaryExpr::createSub(RAFixup,
       MCSymbolRefExpr::create(AP.CurrentFnSymForSize, OutContext), OutContext);
 
-  CSInfos.emplace_back(CSOffsetExpr, ID, std::move(Locations),
-                       std::move(LiveOuts));
+  CSInfos.emplace_back(AP.CurrentFnSym, CSOffsetExpr, ID,
+                       std::move(Locations), std::move(LiveOuts),
+                       std::move(Constants));
 
   // Record the stack size of the current function.
   const MachineFrameInfo *MFI = AP.MF->getFrameInfo();
@@ -411,8 +821,11 @@
 /// StkSizeRecord[NumFunctions] {
 ///   uint64 : Function Address
 ///   uint64 : Stack Size
+///   uint32 : Number of Unwinding Entries
+///   uint32 : Offset into Unwinding Section
 /// }
-void StackMaps::emitFunctionFrameRecords(MCStreamer &OS) {
+void StackMaps::emitFunctionFrameRecords(MCStreamer &OS,
+                                         const UnwindInfo *UI) {
   // Function Frame records.
   DEBUG(dbgs() << WSMP << "functions:\n");
   for (auto const &FR : FnStackSize) {
@@ -420,6 +833,15 @@
                  << " frame size: " << FR.second);
     OS.EmitSymbolValue(FR.first, 8);
     OS.EmitIntValue(FR.second, 8);
+
+    if(UI) {
+      const UnwindInfo::FuncUnwindInfo &FUI = UI->getUnwindInfo(FR.first);
+      DEBUG(dbgs() << " unwind info start: " << FUI.SecOffset
+                   << " (" << FUI.NumUnwindRecord << " entries)\n");
+      OS.EmitIntValue(FUI.NumUnwindRecord, 4);
+      OS.EmitIntValue(FUI.SecOffset, 4);
+    }
+    else OS.EmitIntValue(0, 8);
   }
 }
 
@@ -439,14 +861,20 @@
 ///
 /// StkMapRecord[NumRecords] {
 ///   uint64 : PatchPoint ID
+///   uint32 : Index of Function Record
 ///   uint32 : Instruction Offset
 ///   uint16 : Reserved (record flags)
 ///   uint16 : NumLocations
 ///   Location[NumLocations] {
-///     uint8  : Register | Direct | Indirect | Constant | ConstantIndex
-///     uint8  : Size in Bytes
-///     uint16 : Dwarf RegNum
-///     int32  : Offset
+///     uint8 (4 bits) : Register | Direct | Indirect | Constant | ConstantIndex
+///     uint8 (1 bit)  : Is it a pointer?
+///     uint8 (1 bit)  : Is it an alloca?
+///     uint8 (1 bit)  : Is it a duplicate record for the same live value?
+///     uint8 (1 bit)  : Is it a temporary value created for the stackmap?
+///     uint8          : Size in Bytes
+///     uint16         : Dwarf RegNum
+///     int32          : Offset
+///     uint32         : Size of pointed-to alloca data
 ///   }
 ///   uint16 : Padding
 ///   uint16 : NumLiveOuts
@@ -455,6 +883,25 @@
 ///     uint8  : Reserved
 ///     uint8  : Size in Bytes
 ///   }
+///   uint16 : Padding
+///   uint16 : NumArchValues
+///   ArchValues[NumArchValues] {
+///     Location {
+///       uint8 (4 bits) : Register | Indirect
+///       uint8 (3 bits) : Padding
+///       uint8 (1 bit)  : Is it a pointer?
+///       uint8          : Size in Bytes
+///       uint16         : Dwarf RegNum
+///       int32          : Offset
+///     }
+///     Value {
+///       uint8_t (4 bits) : Instruction
+///       uint8_t (4 bits) : Register | Direct | Constant
+///       uint8_t          : Size
+///       uint16_t         : Dwarf RegNum
+///       int64_t          : Offset or Constant
+///     }
+///   }
 ///   uint32 : Padding (only if required to align to 8 byte)
 /// }
 ///
@@ -470,23 +917,29 @@
   for (const auto &CSI : CSInfos) {
     const LocationVec &CSLocs = CSI.Locations;
     const LiveOutVec &LiveOuts = CSI.LiveOuts;
+    const ArchValues &Values = CSI.Vals;
 
     // Verify stack map entry. It's better to communicate a problem to the
     // runtime than crash in case of in-process compilation. Currently, we do
     // simple overflow checks, but we may eventually communicate other
     // compilation errors this way.
-    if (CSLocs.size() > UINT16_MAX || LiveOuts.size() > UINT16_MAX) {
+    if (CSLocs.size() > UINT16_MAX || LiveOuts.size() > UINT16_MAX ||
+        Values.size() > UINT16_MAX) {
       OS.EmitIntValue(UINT64_MAX, 8); // Invalid ID.
+      OS.EmitIntValue(UINT32_MAX, 4); // Invalid index.
       OS.EmitValue(CSI.CSOffsetExpr, 4);
       OS.EmitIntValue(0, 2); // Reserved.
       OS.EmitIntValue(0, 2); // 0 locations.
       OS.EmitIntValue(0, 2); // padding.
       OS.EmitIntValue(0, 2); // 0 live-out registers.
+      OS.EmitIntValue(0, 2); // padding.
+      OS.EmitIntValue(0, 2); // 0 arch-specific values.
       OS.EmitIntValue(0, 4); // padding.
       continue;
     }
 
     OS.EmitIntValue(CSI.ID, 8);
+    OS.EmitIntValue(FnStackSize.find(CSI.Func) - FnStackSize.begin(), 4);
     OS.EmitValue(CSI.CSOffsetExpr, 4);
 
     // Reserved for flags.
@@ -494,10 +947,14 @@
     OS.EmitIntValue(CSLocs.size(), 2);
 
     for (const auto &Loc : CSLocs) {
-      OS.EmitIntValue(Loc.Type, 1);
+      uint8_t TypeAndFlags =
+        TYPE_AND_FLAGS(Loc.Type, Loc.Ptr, Loc.Alloca,
+                       Loc.Duplicate, Loc.Temporary);
+      OS.EmitIntValue(TypeAndFlags, 1);
       OS.EmitIntValue(Loc.Size, 1);
       OS.EmitIntValue(Loc.Reg, 2);
       OS.EmitIntValue(Loc.Offset, 4);
+      OS.EmitIntValue(Loc.AllocaSize, 4);
     }
 
     // Num live-out registers and padding to align to 4 byte.
@@ -509,6 +966,31 @@
       OS.EmitIntValue(0, 1);
       OS.EmitIntValue(LO.Size, 1);
     }
+
+    // Num arch-specific constants and padding to align to 4 bytes.
+    OS.EmitIntValue(0, 2);
+    OS.EmitIntValue(Values.size(), 2);
+
+    for (const auto &C : Values) {
+      const Location &Loc = C.first;
+      const Operation &Op = C.second;
+
+      uint8_t TypeAndFlags = ARCH_TYPE_AND_FLAGS(Loc.Type, Loc.Ptr);
+      OS.EmitIntValue(TypeAndFlags, 1);
+      OS.EmitIntValue(Loc.Size, 1);
+      OS.EmitIntValue(Loc.Reg, 2);
+      OS.EmitIntValue(Loc.Offset, 4);
+
+      uint8_t OpType = ARCH_OP_TYPE(Op.InstType,
+                                    Op.isGenerated,
+                                    Op.OperandType);
+      OS.EmitIntValue(OpType, 1);
+      OS.EmitIntValue(Op.Size, 1);
+      OS.EmitIntValue(Op.DwarfReg, 2);
+      if(Op.isSymbol) OS.EmitSymbolValue(Op.Symbol, 8);
+      else OS.EmitIntValue(Op.Constant, 8);
+    }
+
     // Emit alignment to 8 byte.
     OS.EmitValueToAlignment(8);
   }
@@ -515,7 +997,7 @@
 }
 
 /// Serialize the stackmap data.
-void StackMaps::serializeToStackMapSection() {
+void StackMaps::serializeToStackMapSection(const UnwindInfo *UI) {
   (void)WSMP;
   // Bail out if there's no stack map data.
   assert((!CSInfos.empty() || (CSInfos.empty() && ConstPool.empty())) &&
@@ -539,7 +1021,7 @@
   // Serialize data.
   DEBUG(dbgs() << "********** Stack Map Output **********\n");
   emitStackmapHeader(OS);
-  emitFunctionFrameRecords(OS);
+  emitFunctionFrameRecords(OS, UI);
   emitConstantPoolEntries(OS);
   emitCallsiteEntries(OS);
   OS.AddBlankLine();
Index: lib/CodeGen/StackSlotColoring.cpp
===================================================================
--- lib/CodeGen/StackSlotColoring.cpp	(revision 320332)
+++ lib/CodeGen/StackSlotColoring.cpp	(working copy)
@@ -278,6 +278,7 @@
   SmallVector<int, 16> SlotMapping(NumObjs, -1);
   SmallVector<float, 16> SlotWeights(NumObjs, 0.0);
   SmallVector<SmallVector<int, 4>, 16> RevMap(NumObjs);
+  SmallDenseMap<int, int, 16> SlotChanges;
   BitVector UsedColors(NumObjs);
 
   DEBUG(dbgs() << "Color spill slot intervals:\n");
@@ -292,7 +293,9 @@
     SlotWeights[NewSS] += li->weight;
     UsedColors.set(NewSS);
     Changed |= (SS != NewSS);
+    if(SS != NewSS) SlotChanges[SS] = NewSS;
   }
+  MF.updateSMStackSlotRefs(SlotChanges);
 
   DEBUG(dbgs() << "\nSpill slots after coloring:\n");
   for (unsigned i = 0, e = SSIntervals.size(); i != e; ++i) {
Index: lib/CodeGen/StackTransformMetadata.cpp
===================================================================
--- lib/CodeGen/StackTransformMetadata.cpp	(nonexistent)
+++ lib/CodeGen/StackTransformMetadata.cpp	(working copy)
@@ -0,0 +1,1384 @@
+//=== llvm/CodeGen/StackTransformMetadata.cpp - Stack Transformation Metadata ===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file accumulates additional data from machine functions needed to do
+// correct and complete stack transformation.
+//
+// Note: the dataflow analysis in this implementation assumes the ISA does not
+// allow memory-to-memory copies.
+//
+//===----------------------------------------------------------------------===//
+
+#include <queue>
+#include "llvm/CodeGen/LiveIntervalAnalysis.h"
+#include "llvm/CodeGen/LiveStackAnalysis.h"
+#include "llvm/CodeGen/MachineFrameInfo.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/MachineInstrBuilder.h"
+#include "llvm/CodeGen/MachineMemOperand.h"
+#include "llvm/CodeGen/MachineRegisterInfo.h"
+#include "llvm/CodeGen/Passes.h"
+#include "llvm/CodeGen/PseudoSourceValue.h"
+#include "llvm/CodeGen/StackMaps.h"
+#include "llvm/CodeGen/StackTransformTypes.h"
+#include "llvm/CodeGen/VirtRegMap.h"
+#include "llvm/IR/DiagnosticInfo.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/LLVMContext.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/Target/TargetInstrInfo.h"
+#include "llvm/Target/TargetValues.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "stacktransform"
+
+static cl::opt<bool>
+NoWarnings("no-sm-warn", cl::desc("Don't issue warnings about stackmaps"),
+           cl::init(false), cl::Hidden);
+
+//===----------------------------------------------------------------------===//
+//                          StackTransformMetadata
+//===----------------------------------------------------------------------===//
+//
+// Run analyses over machine functions (before virtual register rewriting) to
+// glean additional information about live values.  This analysis finds
+// duplicate locations for live values (including backing stack slots and other
+// registers) and architecture-specific live values that must be materialized.
+//
+//===----------------------------------------------------------------------===//
+namespace {
+class StackTransformMetadata : public MachineFunctionPass {
+
+  /* Types */
+
+  /// A bundle tying together a stackmap IR instruction, the generated stackmap
+  /// machine instruction and the call machine instruction that caused the
+  /// stackmap to be emitted in the IR, respectively
+  typedef std::tuple<const CallInst *,
+                     MachineInstr *,
+                     const MachineInstr *> SMInstBundle;
+
+  /// Getters for individual elements of instruction bundles
+  static inline const
+  CallInst *getIRSM(const SMInstBundle &B) { return std::get<0>(B); }
+  static inline
+  MachineInstr *getMISM(const SMInstBundle &B) { return std::get<1>(B); }
+  static inline const
+  MachineInstr *getMICall(const SMInstBundle &B) { return std::get<2>(B); }
+
+  /// A vector of IR values.  Used when mapping from registers/stack slots to
+  /// IR values.
+  typedef SmallVector<const Value *, 4> ValueVec;
+  typedef std::shared_ptr<ValueVec> ValueVecPtr;
+
+  /// Mapping between virtual registers and IR operands
+  typedef std::pair<unsigned, ValueVecPtr> RegValsPair;
+  typedef std::map<unsigned, ValueVecPtr> RegValsMap;
+
+  /// Mapping between stackmaps and virtual registers referenced by the stackmap
+  typedef std::pair<const MachineInstr *, RegValsMap> SMRegPair;
+  typedef std::map<const MachineInstr *, RegValsMap> SMRegMap;
+
+  /// Mapping between stack slots and IR operands
+  typedef std::pair<int, ValueVecPtr> StackValsPair;
+  typedef std::map<int, ValueVecPtr> StackValsMap;
+
+  /// Mapping between stackmaps and stack slots referenced by the stackmap
+  typedef std::pair<const MachineInstr *, StackValsMap> SMStackSlotPair;
+  typedef std::map<const MachineInstr *, StackValsMap> SMStackSlotMap;
+
+  /// A value's spill location
+  class CopyLoc {
+  public:
+    enum Type { NONE, VREG, STACK_LOAD, STACK_STORE };
+    unsigned Vreg;
+    const MachineInstr *Instr;
+    CopyLoc() : Vreg(VirtRegMap::NO_PHYS_REG), Instr(nullptr) {}
+    CopyLoc(unsigned Vreg, const MachineInstr *Instr) :
+      Vreg(Vreg), Instr(Instr) {}
+    virtual CopyLoc *copy() const = 0;
+    virtual ~CopyLoc() {}
+    virtual Type getType() const = 0;
+  };
+  typedef std::shared_ptr<CopyLoc> CopyLocPtr;
+
+  /// A spill to a stack slot
+  class StackCopyLoc : public CopyLoc {
+  public:
+    int StackSlot;
+    StackCopyLoc() : StackSlot(VirtRegMap::NO_STACK_SLOT) {}
+    StackCopyLoc(unsigned Vreg, int StackSlot, const MachineInstr *Instr) :
+      CopyLoc(Vreg, Instr), StackSlot(StackSlot) {}
+    virtual CopyLoc *copy() const = 0;
+    virtual Type getType() const = 0;
+  };
+
+  /// A load from a stack slot
+  class StackLoadLoc : public StackCopyLoc {
+  public:
+    StackLoadLoc() {}
+    StackLoadLoc(unsigned Vreg, int StackSlot, const MachineInstr *Instr) :
+      StackCopyLoc(Vreg, StackSlot, Instr) {}
+    virtual CopyLoc *copy() const
+    { return new StackLoadLoc(Vreg, StackSlot, Instr); }
+    virtual Type getType() const { return CopyLoc::STACK_LOAD; }
+  };
+
+  /// A store to a stack slot
+  class StackStoreLoc : public StackCopyLoc {
+  public:
+    StackStoreLoc() {}
+    StackStoreLoc(unsigned Vreg, int StackSlot, const MachineInstr *Instr) :
+      StackCopyLoc(Vreg, StackSlot, Instr) {}
+    virtual CopyLoc *copy() const
+    { return new StackStoreLoc(Vreg, StackSlot, Instr); }
+    virtual Type getType() const { return CopyLoc::STACK_STORE; }
+  };
+
+  /// A spill to another register
+  class RegCopyLoc : public CopyLoc {
+  public:
+    unsigned SrcVreg;
+    RegCopyLoc() : SrcVreg(VirtRegMap::NO_PHYS_REG) {}
+    RegCopyLoc(unsigned DefVreg, unsigned SrcVreg, const MachineInstr *Instr) :
+      CopyLoc(DefVreg, Instr), SrcVreg(SrcVreg) {}
+    virtual CopyLoc *copy() const
+    { return new RegCopyLoc(Vreg, SrcVreg, Instr); }
+    virtual Type getType() const { return CopyLoc::VREG; }
+  };
+
+  /// Mapping between stack slots and copy locations (e.g., load from or store
+  /// to the stack slot)
+  typedef SmallVector<CopyLocPtr, 8> CopyLocVec;
+  typedef std::shared_ptr<CopyLocVec> CopyLocVecPtr;
+  typedef std::pair<int, CopyLocVecPtr> StackSlotCopyPair;
+  typedef std::map<int, CopyLocVecPtr> StackSlotCopies;
+
+  /// A work item to analyze in dataflow analysis.  Can selectively enable
+  /// traversing definitions.
+  struct WorkItem {
+    WorkItem() : Vreg(0), TraverseDefs(false) {}
+    WorkItem(unsigned Vreg, bool TraverseDefs)
+      : Vreg(Vreg), TraverseDefs(TraverseDefs) {}
+
+    unsigned Vreg;
+    bool TraverseDefs;
+  };
+
+  /* Data */
+
+  /// LLVM-provided analysis & metadata
+  MachineFunction *MF;
+  const MachineFrameInfo *MFI;
+  const MachineRegisterInfo *MRI;
+  const TargetInstrInfo *TII;
+  const TargetRegisterInfo *TRI;
+  const TargetValues *TVG;
+  LiveIntervals *LI;
+  const LiveStacks *LS;
+  const SlotIndexes *Indexes;
+  const VirtRegMap *VRM;
+
+  /// Stackmap/call instructions, mapping of virtual registers & stack slots to
+  /// IR values, stack slots used in the function, list of instructions that
+  /// copy to/from the stack
+  SmallVector<SMInstBundle, 32> SM;
+  SMRegMap SMRegs;
+  SMStackSlotMap SMStackSlots;
+  SmallSet<int, 32> UsedSS;
+  StackSlotCopies SSCopies;
+
+  /* Functions */
+
+  // Reset the analysis for a new function
+  void reset() {
+    SM.clear();
+    SMRegs.clear();
+    SMStackSlots.clear();
+    UsedSS.clear();
+    SSCopies.clear();
+  }
+
+  /// Print information about a virtual register and it's associated IR value
+  void dumpReg(unsigned Reg, const Value *IRVal) const;
+
+  /// Print information about a stack slot and it's associated IR value
+  void dumpStackSlot(int SS, const Value *IRVal) const;
+
+  /// Analyze a machine instruction to see if a value is getting copied from
+  /// another location such as a stack slot or register.
+  CopyLocPtr getCopyLocation(const MachineInstr *MI) const;
+
+  /// Gather stackmap machine instructions, the IR instructions which generated
+  /// the stackmaps, and their associated call machine instructions.  Also,
+  /// find copies to/from stack slots (since there's no other mechanism to
+  /// find/traverse them).
+  void findStackmapsAndStackSlotCopies();
+
+  /// Find all virtual register/stack slot operands in a stackmap and collect
+  /// virtual register/stack slot <-> IR value mappings
+  void mapOpsToIR(const CallInst *IRSM, const MachineInstr *MISM);
+
+  /// Extend the live range for a register to include an instruction.
+  void updateRegisterLiveInterval(MachineOperand &Src,
+                                  const MachineInstr *Inst);
+
+  /// Rather than modifying the backend machinery to prevent hoisting code
+  /// between the stackmap and call site, unwind instructions in order to get
+  /// real live value locations at the function call.
+  bool unwindToCallSite(MachineInstr *SM, const MachineInstr *Call);
+
+  /// Is a virtual register live across the machine instruction?
+  /// Note: returns false if the MI is the last instruction for which the
+  /// virtual register is alive
+  bool isVregLiveAcrossInstr(unsigned Vreg, const MachineInstr *MI) const;
+
+  /// Is a stack slot live across the machine instruction?
+  /// Note: returns false if the MI is the last instruction for which the stack
+  /// slot is alive
+  bool isSSLiveAcrossInstr(int SS, const MachineInstr *MI) const;
+
+  /// Add duplicate location information for a virtual register.  Return true
+  /// if metadata was added, or false if the virtual register is not live
+  /// across the call instruction/stackmap.
+  bool addVregMetadata(unsigned Vreg,
+                       ValueVecPtr IRVals,
+                       const SMInstBundle &SM);
+
+  /// Add duplicate location information for a stack slot.  Return true if
+  /// metadata was added, or false if the stack slot is not live across the
+  /// call instruction/stackmap.
+  bool addSSMetadata(int SS, ValueVecPtr IRVals, const SMInstBundle &SM);
+
+  /// Search stack slot copies for additional virtual registers which are live
+  /// across the stackmap.  Will check to see if the copy instructions have
+  /// already been visited, and if appropriate, will add virtual registers to
+  /// work queue.
+  void inline
+  searchStackSlotCopies(int SS,
+                        ValueVecPtr IRVals,
+                        const SMInstBundle &SM,
+                        SmallPtrSet<const MachineInstr *, 32> &Visited,
+                        std::queue<WorkItem> &work,
+                        bool TraverseDefs);
+
+  /// Find all alternate locations for virtual registers in a stackmap, and add
+  /// them to the metadata to be generated.
+  void findAlternateVregLocs(const SMInstBundle &SM);
+
+  /// Find stackmap operands that have been spilled to alternate locations
+  bool findAlternateOpLocs();
+
+  /// Ensure virtual registers used to generate architecture-specific values
+  /// are handled by the stackmap & convert to physical registers
+  void sanitizeVregs(MachineLiveValPtr &LV, const MachineInstr *SM) const;
+
+  /// Find architecture-specific live values added by the backend
+  void findArchSpecificLiveVals();
+
+  /// Warn about unhandled registers & stack slots
+  void warnUnhandled() const;
+
+public:
+  static char ID;
+  static const std::string SMName;
+
+  StackTransformMetadata() : MachineFunctionPass(ID) {}
+
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const override;
+
+  virtual bool runOnMachineFunction(MachineFunction&) override;
+
+};
+
+} // end anonymous namespace
+
+char &llvm::StackTransformMetadataID = StackTransformMetadata::ID;
+const std::string StackTransformMetadata::SMName("llvm.experimental.stackmap");
+
+INITIALIZE_PASS_BEGIN(StackTransformMetadata, "stacktransformmetadata",
+  "Gather stack transformation metadata", false, false)
+INITIALIZE_PASS_DEPENDENCY(SlotIndexes)
+INITIALIZE_PASS_DEPENDENCY(LiveIntervals)
+INITIALIZE_PASS_DEPENDENCY(LiveStacks)
+INITIALIZE_PASS_DEPENDENCY(VirtRegMap)
+INITIALIZE_PASS_END(StackTransformMetadata, "stacktransformmetadata",
+  "Gather stack transformation metadata", false, false)
+
+char StackTransformMetadata::ID = 0;
+
+void StackTransformMetadata::getAnalysisUsage(AnalysisUsage &AU) const {
+  AU.setPreservesAll();
+  AU.addRequired<LiveIntervals>();
+  AU.addRequired<LiveStacks>();
+  AU.addRequired<SlotIndexes>();
+  AU.addRequired<VirtRegMap>();
+  MachineFunctionPass::getAnalysisUsage(AU);
+}
+
+bool StackTransformMetadata::runOnMachineFunction(MachineFunction &fn) {
+  bool Changed = false;
+
+  if(fn.getFrameInfo()->hasStackMap()) {
+    MF = &fn;
+    MFI = MF->getFrameInfo();
+    MRI = &MF->getRegInfo();
+    TII = MF->getSubtarget().getInstrInfo();
+    TRI = MF->getSubtarget().getRegisterInfo();
+    TVG = MF->getSubtarget().getValues();
+    Indexes = &getAnalysis<SlotIndexes>();
+    LI = &getAnalysis<LiveIntervals>();
+    LS = &getAnalysis<LiveStacks>();
+    VRM = &getAnalysis<VirtRegMap>();
+    reset();
+
+    DEBUG(
+      dbgs() << "\n********** STACK TRANSFORMATION METADATA **********\n"
+             << "********** Function: " << MF->getName() << "\n";
+      VRM->dump();
+    );
+
+    findStackmapsAndStackSlotCopies();
+    Changed = findAlternateOpLocs();
+    findArchSpecificLiveVals();
+    if(!NoWarnings) warnUnhandled();
+  }
+
+  return Changed;
+}
+
+/// Print information about a virtual register and it's associated IR value
+void StackTransformMetadata::dumpReg(unsigned Reg, const Value *IRVal) const {
+  if(IRVal) IRVal->printAsOperand(dbgs());
+  if(TargetRegisterInfo::isPhysicalRegister(Reg))
+    dbgs() << ": in register " << PrintReg(Reg, TRI);
+  else {
+    assert(VRM->hasPhys(Reg) && "Invalid virtual register");
+    unsigned Phys = VRM->getPhys(Reg);
+    dbgs() << ": in register " << PrintReg(Phys, TRI)
+           << " (vreg " << TargetRegisterInfo::virtReg2Index(Reg) << ")";
+  }
+  dbgs() << "\n";
+}
+
+/// Print information about a stack slot and it's associated IR value
+void StackTransformMetadata::dumpStackSlot(int SS, const Value *IRVal) const {
+  assert(!MFI->isDeadObjectIndex(SS) && "Invalid stack slot");
+  if(IRVal) IRVal->printAsOperand(dbgs());
+  dbgs() << ": in stack slot " << SS << " (size: " << MFI->getObjectSize(SS)
+         << ")\n";
+}
+
+/// Analyze a machine instruction to see if a value is getting copied from
+/// another location such as a stack slot or register.
+StackTransformMetadata::CopyLocPtr
+StackTransformMetadata::getCopyLocation(const MachineInstr *MI) const {
+  unsigned SrcVreg = 0;
+  unsigned DefVreg = 0;
+  int SS;
+
+  assert(MI && "Invalid machine instruction");
+
+  // Is it a copy from another register?
+  if(MI->isCopyLike()) {
+    for(unsigned i = 0, e = MI->getNumOperands(); i != e; i++) {
+      const MachineOperand &MO = MI->getOperand(i);
+      if(MO.isReg()) {
+        if(MO.isDef()) DefVreg = MO.getReg();
+        else SrcVreg = MO.getReg();
+      }
+    }
+
+    // TODO does it have to be a virtual register or can it be a physical one?
+    // Liveness analysis seems to apply only to virtual registers.
+    if(TargetRegisterInfo::isVirtualRegister(SrcVreg) &&
+       TargetRegisterInfo::isVirtualRegister(DefVreg))
+      return CopyLocPtr(new RegCopyLoc(DefVreg, SrcVreg, MI));
+  }
+
+  // Is it a load from the stack?
+  if((DefVreg = TII->isLoadFromStackSlot(MI, SS)) &&
+     TargetRegisterInfo::isVirtualRegister(DefVreg))
+    return CopyLocPtr(new StackLoadLoc(DefVreg, SS, MI));
+
+  // Is it a store to the stack?
+  if((SrcVreg = TII->isStoreToStackSlot(MI, SS)) &&
+     TargetRegisterInfo::isVirtualRegister(SrcVreg))
+    return CopyLocPtr(new StackStoreLoc(SrcVreg, SS, MI));
+
+  // A non-copylike instruction
+  return CopyLocPtr(nullptr);
+}
+
+/// Gather stackmap machine instructions, the IR instructions which generated
+/// the stackmaps, and their associated call machine instructions.  Also,
+/// find copies to/from stack slots (since there's no other mechanism to
+/// find/traverse them).
+void StackTransformMetadata::findStackmapsAndStackSlotCopies() {
+  for(auto MBB = MF->begin(), MBBE = MF->end(); MBB != MBBE; MBB++) {
+    for(auto MI = MBB->instr_begin(), ME = MBB->instr_end(); MI != ME; MI++) {
+      if(MI->getOpcode() == TargetOpcode::STACKMAP) {
+        // Find the stackmap IR instruction
+        assert(MI->getOperand(0).isImm() && "Invalid stackmap ID");
+        int64_t ID = MI->getOperand(0).getImm();
+        const BasicBlock *BB = MI->getParent()->getBasicBlock();
+        const CallInst *IRSM = nullptr;
+        for(auto I = BB->begin(), IE = BB->end(); I != IE; I++)
+        {
+          const IntrinsicInst *II;
+          if((II = dyn_cast<IntrinsicInst>(&*I)) &&
+             II->getCalledFunction()->getName() == SMName &&
+             cast<ConstantInt>(II->getArgOperand(0))->getSExtValue() == ID) {
+            IRSM = cast<CallInst>(II);
+            break;
+          }
+        }
+        assert(IRSM && "Could not find stackmap IR instruction");
+
+        // Find the call instruction
+        const MachineInstr *MCI = MI->getPrevNode();
+        while(MCI != nullptr) {
+          if(MCI->isCall()) {
+            if(MCI->getOpcode() == TargetOpcode::STACKMAP)
+              MCI = nullptr;
+            break;
+          }
+          MCI = MCI->getPrevNode();
+        }
+
+        if(!MCI) {
+          DEBUG(dbgs() << "NOTE: stackmap " << ID << " ";
+                IRSM->print(dbgs());
+                dbgs() << ": could not find associated call instruction "
+                          "(lowered to a native instruction?)\n");
+          continue;
+        }
+
+        SM.push_back(SMInstBundle(IRSM, &*MI, MCI));
+      }
+      else {
+        // Record all stack slots that are actually used.  Note that this is
+        // necessary because analysis maintained in MachineFrameInfo/LiveStacks
+        // may denote stack slots as live even though register allocation
+        // actually all references to them.
+        const PseudoSourceValue *PSV;
+        const FixedStackPseudoSourceValue *FI;
+        for(auto MemOp : MI->memoperands()) {
+          PSV = MemOp->getPseudoValue();
+          if(PSV && PSV->isFixed) {
+            FI = cast<FixedStackPseudoSourceValue>(PSV);
+            UsedSS.insert(FI->getFrameIndex());
+          }
+        }
+
+        // See if instruction copies to/from stack slot
+        StackSlotCopies::iterator it;
+        CopyLocPtr loc;
+        if(!(loc = getCopyLocation(&*MI))) continue;
+        enum CopyLoc::Type type = loc->getType();
+        if(type == CopyLoc::STACK_LOAD || type == CopyLoc::STACK_STORE) {
+          StackCopyLoc *SCL = (StackCopyLoc *)loc.get();
+          if((it = SSCopies.find(SCL->StackSlot)) == SSCopies.end())
+            it = SSCopies.emplace(SCL->StackSlot,
+                                  CopyLocVecPtr(new CopyLocVec)).first;
+          it->second->push_back(loc);
+        }
+      }
+    }
+  }
+
+  DEBUG(
+    dbgs() << "\n*** Stack slot copies ***\n\n";
+    for(auto SC = SSCopies.begin(), SCe = SSCopies.end(); SC != SCe; SC++) {
+      dbgs() << "Stack slot " << SC->first << ":\n";
+      for(size_t i = 0, e = SC->second->size(); i < e; i++) {
+        (*SC->second)[i]->Instr->dump();
+      }
+    }
+  );
+}
+
+/// Find all virtual register/stack slot operands in a stackmap and collect
+/// virtual register/stack slot <-> IR value mappings
+void StackTransformMetadata::mapOpsToIR(const CallInst *IRSM,
+                                        const MachineInstr *MISM) {
+  RegValsMap::iterator RegIt;
+  StackValsMap::iterator SSIt;
+  MachineInstr::const_mop_iterator MOit;
+
+  // Initialize new storage location/IR map objects (i.e., for virtual
+  // registers & stack slots) for the stackmap
+  SMRegs.emplace(MISM, RegValsMap());
+  SMStackSlots.emplace(MISM, StackValsMap());
+
+  // Loop over all operands
+  MOit = std::next(MISM->operands_begin(), 2);
+  for(size_t i = 2; i < IRSM->getNumArgOperands(); i++) {
+    const Value *IRVal = IRSM->getArgOperand(i);
+    assert(IRVal && "Invalid stackmap IR operand");
+
+    if(MOit->isImm()) { // Map IR values to stack slots
+      int FrameIdx = INT32_MAX;
+      switch(MOit->getImm()) {
+      case StackMaps::DirectMemRefOp:
+        MOit++;
+        assert(MOit->isFI() && "Invalid operand type");
+        FrameIdx = MOit->getIndex();
+        MOit = std::next(MOit, 2);
+        break;
+      case StackMaps::IndirectMemRefOp:
+        MOit = std::next(MOit, 2);
+        assert(MOit->isFI() && "Invalid operand type");
+        FrameIdx = MOit->getIndex();
+        MOit = std::next(MOit, 2);
+        break;
+      case StackMaps::ConstantOp: MOit = std::next(MOit, 2); continue;
+      default: llvm_unreachable("Unrecognized stackmap operand type"); break;
+      }
+
+      assert(MFI->getObjectIndexBegin() <= FrameIdx &&
+             FrameIdx <= MFI->getObjectIndexEnd() && "Invalid frame index");
+      assert(!MFI->isDeadObjectIndex(FrameIdx) && "Dead frame index");
+      DEBUG(dumpStackSlot(FrameIdx, IRVal););
+
+      // Update the list of IR values mapped to the stack slot (multiple IR
+      // values may be mapped to a single stack slot).
+      SSIt = SMStackSlots[MISM].find(FrameIdx);
+      if(SSIt == SMStackSlots[MISM].end())
+        SSIt = SMStackSlots[MISM].emplace(FrameIdx,
+                                          ValueVecPtr(new ValueVec)).first;
+      SSIt->second->push_back(IRVal);
+    }
+    else if(MOit->isReg()) { // Map IR values to virtual registers
+      unsigned Reg = MOit->getReg();
+      MOit++;
+
+      DEBUG(dumpReg(Reg, IRVal););
+
+      // Update the list of IR values mapped to the virtual register (multiple
+      // IR values may be mapped to a single virtual register).
+      RegIt = SMRegs[MISM].find(Reg);
+      if(RegIt == SMRegs[MISM].end())
+        RegIt = SMRegs[MISM].emplace(Reg, ValueVecPtr(new ValueVec)).first;
+      RegIt->second->push_back(IRVal);
+    } else {
+      llvm_unreachable("Unrecognized stackmap operand type.");
+    }
+  }
+}
+
+/// Extend the live range for a register to include an instruction.
+void
+StackTransformMetadata::updateRegisterLiveInterval(MachineOperand &Src,
+                                                   const MachineInstr *SM) {
+  typedef LiveInterval::Segment Segment;
+
+  assert(Src.isReg() && "Cannot update live range for non-register operand");
+
+  unsigned Vreg = Src.getReg();
+  bool hasRegUnit = false;
+  SlotIndex Slots[2] = {
+    Indexes->getInstructionIndex(Src.getParent()).getRegSlot(),
+    Indexes->getInstructionIndex(SM).getRegSlot()
+  };
+
+  // Find the segment ending at or containing the call instruction.  Note that
+  // we search using the insruction's base index, as the interval may end at
+  // the register index (and the end of the range is non-inclusive).
+  LiveInterval &Reg = LI->getInterval(Vreg);
+  LiveInterval::iterator Seg = Reg.find(Slots[0].getBaseIndex());
+  assert(Seg != Reg.end() && Seg->contains(Slots[0].getBaseIndex()) &&
+         "Invalid live interval");
+
+  if(Seg->end < Slots[1]) {
+    // Update the segment to include the stackmap
+    Seg = Reg.addSegment(Segment(Seg->start, Slots[1], Seg->valno));
+    DEBUG(dbgs() << "    -> Updated register live interval: "; Seg->dump());
+
+    // We also need to update the physical register's register unit (RU) live
+    // range because LiveIntervals::addKillFlags() will use the RU's live range
+    // to avoid marking a physical register dead if two virtual registers
+    // (mapped to that physical register) have overlapping live ranges.
+    MCRegUnitIterator Outer(VRM->getPhys(Vreg), TRI);
+    for(MCRegUnitIterator Unit(Outer); Unit.isValid(); ++Unit) {
+      LiveRange &RURange = LI->getRegUnit(*Unit);
+      LiveRange::iterator RUS;
+
+      for(size_t i = 0; i < 2; i++, RUS = RURange.end()) {
+        RUS = RURange.find(Slots[i]);
+        if(RUS != RURange.end() && RUS->contains(Slots[i])) break;
+      }
+
+      if(RUS != RURange.end()) {
+        hasRegUnit = true;
+        Seg = RURange.addSegment(
+          Segment(RUS->start, Slots[1].getNextIndex(), RUS->valno));
+        DEBUG(
+          dbgs() << "    -> Updated segment for register unit "
+                 << *Unit << ": ";
+          Seg->dump();
+        );
+        break;
+      }
+    }
+
+    // If we can't extend one of the current RU ranges, add a new range.
+    if(!hasRegUnit) {
+      LiveRange &RURange = LI->getRegUnit(*Outer);
+      VNInfo *Valno = RURange.getNextValue(Slots[0], LI->getVNInfoAllocator());
+      Seg = RURange.addSegment(
+        Segment(Slots[0], Slots[1].getNextIndex(), Valno));
+      DEBUG(
+        dbgs() << "    -> Added segment for register unit "
+               << *Outer << ": ";
+        Seg->dump();
+      );
+    }
+  }
+}
+
+/// Rather than modifying the backend machinery to prevent hoisting code
+/// between the stackmap and call site, unwind instructions in order to get
+/// real live value locations at the function call.
+bool StackTransformMetadata::unwindToCallSite(MachineInstr *SM,
+                                              const MachineInstr *Call) {
+  bool Changed = false, Found;
+  MachineOperand *SrcOp;
+  MachineInstr *InB = SM;
+  RegValsMap::iterator VregIt, SrcIt;
+  StackValsMap::iterator SSIt;
+  CopyLocPtr C;
+  RegCopyLoc *RCL;
+  StackCopyLoc *SCL;
+  TemporaryValuePtr Tmp;
+
+  // Note: anything named or related to "Src" refers to the source of the
+  // copy operation, i.e., the originating location for the value
+
+  DEBUG(dbgs() << "\nUnwinding stackmap back to call site:\n\n");
+  while((InB = InB->getPrevNode()) && InB != Call) {
+    if((C = getCopyLocation(InB))) {
+      DEBUG(dbgs() << "  + Copy instruction: "; InB->dump());
+
+      switch(C->getType()) {
+      default: DEBUG(dbgs() << "    Unhandled copy type\n"); break;
+      case CopyLoc::VREG:
+        RCL = (RegCopyLoc *)C.get();
+        SrcOp = &InB->getOperand(InB->findRegisterUseOperandIdx(RCL->SrcVreg));
+
+        // Replace current vreg with source
+        Found = false;
+        for(size_t i = 2; i < SM->getNumOperands(); i++) {
+          MachineOperand &MO = SM->getOperand(i);
+          if(MO.isReg() && MO.getReg() == RCL->Vreg) {
+            MO.ChangeToRegister(RCL->SrcVreg, false, false, SrcOp->isKill(),
+                                SrcOp->isDead(), false, false);
+            InB->clearRegisterKills(RCL->SrcVreg, TRI);
+            InB->clearRegisterDeads(RCL->SrcVreg);
+            updateRegisterLiveInterval(*SrcOp, SM);
+            Found = true;
+          }
+        }
+
+        // Update operand -> IR mapping to source vreg
+        if(Found) {
+          assert(SMRegs[SM].count(RCL->Vreg) &&
+                 "Unhandled register operand in stackmap!");
+          VregIt = SMRegs[SM].find(RCL->Vreg);
+          SrcIt = SMRegs[SM].find(RCL->SrcVreg);
+          if(SrcIt != SMRegs[SM].end()) {
+            for(auto IRVal : *VregIt->second)
+              SrcIt->second->push_back(IRVal);
+          }
+          else SMRegs[SM].emplace(RCL->SrcVreg, VregIt->second);
+          SMRegs[SM].erase(RCL->Vreg);
+          Changed = true;
+        }
+
+        break;
+      case CopyLoc::STACK_LOAD:
+        SCL = (StackCopyLoc *)C.get();
+
+        // Replace current vreg with stack slot.
+        // Note: stack slots don't have liveness information to fix up
+        Found = false;
+        for(size_t i = 2; i < SM->getNumOperands(); i++) {
+          MachineOperand &MO = SM->getOperand(i);
+          if(MO.isReg() && MO.getReg() == SCL->Vreg) {
+            // There's not a great way to add new operands, so trash all
+            // trailing operands up to and including the Vreg, add the spill
+            // slot, and finally add the trailing operands back.
+            SmallVector<MachineOperand, 4> TrailOps(std::next(&MO),
+                                                    SM->operands_end());
+            while(SM->getNumOperands() > (i + 1)) SM->RemoveOperand(i);
+            MachineInstrBuilder Worker(*MF, SM);
+            Worker.addImm(StackMaps::IndirectMemRefOp);
+            Worker.addImm(MFI->getObjectSize(SCL->StackSlot));
+            Worker.addFrameIndex(SCL->StackSlot);
+            Worker.addImm(0);
+            for(auto Trailing : TrailOps) Worker.addOperand(Trailing);
+            Found = true;
+          }
+        }
+
+        // Update operand -> IR mapping to source stack slot
+        if(Found) {
+          assert(SMRegs[SM].count(SCL->Vreg) &&
+                 "Unhandled register operand in stackmap!");
+          SSIt = SMStackSlots[SM].find(SCL->StackSlot);
+          VregIt = SMRegs[SM].find(SCL->Vreg);
+          if(SSIt != SMStackSlots[SM].end()) {
+            for(auto IRVal : *VregIt->second)
+              SSIt->second->push_back(IRVal);
+          }
+          else SMStackSlots[SM].emplace(SCL->StackSlot, VregIt->second);
+          SMRegs[SM].erase(SCL->Vreg);
+          Changed = true;
+        }
+
+        break;
+      case CopyLoc::STACK_STORE:
+        SCL = (StackCopyLoc *)C.get();
+        SrcOp = &InB->getOperand(InB->findRegisterUseOperandIdx(SCL->Vreg));
+
+        // Replace current stack slot with vreg
+        // Note: this *must* be an indirect memory reference (spill slot)
+        // since we're copying to a register!
+        Found = false;
+        for(size_t i = 2; i < SM->getNumOperands(); i++) {
+          MachineOperand &MO = SM->getOperand(i);
+          if(MO.isFI() && MO.getIndex() == SCL->StackSlot) {
+            // TODO if the sibling register is killed/dead in the intervening
+            // instruction we probably need to propagate that to the stackmap
+            // and remove it from the other instruction.
+            unsigned StartIdx = i - 2;
+            SM->getOperand(StartIdx).ChangeToRegister(SCL->Vreg, false);
+            SM->RemoveOperand(StartIdx + 1); // Size
+            SM->RemoveOperand(StartIdx + 1); // Frame index
+            SM->RemoveOperand(StartIdx + 1); // Frame pointer offset
+            Found = true;
+          }
+        }
+
+        // Update operand -> IR mapping to source vreg
+        if(Found) {
+          assert(SMStackSlots[SM].count(SCL->StackSlot) &&
+                 "Unhandled stack slot operand in stackmap!");
+
+          // Update liveness information to include the stackmap
+          InB->clearRegisterKills(SCL->Vreg, TRI);
+          InB->clearRegisterDeads(SCL->Vreg);
+          updateRegisterLiveInterval(*SrcOp, SM);
+
+          VregIt = SMRegs[SM].find(SCL->Vreg);
+          SSIt = SMStackSlots[SM].find(SCL->StackSlot);
+          if(VregIt != SMRegs[SM].end()) {
+            for(auto IRVal : *SSIt->second)
+              VregIt->second->push_back(IRVal);
+          }
+          else SMRegs[SM].emplace(SCL->Vreg, SSIt->second);
+          SMStackSlots[SM].erase(SCL->StackSlot);
+          Changed = true;
+        }
+
+        break;
+      }
+    }
+    else if((Tmp = TVG->getTemporaryValue(InB, VRM))) {
+      DEBUG(dbgs() << "  - Temporary for stackmap: "; InB->dump());
+      assert(Tmp->Type == TemporaryValue::StackSlotRef &&
+             "Unhandled temporary value");
+
+      // Replace current vreg with stack slot reference.
+      // Note: stack slots don't have liveness information to fix up
+      Found = false;
+      for(size_t i = 2; i < SM->getNumOperands(); i++) {
+        MachineOperand &MO = SM->getOperand(i);
+        if(MO.isReg() && MO.getReg() == Tmp->Vreg) {
+          // There's not a great way to add new operands, so trash all trailing
+          // operands up to and including the Vreg, add the metadata, and
+          // finally add the trailing operands back.
+          SmallVector<MachineOperand, 4> TrailOps(std::next(&MO),
+                                                  SM->operands_end());
+          while(SM->getNumOperands() > (i + 1)) SM->RemoveOperand(i);
+          MachineInstrBuilder Worker(*MF, SM);
+          Worker.addImm(StackMaps::TemporaryOp);
+          Worker.addImm(Tmp->Size);
+          Worker.addImm(Tmp->Offset);
+          Worker.addImm(StackMaps::DirectMemRefOp);
+          Worker.addFrameIndex(Tmp->StackSlot);
+          Worker.addImm(0);
+          for(auto Trailing : TrailOps) Worker.addOperand(Trailing);
+          Found = true;
+        }
+      }
+
+      // Update operand -> IR mapping to source stack slot
+      if(Found) {
+        assert(SMRegs[SM].count(Tmp->Vreg) &&
+               "Unhandled register operand in stackmap!");
+        SSIt = SMStackSlots[SM].find(Tmp->StackSlot);
+        VregIt = SMRegs[SM].find(Tmp->Vreg);
+        if(SSIt != SMStackSlots[SM].end()) {
+          for(auto IRVal : *VregIt->second)
+            SSIt->second->push_back(IRVal);
+        }
+        else SMStackSlots[SM].emplace(Tmp->StackSlot, VregIt->second);
+        SMRegs[SM].erase(Tmp->Vreg);
+        Changed = true;
+      }
+    }
+    else DEBUG(dbgs() << "  - Skipping "; InB->dump());
+  }
+
+  if(Changed) DEBUG(dbgs() << "\n  Transformed stackmap: "; SM->dump());
+  return Changed;
+}
+
+/// Is a virtual register live across the machine instruction?
+/// Note: returns false if the MI is the last instruction for which the virtual
+/// register is alive
+bool
+StackTransformMetadata::isVregLiveAcrossInstr(unsigned Vreg,
+                                              const MachineInstr *MI) const {
+  assert(TRI->isVirtualRegister(Vreg) && "Invalid virtual register");
+
+  if(LI->hasInterval(Vreg)) {
+    const LiveInterval &TheLI = LI->getInterval(Vreg);
+    SlotIndex InstrIdx = Indexes->getInstructionIndex(MI);
+    LiveInterval::const_iterator Seg = TheLI.find(InstrIdx);
+    if(Seg != TheLI.end() && Seg->contains(InstrIdx) &&
+       InstrIdx.getInstrDistance(Seg->end) != 0)
+      return true;
+  }
+  return false;
+}
+
+/// Is a stack slot live across the machine instruction?
+/// Note: returns false if the MI is the last instruction for which the stack
+/// slot is alive
+bool
+StackTransformMetadata::isSSLiveAcrossInstr(int SS,
+                                            const MachineInstr *MI) const {
+  if(LS->hasInterval(SS)) {
+    const LiveInterval &TheLI = LS->getInterval(SS);
+    SlotIndex InstrIdx = Indexes->getInstructionIndex(MI);
+    LiveInterval::const_iterator Seg = TheLI.find(InstrIdx);
+    if(Seg != TheLI.end() && Seg->contains(InstrIdx) &&
+       InstrIdx.getInstrDistance(Seg->end) != 0)
+      return true;
+  }
+  return false;
+}
+
+/// Add duplicate location information for a virtual register.
+bool StackTransformMetadata::addVregMetadata(unsigned Vreg,
+                                             ValueVecPtr IRVals,
+                                             const SMInstBundle &SM) {
+  const CallInst *IRSM = getIRSM(SM);
+  const MachineInstr *MICall = getMICall(SM);
+  RegValsMap &Vregs = SMRegs[getMISM(SM)];
+
+  assert(TargetRegisterInfo::isVirtualRegister(Vreg) && VRM->hasPhys(Vreg) &&
+         "Cannot add virtual register metadata -- invalid virtual register");
+
+  if(Vregs.find(Vreg) == Vregs.end() && isVregLiveAcrossInstr(Vreg, MICall))
+  {
+    unsigned Phys = VRM->getPhys(Vreg);
+    for(size_t sz = 0; sz < IRVals->size(); sz++) {
+      DEBUG(dumpReg(Vreg, (*IRVals)[sz]););
+      MF->addSMOpLocation(IRSM, (*IRVals)[sz], MachineLiveReg(Phys));
+    }
+    Vregs[Vreg] = IRVals;
+    return true;
+  }
+  else return false;
+}
+
+/// Add duplicate location information for a stack slot.
+bool StackTransformMetadata::addSSMetadata(int SS,
+                                           ValueVecPtr IRVals,
+                                           const SMInstBundle &SM) {
+  const CallInst *IRSM = getIRSM(SM);
+  const MachineInstr *MICall = getMICall(SM);
+  StackValsMap &SSlots = SMStackSlots[getMISM(SM)];
+
+  assert(!MFI->isDeadObjectIndex(SS) &&
+         "Cannot add stack slot metadata -- invalid stack slot");
+
+  if(SSlots.find(SS) == SSlots.end() && isSSLiveAcrossInstr(SS, MICall))
+  {
+    for(size_t sz = 0; sz < IRVals->size(); sz++) {
+      DEBUG(dumpStackSlot(SS, (*IRVals)[sz]););
+      MF->addSMOpLocation(IRSM, (*IRVals)[sz], MachineLiveStackSlot(SS));
+    }
+    SSlots[SS] = IRVals;
+    return true;
+  }
+  else return false;
+}
+
+/// Search stack slot copies for additional virtual registers which are live
+/// across the stackmap.  Will check to see if the copy instructions have
+/// already been visited, and if appropriate, will add virtual registers to
+/// work queue.
+void inline
+StackTransformMetadata::searchStackSlotCopies(int SS,
+                                 ValueVecPtr IRVals,
+                                 const SMInstBundle &SM,
+                                 SmallPtrSet<const MachineInstr *, 32> &Visited,
+                                 std::queue<WorkItem> &work,
+                                 bool TraverseDefs) {
+  StackSlotCopies::const_iterator Copies;
+  CopyLocVecPtr CL;
+  CopyLocVec::const_iterator Copy, CE;
+
+  if((Copies = SSCopies.find(SS)) != SSCopies.end()) {
+    CL = Copies->second;
+    for(Copy = CL->begin(), CE = CL->end(); Copy != CE; Copy++) {
+      unsigned Vreg = (*Copy)->Vreg;
+      const MachineInstr *Instr = (*Copy)->Instr;
+
+      if(!Visited.count(Instr)) {
+        addVregMetadata(Vreg, IRVals, SM);
+        Visited.insert(Instr);
+        work.emplace(Vreg, TraverseDefs);
+      }
+    }
+  }
+}
+
+/// Find all alternate locations for virtual registers in a stackmap, and add
+/// them to the metadata to be generated.
+void
+StackTransformMetadata::findAlternateVregLocs(const SMInstBundle &SM) {
+  RegValsMap &Regs = SMRegs[getMISM(SM)];
+  std::queue<WorkItem> work;
+  SmallPtrSet<const MachineInstr *, 32> Visited;
+  StackCopyLoc *SCL;
+  RegCopyLoc *RCL;
+
+  DEBUG(dbgs() << "\nDuplicate operand locations:\n\n";);
+
+  // Iterate over all vregs in the stackmap
+  for(RegValsMap::iterator it = Regs.begin(), end = Regs.end();
+      it != end; it++) {
+    unsigned origVreg = it->first;
+    ValueVecPtr IRVals = it->second;
+    Visited.clear();
+
+    // Follow data flow to search for all duplicate locations, including stack
+    // slots and other registers.  It's a duplicate if the following are true:
+    //
+    //   1. It's a copy-like instruction, e.g., a register move or a load
+    //      from/store to stack slot
+    //   2. The alternate location (virtual register/stack slot) is live across
+    //      the machine call instruction
+    //
+    // Note: we *must* search exhaustively (i.e., across copies from registers
+    // that are *not* live across the call) because the following can happen:
+    //
+    //   STORE vreg0, <fi#0>
+    //   ...
+    //   COPY vreg0, vreg1
+    //   ...
+    //   STACKMAP 0, 0, vreg1
+    //
+    // Here, vreg0 is *not* live across the stackmap, but <fi#0> *is*
+    work.emplace(origVreg, true);
+    while(!work.empty()) {
+      WorkItem cur;
+      unsigned vreg;
+      int ss;
+
+      // Walk over definitions
+      cur = work.front();
+      work.pop();
+      if(cur.TraverseDefs) {
+        for(auto instr = MRI->def_instr_begin(cur.Vreg),
+                 ei = MRI->def_instr_end();
+            instr != ei; instr++) {
+
+          if(Visited.count(&*instr)) continue;
+          CopyLocPtr loc = getCopyLocation(&*instr);
+          if(!loc) continue;
+
+          switch(loc->getType()) {
+          case CopyLoc::VREG:
+            RCL = (RegCopyLoc *)loc.get();
+            vreg = RCL->SrcVreg;
+            addVregMetadata(vreg, IRVals, SM);
+            Visited.insert(&*instr);
+            work.emplace(vreg, true);
+            break;
+          case CopyLoc::STACK_LOAD:
+            SCL = (StackCopyLoc *)loc.get();
+            ss = SCL->StackSlot;
+            if(addSSMetadata(ss, IRVals, SM)) {
+              Visited.insert(&*instr);
+              searchStackSlotCopies(ss, IRVals, SM, Visited, work, true);
+            }
+            break;
+          default: llvm_unreachable("Unknown/invalid location type"); break;
+          }
+        }
+      }
+
+      // Walk over uses
+      for(auto instr = MRI->use_instr_begin(cur.Vreg),
+               ei = MRI->use_instr_end();
+          instr != ei; instr++) {
+
+        if(Visited.count(&*instr)) continue;
+        CopyLocPtr loc = getCopyLocation(&*instr);
+        if(!loc) continue;
+
+        // Note: in traversing uses of the given vreg, we *don't* want to
+        // traverse definitions of sibling vregs.  Because we're in pseudo-SSA,
+        // it's possible we could be defining a register in separate dataflow
+        // paths, e.g.:
+        //
+        // BB A:
+        //   %vreg3<def> = COPY %vreg1
+        //   JMP <BB C>
+        //
+        // BB B:
+        //   %vreg3<def> = COPY %vreg2
+        //   JMP <BB C>
+        //
+        // ...
+        //
+        // If we discovered block A through vreg 1, we don't want to explore
+        // through block B in which vreg 3 is defined with a different value.
+        switch(loc->getType()) {
+        case CopyLoc::VREG:
+          RCL = (RegCopyLoc *)loc.get();
+          vreg = RCL->Vreg;
+          addVregMetadata(vreg, IRVals, SM);
+          Visited.insert(&*instr);
+          work.emplace(vreg, false);
+          break;
+        case CopyLoc::STACK_STORE:
+          SCL = (StackCopyLoc *)loc.get();
+          ss = SCL->StackSlot;
+          if(addSSMetadata(ss, IRVals, SM)) {
+            Visited.insert(&*instr);
+            searchStackSlotCopies(ss, IRVals, SM, Visited, work, false);
+          }
+          break;
+        default: llvm_unreachable("Unknown/invalid location type"); break;
+        }
+      }
+    }
+  }
+}
+
+/// Find alternate storage locations for stackmap operands
+bool StackTransformMetadata::findAlternateOpLocs() {
+  bool Changed = false;
+  RegValsMap::iterator vregIt, vregEnd;
+
+  for(auto S = SM.begin(), SE = SM.end(); S != SE; S++) {
+    const CallInst *IRSM = getIRSM(*S);
+    const MachineInstr *MICall = getMICall(*S);
+    MachineInstr *MISM = getMISM(*S);
+
+    DEBUG(
+      dbgs() << "\nStackmap " << MISM->getOperand(0).getImm() << ":\n";
+      MISM->dump();
+      dbgs() << "\n";
+    );
+
+    // Get all virtual register/stack slot operands & their associated IR
+    // values
+    mapOpsToIR(IRSM, MISM);
+
+    // Because the CodeGen machinery is wily (and may hoist instructions above
+    // the stackmap), unwind copies until the call site.
+    Changed |= unwindToCallSite(MISM, MICall);
+
+    // Find alternate locations for vregs in stack map.  Note we don't need to
+    // find alternate stack slot locations, as allocas *should* already be in
+    // the stackmap, so the remaining stack slots are spilled registers (which
+    // are covered here).
+    findAlternateVregLocs(*S);
+  }
+
+  return Changed;
+}
+
+/// Ensure virtual registers used to generate architecture-specific values are
+/// handled by the stackmap & convert to physical registers
+void StackTransformMetadata::sanitizeVregs(MachineLiveValPtr &LV,
+                                           const MachineInstr *SM) const {
+  if(!LV) return;
+  if(LV->isGenerated()) {
+    MachineGeneratedVal *MGV = (MachineGeneratedVal *)LV.get();
+    const ValueGenInstList &Inst = MGV->getInstructions();
+    for(size_t i = 0, num = Inst.size(); i < num; i++) {
+      if(Inst[i]->opType() == ValueGenInst::OpType::Register) {
+        RegInstructionBase *RI = (RegInstructionBase *)Inst[i].get();
+        if(!TRI->isVirtualRegister(RI->getReg())) {
+          if(RI->getReg() == TRI->getFrameRegister(*MF)) continue;
+          // TODO walk through stackmap and see if physical register in
+          // instruction is contained in stackmap
+          LV.reset(nullptr);
+          return;
+        }
+        else if(!SMRegs.at(SM).count(RI->getReg())) {
+          DEBUG(dbgs() << "WARNING: vreg "
+                       << TargetRegisterInfo::virtReg2Index(RI->getReg())
+                       << " used to generate value not handled in stackmap\n");
+          LV.reset(nullptr);
+          return;
+        }
+        else {
+          assert(VRM->hasPhys(RI->getReg()) && "Invalid virtual register");
+          RI->setReg(VRM->getPhys(RI->getReg()));
+        }
+      }
+    }
+  }
+}
+
+/// Filter out register definitions we've previously seen.
+static void
+getUnseenDefinitions(MachineRegisterInfo::def_instr_iterator DefIt,
+                     const SmallPtrSet<const MachineInstr *, 4> &Seen,
+                     SmallPtrSet<const MachineInstr *, 4> &NewDefs) {
+  NewDefs.clear();
+  do { if(!Seen.count(&*DefIt)) NewDefs.insert(&*DefIt);
+  } while((++DefIt) != MachineRegisterInfo::def_instr_end());
+}
+
+/// Try to find the best defining instruction.
+static const MachineInstr *
+tryToBreakDefMITie(const MachineInstr *MICall,
+                   const SmallPtrSet<const MachineInstr *, 4> &Definitions) {
+  // First heuristic -- find closest preceding defining instruction in the same
+  // machine basic block.
+  const MachineInstr *Cur, *BestDef = nullptr;
+  unsigned Distance, Best = UINT32_MAX;
+  SmallVector<std::pair<const MachineInstr *, unsigned>, 4> SearchDefs;
+  for(auto Def : Definitions) {
+    Cur = MICall;
+    Distance = 1;
+    while((Cur = Cur->getPrevNode())) {
+      if(Cur == Def) {
+        SearchDefs.emplace_back(Def, Distance);
+        break;
+      }
+      Distance++;
+    }
+  }
+
+  for(auto Pair : SearchDefs) {
+    if(Pair.second < Best) {
+      BestDef = Pair.first;
+      Best = Pair.second;
+    }
+  }
+
+  if(BestDef)
+    DEBUG(dbgs() << "Choosing defining instruction"; BestDef->dump());
+  return BestDef;
+}
+
+/// Find architecture-specific live values added by the backend
+void StackTransformMetadata::findArchSpecificLiveVals() {
+  DEBUG(dbgs() << "\n*** Finding architecture-specific live values ***\n\n";);
+
+  for(auto S = SM.begin(), SE = SM.end(); S != SE; S++)
+  {
+    const MachineInstr *MISM = getMISM(*S);
+    const MachineInstr *MICall = getMICall(*S);
+    const CallInst *IRSM = getIRSM(*S);
+    RegValsMap &CurVregs = SMRegs[MISM];
+    StackValsMap &CurSS = SMStackSlots[MISM];
+
+    DEBUG(
+      MISM->dump();
+      dbgs() << "  -> Call instruction SlotIndex ";
+      Indexes->getInstructionIndex(MICall).print(dbgs());
+      dbgs() << ", searching vregs 0 -> " << MRI->getNumVirtRegs()
+             << " and stack slots " << MFI->getObjectIndexBegin() << " -> "
+             << MFI->getObjectIndexEnd() << "\n";
+    );
+
+    // Include any mandatory architecture-specific live values
+    TVG->addRequiredArchLiveValues(MF, MISM, IRSM);
+
+    // Search for virtual registers not handled by the stackmap.  Registers
+    // spilled to the stack should have been converted to frame index
+    // references by now.
+    for(unsigned i = 0, numVregs = MRI->getNumVirtRegs(); i < numVregs; i++) {
+      unsigned Vreg = TargetRegisterInfo::index2VirtReg(i);
+      MachineLiveValPtr MLV;
+      MachineLiveReg MLR(0);
+
+      if(VRM->hasPhys(Vreg) && isVregLiveAcrossInstr(Vreg, MICall) &&
+         CurVregs.find(Vreg) == CurVregs.end()) {
+        DEBUG(dbgs() << "    + vreg" << i
+                     << " is live in register but not in stackmap\n";);
+
+        // Walk the use-def chain to see if we can find a valid value.  Note we
+        // keep track of seen definitions because even though we're supposed to
+        // be in SSA form it's possible to find definition cycles.
+        const MachineInstr *DefMI;
+        unsigned ChainVreg = Vreg;
+        SmallPtrSet<const MachineInstr *, 4> SeenDefs, NewDefs;
+        do {
+          getUnseenDefinitions(MRI->def_instr_begin(ChainVreg),
+                               SeenDefs, NewDefs);
+
+          // Try to find a suitable defining instruction
+          if(NewDefs.size() == 0) {
+            DEBUG(dbgs() << "WARNING: no unseen definition\n");
+            break;
+          }
+          else if(NewDefs.size() == 1) DefMI = *NewDefs.begin();
+          else if(!(DefMI = tryToBreakDefMITie(MICall, NewDefs))) {
+            // No suitable defining instruction, not much we can do...
+            DEBUG(
+              dbgs() << "WARNING: multiple definitions for virtual "
+                        "register, missed in live-value analysis?\n";
+              for(auto d = MRI->def_instr_begin(ChainVreg),
+                  e = MRI->def_instr_end(); d != e; d++)
+                d->dump();
+            );
+            break;
+          }
+
+          SeenDefs.insert(DefMI);
+          MLV = TVG->getMachineValue(DefMI);
+          sanitizeVregs(MLV, MISM);
+
+          if(MLV) break; // We got a value!
+          else {
+            // Couldn't get a value, follow the use-def chain
+            CopyLocPtr Copy = getCopyLocation(DefMI);
+            if(Copy) {
+              switch(Copy->getType()) {
+              default: ChainVreg = 0; break;
+              case CopyLoc::VREG:
+                ChainVreg = ((RegCopyLoc *)Copy.get())->SrcVreg;
+                break;
+              }
+            }
+            else ChainVreg = 0;
+          }
+        } while(TargetRegisterInfo::isVirtualRegister(ChainVreg));
+
+        if(MLV) {
+          DEBUG(dbgs() << "      Defining instruction: ";
+                MLV->getDefiningInst()->print(dbgs());
+                dbgs() << "      Value: " << MLV->toString() << "\n");
+
+          MLR.setReg(VRM->getPhys(Vreg));
+          MF->addSMArchSpecificLocation(IRSM, MLR, *MLV);
+          CurVregs.emplace(Vreg, ValueVecPtr(nullptr));
+        }
+        else {
+          DEBUG(
+            DefMI = &*MRI->def_instr_begin(Vreg);
+            StringRef BBName = DefMI->getParent()->getName();
+            dbgs() << "      Unhandled defining instruction in basic block "
+                   << BBName << ":";
+            DefMI->print(dbgs());
+          );
+        }
+      }
+    }
+
+    // Search for stack slots not handled by the stackmap
+    for(int SS = MFI->getObjectIndexBegin(), e = MFI->getObjectIndexEnd();
+        SS < e; SS++) {
+      if(UsedSS.count(SS) && !MFI->isDeadObjectIndex(SS) &&
+         isSSLiveAcrossInstr(SS, MICall) && CurSS.find(SS) == CurSS.end()) {
+        DEBUG(dbgs() << "    + stack slot " << SS
+                     << " is live but not in stackmap\n";);
+        // TODO add arch-specific stack slot information to machine function
+      }
+    }
+
+    DEBUG(dbgs() << "\n";);
+  }
+}
+
+/// Find IR call instruction which generated the stackmap
+static inline const CallInst *findCalledFunc(const llvm::CallInst *IRSM) {
+  const Instruction *Func = IRSM->getPrevNode();
+  while(Func && !isa<CallInst>(Func)) Func = Func->getPrevNode();
+  return dyn_cast<CallInst>(Func);
+}
+
+/// Display a warning about unhandled values
+static inline void displayWarning(std::string &Msg,
+                                  const CallInst *CI,
+                                  const Function *F) {
+  assert(CI && "Invalid arguments");
+
+  // Note: it may be possible for us to not have a called function, for example
+  // if we call a function using a function pointer
+  const Function *CurF = CI->getParent()->getParent();
+  const std::string &Triple = CurF->getParent()->getTargetTriple();
+  Msg = "(" + Triple + ") " + Msg;
+  if(F && F->hasName()) {
+    Msg += " across call to ";
+    Msg += F->getName();
+  }
+  DiagnosticInfoOptimizationFailure DI(*CurF, CI->getDebugLoc(), Msg);
+  CurF->getContext().diagnose(DI);
+}
+
+/// Warn about unhandled registers & stack slots
+void StackTransformMetadata::warnUnhandled() const {
+  std::string Msg;
+  const CallInst *IRCall;
+  const Function *CalledFunc;
+
+  for(auto S = SM.begin(), SE = SM.end(); S != SE; S++)
+  {
+    const MachineInstr *MISM = getMISM(*S);
+    const MachineInstr *MICall = getMICall(*S);
+    const RegValsMap &CurVregs = SMRegs.at(MISM);
+    const StackValsMap &CurSS = SMStackSlots.at(MISM);
+    IRCall = findCalledFunc(getIRSM(*S));
+    CalledFunc = IRCall->getCalledFunction();
+    assert(IRCall && "No call instruction for stackmap");
+
+    // Search for virtual registers not handled by the stackmap
+    for(unsigned i = 0; i < MRI->getNumVirtRegs(); i++) {
+      unsigned Vreg = TargetRegisterInfo::index2VirtReg(i);
+
+      // Virtual register allocated to physical register
+      if(VRM->hasPhys(Vreg) && isVregLiveAcrossInstr(Vreg, MICall) &&
+         CurVregs.find(Vreg) == CurVregs.end()) {
+        Msg = "Stack transformation: unhandled register ";
+        Msg += TRI->getName(VRM->getPhys(Vreg));
+        displayWarning(Msg, IRCall, CalledFunc);
+      }
+    }
+
+    // Search for all stack slots not handled by the stackmap
+    for(int SS = MFI->getObjectIndexBegin(), e = MFI->getObjectIndexEnd();
+        SS < e; SS++) {
+      if(UsedSS.count(SS) && !MFI->isDeadObjectIndex(SS) &&
+         isSSLiveAcrossInstr(SS, MICall) && CurSS.find(SS) == CurSS.end()) {
+        Msg = "Stack transformation: unhandled stack slot ";
+        Msg += std::to_string(SS);
+        displayWarning(Msg, IRCall, CalledFunc);
+      }
+    }
+  }
+}
+
Index: lib/CodeGen/StackTransformTypes.cpp
===================================================================
--- lib/CodeGen/StackTransformTypes.cpp	(nonexistent)
+++ lib/CodeGen/StackTransformTypes.cpp	(working copy)
@@ -0,0 +1,303 @@
+//===-- llvm/Target/TargetValueGenerator.cpp - Value Generator --*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/CodeGen/AsmPrinter.h"
+#include "llvm/CodeGen/MachineFrameInfo.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/StackTransformTypes.h"
+#include "llvm/IR/Mangler.h"
+#include "llvm/MC/MCContext.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Target/TargetFrameLowering.h"
+#include "llvm/Target/TargetMachine.h"
+#include "llvm/Target/TargetRegisterInfo.h"
+#include "llvm/Target/TargetSubtargetInfo.h"
+
+#define DEBUG_TYPE "stacktransform"
+
+using namespace llvm;
+
+//===----------------------------------------------------------------------===//
+// Common functions
+//
+
+static MCSymbol *GetExternalSymbol(AsmPrinter &AP, StringRef Sym) {
+  SmallString<60> Name;
+  Mangler::getNameWithPrefix(Name, Sym, *AP.TM.getDataLayout());
+  return AP.OutContext.lookupSymbol(Name);
+}
+
+//===----------------------------------------------------------------------===//
+// Types for generating more complex architecture-specific live values
+//
+
+const char *ValueGenInst::InstTypeStr[] = {
+#define X(type) #type ,
+  VALUE_GEN_INST
+#undef X
+};
+
+const char *ValueGenInst::getInstName(enum InstType Type) {
+  switch(Type) {
+#define X(type) case type: 
+    VALUE_GEN_INST
+#undef X
+    return InstTypeStr[Type];
+    break;
+  default:
+    return "unknown";
+  };
+}
+
+std::string ValueGenInst::getInstNameStr(enum InstType Type) {
+  return std::string(getInstName(Type));
+}
+
+std::string RefInstruction::str() const {
+  std::string buf = "reference to '";
+  switch(Symbol.getType()) {
+  case MachineOperand::MO_GlobalAddress:
+    buf += Symbol.getGlobal()->getName();
+    buf += "' (global)";
+    break;
+  case MachineOperand::MO_ExternalSymbol:
+    buf += Symbol.getSymbolName();
+    buf += "' (external)";
+    break;
+  case MachineOperand::MO_MCSymbol:
+    buf += Symbol.getMCSymbol()->getName();
+    buf += "' (MC symbol)";
+    break;
+  default:
+    DEBUG(dbgs() << "Unhandled reference type: ";
+          Symbol.print(dbgs());
+          dbgs() << "\n";);
+    buf += "n/a' (unhandled type)";
+    break;
+  }
+  return buf;
+}
+
+MCSymbol *RefInstruction::getReference(AsmPrinter &AP) const {
+  switch(Symbol.getType()) {
+  case MachineOperand::MO_ExternalSymbol:
+    return GetExternalSymbol(AP, Symbol.getSymbolName());
+  case MachineOperand::MO_GlobalAddress:
+    return AP.TM.getSymbol(Symbol.getGlobal(), *AP.Mang);
+  case MachineOperand::MO_MCSymbol:
+    return Symbol.getMCSymbol();
+  default:
+    DEBUG(dbgs() << "Unhandled reference type: ";
+          Symbol.print(dbgs());
+          dbgs() << "\n";);
+    return nullptr;
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// MachineSymbolRef implementation
+//
+
+bool MachineSymbolRef::operator==(const MachineLiveVal &RHS) const {
+  if(RHS.isSymbolRef()) {
+    const MachineSymbolRef &MSR = (const MachineSymbolRef &)RHS;
+    if(&MSR.Symbol == &Symbol && MSR.Load == Load) return true;
+  }
+  return false;
+}
+
+std::string MachineSymbolRef::toString() const {
+  std::string buf;
+  if(Load) buf = "dereference symbol '";
+  else buf = "reference to symbol '";
+  switch(Symbol.getType()) {
+  case MachineOperand::MO_GlobalAddress:
+    buf += Symbol.getGlobal()->getName();
+    buf += "' (global)";
+    break;
+  case MachineOperand::MO_ExternalSymbol:
+    buf += Symbol.getSymbolName();
+    buf += "' (external)";
+    break;
+  case MachineOperand::MO_MCSymbol:
+    buf += Symbol.getMCSymbol()->getName();
+    buf += "' (MC symbol)";
+    break;
+  default:
+    DEBUG(dbgs() << "Unhandled reference type: ";
+          Symbol.print(dbgs());
+          dbgs() << "\n";);
+    buf += "n/a' (unhandled type)";
+    break;
+  }
+  return buf;
+}
+
+MCSymbol *MachineSymbolRef::getReference(AsmPrinter &AP) const {
+  switch(Symbol.getType()) {
+  case MachineOperand::MO_ExternalSymbol:
+    return GetExternalSymbol(AP, Symbol.getSymbolName());
+  case MachineOperand::MO_GlobalAddress:
+    return AP.TM.getSymbol(Symbol.getGlobal(), *AP.Mang);
+  case MachineOperand::MO_MCSymbol:
+    return Symbol.getMCSymbol();
+  default:
+    DEBUG(dbgs() << "Unhandled reference type: ";
+          Symbol.print(dbgs());
+          dbgs() << "\n";);
+    return nullptr;
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// MachineConstPoolRef implementation
+//
+
+bool MachineConstPoolRef::operator==(const MachineLiveVal &RHS) const {
+  if(RHS.isConstPoolRef()) {
+    const MachineConstPoolRef &MCPR = (const MachineConstPoolRef &)RHS;
+    if(MCPR.Index == Index) return true;
+  }
+  return false;
+}
+
+MCSymbol *MachineConstPoolRef::getReference(AsmPrinter &AP) const {
+  MCSymbol *Sym = AP.GetCPISymbol(Index);
+  assert(Sym && "Could not get constant pool reference");
+  return Sym;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineStackObject implementation
+//
+
+bool MachineStackObject::operator==(const MachineLiveVal &RHS) const {
+  if(RHS.isStackObject()) {
+    const MachineStackObject &MSO = (const MachineStackObject &)RHS;
+    if(MSO.Index == Index) return true;
+  }
+  return false;
+}
+
+std::string MachineStackObject::toString() const {
+  std::string buf;
+  if(Load) buf = "load from ";
+  else buf = "reference to ";
+  return buf + "stack slot " + std::to_string(Index);
+}
+
+int
+MachineStackObject::getOffsetFromReg(AsmPrinter &AP, unsigned &BR) const {
+  const TargetFrameLowering *TFL = AP.MF->getSubtarget().getFrameLowering();
+  return TFL->getFrameIndexReference(*AP.MF, Index, BR);
+}
+
+//===----------------------------------------------------------------------===//
+// ReturnAddress implementation
+//
+
+int ReturnAddress::getOffsetFromReg(AsmPrinter &AP, unsigned &BR) const {
+  int Off = AP.MF->getSubtarget().getRegisterInfo()->getReturnAddrLoc(*AP.MF,
+                                                                      BR);
+  if(BR == 0) llvm_unreachable("No saved return address!");
+  return Off;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineImmediate implementation
+//
+
+MachineImmediate::MachineImmediate(unsigned Size,
+                                   uint64_t Value,
+                                   const MachineInstr *DefMI,
+                                   bool Ptr)
+  : MachineLiveVal(DefMI, Ptr), Size(Size), Value(Value)
+{
+  if(Size > 8)
+    llvm_unreachable("Unsupported immediate value size of > 8 bytes");
+}
+
+bool MachineImmediate::operator==(const MachineLiveVal &RHS) const {
+  if(RHS.isImm()) {
+    const MachineImmediate &MI = (const MachineImmediate &)RHS;
+    if(MI.Size == Size && MI.Value == Value) return true;
+  }
+  return false;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineGeneratedVal implementation
+//
+
+bool MachineGeneratedVal::operator==(const MachineLiveVal &RHS) const {
+  if(!RHS.isGenerated()) return false;
+  const MachineGeneratedVal &MGV = (const MachineGeneratedVal &)RHS;
+
+  if(VG.size() != MGV.VG.size()) return false;
+  for(size_t i = 0, num = VG.size(); i < num; i++)
+    if(VG[i] != MGV.VG[i]) return false;
+  return true;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineLiveReg implementation
+//
+
+bool MachineLiveReg::operator==(const MachineLiveLoc &RHS) const {
+  if(RHS.isReg()) {
+    const MachineLiveReg &MLR = (const MachineLiveReg &)RHS;
+    if(MLR.Reg == Reg) return true;
+  }
+  return false;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineLiveStackAddr implementation
+//
+
+bool MachineLiveStackAddr::operator==(const MachineLiveLoc &RHS) const {
+  if(RHS.isStackAddr() && !RHS.isStackSlot()) {
+    const MachineLiveStackAddr &MLSA = (const MachineLiveStackAddr &)RHS;
+    if(Offset != INT32_MAX && MLSA.Offset != INT32_MAX &&
+       Offset == MLSA.Offset && Reg == MLSA.Reg && Size == MLSA.Size)
+      return true;
+  }
+  return false;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineLiveStackSlot implementation
+//
+
+bool MachineLiveStackSlot::operator==(const MachineLiveLoc &RHS) const {
+  if(RHS.isStackSlot()) {
+    const MachineLiveStackSlot &MLSS = (const MachineLiveStackSlot &)RHS;
+    if(MLSS.Index == Index) return true;
+  }
+  return false;
+}
+
+int MachineLiveStackSlot::calcAndGetRegOffset(const AsmPrinter &AP, unsigned &BP) {
+  if(Offset == INT32_MAX) {
+    const TargetFrameLowering *TFL = AP.MF->getSubtarget().getFrameLowering();
+    Offset = TFL->getFrameIndexReference(*AP.MF, Index, Reg);
+  }
+  BP = Reg;
+  return Offset;
+}
+
+unsigned MachineLiveStackSlot::getSize(const AsmPrinter &AP) {
+  if(Size == 0) {
+    const MachineFrameInfo *MFI = AP.MF->getFrameInfo();
+    Size = MFI->getObjectSize(Index);
+  }
+  return Size;
+}
+
Index: lib/CodeGen/UnwindInfo.cpp
===================================================================
--- lib/CodeGen/UnwindInfo.cpp	(nonexistent)
+++ lib/CodeGen/UnwindInfo.cpp	(working copy)
@@ -0,0 +1,186 @@
+//===--------------------------- UnwindInfo.cpp ---------------------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/CodeGen/UnwindInfo.h"
+#include "llvm/MC/MCSectionELF.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/MC/MCObjectFileInfo.h"
+#include "llvm/Target/TargetFrameLowering.h"
+#include "llvm/Target/TargetRegisterInfo.h"
+#include "llvm/Target/TargetSubtargetInfo.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "unwindinfo"
+
+static const char *UIDbg = "Unwind Info: ";
+
+void UnwindInfo::recordUnwindInfo(const MachineFunction &MF) {
+  // We *only* need this information for functions which have a stackmap, as
+  // only those function activations can be unwound during stack
+  // transformation.  This may also be a correctness criterion since we record
+  // offsets from the FBP, and not all functions may have one (stackmaps are
+  // implemented using FBPs, and thus prevent the FP-elimination optimization).
+  if(!MF.getFrameInfo()->hasStackMap()) return;
+
+  DEBUG(dbgs() << "**** UnwindInfo: Analyzing " << MF.getName() << "****\n");
+
+  const MachineFrameInfo *MFI = MF.getFrameInfo();
+  assert(MFI->isCalleeSavedInfoValid() && "No callee-saved information!");
+
+  // Get this function's saved registers
+  unsigned FrameReg;
+  const TargetFrameLowering *TFL = MF.getSubtarget().getFrameLowering();
+  const TargetRegisterInfo *TRI = MF.getSubtarget().getRegisterInfo();
+  const std::vector<CalleeSavedInfo> &CSI = MFI->getCalleeSavedInfo();
+
+  DEBUG(dbgs() << CSI.size() << " saved registers\n");
+
+  // Get DWARF register number and FBP offset using callee saved information
+  CalleeSavedRegisters SavedRegs(CSI.size());
+  for(unsigned i = 0; i < CSI.size(); i++) {
+    SavedRegs[i].DwarfReg = TRI->getDwarfRegNum(CSI[i].getReg(), false);
+    SavedRegs[i].Offset =
+      TFL->getFrameIndexReferenceFromFP(MF, CSI[i].getFrameIdx(), FrameReg);
+
+    DEBUG(dbgs() << "Register " << SavedRegs[i].DwarfReg << " at register "
+                 << PrintReg(FrameReg, TRI) << " + " << SavedRegs[i].Offset
+                 << "\n");
+    assert(FrameReg == TRI->getFrameRegister(MF) &&
+           "Invalid register used as offset for unwinding information");
+  }
+
+  // Save the information for when we emit the section
+  const MCSymbol *FuncSym = OutContext.lookupSymbol(MF.getName());
+  assert(FuncSym && "Could not find function symbol");
+  FuncCalleeSaved.insert(FuncCalleePair(FuncSym, std::move(SavedRegs)));
+}
+
+void UnwindInfo::addRegisterUnwindInfo(const MachineFunction &MF,
+                                       uint32_t MachineReg,
+                                       int32_t Offset) {
+  if(!MF.getFrameInfo()->hasStackMap()) return;
+
+  const MCSymbol *FuncSym = OutContext.lookupSymbol(MF.getName());
+  assert(FuncSym && "Could not find function symbol");
+  assert(FuncCalleeSaved.find(FuncSym) != FuncCalleeSaved.end() &&
+         "Cannot add register restore information -- function not found");
+  const TargetRegisterInfo *TRI = MF.getSubtarget().getRegisterInfo();
+  FuncCalleeSaved[FuncSym].push_back(
+    RegOffset(TRI->getDwarfRegNum(MachineReg, false), Offset));
+}
+
+void UnwindInfo::emitUnwindInfo(MCStreamer &OS) {
+  unsigned curIdx = 0;
+  unsigned startIdx;
+  FuncCalleeMap::const_iterator f, e;
+  for(f = FuncCalleeSaved.begin(), e = FuncCalleeSaved.end(); f != e; f++) {
+    const MCSymbol *FuncSym = f->first;
+    const CalleeSavedRegisters &CSR = f->second;
+
+    assert(FuncSym && "Invalid machine function");
+    if(CSR.size() < 2)
+      DEBUG(dbgs() << "WARNING: should have at least 2 registers to restore "
+                               "(return address & saved FBP");
+
+    DEBUG(dbgs() << UIDbg << "Function " << FuncSym->getName()
+                 << " (offset " << curIdx << ", "
+                 << CSR.size() << " entries):\n");
+
+    startIdx = curIdx;
+    CalleeSavedRegisters::const_iterator cs, cse;
+    for(cs = CSR.begin(), cse = CSR.end(); cs != cse; cs++) {
+      assert(cs->DwarfReg < UINT16_MAX &&
+             "Register number too large for resolution");
+      assert(INT16_MIN < cs->Offset && cs->Offset < INT16_MAX &&
+             "Register save offset too large for resolution");
+
+      DEBUG(dbgs() << UIDbg << "  Register " << cs->DwarfReg
+                   << " saved at " << cs->Offset << "\n";);
+
+      OS.EmitIntValue(cs->DwarfReg, 2);
+      OS.EmitIntValue(cs->Offset, 2);
+      curIdx++;
+    }
+    FuncUnwindInfo FUI(startIdx, curIdx - startIdx);
+    FuncUnwindMetadata.insert(FuncUnwindPair(FuncSym, std::move(FUI)));
+  }
+}
+
+void UnwindInfo::emitAddrRangeInfo(MCStreamer &OS) {
+  FuncUnwindMap::const_iterator f, e;
+  for(f = FuncUnwindMetadata.begin(), e = FuncUnwindMetadata.end();
+      f != e;
+      f++) {
+    const MCSymbol *Func = f->first;
+    const FuncUnwindInfo &FUI = f->second;
+    OS.EmitSymbolValue(Func, 8);
+    OS.EmitIntValue(FUI.NumUnwindRecord, 4);
+    OS.EmitIntValue(FUI.SecOffset, 4);
+  }
+}
+
+/// Serialize the unwinding information.
+void UnwindInfo::serializeToUnwindInfoSection() {
+  // Bail out if there's no unwind info.
+  if(FuncCalleeSaved.empty()) return;
+
+  // Emit unwinding record information.
+  // FIXME: we only support ELF object files for now
+
+  // Switch to the unwind info section
+  MCStreamer &OS = *AP.OutStreamer;
+  MCSection *UnwindInfoSection =
+      OutContext.getObjectFileInfo()->getUnwindInfoSection();
+  OS.SwitchSection(UnwindInfoSection);
+
+  // Emit a dummy symbol to force section inclusion.
+  OS.EmitLabel(OutContext.getOrCreateSymbol(Twine("__StackTransform_UnwindInfo")));
+
+  // Serialize data.
+  DEBUG(dbgs() << "********** Unwind Info Output **********\n");
+  emitUnwindInfo(OS);
+  OS.AddBlankLine();
+
+  // Switch to the unwind address range section & emit section
+  MCSection *UnwindAddrRangeSection =
+      OutContext.getObjectFileInfo()->getUnwindAddrRangeSection();
+  OS.SwitchSection(UnwindAddrRangeSection);
+  OS.EmitLabel(OutContext.getOrCreateSymbol(Twine("__StackTransform_UnwindAddrRange")));
+  emitAddrRangeInfo(OS);
+  OS.AddBlankLine();
+
+  Emitted = true;
+}
+
+const UnwindInfo::FuncUnwindInfo &
+UnwindInfo::getUnwindInfo(const MCSymbol *Func) const {
+  assert(Emitted && "Have not yet calculated per-function unwinding metadata");
+
+  FuncUnwindMap::const_iterator it = FuncUnwindMetadata.find(Func);
+  assert(it != FuncUnwindMetadata.end() && "Invalid function");
+  return it->second;
+}
+
+void UnwindInfo::print(raw_ostream &OS) {
+  OS << UIDbg << "Function unwinding information\n";
+  FuncCalleeMap::const_iterator b, e;
+  for(b = FuncCalleeSaved.begin(), e = FuncCalleeSaved.end();
+      b != e;
+      b++) {
+    OS << UIDbg << "Function - " << b->first->getName() << "\n";
+    const CalleeSavedRegisters &CSR = b->second;
+    CalleeSavedRegisters::const_iterator br, be;
+    for(br = CSR.begin(), be = CSR.end(); br != be; br++) {
+      OS << UIDbg << "Register " << br->DwarfReg
+                  << " at offset " << br->Offset << "\n";
+    }
+  }
+}
+
Index: lib/IR/AsmWriter.cpp
===================================================================
--- lib/IR/AsmWriter.cpp	(revision 320332)
+++ lib/IR/AsmWriter.cpp	(working copy)
@@ -691,6 +691,11 @@
   this->F = &F;
 }
 
+int ModuleSlotTracker::getLocalSlot(const Value *V) {
+  assert(F && "No function incorporated");
+  return Machine->getLocalSlot(V);
+}
+
 static SlotTracker *createSlotTracker(const Module *M) {
   return new SlotTracker(M);
 }
Index: lib/IR/DiagnosticInfo.cpp
===================================================================
--- lib/IR/DiagnosticInfo.cpp	(revision 320332)
+++ lib/IR/DiagnosticInfo.cpp	(working copy)
@@ -201,6 +201,11 @@
   return getSeverity() == DS_Warning;
 }
 
+bool DiagnosticInfoOptimizationError::isEnabled() const {
+  // Only print errors.
+  return getSeverity() == DS_Error;
+}
+
 void llvm::emitLoopVectorizeWarning(LLVMContext &Ctx, const Function &Fn,
                                     const DebugLoc &DLoc, const Twine &Msg) {
   Ctx.diagnose(DiagnosticInfoOptimizationFailure(
Index: lib/MC/MCCodeGenInfo.cpp
===================================================================
--- lib/MC/MCCodeGenInfo.cpp	(revision 320332)
+++ lib/MC/MCCodeGenInfo.cpp	(working copy)
@@ -20,4 +20,5 @@
   RelocationModel = RM;
   CMModel = CM;
   OptLevel = OL;
+  ArchIROptLevel = OL;
 }
Index: lib/MC/MCObjectFileInfo.cpp
===================================================================
--- lib/MC/MCObjectFileInfo.cpp	(revision 320332)
+++ lib/MC/MCObjectFileInfo.cpp	(working copy)
@@ -519,6 +519,15 @@
   DwarfAddrSection =
       Ctx->getELFSection(".debug_addr", ELF::SHT_PROGBITS, 0, "addr_sec");
 
+  UnwindAddrRangeSection =
+      Ctx->getELFSection(".stack_transform.unwind_arange", ELF::SHT_PROGBITS,
+                         0, sizeof(uint64_t) + sizeof(uint64_t), "");
+  UnwindInfoSection =
+      Ctx->getELFSection(".stack_transform.unwind", ELF::SHT_PROGBITS, 0,
+                         sizeof(uint16_t) + sizeof(int16_t), "");
+  UnwindAddrRangeSection->setAlignment(sizeof(uint64_t));
+  UnwindInfoSection->setAlignment(sizeof(uint16_t) + sizeof(int16_t));
+
   StackMapSection =
       Ctx->getELFSection(".llvm_stackmaps", ELF::SHT_PROGBITS, ELF::SHF_ALLOC);
 
Index: lib/Target/AArch64/AArch64AsmPrinter.cpp
===================================================================
--- lib/Target/AArch64/AArch64AsmPrinter.cpp	(revision 320332)
+++ lib/Target/AArch64/AArch64AsmPrinter.cpp	(working copy)
@@ -28,6 +28,7 @@
 #include "llvm/CodeGen/MachineModuleInfoImpls.h"
 #include "llvm/CodeGen/StackMaps.h"
 #include "llvm/CodeGen/TargetLoweringObjectFileImpl.h"
+#include "llvm/CodeGen/UnwindInfo.h"
 #include "llvm/IR/DataLayout.h"
 #include "llvm/IR/DebugInfo.h"
 #include "llvm/MC/MCAsmInfo.h"
@@ -49,11 +50,12 @@
 class AArch64AsmPrinter : public AsmPrinter {
   AArch64MCInstLower MCInstLowering;
   StackMaps SM;
+  UnwindInfo UI;
 
 public:
   AArch64AsmPrinter(TargetMachine &TM, std::unique_ptr<MCStreamer> Streamer)
       : AsmPrinter(TM, std::move(Streamer)), MCInstLowering(OutContext, *this),
-        SM(*this), AArch64FI(nullptr) {}
+        SM(*this), UI(*this), AArch64FI(nullptr) {}
 
   const char *getPassName() const override {
     return "AArch64 Assembly Printer";
@@ -83,7 +85,9 @@
 
   bool runOnMachineFunction(MachineFunction &F) override {
     AArch64FI = F.getInfo<AArch64FunctionInfo>();
-    return AsmPrinter::runOnMachineFunction(F);
+    bool retval = AsmPrinter::runOnMachineFunction(F);
+    UI.recordUnwindInfo(F);
+    return retval;
   }
 
 private:
@@ -129,8 +133,10 @@
     // linker can safely perform dead code stripping.  Since LLVM never
     // generates code that does this, it is always safe to set.
     OutStreamer->EmitAssemblerFlag(MCAF_SubsectionsViaSymbols);
-    SM.serializeToStackMapSection();
   }
+  UI.serializeToUnwindInfoSection();
+  SM.serializeToStackMapSection(&UI);
+  UI.reset(); // Must reset after SM serialization to clear metadata
 }
 
 MachineLocation
Index: lib/Target/AArch64/AArch64FrameLowering.cpp
===================================================================
--- lib/Target/AArch64/AArch64FrameLowering.cpp	(revision 320332)
+++ lib/Target/AArch64/AArch64FrameLowering.cpp	(working copy)
@@ -655,6 +655,15 @@
   return resolveFrameIndexReference(MF, FI, FrameReg);
 }
 
+/// getFrameIndexReferenceFromFP - Provide a base+offset reference to an FI
+/// slot for debug info, but force base to be the frame pointer (x29).
+int
+AArch64FrameLowering::getFrameIndexReferenceFromFP(const MachineFunction &MF,
+                                                   int FI,
+                                                   unsigned &FrameReg) const {
+  return resolveFrameIndexReference(MF, FI, FrameReg, true);
+}
+
 int AArch64FrameLowering::resolveFrameIndexReference(const MachineFunction &MF,
                                                      int FI, unsigned &FrameReg,
                                                      bool PreferFP) const {
Index: lib/Target/AArch64/AArch64FrameLowering.h
===================================================================
--- lib/Target/AArch64/AArch64FrameLowering.h	(revision 320332)
+++ lib/Target/AArch64/AArch64FrameLowering.h	(working copy)
@@ -40,6 +40,8 @@
   int getFrameIndexOffset(const MachineFunction &MF, int FI) const override;
   int getFrameIndexReference(const MachineFunction &MF, int FI,
                              unsigned &FrameReg) const override;
+  int getFrameIndexReferenceFromFP(const MachineFunction &MF, int FI,
+                                   unsigned &FrameReg) const override;
   int resolveFrameIndexReference(const MachineFunction &MF, int FI,
                                  unsigned &FrameReg,
                                  bool PreferFP = false) const;
Index: lib/Target/AArch64/AArch64ISelLowering.cpp
===================================================================
--- lib/Target/AArch64/AArch64ISelLowering.cpp	(revision 320332)
+++ lib/Target/AArch64/AArch64ISelLowering.cpp	(working copy)
@@ -2032,6 +2032,8 @@
     return LowerFSINCOS(Op, DAG);
   case ISD::MUL:
     return LowerMUL(Op, DAG);
+  case (uint16_t)~TargetOpcode::STACKMAP:
+    return SDValue(); // Use generic stackmap type legalizer
   }
 }
 
Index: lib/Target/AArch64/AArch64InstrInfo.td
===================================================================
--- lib/Target/AArch64/AArch64InstrInfo.td	(revision 320332)
+++ lib/Target/AArch64/AArch64InstrInfo.td	(working copy)
@@ -123,7 +123,7 @@
 def AArch64LOADgot       : SDNode<"AArch64ISD::LOADgot", SDTIntUnaryOp>;
 def AArch64callseq_start : SDNode<"ISD::CALLSEQ_START",
                                 SDCallSeqStart<[ SDTCisVT<0, i32> ]>,
-                                [SDNPHasChain, SDNPOutGlue]>;
+                                [SDNPHasChain, SDNPOptInGlue, SDNPOutGlue]>;
 def AArch64callseq_end   : SDNode<"ISD::CALLSEQ_END",
                                 SDCallSeqEnd<[ SDTCisVT<0, i32>,
                                                SDTCisVT<1, i32> ]>,
Index: lib/Target/AArch64/AArch64RegisterInfo.cpp
===================================================================
--- lib/Target/AArch64/AArch64RegisterInfo.cpp	(revision 320332)
+++ lib/Target/AArch64/AArch64RegisterInfo.cpp	(working copy)
@@ -215,6 +215,23 @@
   return TFI->hasFP(MF) ? AArch64::FP : AArch64::SP;
 }
 
+int AArch64RegisterInfo::getReturnAddrLoc(const MachineFunction &MF,
+                                          unsigned &BaseReg) const {
+  const TargetFrameLowering *TFL = MF.getSubtarget().getFrameLowering();
+  const MachineFrameInfo *MFI = MF.getFrameInfo();
+  assert(MFI->isCalleeSavedInfoValid() && "No callee-saved information");
+  const std::vector<CalleeSavedInfo> &CSI = MFI->getCalleeSavedInfo();
+
+  // The return address' location is the the link register's spill slot
+  for(unsigned i = 0, e = CSI.size(); i < e; i++)
+    if(CSI[i].getReg() == AArch64::LR)
+      return TFL->getFrameIndexReference(MF, CSI[i].getFrameIdx(), BaseReg);
+
+  // We didn't find it, is it actually saved?
+  BaseReg = 0;
+  return INT32_MAX;
+}
+
 bool AArch64RegisterInfo::requiresRegisterScavenging(
     const MachineFunction &MF) const {
   return true;
Index: lib/Target/AArch64/AArch64RegisterInfo.h
===================================================================
--- lib/Target/AArch64/AArch64RegisterInfo.h	(revision 320332)
+++ lib/Target/AArch64/AArch64RegisterInfo.h	(working copy)
@@ -91,6 +91,9 @@
   // Debug information queries.
   unsigned getFrameRegister(const MachineFunction &MF) const override;
 
+  int getReturnAddrLoc(const MachineFunction &MF,
+                       unsigned &BaseReg) const override;
+
   unsigned getRegPressureLimit(const TargetRegisterClass *RC,
                                MachineFunction &MF) const override;
   // Base pointer (stack realignment) support.
Index: lib/Target/AArch64/AArch64Subtarget.h
===================================================================
--- lib/Target/AArch64/AArch64Subtarget.h	(revision 320332)
+++ lib/Target/AArch64/AArch64Subtarget.h	(working copy)
@@ -19,6 +19,7 @@
 #include "AArch64InstrInfo.h"
 #include "AArch64RegisterInfo.h"
 #include "AArch64SelectionDAGInfo.h"
+#include "AArch64Values.h"
 #include "llvm/IR/DataLayout.h"
 #include "llvm/Target/TargetSubtargetInfo.h"
 #include <string>
@@ -63,6 +64,7 @@
   AArch64InstrInfo InstrInfo;
   AArch64SelectionDAGInfo TSInfo;
   AArch64TargetLowering TLInfo;
+  AArch64Values VGen;
 private:
   /// initializeSubtargetDependencies - Initializes using CPUString and the
   /// passed in feature string so that we can use initializer lists for
@@ -89,6 +91,9 @@
   const AArch64RegisterInfo *getRegisterInfo() const override {
     return &getInstrInfo()->getRegisterInfo();
   }
+  const AArch64Values *getValues() const override {
+    return &VGen;
+  }
   const Triple &getTargetTriple() const { return TargetTriple; }
   bool enableMachineScheduler() const override { return true; }
   bool enablePostRAScheduler() const override {
Index: lib/Target/AArch64/AArch64TargetMachine.cpp
===================================================================
--- lib/Target/AArch64/AArch64TargetMachine.cpp	(revision 320332)
+++ lib/Target/AArch64/AArch64TargetMachine.cpp	(working copy)
@@ -220,16 +220,20 @@
   // Cmpxchg instructions are often used with a subsequent comparison to
   // determine whether it succeeded. We can exploit existing control-flow in
   // ldrex/strex loops to simplify this, but it needs tidying up.
-  if (TM->getOptLevel() != CodeGenOpt::None && EnableAtomicTidy)
+  if (TM->getOptLevel() != CodeGenOpt::None &&
+      TM->getArchIROptLevel() != CodeGenOpt::None &&
+      EnableAtomicTidy)
     addPass(createCFGSimplificationPass());
 
   TargetPassConfig::addIRPasses();
 
   // Match interleaved memory accesses to ldN/stN intrinsics.
-  if (TM->getOptLevel() != CodeGenOpt::None)
+  if (TM->getOptLevel() != CodeGenOpt::None &&
+      TM->getArchIROptLevel() != CodeGenOpt::None)
     addPass(createInterleavedAccessPass(TM));
 
-  if (TM->getOptLevel() == CodeGenOpt::Aggressive && EnableGEPOpt) {
+  if (TM->getOptLevel() == CodeGenOpt::Aggressive &&
+      TM->getArchIROptLevel() != CodeGenOpt::None && EnableGEPOpt) {
     // Call SeparateConstOffsetFromGEP pass to extract constants within indices
     // and lower a GEP with multiple indices to either arithmetic operations or
     // multiple GEPs with single index.
@@ -247,12 +251,14 @@
 bool AArch64PassConfig::addPreISel() {
   // Run promote constant before global merge, so that the promoted constants
   // get a chance to be merged
-  if (TM->getOptLevel() != CodeGenOpt::None && EnablePromoteConstant)
+  if (TM->getOptLevel() != CodeGenOpt::None &&
+      TM->getArchIROptLevel() != CodeGenOpt::None && EnablePromoteConstant)
     addPass(createAArch64PromoteConstantPass());
   // FIXME: On AArch64, this depends on the type.
   // Basically, the addressable offsets are up to 4095 * Ty.getSizeInBytes().
   // and the offset has to be a multiple of the related size in bytes.
   if ((TM->getOptLevel() != CodeGenOpt::None &&
+       TM->getArchIROptLevel() != CodeGenOpt::None &&
        EnableGlobalMerge == cl::BOU_UNSET) ||
       EnableGlobalMerge == cl::BOU_TRUE) {
     bool OnlyOptimizeForSize = (TM->getOptLevel() < CodeGenOpt::Aggressive) &&
@@ -260,7 +266,8 @@
     addPass(createGlobalMergePass(TM, 4095, OnlyOptimizeForSize));
   }
 
-  if (TM->getOptLevel() != CodeGenOpt::None)
+  if (TM->getOptLevel() != CodeGenOpt::None &&
+      TM->getArchIROptLevel() != CodeGenOpt::None)
     addPass(createAArch64AddressTypePromotionPass());
 
   return false;
Index: lib/Target/AArch64/AArch64Values.cpp
===================================================================
--- lib/Target/AArch64/AArch64Values.cpp	(nonexistent)
+++ lib/Target/AArch64/AArch64Values.cpp	(working copy)
@@ -0,0 +1,253 @@
+//===- AArch64TargetValues.cpp - AArch64 specific value generator -===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "AArch64Values.h"
+#include "AArch64.h"
+#include "MCTargetDesc/AArch64AddressingModes.h"
+#include "llvm/CodeGen/MachineConstantPool.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Target/TargetInstrInfo.h"
+#include "llvm/Target/TargetSubtargetInfo.h"
+
+#define DEBUG_TYPE "stacktransform"
+
+using namespace llvm;
+
+static TemporaryValue *getTemporaryReference(const MachineInstr *MI,
+                                             const VirtRegMap *VRM,
+                                             unsigned Size) {
+  TemporaryValue *Val = nullptr;
+  if(MI->getOperand(0).isReg()) {
+    // Instruction format:    ADDXri  xd    xn    imm#  lsl#
+    // Stack slot reference:                <fi>  0     0
+    if(MI->getOperand(1).isFI() &&
+       MI->getOperand(2).isImm() && MI->getOperand(2).getImm() == 0 &&
+       MI->getOperand(3).isImm() && MI->getOperand(3).getImm() == 0) {
+      Val = new TemporaryValue;
+      Val->Type = TemporaryValue::StackSlotRef;
+      Val->Size = Size;
+      Val->Vreg = MI->getOperand(0).getReg();
+      Val->StackSlot = MI->getOperand(1).getIndex();
+      Val->Offset = 0;
+    }
+  }
+
+  return Val;
+}
+
+TemporaryValuePtr
+AArch64Values::getTemporaryValue(const MachineInstr *MI,
+                                 const VirtRegMap *VRM) const {
+  TemporaryValue *Val = nullptr;
+  switch(MI->getOpcode()) {
+  case AArch64::ADDXri: Val = getTemporaryReference(MI, VRM, 8); break;
+  default: break;
+  }
+  return TemporaryValuePtr(Val);
+}
+
+typedef ValueGenInst::InstType InstType;
+template <InstType T> using RegInstruction = RegInstruction<T>;
+template <InstType T> using ImmInstruction = ImmInstruction<T>;
+
+// Bitwise-conversions between floats & ints
+union IntFloat64 { double d; uint64_t i; };
+union IntFloat32 { float f; uint64_t i; };
+
+MachineLiveVal *
+AArch64Values::genADDInstructions(const MachineInstr *MI) const {
+  int Index;
+
+  switch(MI->getOpcode()) {
+  case AArch64::ADDXri:
+    if(MI->getOperand(1).isFI()) {
+      Index = MI->getOperand(1).getIndex();
+      assert(MI->getOperand(2).isImm() && MI->getOperand(2).getImm() == 0);
+      assert(MI->getOperand(3).isImm() && MI->getOperand(3).getImm() == 0);
+      return new MachineStackObject(Index, false, MI, true);
+    }
+    break;
+  default:
+    DEBUG(dbgs() << "Unhandled ADD machine instruction");
+    break;
+  }
+  return nullptr;
+}
+
+MachineLiveVal *
+AArch64Values::genADRPInstructions(const MachineInstr *MI) const {
+  ValueGenInstList IL;
+  if(isSymbolValue(MI->getOperand(1))) {
+    IL.emplace_back(new RefInstruction(MI->getOperand(1)));
+    IL.emplace_back(new ImmInstruction<InstType::Mask>(8, ~0xfff));
+    return new MachineGeneratedVal(IL, MI, false);
+  }
+  return nullptr;
+}
+
+MachineLiveVal *
+AArch64Values::genBitfieldInstructions(const MachineInstr *MI) const {
+  int64_t R, S;
+  unsigned Size, Bits;
+  uint64_t Mask;
+  ValueGenInstList IL;
+
+  switch(MI->getOpcode()) {
+  case AArch64::UBFMXri:
+    Size = 8;
+    Bits = 64;
+    Mask = UINT64_MAX;
+
+    assert(MI->getOperand(1).isReg() &&
+           MI->getOperand(2).isImm() &&
+           MI->getOperand(3).isImm());
+
+    // TODO ensure this is correct
+    IL.emplace_back(
+      new RegInstruction<InstType::Set>(MI->getOperand(1).getReg()));
+    R = MI->getOperand(2).getImm();
+    S = MI->getOperand(3).getImm();
+    if(S >= R) {
+      IL.emplace_back(new ImmInstruction<InstType::RightShiftLog>(Size, R));
+      IL.emplace_back(
+        new ImmInstruction<InstType::Mask>(Size, ~(Mask << (S - R + 1))));
+    }
+    else {
+      IL.emplace_back(
+        new ImmInstruction<InstType::Mask>(Size, ~(Mask << (S + 1))));
+      IL.emplace_back(new ImmInstruction<InstType::LeftShift>(Size, Bits - R));
+    }
+    return new MachineGeneratedVal(IL, MI, false);
+    break;
+  default:
+    DEBUG(dbgs() << "Unhandled bitfield instruction");
+    break;
+  }
+  return nullptr;
+}
+
+MachineLiveVal *
+AArch64Values::genLoadRegValue(const MachineInstr *MI) const {
+  switch(MI->getOpcode()) {
+  case AArch64::LDRDui:
+    if(MI->getOperand(2).isCPI()) {
+      int Idx = MI->getOperand(2).getIndex();
+      const MachineFunction *MF = MI->getParent()->getParent();
+      const MachineConstantPool *MCP = MF->getConstantPool();
+      const std::vector<MachineConstantPoolEntry> &CP = MCP->getConstants();
+      if(CP[Idx].isMachineConstantPoolEntry()) {
+        // TODO unhandled for now
+      }
+      else {
+        const Constant *Val = CP[Idx].Val.ConstVal;
+        if(isa<ConstantFP>(Val)) {
+          const ConstantFP *FPVal = cast<ConstantFP>(Val);
+          const APFloat &Flt = FPVal->getValueAPF();
+          switch(APFloat::getSizeInBits(Flt.getSemantics())) {
+          case 32: {
+            IntFloat32 I2F = { Flt.convertToFloat() };
+            return new MachineImmediate(4, I2F.i, MI, false);
+          }
+          case 64: {
+            IntFloat64 I2D = { Flt.convertToDouble() };
+            return new MachineImmediate(8, I2D.i, MI, false);
+          }
+          default: break;
+          }
+        }
+      }
+    }
+    break;
+  case AArch64::LDRXui:
+    // Note: if this is of the form %vreg, <ga:...>, then the compiler has
+    // emitted multiple instructions in order to form the full address.  We,
+    // however, don't have the instruction encoding limitations.
+    // TODO verify this note above is true, maybe using MO::getTargetFlags?
+    // Note 2: we *must* ensure the symbol is const-qualified, otherwise we
+    // risk creating a new value if the symbol's value changes between when the
+    // initial load would have occurred and the transformation, e.g.,
+    //
+    //   ldr x20, <ga:mysym>
+    //   ... (somebody changes mysym's value) ...
+    //   bl <ga:myfunc>
+    //
+    // In this situation, the transformation occurs at the call site and
+    // retrieves the updated value rather than the value that would have been
+    // loaded at the ldr instruction.
+    if(TargetValues::isSymbolValue(MI->getOperand(2)) &&
+       TargetValues::isSymbolValueConstant(MI->getOperand(2)))
+      return new MachineSymbolRef(MI->getOperand(2), true, MI);
+    break;
+  default: break;
+  }
+  return nullptr;
+}
+
+MachineLiveValPtr AArch64Values::getMachineValue(const MachineInstr *MI) const {
+  IntFloat64 Conv64;
+  MachineLiveVal* Val = nullptr;
+  const MachineOperand *MO;
+  const TargetInstrInfo *TII;
+
+  switch(MI->getOpcode()) {
+  case AArch64::ADDXri:
+    Val = genADDInstructions(MI);
+    break;
+  case AArch64::ADRP:
+    Val = genADRPInstructions(MI);
+    break;
+  case AArch64::MOVaddr:
+    MO = &MI->getOperand(1);
+    if(MO->isCPI())
+      Val = new MachineConstPoolRef(MO->getIndex(), MI);
+    else if(TargetValues::isSymbolValue(MO))
+      Val = new MachineSymbolRef(*MO, false, MI);
+    break;
+  case AArch64::COPY:
+    MO = &MI->getOperand(1);
+    if(MO->isReg() && MO->getReg() == AArch64::LR) Val = new ReturnAddress(MI);
+    break;
+  case AArch64::FMOVD0:
+    Conv64.d = 0.0;
+    Val = new MachineImmediate(8, Conv64.i, MI, false);
+    break;
+  case AArch64::FMOVDi:
+    Conv64.d = (double)AArch64_AM::getFPImmFloat(MI->getOperand(1).getImm());
+    Val = new MachineImmediate(8, Conv64.i, MI, false);
+    break;
+  case AArch64::LDRXui:
+  case AArch64::LDRDui:
+    Val = genLoadRegValue(MI);
+    break;
+  case AArch64::MOVi32imm:
+    MO = &MI->getOperand(1);
+    assert(MO->isImm() && "Invalid immediate for MOVi32imm");
+    Val = new MachineImmediate(4, MO->getImm(), MI, false);
+    break;
+  case AArch64::MOVi64imm:
+    MO = &MI->getOperand(1);
+    assert(MO->isImm() && "Invalid immediate for MOVi64imm");
+    Val = new MachineImmediate(8, MO->getImm(), MI, false);
+    break;
+  case AArch64::UBFMXri:
+    Val = genBitfieldInstructions(MI);
+    break;
+  default:
+    TII =  MI->getParent()->getParent()->getSubtarget().getInstrInfo();
+    DEBUG(dbgs() << "Unhandled opcode: "
+                 << TII->getName(MI->getOpcode()) << "\n");
+    break;
+  }
+
+  return MachineLiveValPtr(Val);
+}
+
Index: lib/Target/AArch64/AArch64Values.h
===================================================================
--- lib/Target/AArch64/AArch64Values.h	(nonexistent)
+++ lib/Target/AArch64/AArch64Values.h	(working copy)
@@ -0,0 +1,29 @@
+//===----- AArch64TargetValues.cpp - AArch64 specific value generator -----===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Target/TargetValues.h"
+
+namespace llvm {
+
+class AArch64Values final : public TargetValues {
+public:
+  AArch64Values() {}
+  virtual TemporaryValuePtr getTemporaryValue(const MachineInstr *MI,
+                                              const VirtRegMap *VRM) const;
+  virtual MachineLiveValPtr getMachineValue(const MachineInstr *MI) const;
+
+private:
+  MachineLiveVal *genADDInstructions(const MachineInstr *MI) const;
+  MachineLiveVal *genADRPInstructions(const MachineInstr *MI) const;
+  MachineLiveVal *genBitfieldInstructions(const MachineInstr *MI) const;
+  MachineLiveVal *genLoadRegValue(const MachineInstr *MI) const;
+};
+
+}
+
Index: lib/Target/AArch64/CMakeLists.txt
===================================================================
--- lib/Target/AArch64/CMakeLists.txt	(revision 320332)
+++ lib/Target/AArch64/CMakeLists.txt	(working copy)
@@ -43,6 +43,7 @@
   AArch64TargetMachine.cpp
   AArch64TargetObjectFile.cpp
   AArch64TargetTransformInfo.cpp
+  AArch64Values.cpp
 )
 
 add_dependencies(LLVMAArch64CodeGen intrinsics_gen)
Index: lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp
===================================================================
--- lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp	(revision 320332)
+++ lib/Target/AArch64/MCTargetDesc/AArch64MCCodeEmitter.cpp	(working copy)
@@ -281,7 +281,13 @@
 
   ++MCNumFixups;
 
-  return 0;
+  // Set the shift bit of the add instruction for relocation types
+  // R_AARCH64_TLSLE_ADD_TPREL_HI12 and R_AARCH64_TLSLD_ADD_DTPREL_HI12.
+  AArch64MCExpr::VariantKind RefKind = cast<AArch64MCExpr>(Expr)->getKind();
+  if (RefKind == AArch64MCExpr::VK_TPREL_HI12 ||
+      RefKind == AArch64MCExpr::VK_DTPREL_HI12)
+    ShiftVal = 12;
+  return ShiftVal == 0 ? 0 : (1 << ShiftVal);
 }
 
 /// getCondBranchTargetOpValue - Return the encoded value for a conditional
Index: lib/Target/CMakeLists.txt
===================================================================
--- lib/Target/CMakeLists.txt	(revision 320332)
+++ lib/Target/CMakeLists.txt	(working copy)
@@ -8,6 +8,7 @@
   TargetMachineC.cpp
   TargetRecip.cpp
   TargetSubtargetInfo.cpp
+  TargetValues.cpp
 
   ADDITIONAL_HEADER_DIRS
   ${LLVM_MAIN_INCLUDE_DIR}/llvm/Target
Index: lib/Target/PowerPC/CMakeLists.txt
===================================================================
--- lib/Target/PowerPC/CMakeLists.txt	(revision 320332)
+++ lib/Target/PowerPC/CMakeLists.txt	(working copy)
@@ -34,6 +34,7 @@
   PPCTargetTransformInfo.cpp
   PPCTOCRegDeps.cpp
   PPCTLSDynamicCall.cpp
+  PPCValues.cpp
   PPCVSXCopy.cpp
   PPCVSXFMAMutate.cpp
   PPCVSXSwapRemoval.cpp
Index: lib/Target/PowerPC/PPCAsmPrinter.cpp
===================================================================
--- lib/Target/PowerPC/PPCAsmPrinter.cpp	(revision 320332)
+++ lib/Target/PowerPC/PPCAsmPrinter.cpp	(working copy)
@@ -36,6 +36,7 @@
 #include "llvm/CodeGen/MachineRegisterInfo.h"
 #include "llvm/CodeGen/StackMaps.h"
 #include "llvm/CodeGen/TargetLoweringObjectFileImpl.h"
+#include "llvm/CodeGen/UnwindInfo.h"
 #include "llvm/IR/Constants.h"
 #include "llvm/IR/DebugInfo.h"
 #include "llvm/IR/DerivedTypes.h"
@@ -70,10 +71,11 @@
     MapVector<MCSymbol*, MCSymbol*> TOC;
     const PPCSubtarget *Subtarget;
     StackMaps SM;
+    UnwindInfo UI;
   public:
     explicit PPCAsmPrinter(TargetMachine &TM,
                            std::unique_ptr<MCStreamer> Streamer)
-        : AsmPrinter(TM, std::move(Streamer)), SM(*this) {}
+        : AsmPrinter(TM, std::move(Streamer)), SM(*this), UI(*this) {}
 
     const char *getPassName() const override {
       return "PowerPC Assembly Printer";
@@ -99,9 +101,33 @@
     void LowerPATCHPOINT(MCStreamer &OutStreamer, StackMaps &SM,
                          const MachineInstr &MI);
     void EmitTlsCall(const MachineInstr *MI, MCSymbolRefExpr::VariantKind VK);
+
+    virtual int getCanonicalReturnAddr(const MachineInstr *Call) const override;
+
     bool runOnMachineFunction(MachineFunction &MF) override {
       Subtarget = &MF.getSubtarget<PPCSubtarget>();
-      return AsmPrinter::runOnMachineFunction(MF);
+      bool retval = AsmPrinter::runOnMachineFunction(MF);
+
+      // Add this function's register unwind info.  The PowerPC backend doesn't
+      // maintain the saved FBP (old r31) and link register as callee-saved
+      // registers, so manually add where they're saved.
+      if(MF.getFrameInfo()->hasStackMap()) {
+        UI.recordUnwindInfo(MF);
+
+        // Add the LR & FP save slots
+        const TargetFrameLowering *TFL = Subtarget->getFrameLowering();
+        const PPCFunctionInfo *FI = MF.getInfo<PPCFunctionInfo>();
+        int Index, Offset;
+        unsigned BaseReg;
+
+        Offset = MF.getFrameInfo()->getStackSize() + 16;
+        UI.addRegisterUnwindInfo(MF, PPC::LR8, Offset);
+
+        Index = FI->getFramePointerSaveIndex();
+        Offset = TFL->getFrameIndexReferenceFromFP(MF, Index, BaseReg);
+        UI.addRegisterUnwindInfo(MF, PPC::X31, Offset);
+      }
+      return retval;
     }
   };
 
@@ -327,7 +353,9 @@
 }
 
 void PPCAsmPrinter::EmitEndOfAsmFile(Module &M) {
-  SM.serializeToStackMapSection();
+  UI.serializeToUnwindInfoSection();
+  SM.serializeToStackMapSection(&UI);
+  UI.reset(); // Must reset after SM serialization to clear metadata
 }
 
 void PPCAsmPrinter::LowerSTACKMAP(MCStreamer &OutStreamer, StackMaps &SM,
@@ -490,6 +518,19 @@
                  .addExpr(SymVar));
 }
 
+/// getCanonicalReturnAddr -- for machine instructions which actually codegen
+/// a call + other instructions, return an offset which would correct a label
+/// to point to the call's actual return address.
+int PPCAsmPrinter::getCanonicalReturnAddr(const MachineInstr *Call) const {
+  switch(Call->getOpcode()) {
+  case PPC::BL8_NOP:
+  case PPC::BLA8_NOP:
+  case PPC::BL8_NOP_TLS:
+  case PPC::BCTRL8_LDinto_toc: return 4;
+  default: return 0;
+  }
+}
+
 /// EmitInstruction -- Print out a single PowerPC MI in Darwin syntax to
 /// the current output stream.
 ///
Index: lib/Target/PowerPC/PPCInstrInfo.td
===================================================================
--- lib/Target/PowerPC/PPCInstrInfo.td	(revision 320332)
+++ lib/Target/PowerPC/PPCInstrInfo.td	(working copy)
@@ -162,7 +162,7 @@
 
 // These are target-independent nodes, but have target-specific formats.
 def callseq_start : SDNode<"ISD::CALLSEQ_START", SDT_PPCCallSeqStart,
-                           [SDNPHasChain, SDNPOutGlue]>;
+                           [SDNPHasChain, SDNPOptInGlue, SDNPOutGlue]>;
 def callseq_end   : SDNode<"ISD::CALLSEQ_END",   SDT_PPCCallSeqEnd,
                            [SDNPHasChain, SDNPOptInGlue, SDNPOutGlue]>;
 
Index: lib/Target/PowerPC/PPCSubtarget.h
===================================================================
--- lib/Target/PowerPC/PPCSubtarget.h	(revision 320332)
+++ lib/Target/PowerPC/PPCSubtarget.h	(working copy)
@@ -17,6 +17,7 @@
 #include "PPCFrameLowering.h"
 #include "PPCISelLowering.h"
 #include "PPCInstrInfo.h"
+#include "PPCValues.h"
 #include "llvm/ADT/Triple.h"
 #include "llvm/IR/DataLayout.h"
 #include "llvm/MC/MCInstrItineraries.h"
@@ -129,6 +130,7 @@
   PPCFrameLowering FrameLowering;
   PPCInstrInfo InstrInfo;
   PPCTargetLowering TLInfo;
+  PPCValues VGen;
   TargetSelectionDAGInfo TSInfo;
 
 public:
@@ -171,6 +173,7 @@
     return &getInstrInfo()->getRegisterInfo();
   }
   const PPCTargetMachine &getTargetMachine() const { return TM; }
+  const PPCValues *getValues() const override { return &VGen; }
 
   /// initializeSubtargetDependencies - Initializes using a CPU and feature string
   /// so that we can use initializer lists for subtarget initialization.
Index: lib/Target/PowerPC/PPCValues.cpp
===================================================================
--- lib/Target/PowerPC/PPCValues.cpp	(nonexistent)
+++ lib/Target/PowerPC/PPCValues.cpp	(working copy)
@@ -0,0 +1,46 @@
+//===--------- PPCTargetValues.cpp - PPC specific value generator ---------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "PPCFrameLowering.h"
+#include "PPCValues.h"
+#include "PPC.h"
+#include "llvm/CodeGen/MachineRegisterInfo.h"
+#include "llvm/CodeGen/MachineOperand.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+#define DEBUG_TYPE "stacktransform"
+
+using namespace llvm;
+
+MachineLiveValPtr PPCValues::getMachineValue(const MachineInstr *MI) const {
+  // TODO
+  return nullptr;
+}
+
+void PPCValues::addRequiredArchLiveValues(MachineFunction *MF,
+                                          const MachineInstr *MIStackMap,
+                                          const CallInst *IRStackMap) const {
+  if(!MF->getRegInfo().use_empty(PPC::X2)) {
+    MachineOperand TOCRef = MachineOperand::CreateES(".TOC.");
+    MachineSymbolRef TOCSym(TOCRef, false, MIStackMap);
+
+    DEBUG(dbgs() << "   + Setting R2 to be TOC pointer\n");
+    MachineLiveReg TOCPtr(PPC::X2);
+    MF->addSMArchSpecificLocation(IRStackMap, TOCPtr, TOCSym);
+
+    // Per the ELFv2 ABI, the TOC Pointer Doubleword save area is at SP + 24
+    DEBUG(dbgs() << "   + Setting TOC pointer save slot to be TOC pointer\n");
+    const PPCFrameLowering *PFL =
+      (const PPCFrameLowering *)MF->getSubtarget().getFrameLowering();
+    MachineLiveStackAddr TOCSS(PFL->getTOCSaveOffset(), PPC::X1, 8);
+    MF->addSMArchSpecificLocation(IRStackMap, TOCSS, TOCSym);
+  }
+}
+
Index: lib/Target/PowerPC/PPCValues.h
===================================================================
--- lib/Target/PowerPC/PPCValues.h	(nonexistent)
+++ lib/Target/PowerPC/PPCValues.h	(working copy)
@@ -0,0 +1,24 @@
+//===--------- PPCTargetValues.cpp - PPC specific value generator ---------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Target/TargetValues.h"
+
+namespace llvm {
+
+class PPCValues final : public TargetValues {
+public:
+  PPCValues() {}
+  virtual MachineLiveValPtr getMachineValue(const MachineInstr *MI) const;
+  virtual void addRequiredArchLiveValues(MachineFunction *MF,
+                                         const MachineInstr *MIStackMap,
+                                         const CallInst *IRStackMap) const;
+};
+
+}
+
Index: lib/Target/TargetMachine.cpp
===================================================================
--- lib/Target/TargetMachine.cpp	(revision 320332)
+++ lib/Target/TargetMachine.cpp	(working copy)
@@ -149,6 +149,17 @@
     CodeGenInfo->setOptLevel(Level);
 }
 
+CodeGenOpt::Level TargetMachine::getArchIROptLevel() const {
+  if (!CodeGenInfo)
+    return CodeGenOpt::Default;
+  return CodeGenInfo->getArchIROptLevel();
+}
+
+void TargetMachine::setArchIROptLevel(CodeGenOpt::Level Level) const {
+  if (CodeGenInfo)
+    CodeGenInfo->setArchIROptLevel(Level);
+}
+
 TargetIRAnalysis TargetMachine::getTargetIRAnalysis() {
   return TargetIRAnalysis([this](Function &F) {
     return TargetTransformInfo(F.getParent()->getDataLayout());
Index: lib/Target/TargetValues.cpp
===================================================================
--- lib/Target/TargetValues.cpp	(nonexistent)
+++ lib/Target/TargetValues.cpp	(working copy)
@@ -0,0 +1,41 @@
+//===--------- TargetValues.cpp - Target value generator helpers ----------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/IR/GlobalVariable.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Target/TargetValues.h"
+
+#define DEBUG_TYPE "stacktransform"
+
+using namespace llvm;
+
+bool TargetValues::isSymbolValueConstant(const MachineOperand &MO) {
+  const GlobalValue *GV;
+  const GlobalVariable *GVar;
+
+  switch(MO.getType()) {
+  case MachineOperand::MO_GlobalAddress:
+    GV = MO.getGlobal();
+    if(isa<Function>(GV)) return true;
+    else if((GVar = dyn_cast<GlobalVariable>(GV)) && GVar->isConstant())
+      return true;
+    break;
+  case MachineOperand::MO_ExternalSymbol:
+    // TODO
+    break;
+  case MachineOperand::MO_MCSymbol:
+    // TODO
+    break;
+  default:
+    DEBUG(dbgs() << "Unhandled reference type\n");
+    break;
+  }
+  return false;
+}
+
Index: lib/Target/X86/CMakeLists.txt
===================================================================
--- lib/Target/X86/CMakeLists.txt	(revision 320332)
+++ lib/Target/X86/CMakeLists.txt	(working copy)
@@ -34,6 +34,7 @@
   X86VZeroUpper.cpp
   X86FixupLEAs.cpp
   X86WinEHState.cpp
+  X86Values.cpp
   )
 
 if( CMAKE_CL_64 )
Index: lib/Target/X86/X86AsmPrinter.cpp
===================================================================
--- lib/Target/X86/X86AsmPrinter.cpp	(revision 320332)
+++ lib/Target/X86/X86AsmPrinter.cpp	(working copy)
@@ -24,6 +24,7 @@
 #include "llvm/CodeGen/TargetLoweringObjectFileImpl.h"
 #include "llvm/IR/DebugInfo.h"
 #include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/DiagnosticInfo.h"
 #include "llvm/IR/Mangler.h"
 #include "llvm/IR/Module.h"
 #include "llvm/IR/Type.h"
@@ -49,6 +50,8 @@
 bool X86AsmPrinter::runOnMachineFunction(MachineFunction &MF) {
   Subtarget = &MF.getSubtarget<X86Subtarget>();
 
+  bool modified = TagCallSites(MF);
+
   SMShadowTracker.startFunction(MF);
 
   SetupMachineFunction(MF);
@@ -66,8 +69,17 @@
   // Emit the rest of the function body.
   EmitFunctionBody();
 
-  // We didn't modify anything.
-  return false;
+  // Add this function's register unwind info.  The x86 backend doesn't
+  // maintain the saved FBP (old RBP) and return address (RIP) as callee-saved
+  // registers, so manually add where they're saved.
+  if(MF.getFrameInfo()->hasStackMap()) {
+    UI.recordUnwindInfo(MF);
+    UI.addRegisterUnwindInfo(MF, X86::RIP, 8);
+    UI.addRegisterUnwindInfo(MF, X86::RBP, 0);
+  }
+
+  // We may have modified where stack map intrinsics are located.
+  return modified;
 }
 
 /// printSymbolOperand - Print a raw symbol reference operand.  This handles
@@ -689,8 +701,10 @@
   }
 
   if (TT.isOSBinFormatELF()) {
-    SM.serializeToStackMapSection();
+    UI.serializeToUnwindInfoSection();
+    SM.serializeToStackMapSection(&UI);
     FM.serializeToFaultMapSection();
+    UI.reset(); // Must reset after SM serialization to clear metadata
   }
 }
 
Index: lib/Target/X86/X86AsmPrinter.h
===================================================================
--- lib/Target/X86/X86AsmPrinter.h	(revision 320332)
+++ lib/Target/X86/X86AsmPrinter.h	(working copy)
@@ -14,6 +14,7 @@
 #include "llvm/CodeGen/AsmPrinter.h"
 #include "llvm/CodeGen/FaultMaps.h"
 #include "llvm/CodeGen/StackMaps.h"
+#include "llvm/CodeGen/UnwindInfo.h"
 #include "llvm/Target/TargetMachine.h"
 
 // Implemented in X86MCInstLower.cpp
@@ -28,6 +29,7 @@
 class LLVM_LIBRARY_VISIBILITY X86AsmPrinter : public AsmPrinter {
   const X86Subtarget *Subtarget;
   StackMaps SM;
+  UnwindInfo UI;
   FaultMaps FM;
 
   // This utility class tracks the length of a stackmap instruction's 'shadow'.
@@ -90,8 +92,8 @@
  public:
    explicit X86AsmPrinter(TargetMachine &TM,
                           std::unique_ptr<MCStreamer> Streamer)
-       : AsmPrinter(TM, std::move(Streamer)), SM(*this), FM(*this),
-         SMShadowTracker(TM) {}
+       : AsmPrinter(TM, std::move(Streamer)), SM(*this), UI(*this),
+         FM(*this), SMShadowTracker(TM) {}
 
   const char *getPassName() const override {
     return "X86 Assembly / Object Emitter";
Index: lib/Target/X86/X86ISelLowering.cpp
===================================================================
--- lib/Target/X86/X86ISelLowering.cpp	(revision 320332)
+++ lib/Target/X86/X86ISelLowering.cpp	(working copy)
@@ -18621,6 +18621,8 @@
   case ISD::GC_TRANSITION_START:
                                 return LowerGC_TRANSITION_START(Op, DAG);
   case ISD::GC_TRANSITION_END:  return LowerGC_TRANSITION_END(Op, DAG);
+  case (uint16_t)~TargetOpcode::STACKMAP:
+    return SDValue(); // Use generic stackmap type legalizer
   }
 }
 
Index: lib/Target/X86/X86InstrInfo.td
===================================================================
--- lib/Target/X86/X86InstrInfo.td	(revision 320332)
+++ lib/Target/X86/X86InstrInfo.td	(working copy)
@@ -169,7 +169,7 @@
                          SDNPMemOperand]>;
 def X86callseq_start :
                  SDNode<"ISD::CALLSEQ_START", SDT_X86CallSeqStart,
-                        [SDNPHasChain, SDNPOutGlue]>;
+                        [SDNPHasChain, SDNPOptInGlue, SDNPOutGlue]>;
 def X86callseq_end :
                  SDNode<"ISD::CALLSEQ_END",   SDT_X86CallSeqEnd,
                         [SDNPHasChain, SDNPOptInGlue, SDNPOutGlue]>;
Index: lib/Target/X86/X86RegisterInfo.cpp
===================================================================
--- lib/Target/X86/X86RegisterInfo.cpp	(revision 320332)
+++ lib/Target/X86/X86RegisterInfo.cpp	(working copy)
@@ -596,6 +596,13 @@
   return FrameReg;
 }
 
+int X86RegisterInfo::getReturnAddrLoc(const MachineFunction &MF,
+                                      unsigned &BaseReg) const {
+  const X86MachineFunctionInfo *X86FI = MF.getInfo<X86MachineFunctionInfo>();
+  const TargetFrameLowering *TFL = MF.getSubtarget().getFrameLowering();
+  return TFL->getFrameIndexReference(MF, X86FI->getRAIndex(), BaseReg);
+}
+
 namespace llvm {
 unsigned getX86SubSuperRegisterOrZero(unsigned Reg, MVT::SimpleValueType VT,
                                       bool High) {
Index: lib/Target/X86/X86RegisterInfo.h
===================================================================
--- lib/Target/X86/X86RegisterInfo.h	(revision 320332)
+++ lib/Target/X86/X86RegisterInfo.h	(working copy)
@@ -126,6 +126,9 @@
   unsigned getBaseRegister() const { return BasePtr; }
   // FIXME: Move to FrameInfok
   unsigned getSlotSize() const { return SlotSize; }
+
+  int getReturnAddrLoc(const MachineFunction &MF,
+                       unsigned &BaseReg) const override;
 };
 
 /// Returns the sub or super register of a specific X86 register.
Index: lib/Target/X86/X86Subtarget.h
===================================================================
--- lib/Target/X86/X86Subtarget.h	(revision 320332)
+++ lib/Target/X86/X86Subtarget.h	(working copy)
@@ -18,6 +18,7 @@
 #include "X86ISelLowering.h"
 #include "X86InstrInfo.h"
 #include "X86SelectionDAGInfo.h"
+#include "X86Values.h"
 #include "llvm/ADT/Triple.h"
 #include "llvm/IR/CallingConv.h"
 #include "llvm/Target/TargetSubtargetInfo.h"
@@ -248,7 +249,7 @@
   X86InstrInfo InstrInfo;
   X86TargetLowering TLInfo;
   X86FrameLowering FrameLowering;
-
+  X86Values VGen;
 public:
   /// This constructor initializes the data members to match that
   /// of the specified triple.
@@ -269,6 +270,7 @@
   const X86RegisterInfo *getRegisterInfo() const override {
     return &getInstrInfo()->getRegisterInfo();
   }
+  const X86Values *getValues() const override { return &VGen; }
 
   /// Returns the minimum alignment known to hold of the
   /// stack frame on entry to the function and which must be maintained by every
Index: lib/Target/X86/X86Values.cpp
===================================================================
--- lib/Target/X86/X86Values.cpp	(nonexistent)
+++ lib/Target/X86/X86Values.cpp	(working copy)
@@ -0,0 +1,212 @@
+//===--------- X86TargetValues.cpp - X86 specific value generator ---------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "X86Values.h"
+#include "X86InstrInfo.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Target/TargetInstrInfo.h"
+#include "llvm/Target/TargetSubtargetInfo.h"
+
+#define DEBUG_TYPE "stacktransform"
+
+using namespace llvm;
+
+static TemporaryValue *getTemporaryReference(const MachineInstr *MI,
+                                             const VirtRegMap *VRM,
+                                             unsigned Size) {
+  TemporaryValue *Val = nullptr;
+  if(MI->getOperand(0).isReg()) {
+    // Instruction format:  LEA64  rd     rbase  scale# ridx   disp#  rseg
+    // Stack slot reference:              <fi>   1      noreg  off    noreg
+    // TODO check for noreg in ridx & rseg?
+    if(MI->getOperand(1 + X86::AddrBaseReg).isFI() &&
+       MI->getOperand(1 + X86::AddrScaleAmt).isImm() &&
+       MI->getOperand(1 + X86::AddrScaleAmt).getImm() == 1) {
+      assert(MI->getOperand(1 + X86::AddrDisp).isImm() && "Invalid encoding");
+      Val = new TemporaryValue;
+      Val->Type = TemporaryValue::StackSlotRef;
+      Val->Size = Size;
+      Val->Vreg = MI->getOperand(0).getReg();
+      Val->StackSlot = MI->getOperand(1 + X86::AddrBaseReg).getIndex();
+      Val->Offset = MI->getOperand(1 + X86::AddrDisp).getImm();
+    }
+  }
+  return Val;
+}
+
+TemporaryValuePtr
+X86Values::getTemporaryValue(const MachineInstr *MI,
+                             const VirtRegMap *VRM) const {
+  TemporaryValue *Val = nullptr;
+  switch(MI->getOpcode()) {
+  case X86::LEA64r: Val = getTemporaryReference(MI, VRM, 8); break;
+  default: break;
+  }
+  return TemporaryValuePtr(Val);
+}
+
+typedef ValueGenInst::InstType InstType;
+template <InstType T> using RegInstruction = RegInstruction<T>;
+template <InstType T> using ImmInstruction = ImmInstruction<T>;
+
+/// Return whether the machine operand is a specific immediate value.
+static bool isImmOp(const MachineOperand &MO, int64_t Imm) {
+  if(!MO.isImm()) return false;
+  else if(MO.getImm() != Imm) return false;
+  else return true;
+}
+
+/// Return whether the machine operand is a specific register.
+static bool isRegOp(const MachineOperand &MO, unsigned Reg) {
+  if(!MO.isReg()) return false;
+  else if(MO.getReg() != Reg) return false;
+  else return true;
+}
+
+/// Return whether the machine operand is the sentinal %noreg register.
+static bool isNoRegOp(const MachineOperand &MO) { return isRegOp(MO, 0); }
+
+MachineLiveVal *X86Values::genLEAInstructions(const MachineInstr *MI) const {
+  unsigned Reg, Size;
+  int64_t Imm;
+  ValueGenInstList IL;
+
+  // TODO do we need to handle the segment register operand?
+  switch(MI->getOpcode()) {
+  case X86::LEA64r:
+    Size = 8;
+
+    if(MI->getOperand(1 + X86::AddrBaseReg).isFI()) {
+      // Stack slot address
+      if(!isImmOp(MI->getOperand(1 + X86::AddrScaleAmt), 1)) {
+        DEBUG(dbgs() << "Unhandled scale amount for frame index\n");
+        break;
+      }
+
+      if(!isNoRegOp(MI->getOperand(1 + X86::AddrIndexReg))) {
+        DEBUG(dbgs() <<  "Unhandled index register for frame index\n");
+        break;
+      }
+
+      if(!isImmOp(MI->getOperand(1 + X86::AddrDisp), 0)) {
+        DEBUG(dbgs() << "Unhandled index register for frame index\n");
+        break;
+      }
+
+      return new
+        MachineStackObject(MI->getOperand(1 + X86::AddrBaseReg).getIndex(),
+                           false, MI, true);
+    }
+    else if(isRegOp(MI->getOperand(1 + X86::AddrBaseReg), X86::RIP)) {
+      // PC-relative symbol address
+      if(!isImmOp(MI->getOperand(1 + X86::AddrScaleAmt), 1)) {
+        DEBUG(dbgs() << "Unhandled scale amount for PC-relative address\n");
+        break;
+      }
+
+      if(!isNoRegOp(MI->getOperand(1 + X86::AddrIndexReg))) {
+        DEBUG(dbgs() << "Unhandled index register for PC-relative address\n");
+        break;
+      }
+
+      return new
+        MachineSymbolRef(MI->getOperand(1 + X86::AddrDisp), false, MI);
+    }
+    else {
+      // Raw form of LEA
+      if(!MI->getOperand(1 + X86::AddrBaseReg).isReg() ||
+         !MI->getOperand(1 + X86::AddrDisp).isImm()) {
+        DEBUG(dbgs() << "Unhandled base register/displacement operands\n");
+        break;
+      }
+
+      // Initialize to index register * scale if indexing, or zero otherwise
+      Reg = MI->getOperand(1 + X86::AddrIndexReg).getReg();
+      if(Reg) {
+        Imm = MI->getOperand(1 + X86::AddrScaleAmt).getImm();
+        IL.emplace_back(new RegInstruction<InstType::Set>(Reg));
+        IL.emplace_back(new ImmInstruction<InstType::Multiply>(Size, Imm));
+      }
+      else IL.emplace_back(new ImmInstruction<InstType::Set>(Size, 0));
+
+      // Add the base register & displacement
+      Reg = MI->getOperand(1 + X86::AddrBaseReg).getReg();
+      Imm = MI->getOperand(1 + X86::AddrDisp).getImm();
+      IL.emplace_back(new RegInstruction<InstType::Add>(Reg));
+      IL.emplace_back(new ImmInstruction<InstType::Add>(Size, Imm));
+      return new MachineGeneratedVal(IL, MI, true);
+    }
+
+    break;
+  default:
+    DEBUG(dbgs() << "Unhandled LEA machine instruction");
+    break;
+  }
+  return nullptr;
+}
+
+MachineLiveValPtr X86Values::getMachineValue(const MachineInstr *MI) const {
+  MachineLiveVal* Val = nullptr;
+  const MachineOperand *MO, *MO2;
+  const TargetInstrInfo *TII;
+
+  switch(MI->getOpcode()) {
+  case X86::LEA64r:
+    Val = genLEAInstructions(MI);
+    break;
+  case X86::MOV32r0:
+    Val = new MachineImmediate(4, 0, MI, false);
+    break;
+  case X86::MOV32ri:
+    MO = &MI->getOperand(1);
+    if(MO->isImm()) Val = new MachineImmediate(4, MO->getImm(), MI, false);
+    break;
+  case X86::MOV32ri64:
+    // TODO the upper 32 bits of this reference are supposed to be masked
+    MO = &MI->getOperand(1);
+    if(TargetValues::isSymbolValue(MO))
+      Val = new MachineSymbolRef(*MO, false, MI);
+    break;
+  case X86::MOV64ri:
+    MO = &MI->getOperand(1);
+    if(MO->isImm()) Val = new MachineImmediate(8, MO->getImm(), MI, false);
+    else if(TargetValues::isSymbolValue(MO))
+      Val = new MachineSymbolRef(*MO, false, MI);
+    break;
+  case X86::MOV64rm:
+    MO = &MI->getOperand(1 + X86::AddrBaseReg);
+    MO2 = &MI->getOperand(1 + X86::AddrDisp);
+    // Note: codegen'd a PC relative symbol reference
+    // Note 2: we *must* ensure the symbol is const-qualified, otherwise we
+    // risk creating a new value if the symbol's value changes between when the
+    // initial load would have occurred and the transformation, e.g.,
+    //
+    //   movq <ga:mysym>, %rax
+    //   ... (somebody changes mysym's value) ...
+    //   callq <ga:myfunc>
+    //
+    // In this situation, the transformation occurs at the call site and
+    // retrieves the updated value rather than the value that would have been
+    // loaded at the ldr instruction.
+    if(MO->isReg() && MO->getReg() == X86::RIP &&
+       TargetValues::isSymbolValue(MO2) &&
+       TargetValues::isSymbolValueConstant(MO2))
+        Val = new MachineSymbolRef(*MO2, true, MI);
+    break;
+  default:
+    TII =  MI->getParent()->getParent()->getSubtarget().getInstrInfo();
+    DEBUG(dbgs() << "Unhandled opcode: "
+                 << TII->getName(MI->getOpcode()) << "\n");
+    break;
+  }
+
+  return MachineLiveValPtr(Val);
+}
+
Index: lib/Target/X86/X86Values.h
===================================================================
--- lib/Target/X86/X86Values.h	(nonexistent)
+++ lib/Target/X86/X86Values.h	(working copy)
@@ -0,0 +1,26 @@
+//===--------- X86TargetValues.cpp - X86 specific value generator ---------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Target/TargetValues.h"
+
+namespace llvm {
+
+class X86Values final : public TargetValues {
+public:
+  X86Values() {}
+  virtual TemporaryValuePtr getTemporaryValue(const MachineInstr *MI,
+                                              const VirtRegMap *VRM) const;
+  virtual MachineLiveValPtr getMachineValue(const MachineInstr *MI) const;
+
+private:
+  MachineLiveVal *genLEAInstructions(const MachineInstr *LEA) const;
+};
+
+}
+
Index: lib/Transforms/Instrumentation/CMakeLists.txt
===================================================================
--- lib/Transforms/Instrumentation/CMakeLists.txt	(revision 320332)
+++ lib/Transforms/Instrumentation/CMakeLists.txt	(working copy)
@@ -2,10 +2,13 @@
   AddressSanitizer.cpp
   BoundsChecking.cpp
   DataFlowSanitizer.cpp
+  MigrationPoints.cpp
   GCOVProfiling.cpp
   MemorySanitizer.cpp
+  InsertStackMaps.cpp
   Instrumentation.cpp
   InstrProfiling.cpp
+  LibcStackMaps.cpp
   SafeStack.cpp
   SanitizerCoverage.cpp
   ThreadSanitizer.cpp
Index: lib/Transforms/Instrumentation/InsertStackMaps.cpp
===================================================================
--- lib/Transforms/Instrumentation/InsertStackMaps.cpp	(nonexistent)
+++ lib/Transforms/Instrumentation/InsertStackMaps.cpp	(working copy)
@@ -0,0 +1,357 @@
+#include <map>
+#include <set>
+#include <vector>
+#include "llvm/Pass.h"
+#include "llvm/Analysis/LiveValues.h"
+#include "llvm/Analysis/PopcornUtil.h"
+#include "llvm/IR/Dominators.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/InstIterator.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/ModuleSlotTracker.h"
+#include "llvm/IR/Type.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+#define DEBUG_TYPE "insert-stackmaps"
+
+using namespace llvm;
+
+static cl::opt<bool>
+NoLiveVals("no-live-vals",
+           cl::desc("Don't add live values to inserted stackmaps"),
+           cl::init(false),
+           cl::Hidden);
+
+namespace {
+
+/* Track slots for unnamed values */
+static ModuleSlotTracker *SlotTracker = nullptr;
+
+/* Sort values based on name */
+struct ValueComp
+{
+  bool operator ()(const Value *a, const Value *b)
+  {
+    if(a->hasName() && b->hasName())
+      return a->getName().compare(b->getName()) < 0;
+    else if(a->hasName()) return true;
+    else if(b->hasName()) return false;
+    else {
+      int slot_a = SlotTracker->getLocalSlot(a),
+          slot_b = SlotTracker->getLocalSlot(b);
+      return slot_a < slot_b;
+    }
+  }
+};
+
+/**
+ * This class instruments equivalence points in the IR with LLVM's stackmap
+ * intrinsic.  This tells the backend to record the locations of IR values
+ * after register allocation in a separate ELF section.
+ */
+class InsertStackMaps : public ModulePass
+{
+private:
+  /* Some useful typedefs */
+  typedef SmallVector<const Instruction *, 4> InstVec;
+  typedef DenseMap<const Instruction *, InstVec> InstHidingMap;
+  typedef SmallVector<const Argument *, 4> ArgVec;
+  typedef DenseMap<const Instruction *, ArgVec> ArgHidingMap;
+
+public:
+  static char ID;
+  size_t callSiteID;
+  size_t numInstrumented;
+
+  InsertStackMaps() : ModulePass(ID), callSiteID(0), numInstrumented(0) {
+    initializeInsertStackMapsPass(*PassRegistry::getPassRegistry());
+  }
+  ~InsertStackMaps() {}
+
+  /* ModulePass virtual methods */
+  virtual const char *getPassName() const { return "Insert stackmaps"; }
+
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const
+  {
+    AU.addRequired<LiveValues>();
+    AU.addRequired<DominatorTreeWrapperPass>();
+    AU.setPreservesCFG();
+  }
+
+  /**
+   * Use liveness analysis to insert stackmap intrinsics into the IR to record
+   * live values at equivalence points.
+   *
+   * Note: currently we only insert stackmaps at function call sites.
+   */
+  virtual bool runOnModule(Module &M)
+  {
+    bool modified = false;
+
+    std::set<const Value *> *live;
+    std::set<const Value *, ValueComp> sortedLive;
+    InstHidingMap hiddenInst;
+    ArgHidingMap hiddenArgs;
+
+    DEBUG(errs() << "\n********** Begin InsertStackMaps **********\n"
+                 << "********** Module: " << M.getName() << " **********\n\n");
+
+    this->createSMType(M);
+    if(this->addSMDeclaration(M)) modified = true;
+    SlotTracker = new ModuleSlotTracker(&M);
+
+    modified |= this->removeOldStackmaps(M);
+
+    /* Iterate over all functions/basic blocks/instructions. */
+    for(Module::iterator f = M.begin(), fe = M.end(); f != fe; f++)
+    {
+      if(f->isDeclaration()) continue;
+
+      DEBUG(errs() << "InsertStackMaps: entering function "
+                   << f->getName() << "\n");
+
+      LiveValues &liveVals = getAnalysis<LiveValues>(*f);
+      DominatorTree &DT = getAnalysis<DominatorTreeWrapperPass>(*f).getDomTree();
+      SlotTracker->incorporateFunction(*f);
+      std::set<const Value *>::const_iterator v, ve;
+      getHiddenVals(*f, hiddenInst, hiddenArgs);
+
+      /* Find call sites in the function. */
+      for(Function::iterator b = f->begin(), be = f->end(); b != be; b++)
+      {
+        DEBUG(
+          errs() << "InsertStackMaps: entering basic block ";
+          b->printAsOperand(errs(), false);
+          errs() << "\n"
+        );
+
+        for(BasicBlock::iterator i = b->begin(), ie = b->end(); i != ie; i++)
+        {
+          if(Popcorn::isCallSite(&*i))
+          {
+            CallSite CS(i);
+            if(CS.isInvoke())
+            {
+              DEBUG(dbgs() << "WARNING: unhandled invoke:"; CS->dump());
+              continue;
+            }
+
+            IRBuilder<> builder(CS->getNextNode());
+            std::vector<Value *> args(2);
+            args[0] = ConstantInt::getSigned(Type::getInt64Ty(M.getContext()),
+                                             this->callSiteID++);
+            args[1] = ConstantInt::getSigned(Type::getInt32Ty(M.getContext()),
+                                             0);
+
+            if(NoLiveVals) {
+              builder.CreateCall(this->SMFunc, ArrayRef<Value*>(args));
+              this->numInstrumented++;
+              continue;
+            }
+
+            live = liveVals.getLiveValues(&*i);
+            for(const Value *val : *live) sortedLive.insert(val);
+            for(const auto &pair : hiddenInst) {
+              /*
+               * The two criteria for inclusion of a hidden value are:
+               *   1. The value's definition dominates the call
+               *   2. A use which hides the definition is in the stackmap
+               */
+              if(DT.dominates(pair.first, i) && live->count(pair.first))
+                for(auto &inst : pair.second) sortedLive.insert(inst);
+            }
+            for(const auto &pair : hiddenArgs) {
+              /*
+               * Similar criteria apply as above, except we know arguments
+               * dominate the entire function.
+               */
+              if(live->count(pair.first))
+                for(auto &inst : pair.second) sortedLive.insert(inst);
+            }
+            delete live;
+
+            DEBUG(
+              const Function *calledFunc;
+
+              errs() << "  ";
+              if(!CS->getType()->isVoidTy()) {
+                CS->printAsOperand(errs(), false);
+                errs() << " ";
+              }
+              else errs() << "(void) ";
+
+              calledFunc = CS.getCalledFunction();
+              if(calledFunc && calledFunc->hasName())
+              {
+                StringRef name = CS.getCalledFunction()->getName();
+                errs() << name << " ";
+              }
+              errs() << "ID: " << this->callSiteID;
+
+              errs() << ", " << sortedLive.size() << " live value(s)\n   ";
+              for(const Value *val : sortedLive) {
+                errs() << " ";
+                val->printAsOperand(errs(), false);
+              }
+              errs() << "\n";
+            );
+
+            for(v = sortedLive.begin(), ve = sortedLive.end(); v != ve; v++)
+              args.push_back((Value*)*v);
+            builder.CreateCall(this->SMFunc, ArrayRef<Value*>(args));
+            sortedLive.clear();
+            this->numInstrumented++;
+          }
+        }
+      }
+
+      hiddenInst.clear();
+      hiddenArgs.clear();
+      this->callSiteID = 0;
+    }
+
+    DEBUG(
+      errs() << "InsertStackMaps: finished module " << M.getName() << ", added "
+             << this->numInstrumented << " stackmaps\n\n";
+    );
+
+    if(numInstrumented > 0) modified = true;
+    delete SlotTracker;
+
+    return modified;
+  }
+
+private:
+  /* Name of stack map intrinsic */
+  static const StringRef SMName;
+
+  /* Stack map instruction creation */
+  Function *SMFunc;
+  FunctionType *SMTy; // Used for creating function declaration
+
+  /**
+   * Create the function type for the stack map intrinsic.
+   */
+  void createSMType(const Module &M)
+  {
+    std::vector<Type*> params(2);
+    params[0] = Type::getInt64Ty(M.getContext());
+    params[1] = Type::getInt32Ty(M.getContext());
+    this->SMTy = FunctionType::get(Type::getVoidTy(M.getContext()),
+                                                   ArrayRef<Type*>(params),
+                                                   true);
+  }
+
+  /**
+   * Add the stackmap intrinisic's function declaration if not already present.
+   * Return true if the declaration was added, or false if it's already there.
+   */
+  bool addSMDeclaration(Module &M)
+  {
+    if(!(this->SMFunc = M.getFunction(this->SMName)))
+    {
+      DEBUG(errs() << "Adding stackmap function declaration to " << M.getName() << "\n");
+      this->SMFunc = cast<Function>(M.getOrInsertFunction(this->SMName, this->SMTy));
+      this->SMFunc->setCallingConv(CallingConv::C);
+      return true;
+    }
+    else return false;
+  }
+
+  /**
+   * Iterate over all instructions, removing previously found stackmaps.
+   */
+  bool removeOldStackmaps(Module &M)
+  {
+    bool modified = false;
+    CallInst* CI;
+    const Function *F;
+
+    DEBUG(dbgs() << "Searching for/removing old stackmaps\n";);
+
+    for(Module::iterator f = M.begin(), fe = M.end(); f != fe; f++) {
+      for(Function::iterator bb = f->begin(), bbe = f->end(); bb != bbe; bb++) {
+        for(BasicBlock::iterator i = bb->begin(), ie = bb->end(); i != ie; i++) {
+          if((CI = dyn_cast<CallInst>(&*i))) {
+            F = CI->getCalledFunction();
+            if(F && F->hasName() && F->getName() == SMName) {
+              i = i->eraseFromParent()->getPrevNode();
+              modified = true;
+            }
+          }
+        }
+      }
+    }
+
+    DEBUG(if(modified)
+            dbgs() << "WARNING: found previous run of Popcorn passes!\n";);
+
+    return modified;
+  }
+
+  /**
+   * Gather a list of values which may be "hidden" from live value analysis.
+   * This function collects the values used in these instructions, which are
+   * later added to the appropriate stackmaps.
+   *
+   *  - Instructions which access fields of structs or entries of arrays, like
+   *    getelementptr, can interfere with the live value analysis to hide the
+   *    backing values used in the instruction.  For example, the following IR
+   *    obscures %arr from the live value analysis:
+   *
+   *  %arr = alloca [4 x double], align 8
+   *  %arrayidx = getelementptr inbounds [4 x double], [4 x double]* %arr, i64 0, i64 0
+   *
+   *  -> Access to %arr might only happen through %arrayidx, and %arr may not
+   *     be used any more
+   *
+   */
+  void getHiddenVals(Function &F, InstHidingMap &inst, ArgHidingMap &args)
+  {
+    /* Does the instruction potentially hide values from liveness analysis? */
+    auto hidesValues = [](const Instruction *I) {
+      if(isa<ExtractElementInst>(I) || isa<InsertElementInst>(I) ||
+         isa<ExtractValueInst>(I) || isa<InsertValueInst>(I) ||
+         isa<GetElementPtrInst>(I) || isa<BitCastInst>(I))
+        return true;
+      else return false;
+    };
+
+    /* Search for instructions that obscure live values & record operands */
+    for(inst_iterator i = inst_begin(F), e = inst_end(F); i != e; ++i) {
+      InstVec &InstsHidden = inst[&*i];
+      ArgVec &ArgsHidden = args[&*i];
+
+      if(hidesValues(&*i)) {
+        for(unsigned op = 0; op < i->getNumOperands(); op++) {
+          if(isa<Instruction>(i->getOperand(op)))
+            InstsHidden.push_back(cast<Instruction>(i->getOperand(op)));
+          else if(isa<Argument>(i->getOperand(op)))
+            ArgsHidden.push_back(cast<Argument>(i->getOperand(op)));
+        }
+      }
+    }
+  }
+};
+
+} /* end anonymous namespace */
+
+char InsertStackMaps::ID = 0;
+const StringRef InsertStackMaps::SMName = "llvm.experimental.stackmap";
+
+INITIALIZE_PASS_BEGIN(InsertStackMaps, "insert-stackmaps",
+                      "Instrument equivalence points with stack maps",
+                      false, false)
+INITIALIZE_PASS_DEPENDENCY(LiveValues)
+INITIALIZE_PASS_DEPENDENCY(DominatorTreeWrapperPass)
+INITIALIZE_PASS_END(InsertStackMaps, "insert-stackmaps",
+                    "Instrument equivalence points with stack maps",
+                    false, false)
+
+namespace llvm {
+  ModulePass *createInsertStackMapsPass() { return new InsertStackMaps(); }
+}
+
Index: lib/Transforms/Instrumentation/Instrumentation.cpp
===================================================================
--- lib/Transforms/Instrumentation/Instrumentation.cpp	(revision 320332)
+++ lib/Transforms/Instrumentation/Instrumentation.cpp	(working copy)
@@ -24,8 +24,11 @@
   initializeAddressSanitizerPass(Registry);
   initializeAddressSanitizerModulePass(Registry);
   initializeBoundsCheckingPass(Registry);
+  initializeMigrationPointsPass(Registry);
   initializeGCOVProfilerPass(Registry);
+  initializeInsertStackMapsPass(Registry);
   initializeInstrProfilingPass(Registry);
+  initializeLibcStackMapsPass(Registry);
   initializeMemorySanitizerPass(Registry);
   initializeThreadSanitizerPass(Registry);
   initializeSanitizerCoverageModulePass(Registry);
Index: lib/Transforms/Instrumentation/LibcStackMaps.cpp
===================================================================
--- lib/Transforms/Instrumentation/LibcStackMaps.cpp	(nonexistent)
+++ lib/Transforms/Instrumentation/LibcStackMaps.cpp	(working copy)
@@ -0,0 +1,229 @@
+#include <map>
+#include <vector>
+#include "llvm/Pass.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/Type.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/Path.h"
+#include "llvm/Support/raw_ostream.h"
+
+#define DEBUG_TYPE "libc-stackmaps"
+
+using namespace llvm;
+
+namespace {
+
+/**
+ * Instrument thread starting points with stackmaps.  These are the only
+ * functions inside of libc for which we want to generate metadata, since we
+ * disallow migration inside the public libc API.
+ */
+// TODO: only implemented for musl-libc!
+class LibcStackMaps : public ModulePass
+{
+public:
+  static char ID;
+  size_t numInstrumented;
+
+  LibcStackMaps() : ModulePass(ID), numInstrumented(0) {
+    initializeLibcStackMapsPass(*PassRegistry::getPassRegistry());
+  }
+  ~LibcStackMaps() {}
+
+  /* ModulePass virtual methods */
+  virtual const char *getPassName() const
+  { return "Insert stackmaps in libc thread start functions"; }
+
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const
+  { AU.setPreservesCFG(); }
+
+  virtual bool runOnModule(Module &M)
+  {
+    int64_t smid;
+    bool modified = false;
+    Function *F;
+    std::map<std::string, std::vector<std::string> >::const_iterator file;
+
+    /* Is this a module (i.e., source file) we're interested in? */
+    if((file = funcs.find(sys::path::stem(M.getName()))) != funcs.end())
+    {
+      DEBUG(dbgs() << "\n********** Begin LibcStackMaps **********\n"
+                   << "********** Module: " << file->first << " **********\n\n");
+
+      this->createSMType(M);
+      modified |= this->addSMDeclaration(M);
+
+      /* Iterate over thread starting functions in the module */
+      for(size_t f = 0, fe = file->second.size(); f < fe; f++)
+      {
+        DEBUG(dbgs() << "LibcStackMaps: entering thread starting function "
+                     << file->second[f] << "\n");
+
+        F = M.getFunction(file->second[f]);
+        assert(F && !F->isDeclaration() && "No thread function definition");
+        modified |= this->removeOldStackmaps(F);
+        assert(smids.find(file->second[f]) != smids.end() && "No ID for function");
+        smid = smids.find(file->second[f])->second;
+
+        /*
+         * Look for & instrument a generic call instruction followed by a call
+         * to an exit function, e.g.,
+         *
+         *   %call = call i32 %main(...)
+         *   call void @exit(i32 %call)
+         */
+        for(Function::iterator bb = F->begin(), be = F->end(); bb != be; bb++)
+        {
+          bool track = false;
+          for(BasicBlock::reverse_iterator i = bb->rbegin(), ie = bb->rend();
+              i != ie; i++)
+          {
+            if(isExitCall(*i)) track = true;
+            else if(track && isa<CallInst>(*i))
+            {
+              IRBuilder<> builder(i->getNextNode());
+              std::vector<Value *> args(2);
+              args[0] = ConstantInt::getSigned(Type::getInt64Ty(M.getContext()), smid);
+              args[1] = ConstantInt::getSigned(Type::getInt32Ty(M.getContext()), 0);
+              builder.CreateCall(this->SMFunc, ArrayRef<Value*>(args));
+              this->numInstrumented++;
+              break;
+            }
+          }
+        }
+      }
+
+      DEBUG(dbgs() << "LibcStackMaps: finished module " << M.getName()
+                   << ", added " << this->numInstrumented << " stackmaps\n\n";);
+    }
+
+    if(numInstrumented > 0) modified = true;
+    return modified;
+  }
+
+private:
+  /* Name of stack map intrinsic */
+  static const StringRef SMName;
+
+  /* Stack map instruction creation */
+  Function *SMFunc;
+  FunctionType *SMTy; // Used for creating function declaration
+
+  /* Files, functions & IDs */
+  static const std::map<std::string, std::vector<std::string> > funcs;
+  static const std::map<std::string, int64_t> smids;
+  static const std::vector<std::string> exitFuncs;
+
+  /**
+   * Create the function type for the stack map intrinsic.
+   */
+  void createSMType(const Module &M)
+  {
+    std::vector<Type*> params(2);
+    params[0] = Type::getInt64Ty(M.getContext());
+    params[1] = Type::getInt32Ty(M.getContext());
+    this->SMTy = FunctionType::get(Type::getVoidTy(M.getContext()),
+                                                   ArrayRef<Type*>(params),
+                                                   true);
+  }
+
+  /**
+   * Add the stackmap intrinisic's function declaration if not already present.
+   * Return true if the declaration was added, or false if it's already there.
+   */
+  bool addSMDeclaration(Module &M)
+  {
+    if(!(this->SMFunc = M.getFunction(this->SMName)))
+    {
+      DEBUG(dbgs() << "Adding stackmap function declaration to " << M.getName() << "\n");
+      this->SMFunc = cast<Function>(M.getOrInsertFunction(this->SMName, this->SMTy));
+      this->SMFunc->setCallingConv(CallingConv::C);
+      return true;
+    }
+    else return false;
+  }
+
+  /**
+   * Iterate over all instructions, removing previously found stackmaps.
+   */
+  bool removeOldStackmaps(Function *F)
+  {
+    bool modified = false;
+    CallInst* CI;
+    const Function *CurF;
+
+    DEBUG(dbgs() << "Searching for/removing old stackmaps\n";);
+
+    for(Function::iterator bb = F->begin(), bbe = F->end(); bb != bbe; bb++) {
+      for(BasicBlock::iterator i = bb->begin(), ie = bb->end(); i != ie; i++) {
+        if((CI = dyn_cast<CallInst>(&*i))) {
+          CurF = CI->getCalledFunction();
+          if(CurF && CurF->hasName() && CurF->getName() == SMName) {
+            i = i->eraseFromParent()->getPrevNode();
+            modified = true;
+          }
+        }
+      }
+    }
+
+    DEBUG(if(modified) dbgs() << "WARNING: found previous stackmaps!\n";);
+    return modified;
+  }
+
+  /**
+   * Return whether or not the instruction is a call to an exit function.
+   */
+  bool isExitCall(Instruction &I)
+  {
+    CallInst *CI;
+    Function *F;
+
+    if((CI = dyn_cast<CallInst>(&I)))
+    {
+      F = CI->getCalledFunction();
+      if(F && F->hasName())
+        for(size_t i = 0, e = exitFuncs.size(); i < e; i++)
+          if(F->getName() == exitFuncs[i]) return true;
+    }
+
+    return false;
+  }
+};
+
+} /* end anonymous namespace */
+
+char LibcStackMaps::ID = 0;
+const StringRef LibcStackMaps::SMName = "llvm.experimental.stackmap";
+
+/**
+ * Map a source code filename (minus the extension) to the names of functions
+ * inside which are to be instrumented.
+ */
+const std::map<std::string, std::vector<std::string> > LibcStackMaps::funcs = {
+  {"__libc_start_main", {"__libc_start_main"}},
+  {"pthread_create", {"start", "start_c11"}}
+};
+
+/* Map a function name to the stackmap ID representing that function. */
+const std::map<std::string, int64_t> LibcStackMaps::smids = {
+  {"__libc_start_main", UINT64_MAX},
+  {"start", UINT64_MAX - 1},
+  {"start_c11", UINT64_MAX - 2}
+};
+
+/**
+ * Thread exit function names, used to search for starting function call site
+ * to be instrumented with stackmap.
+ */
+const std::vector<std::string> LibcStackMaps::exitFuncs = {
+  "exit", "pthread_exit", "__pthread_exit"
+};
+
+INITIALIZE_PASS(LibcStackMaps, "libc-stackmaps",
+  "Instrument libc thread start functions with stack maps", false, false)
+
+namespace llvm {
+  ModulePass *createLibcStackMapsPass() { return new LibcStackMaps(); }
+}
+
Index: lib/Transforms/Instrumentation/MigrationPoints.cpp
===================================================================
--- lib/Transforms/Instrumentation/MigrationPoints.cpp	(nonexistent)
+++ lib/Transforms/Instrumentation/MigrationPoints.cpp	(working copy)
@@ -0,0 +1,490 @@
+//===- MigrationPoints.cpp ------------------------------------------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Instruments the migration points selected by SelectMigrationPoints.  May
+// additionally add HTM instrumentation if selected by user & supported by the
+// architecture.
+//
+//===----------------------------------------------------------------------===//
+
+#include <fstream>
+#include <map>
+#include "llvm/Pass.h"
+#include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/Triple.h"
+#include "llvm/Analysis/PopcornUtil.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/Module.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_os_ostream.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "migration-points"
+#define MIGRATE_FLAG_NAME "__migrate_flag"
+
+/// Disable rollback-only transactions for PowerPC.
+const static cl::opt<bool>
+NoROTPPC("htm-ppc-no-rot", cl::Hidden, cl::init(false),
+  cl::desc("Disable rollback-only transactions in HTM instrumentation "
+           "(PowerPC only)"));
+
+/// Add counters to abort handlers for the specified function.  Allows in-depth
+/// profiling of which HTM sections added to the function are causing aborts.
+const static cl::opt<std::string>
+AbortCount("abort-count", cl::Hidden, cl::init(""),
+  cl::desc("Add counters for each abort handler in the specified function"),
+  cl::value_desc("function"));
+
+STATISTIC(NumMigPoints, "Number of migration points added");
+STATISTIC(NumHTMBegins, "Number of HTM begin intrinsics added");
+STATISTIC(NumHTMEnds, "Number of HTM end intrinsics added");
+
+namespace {
+
+/// MigrationPoints - insert migration points into functions, optionally adding
+/// HTM execution.
+class MigrationPoints : public FunctionPass
+{
+public:
+  static char ID;
+
+  MigrationPoints() : FunctionPass(ID) {}
+  ~MigrationPoints() {}
+
+  virtual const char *getPassName() const
+  { return "Insert migration points"; }
+
+  /// Generate the migration library API function declaration.
+  void addMigrationIntrinsic(Module &M, bool DoHTM) {
+    LLVMContext &C = M.getContext();
+    Type *VoidTy = Type::getVoidTy(C);
+    PointerType *VoidPtrTy = Type::getInt8PtrTy(C, 0);
+    std::vector<Type *> FuncPtrArgTy = { VoidPtrTy };
+    FunctionType *FuncPtrTy = FunctionType::get(VoidTy, FuncPtrArgTy, false);
+    CallbackType = PointerType::get(FuncPtrTy, 0);
+    std::vector<Type *> ArgTy = { CallbackType, VoidPtrTy };
+    FunctionType *FuncTy = FunctionType::get(VoidTy, ArgTy, false);
+    if(DoHTM) {
+      MigrateAPI = M.getOrInsertFunction("migrate", FuncTy);
+      MigrateFlag = cast<GlobalValue>(
+        M.getOrInsertGlobal(MIGRATE_FLAG_NAME, Type::getInt32Ty(C)));
+      // TODO this needs to be thread-local storage
+      //MigrateFlag->setThreadLocal(true);
+    }
+    else {
+      MigrateAPI = M.getOrInsertFunction("check_migrate", FuncTy);
+      MigrateFlag = nullptr;
+    }
+  }
+
+  virtual bool doInitialization(Module &M) {
+    Triple TheTriple(M.getTargetTriple());
+    Arch = TheTriple.getArch();
+    Ty = Popcorn::getInstrumentationType(M);
+
+    switch(Ty) {
+    case Popcorn::HTM:
+      if(HTMBegin.find(Arch) != HTMBegin.end()) {
+        DEBUG(dbgs() << "\n-> MigrationPoints: Adding HTM intrinsics for '"
+                     << TheTriple.getArchName() << "' <-\n");
+        HTMBeginDecl =
+          Intrinsic::getDeclaration(&M, HTMBegin.find(Arch)->second);
+        HTMEndDecl = Intrinsic::getDeclaration(&M, HTMEnd.find(Arch)->second);
+        HTMTestDecl =
+          Intrinsic::getDeclaration(&M, HTMTest.find(Arch)->second);
+        addMigrationIntrinsic(M, true);
+      }
+      else {
+        DEBUG(
+          dbgs() << "\n-> MigrationPoints: Selected HTM instrumentation but '"
+                 << TheTriple.getArchName()
+                 << "' is not supported, falling back to call-outs <-\n"
+        );
+        Ty = Popcorn::Cycles;
+        addMigrationIntrinsic(M, false);
+      }
+      break;
+    case Popcorn::Cycles:
+      addMigrationIntrinsic(M, false);
+      break;
+    case Popcorn::None:
+      return false;
+    default: llvm_unreachable("Unknown instrumentation type"); break;
+    }
+
+    // Add abort counters if somebody requested abort profiling.
+    const Function *CounterFunc;
+    if(AbortCount != "" && (CounterFunc = M.getFunction(AbortCount)) &&
+       !CounterFunc->isDeclaration()) {
+      LLVMContext &C = M.getContext();
+      IntegerType *Unsigned = Type::getInt32Ty(C);
+      GlobalVariable *NumCtrs = cast<GlobalVariable>(
+        M.getOrInsertGlobal("__num_abort_counters", Unsigned));
+      NumCtrs->setInitializer(ConstantInt::get(Unsigned, 1024, false));
+      Type *ArrType = ArrayType::get(Type::getInt64Ty(C), 1024);
+      AbortCounters = cast<GlobalVariable>(
+        M.getOrInsertGlobal("__abort_counters", ArrType));
+      AbortCounters->setInitializer(ConstantAggregateZero::get(ArrType));
+    }
+
+    return true;
+  }
+
+  /// Insert migration points into functions
+  virtual bool runOnFunction(Function &F)
+  {
+    if(Ty == Popcorn::None) return false;
+
+    DEBUG(dbgs() << "\n********** ADD MIGRATION POINTS **********\n"
+                 << "********** Function: " << F.getName() << "\n\n");
+
+    initializeAnalysis(F);
+
+    // Find all instrumentation points marked by previous analysis passes.
+    findInstrumentationPoints(F);
+
+    // Apply code transformations to marked instructions, including adding
+    // migration points & HTM instrumentation.
+    addMigrationPoints(F);
+
+    // Write the modified IR & close the abort handler map file if we
+    // instrumented the code to profile abort handlers.
+    if(MapFile.is_open()) {
+      MapFile.close();
+      std::fstream TheIR("htm-abort-ir.ll", std::ios::out | std::ios::trunc);
+      raw_os_ostream IRStream(TheIR);
+      F.print(IRStream);
+      TheIR.close();
+    }
+
+    return true;
+  }
+
+  /// Reset all analysis.
+  void initializeAnalysis(const Function &F) {
+    DoHTMInst = false;
+    DoAbortInstrument = false;
+    MigPointInsts.clear();
+    HTMBeginInsts.clear();
+    HTMEndInsts.clear();
+
+    if(Ty == Popcorn::HTM) {
+      // We've checked at the global scope whether HTM is enabled for the
+      // module.  Check whether the target-specific feature for HTM is enabled
+      // for the current function.
+      if(!F.hasFnAttribute("target-features")) {
+        DEBUG(dbgs() << "-> Disabled HTM instrumentation, "
+                        "no 'target-features' attribute\n");
+        return;
+      }
+
+      Attribute TargetAttr = F.getFnAttribute("target-features");
+      assert(TargetAttr.isStringAttribute() && "Invalid target features");
+      StringRef AttrVal = TargetAttr.getValueAsString();
+      size_t pos = StringRef::npos;
+
+      switch(Arch) {
+      case Triple::ppc64le: pos = AttrVal.find("+htm"); break;
+      case Triple::x86_64: pos = AttrVal.find("+rtm"); break;
+      default: break;
+      }
+
+      DoHTMInst = (pos != StringRef::npos);
+
+      DEBUG(if(!DoHTMInst) dbgs() << "-> Disabled HTM instrumentation, HTM "
+                                     "not listed in target-features\n");
+
+      // Enable HTM abort handler profiling if specified
+      if(DoHTMInst && AbortCount == F.getName()) {
+        DoAbortInstrument = true;
+        AbortHandlerCount = 0;
+        MapFile.open("htm-abort.map", std::ios::out | std::ios::trunc);
+        assert(MapFile.is_open() && MapFile.good() &&
+               "Could not open abort handler map file");
+      }
+    }
+
+    DEBUG(
+      if(DoHTMInst) {
+        dbgs() << "-> Adding HTM instrumentation\n";
+        if(DoAbortInstrument) dbgs() << "  - Adding abort counters\n";
+      }
+      else dbgs() << "-> Adding call-out instrumentation\n";
+    );
+  }
+
+private:
+  //===--------------------------------------------------------------------===//
+  // Types & fields
+  //===--------------------------------------------------------------------===//
+
+  /// Type of the instrumentation to be applied to functions.
+  enum Popcorn::InstrumentType Ty;
+
+  /// The current architecture - used to access architecture-specific HTM calls
+  Triple::ArchType Arch;
+
+  /// Should we instrument code with HTM execution?  Set if HTM is enabled on
+  /// the command line and if the target is supported
+  bool DoHTMInst;
+
+  /// Should we instrument HTM abort handlers with counters for precise
+  /// profiling of which code locations cause aborts & all associated state.
+  bool DoAbortInstrument;
+  GlobalVariable *AbortCounters;
+  unsigned AbortHandlerCount;
+  std::ofstream MapFile;
+
+  /// Function declaration & migration node ID for migration library API
+  Constant *MigrateAPI;
+  GlobalValue *MigrateFlag;
+  PointerType *CallbackType;
+
+  /// Function declarations for HTM intrinsics
+  Value *HTMBeginDecl;
+  Value *HTMEndDecl;
+  Value *HTMTestDecl;
+
+  /// Per-architecture LLVM intrinsic IDs for HTM begin, HTM end, and testing
+  /// if executing transactionally
+  typedef std::map<Triple::ArchType, Intrinsic::ID> IntrinsicMap;
+  const static IntrinsicMap HTMBegin;
+  const static IntrinsicMap HTMEnd;
+  const static IntrinsicMap HTMTest;
+
+  /// Code locations marked for instrumentation.
+  SmallPtrSet<Instruction *, 32> MigPointInsts;
+  SmallPtrSet<Instruction *, 32> HTMBeginInsts;
+  SmallPtrSet<Instruction *, 32> HTMEndInsts;
+
+  //===--------------------------------------------------------------------===//
+  // Instrumentation implementation
+  //===--------------------------------------------------------------------===//
+
+  /// Find instructions tagged by SelectMigrationPoints with instrumentation
+  /// metadata.
+  void findInstrumentationPoints(Function &F) {
+    for(Function::iterator BB = F.begin(), BBE = F.end(); BB != BBE; BB++) {
+      for(BasicBlock::iterator I = BB->begin(), IE = BB->end(); I != IE; I++) {
+        if(Popcorn::hasEquivalencePointMetadata(I)) MigPointInsts.insert(I);
+        if(Popcorn::isHTMBeginPoint(I)) HTMBeginInsts.insert(I);
+        if(Popcorn::isHTMEndPoint(I)) HTMEndInsts.insert(I);
+      }
+    }
+  }
+
+  /// Add a migration point directly before an instruction.
+  void addMigrationPoint(Instruction *I) {
+    LLVMContext &C = I->getContext();
+    IRBuilder<> Worker(I);
+    std::vector<Value *> Args = {
+      ConstantPointerNull::get(CallbackType),
+      ConstantPointerNull::get(Type::getInt8PtrTy(C, 0))
+    };
+    Worker.CreateCall(MigrateAPI, Args);
+  }
+
+  // Note: because we're only supporting 2 architectures for now, we're not
+  // going to abstract this out into the appropriate Target/* folders
+
+  /// Add HTM begin which avoids doing any work unless there's an abort.  In
+  /// the event of an abort, the instrumentation checks if it should migrate,
+  /// and if so, invokes the migration API.
+  void addHTMBeginInternal(Instruction *I, Value *Begin, Value *Comparison) {
+    LLVMContext &C = I->getContext();
+    BasicBlock *CurBB = I->getParent(), *NewSuccBB, *FlagCheckBB, *MigPointBB;
+
+    // Set up each of the new basic blocks
+    NewSuccBB =
+      CurBB->splitBasicBlock(I, "migpointsucc" + std::to_string(NumMigPoints));
+    MigPointBB =
+      BasicBlock::Create(C, "migpoint" + std::to_string(NumMigPoints),
+                         CurBB->getParent(), NewSuccBB);
+    FlagCheckBB =
+      BasicBlock::Create(C, "migflagcheck" + std::to_string(NumMigPoints),
+                         CurBB->getParent(), MigPointBB);
+
+    // Add check & branch based on HTM begin result Comparison.  The true
+    // target of the branch is when we've started the transaction.
+    IRBuilder<> HTMWorker(CurBB->getTerminator());
+    HTMWorker.CreateCondBr(Comparison, NewSuccBB, FlagCheckBB);
+    CurBB->getTerminator()->eraseFromParent();
+
+    // Check flag to see if we should invoke migration library API.
+    IRBuilder<> FlagCheckWorker(FlagCheckBB);
+    if(DoAbortInstrument) {
+      assert(AbortHandlerCount < 1024 && "Too abort handler many counters!");
+
+      // Write the name of the basic block to the map file so we can map abort
+      // counters to their basic blocks.
+      if(!AbortHandlerCount) MapFile << FlagCheckBB->getName().str();
+      else MapFile << " " << FlagCheckBB->getName().str();
+
+      // Add instrumentation to increment the counter's value.
+      std::string CtrNum(std::to_string(AbortHandlerCount));
+      std::vector<Value *> Idx = {
+        ConstantInt::get(Type::getInt64Ty(C), 0),
+        ConstantInt::get(Type::getInt64Ty(C), AbortHandlerCount),
+      };
+      Value *One = ConstantInt::get(Type::getInt64Ty(C), 1, false);
+      Value *GEP = FlagCheckWorker.CreateInBoundsGEP(AbortCounters, Idx,
+                                                     "ctrptr" + CtrNum);
+      Value *CtrVal = FlagCheckWorker.CreateLoad(GEP, "ctr" + CtrNum);
+      Value *Inc = FlagCheckWorker.CreateAdd(CtrVal, One);
+      FlagCheckWorker.CreateStore(Inc, GEP);
+
+      AbortHandlerCount++;
+    }
+    Value *Flag = FlagCheckWorker.CreateLoad(MigrateFlag);
+    Value *NegOne = ConstantInt::get(Type::getInt32Ty(C), -1, true);
+    Value *Cmp = FlagCheckWorker.CreateICmpEQ(Flag, NegOne);
+    FlagCheckWorker.CreateCondBr(Cmp, NewSuccBB, MigPointBB);
+
+    // Add call to migration library API.
+    IRBuilder<> MigPointWorker(MigPointBB);
+    std::vector<Value *> Args = {
+      ConstantPointerNull::get(CallbackType),
+      ConstantPointerNull::get(Type::getInt8PtrTy(C, 0))
+    };
+    MigPointWorker.CreateCall(MigrateAPI, Args);
+    MigPointWorker.CreateBr(NewSuccBB);
+  }
+
+  /// Add a transactional execution begin intrinsic for PowerPC, optionally
+  /// with rollback-only transactions.
+  void addPowerPCHTMBegin(Instruction *I) {
+    LLVMContext &C = I->getContext();
+    IRBuilder<> Worker(I);
+    std::vector<Value *> Args = { ConstantInt::get(Type::getInt32Ty(C),
+                                                   NoROTPPC ? 0 : 1,
+                                                   false) };
+    Value *HTMBeginVal = Worker.CreateCall(HTMBeginDecl, Args);
+    Value *Zero = ConstantInt::get(Type::getInt32Ty(C), 0, false);
+    Value *Cmp = Worker.CreateICmpNE(HTMBeginVal, Zero);
+    addHTMBeginInternal(I, HTMBeginVal, Cmp);
+  }
+
+  /// Add a transactional execution begin intrinsice for x86.
+  void addX86HTMBegin(Instruction *I) {
+    LLVMContext &C = I->getContext();
+    IRBuilder<> Worker(I);
+    Value *HTMBeginVal = Worker.CreateCall(HTMBeginDecl);
+    Value *Success = ConstantInt::get(Type::getInt32Ty(C), 0xffffffff, false);
+    Value *Cmp = Worker.CreateICmpEQ(HTMBeginVal, Success);
+    addHTMBeginInternal(I, HTMBeginVal, Cmp);
+  }
+
+  /// Add transactional execution end intrinsic for PowerPC.
+  void addPowerPCHTMEnd(Instruction *I) {
+    LLVMContext &C = I->getContext();
+    IRBuilder<> EndWorker(I);
+    ConstantInt *One = ConstantInt::get(IntegerType::getInt32Ty(C),
+                                        1, false);
+    EndWorker.CreateCall(HTMEndDecl, ArrayRef<Value *>(One));
+  }
+
+  /// Add transactional execution check & end intrinsics for x86.
+  void addX86HTMCheckAndEnd(Instruction *I) {
+    // Note: x86's HTM facility will cause a segfault if an xend instruction is
+    // called outside of a transaction, hence we need to check if we're in a
+    // transaction before actually trying to end it.
+    LLVMContext &C = I->getContext();
+    BasicBlock *CurBB = I->getParent(), *NewSuccBB, *HTMEndBB;
+    Function *CurF = CurBB->getParent();
+
+    // Create a new successor which contains all instructions after the HTM
+    // check & end
+    NewSuccBB = CurBB->splitBasicBlock(I,
+      ".htmendsucc" + std::to_string(NumHTMEnds));
+
+    // Create an HTM end block, which ends the transaction and jumps to the
+    // new successor
+    HTMEndBB = BasicBlock::Create(C,
+      ".htmend" + std::to_string(NumHTMEnds), CurF, NewSuccBB);
+    IRBuilder<> EndWorker(HTMEndBB);
+    EndWorker.CreateCall(HTMEndDecl);
+    EndWorker.CreateBr(NewSuccBB);
+
+    // Finally, add the HTM test & replace the unconditional branch created by
+    // splitBasicBlock() with a conditional branch to either end the
+    // transaction or continue on to the new successor
+    IRBuilder<> PredWorker(CurBB->getTerminator());
+    CallInst *HTMTestVal = PredWorker.CreateCall(HTMTestDecl);
+    ConstantInt *Zero = ConstantInt::get(IntegerType::getInt32Ty(C), 0, true);
+    Value *Cmp = PredWorker.CreateICmpNE(HTMTestVal, Zero,
+      "htmcmp" + std::to_string(NumHTMEnds));
+    PredWorker.CreateCondBr(Cmp, HTMEndBB, NewSuccBB);
+    CurBB->getTerminator()->eraseFromParent();
+  }
+
+  /// Insert migration points & HTM instrumentation for instructions.
+  void addMigrationPoints(Function &F) {
+    if(DoHTMInst) {
+      // Note: need to add the HTM ends before begins
+      for(auto I = HTMEndInsts.begin(), E = HTMEndInsts.end(); I != E; ++I) {
+        switch(Arch) {
+        case Triple::ppc64le: addPowerPCHTMEnd(*I); break;
+        case Triple::x86_64: addX86HTMCheckAndEnd(*I); break;
+        default: llvm_unreachable("HTM -- unsupported architecture");
+        }
+        NumHTMEnds++;
+      }
+
+      // The following APIs both insert HTM begins & migration points because
+      // the control flow with/without abort handlers is intertwined
+      for(auto I = HTMBeginInsts.begin(), E = HTMBeginInsts.end();
+          I != E; ++I) {
+        switch(Arch) {
+        case Triple::ppc64le: addPowerPCHTMBegin(*I); break;
+        case Triple::x86_64: addX86HTMBegin(*I); break;
+        default: llvm_unreachable("HTM -- unsupported architecture");
+        }
+        NumHTMBegins++;
+        NumMigPoints++;
+      }
+    }
+    else {
+      for(auto I = MigPointInsts.begin(), E = MigPointInsts.end();
+          I != E; ++I) {
+        addMigrationPoint(*I);
+        NumMigPoints++;
+      }
+    }
+  }
+};
+
+} /* end anonymous namespace */
+
+char MigrationPoints::ID = 0;
+
+const MigrationPoints::IntrinsicMap MigrationPoints::HTMBegin = {
+  {Triple::x86_64, Intrinsic::x86_xbegin},
+  {Triple::ppc64le, Intrinsic::ppc_tbegin}
+};
+
+const MigrationPoints::IntrinsicMap MigrationPoints::HTMEnd = {
+  {Triple::x86_64, Intrinsic::x86_xend},
+  {Triple::ppc64le, Intrinsic::ppc_tend}
+};
+
+const MigrationPoints::IntrinsicMap MigrationPoints::HTMTest = {
+  {Triple::x86_64, Intrinsic::x86_xtest},
+  {Triple::ppc64le, Intrinsic::ppc_ttest}
+};
+
+INITIALIZE_PASS(MigrationPoints, "migration-points",
+                "Insert migration points into functions", true, false)
+
+namespace llvm {
+  FunctionPass *createMigrationPointsPass()
+  { return new MigrationPoints(); }
+}
+
Index: lib/Transforms/Utils/CMakeLists.txt
===================================================================
--- lib/Transforms/Utils/CMakeLists.txt	(revision 320332)
+++ lib/Transforms/Utils/CMakeLists.txt	(working copy)
@@ -28,6 +28,7 @@
   Mem2Reg.cpp
   MetaRenamer.cpp
   ModuleUtils.cpp
+  NameStringLiterals.cpp
   PromoteMemoryToRegister.cpp
   SSAUpdater.cpp
   SimplifyCFG.cpp
@@ -34,6 +35,7 @@
   SimplifyIndVar.cpp
   SimplifyInstructions.cpp
   SimplifyLibCalls.cpp
+  StaticVarSections.cpp
   SymbolRewriter.cpp
   UnifyFunctionExitNodes.cpp
   Utils.cpp
Index: lib/Transforms/Utils/NameStringLiterals.cpp
===================================================================
--- lib/Transforms/Utils/NameStringLiterals.cpp	(nonexistent)
+++ lib/Transforms/Utils/NameStringLiterals.cpp	(working copy)
@@ -0,0 +1,119 @@
+#include <algorithm>
+#include <cctype>
+#include "llvm/Pass.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/GlobalVariable.h"
+#include "llvm/IR/GlobalValue.h"
+#include "llvm/IR/Module.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+#define DEBUG_TYPE "name-string-literals"
+#define CHARS_FOR_NAME 10
+
+using namespace llvm;
+
+namespace
+{
+
+/**
+ * Generate unique name for private anonymous string literals.  Uses the
+ * filename, LLVM's temporary name and (up to) the first 10 characters of the
+ * string.  Converts non-alphanumeric characters to underscores.
+ */
+std::string UniquifySymbol(const Module &M, GlobalVariable &Sym)
+{
+  std::string newName;
+  std::string::size_type loc;
+  auto filter = [](char c){ return !isalnum(c); };
+
+  newName = M.getName();
+  loc = newName.find_last_of('.');
+  newName = newName.substr(0, loc) + "_" + Sym.getName().str() + "_";
+  std::replace_if(newName.begin(), newName.end(), filter, '_');
+
+  // Check if it's a string, and if so use string content to uniquify
+  if(Sym.hasInitializer()) {
+    Constant *Initializer = Sym.getInitializer();
+    if(isa<ConstantDataSequential>(Initializer)) {
+      ConstantDataSequential *CDS = cast<ConstantDataSequential>(Initializer);
+      if(CDS->isString()) {
+        std::string data = CDS->getAsString().substr(0, CHARS_FOR_NAME);
+        std::replace_if(data.begin(), data.end(), filter, '_');
+        newName += data;
+      }
+    }
+  }
+
+  return newName;
+}
+
+/**
+ * This pass searches for anonymous read-only data for which there is no symbol
+ * and generates a symbol for the data.  This is required by the Popcorn
+ * compiler in order to align the data at link-time.
+ */
+class NameStringLiterals : public ModulePass
+{
+public:
+	static char ID;
+
+  NameStringLiterals() : ModulePass(ID) {}
+  ~NameStringLiterals() {}
+
+	/* ModulePass virtual methods */
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const { AU.setPreservesCFG(); }
+	virtual bool runOnModule(Module &M)
+  {
+    bool modified = false;
+    std::string newName;
+    Module::global_iterator gl, gle; // for global variables
+
+    DEBUG(errs() << "\n********** Begin NameStringLiterals **********\n"
+                 << "********** Module: " << M.getName() << " **********\n\n");
+
+    // Iterate over all globals and generate symbol for anonymous string
+    // literals in each module
+    for(gl = M.global_begin(), gle = M.global_end(); gl != gle; gl++) {
+      // DONT NEED TO CHANGE NAME PER-SE just change type
+      // PrivateLinkage does NOT show up in any symbol table in the object file!
+      if(gl->getLinkage() == GlobalValue::PrivateLinkage) {
+        //change Linkage
+        //FROM private unnamed_addr constant [num x i8]
+        //TO global [num x i8]
+        gl->setLinkage(GlobalValue::ExternalLinkage);
+
+        // Make the global's name unique so we don't clash when linking with
+        // other files
+        newName = UniquifySymbol(M, *gl);
+        gl->setName(newName);
+
+        // Also REMOVE unnamed_addr value
+        if(gl->hasUnnamedAddr()) {
+          gl->setUnnamedAddr(false);
+        }
+
+        modified = true;
+
+        DEBUG(errs() << "New anonymous string name: " << newName << "\n";);
+      } else {
+        DEBUG(errs() << "> " <<  *gl << ", linkage: "
+                     << gl->getLinkage() << "\n");
+      }
+    }
+  
+    return modified;
+  }
+  virtual const char *getPassName() const { return "Name string literals"; }
+};
+
+} /* end anonymous namespace */
+
+char NameStringLiterals::ID = 0;
+INITIALIZE_PASS(NameStringLiterals, "name-string-literals",
+  "Generate symbols for anonymous string literals", false, false)
+
+namespace llvm {
+  ModulePass *createNameStringLiteralsPass() { return new NameStringLiterals(); }
+}
+
Index: lib/Transforms/Utils/StaticVarSections.cpp
===================================================================
--- lib/Transforms/Utils/StaticVarSections.cpp	(nonexistent)
+++ lib/Transforms/Utils/StaticVarSections.cpp	(working copy)
@@ -0,0 +1,106 @@
+#include <algorithm>
+#include "llvm/Pass.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/GlobalVariable.h"
+#include "llvm/IR/GlobalValue.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+#define DEBUG_TYPE "static-var-sections"
+
+using namespace llvm;
+
+namespace
+{
+
+std::string UniquifySymbol(const Module &M,
+                           std::string &section,
+                           GlobalVariable &Sym)
+{
+  std::string newName;
+  auto filter = [](char c){ return !isalnum(c); };
+
+  newName = M.getName().str() + "_" + Sym.getName().str();
+  std::replace_if(newName.begin(), newName.end(), filter, '_');
+
+  return section + newName;
+}
+
+/**
+ * This pass searches for static, i.e., module-private, global variables and
+ * modifies their linkage to be in their own sections similarly to other
+ * global variables with the -fdata-sections switch.  By default, LLVM doesn't
+ * apply -fdata-sections to static global variables.
+ */
+class StaticVarSections : public ModulePass
+{
+public:
+	static char ID;
+
+	StaticVarSections() : ModulePass(ID) {}
+	~StaticVarSections() {}
+
+	/* ModulePass virtual methods */
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const { AU.setPreservesCFG(); }
+	virtual bool runOnModule(Module &M)
+  {
+    bool modified = false;
+    Module::iterator it, ite;
+    Module::global_iterator gl, gle; // for global variables
+  
+    DEBUG(errs() << "\n********** Beginning StaticVarSections **********\n"
+                 << "********** Module: " << M.getName() << " **********\n\n");
+  
+    // Iterate over all static globals and place them in their own section
+    for(gl = M.global_begin(), gle = M.global_end(); gl != gle; gl++) {
+      std::string secName = ".";
+      if(gl->isThreadLocal()) secName += "t";
+  
+      if(gl->hasCommonLinkage() &&
+         gl->getName().find(".cache.") != std::string::npos) {
+        gl->setLinkage(GlobalValue::InternalLinkage);
+      }
+  
+      // InternalLinkage is specifically for STATIC variables
+      if(gl->hasInternalLinkage() && !gl->hasSection()) {
+        if(gl->isConstant()) {
+          //Belongs in RODATA
+          assert(!gl->isThreadLocal() && "TLS data should not be in .rodata");
+          secName += "rodata.";
+        }
+        else if(gl->getInitializer()->isZeroValue()) {
+          //Belongs in BSS
+          secName += "bss.";
+        }
+        else {
+          //Belongs in DATA
+          secName += "data.";
+        }
+
+        secName = UniquifySymbol(M, secName, *gl);
+        gl->setSection(secName);
+        modified = true;
+
+        DEBUG(errs() << *gl << " - new section: " << secName << "\n");
+      } else {
+        DEBUG(errs() << "> " <<  *gl << ", linkage: "
+                     << gl->getLinkage() << "\n");
+        continue;
+      }
+    }
+    
+    return modified;
+  }
+  virtual const char *getPassName() const { return "Static variables in separate sections"; }
+};
+
+} /* end anonymous namespace */
+
+char StaticVarSections::ID = 0;
+INITIALIZE_PASS(StaticVarSections, "static-var-sections",
+  "Put static variables into separate sections", false, false)
+
+namespace llvm {
+  ModulePass *createStaticVarSectionsPass() { return new StaticVarSections(); }
+}
+
Index: lib/Transforms/Utils/Utils.cpp
===================================================================
--- lib/Transforms/Utils/Utils.cpp	(revision 320332)
+++ lib/Transforms/Utils/Utils.cpp	(working copy)
@@ -28,7 +28,9 @@
   initializeLoopSimplifyPass(Registry);
   initializeLowerInvokePass(Registry);
   initializeLowerSwitchPass(Registry);
+  initializeNameStringLiteralsPass(Registry);
   initializePromotePassPass(Registry);
+  initializeStaticVarSectionsPass(Registry);
   initializeUnifyFunctionExitNodesPass(Registry);
   initializeInstSimplifierPass(Registry);
   initializeMetaRenamerPass(Registry);
