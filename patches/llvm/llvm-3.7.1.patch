Index: include/llvm/ADT/StringSet.h
===================================================================
--- include/llvm/ADT/StringSet.h	(revision 277823)
+++ include/llvm/ADT/StringSet.h	(working copy)
@@ -23,7 +23,16 @@
   class StringSet : public llvm::StringMap<char, AllocatorTy> {
     typedef llvm::StringMap<char, AllocatorTy> base;
   public:
+    StringSet() : StringMap<char, AllocatorTy>() {}
 
+    StringSet(std::initializer_list<StringRef> List)
+      : StringMap<char, AllocatorTy>() {
+      for(auto Key : List) {
+        assert(!Key.empty());
+        base::insert(std::make_pair(Key, '\0'));
+      }
+    }
+
     std::pair<typename base::iterator, bool> insert(StringRef Key) {
       assert(!Key.empty());
       return base::insert(std::make_pair(Key, '\0'));
Index: include/llvm/ADT/ilist_node.h
===================================================================
--- include/llvm/ADT/ilist_node.h	(revision 277823)
+++ include/llvm/ADT/ilist_node.h	(working copy)
@@ -70,7 +70,7 @@
     const NodeTy *Prev = this->getPrev();
 
     // Check for sentinel.
-    if (!Prev->getNext())
+    if (!Prev || !Prev->getNext())
       return nullptr;
 
     return Prev;
@@ -81,7 +81,7 @@
     NodeTy *Next = getNext();
 
     // Check for sentinel.
-    if (!Next->getNext())
+    if (!Next || !Next->getNext())
       return nullptr;
 
     return Next;
Index: include/llvm/Analysis/LiveValues.h
===================================================================
--- include/llvm/Analysis/LiveValues.h	(nonexistent)
+++ include/llvm/Analysis/LiveValues.h	(working copy)
@@ -0,0 +1,209 @@
+/*
+ * Calculate live-value sets for functions.
+ *
+ * Liveness-analysis is based on the non-iterative dataflow algorithm for
+ * reducible graphs by Brandner et. al in:
+ *
+ * "Computing Liveness Sets for SSA-Form Programs"
+ * URL: https://hal.inria.fr/inria-00558509v1/document
+ * Accessed: 5/19/2016
+ *
+ * Author: Rob Lyerly <rlyerly@vt.edu>
+ * Date: 5/19/2016
+ */
+
+#ifndef _LIVE_VALUES_H
+#define _LIVE_VALUES_H
+
+#include <map>
+#include <set>
+#include <list>
+#include "llvm/Pass.h"
+#include "llvm/Analysis/LoopNestingTree.h"
+#include "llvm/IR/Function.h"
+#include "llvm/Support/raw_ostream.h"
+
+namespace llvm {
+
+class LiveValues : public FunctionPass
+{
+public:
+  typedef std::pair<const BasicBlock *, const BasicBlock *> Edge;
+
+  static char ID;
+
+  /**
+   * Default constructor.
+   */
+  LiveValues(void);
+
+  /**
+   * Default destructor.
+   */
+  ~LiveValues(void) {}
+
+  /**
+   * Return whether or not a given type should be included in the analysis.
+   * @return true if the type is included in liveness sets, false otherwise
+   */
+  bool includeAsm(void) const { return inlineasm; }
+  bool includeBitcasts(void) const { return bitcasts; }
+  bool includeComparisons(void) const { return comparisons; }
+  bool includeConstants(void) const { return constants; }
+  bool includeMetadata(void) const { return metadata; }
+
+  /**
+   * Set whether or not to include the specified type in the analysis (all
+   * are set to false by default by the constructor).
+   * @param include true if it should be included, false otherwise
+   */
+  void includeAsm(bool include) { inlineasm = include; }
+  void includeBitcasts(bool include) { bitcasts = include; }
+  void includeComparisons(bool include) { comparisons = include; }
+  void includeConstants(bool include) { constants = include; }
+  void includeMetadata(bool include) { metadata = include; }
+
+  /**
+   * Register which analysis passes we need.
+   * @param AU an analysis usage object
+   */
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const;
+
+  /**
+   * Calculate liveness sets for a function.
+   * @param F a function for which to calculate live values.
+   * @return false, always
+   */
+  virtual bool runOnFunction(Function &F);
+
+  /**
+   * Get the human-readable name of the pass.
+   * @return the pass name
+   */
+  virtual const char *getPassName() const { return "Live value analysis"; }
+
+  /**
+   * Print a human-readable version of the analysis.
+   * @param O an output stream
+   * @param F the function for which to print analysis
+   */
+  virtual void print(raw_ostream &O, const Function *F) const;
+
+  /**
+   * Return the live-in set for a basic block.
+   * @param BB a basic block
+   * @return a set of live-in values for the basic block; this set must be
+   *         freed by the user.
+   */
+  std::set<const Value *> *getLiveIn(const BasicBlock *BB) const;
+
+  /**
+   * Return the live-out set for a basic block.
+   * @param BB a basic block
+   * @return a set of live-out values for the basic block; this set must be
+   *         freed by the user.
+   */
+  std::set<const Value *> *getLiveOut(const BasicBlock *BB) const;
+
+  /**
+   * Get the live values across a given instruction, i.e., values live right
+   * after the invocation of the instruction (excluding the value defined by
+   * the instruction itself).
+   * @param inst an instruction
+   * @return the set of values live directly before the instruction; this set
+   *         must be freed by the user.
+   */
+  std::set<const Value *> *
+  getLiveValues(const Instruction *inst) const;
+
+private:
+  /* Should values of each type be included? */
+  bool inlineasm;
+  bool bitcasts;
+  bool comparisons;
+  bool constants;
+  bool metadata;
+
+  /* A loop nesting forest composed of 0 or more loop nesting trees. */
+  typedef std::list<LoopNestingTree> LoopNestingForest;
+
+  /* Maps live values to a basic block. */
+  typedef std::map<const BasicBlock *, std::set<const Value *> > LiveVals;
+  typedef std::pair<const BasicBlock *, std::set<const Value *> > LiveValsPair;
+
+  /* Store analysis for all functions. */
+  std::map<const Function *, LiveVals> FuncBBLiveIn;
+  std::map<const Function *, LiveVals> FuncBBLiveOut;
+
+  /**
+   * Return whether or not a value is a variable that should be tracked.
+   * @param val a value
+   * @return true if the value is a variable to be tracked, false otherwise
+   */
+  bool includeVal(const Value *val) const;
+
+  /**
+   * Insert the values used in phi-nodes at the beginning of basic block S (as
+   * values live from B) into the set uses.
+   * @param B a basic block which passes live values into phi-nodes in S
+   * @param S a basic block, successor to B
+   * @param uses set in which to add values used in phi-nodes in B
+   * @return the number of values added to the set
+   */
+  unsigned phiUses(const BasicBlock *B,
+                   const BasicBlock *S,
+                   std::set<const Value *> &uses);
+
+  /**
+   * Insert the values defined by the phi-nodes at the beginning of basic block
+   * B into the set defs.
+   * @param B a basic block
+   * @param defs set in which to add values defined by phi-nodes in B
+   * @return the number of values added to the set
+   */
+  unsigned phiDefs(const BasicBlock *B,
+                   std::set<const Value *> &defs);
+
+  /**
+   * Do a post-order traversal of the control flow graph to calculate partial
+   * liveness sets.
+   * @param F a function for which to calculate per-basic block partial
+   *          liveness sets
+   * @param liveIn per-basic block live-in values
+   * @param liveOut per-basic block live-out values
+   */
+  void dagDFS(Function &F, LiveVals &liveIn, LiveVals &liveOut);
+
+  /**
+   * Construct the loop-nesting forest for a function.
+   * @param F a function for which to calculate the loop-nesting forest.
+   * @param LNF a loop nesting forest to populate with loop nesting trees.
+   */
+  void constructLoopNestingForest(Function &F, LoopNestingForest &LNF);
+
+  /**
+   * Propagate live values throughout the loop-nesting tree.
+   * @param loopNest a loop-nesting tree
+   * @param liveIn per-basic block live-in values
+   * @param liveOut per-basic block live-out values
+   */
+  void propagateValues(const LoopNestingTree &loopNest,
+                       LiveVals &liveIn,
+                       LiveVals &liveOut);
+
+  /**
+   * Propagate live values within loops for all loop-nesting trees in the
+   * function's loop-nesting forest.
+   * @param LNF a loop nesting forest
+   * @param liveIn per-basic block live-in values
+   * @param liveOut per-basic block live-out values
+   */
+  void loopTreeDFS(LoopNestingForest &LNF,
+                   LiveVals &liveIn,
+                   LiveVals &liveOut);
+};
+
+} /* llvm namespace */
+
+#endif /* _LIVE_VALUES_H */
+
Index: include/llvm/Analysis/LoopNestingTree.h
===================================================================
--- include/llvm/Analysis/LoopNestingTree.h	(nonexistent)
+++ include/llvm/Analysis/LoopNestingTree.h	(working copy)
@@ -0,0 +1,191 @@
+/*
+ * Loop-nesting tree for a loop.  The root of a loop-nesting tree is the loop
+ * header of the outermost loop.  The children of any given node (including the
+ * root) are the basic blocks contained within the loop and the loop headers of
+ * nested loops.
+ *
+ * Note: we assume that the control-flow graphs are reducible
+ *
+ * Author: Rob Lyerly <rlyerly@vt.edu>
+ * Date: 5/23/2016
+ */
+
+#ifndef _LOOP_NESTING_TREE_H
+#define _LOOP_NESTING_TREE_H
+
+#include <list>
+#include <vector>
+#include <queue>
+#include "llvm/IR/BasicBlock.h"
+#include "llvm/Analysis/LoopInfo.h"
+#include "llvm/Support/raw_ostream.h"
+
+class LoopNestingTree {
+private:
+  /*
+   * Tree node object.
+   */
+  class Node {
+  public:
+    /**
+     * Construct a node for a basic block.
+     * @param _bb a basic block
+     * @param _parent the parent of this node, i.e. the loop header of the
+     *                containing loop
+     * @param _isLoopHeader is the basic block a loop header?
+     */
+    Node(const llvm::BasicBlock *_bb, const Node *_parent, bool _isLoopHeader)
+      : bb(_bb), parent(_parent), isLoopHeader(_isLoopHeader) {}
+
+    /**
+     * Add a child to the node.
+     * @param child a child to add to the node
+     */
+    void addChild(Node *child) { children.push_back(child); }
+
+    const llvm::BasicBlock *bb; /* Basic block encapsulated by the node. */
+    const Node *parent; /* Parent node, i.e. header of containing loop. */
+    std::list<Node *> children; /* Regular child nodes in the tree. */
+    bool isLoopHeader; /* Is the basic block a loop header? */
+  };
+
+  unsigned _size; /* Number of nodes (i.e., basic blocks) in the tree. */
+  unsigned _depth; /* Number of nested loops in the tree. */
+  Node *_root; /* Root of the tree, i.e. loop header of outermost loop. */
+
+  /**
+   * Print a node & its children.  Recurses into nested loops.
+   * @param O an output stream on which to print the tree
+   * @param node a node to print
+   * @param depth the current depth
+   */
+  void print(llvm::raw_ostream &O, Node *node, unsigned depth) const;
+
+  /**
+   * Delete the node's children & the node itself.  Recurses into nested loops.
+   * @param node the node being deleted
+   */
+  void deleteRecursive(Node *node);
+
+public:
+  /**
+   * Construct a loop-nesting tree from a strongly-connected component of the
+   * control-flow graph.
+   * @param SCC a strongly-connected component of the control-flow graph
+   * @param LI analysis from the loop info pass
+   */
+  LoopNestingTree(const std::vector<llvm::BasicBlock *> &SCC,
+                  const llvm::LoopInfo &LI);
+
+  /**
+   * Destroy a loop-nesting tree.
+   */
+  ~LoopNestingTree() { deleteRecursive(this->_root); }
+
+  /**
+   * Return the size of the loop-nesting tree, that is the number of nodes in
+   * the loop (and all nested loops).
+   * @return the number of nodes in the tree
+   */
+  unsigned size() const { return this->_size; }
+
+  /**
+   * Return the depth of the loop-nesting tree, that is the number of nested
+   * loops.  A value of one indicates that there are no nested loops.
+   * @return the number of nested loops in the tree
+   */
+  unsigned depth() const { return this->_depth; }
+
+  /**
+   * Print the tree.
+   * @param O the output stream on which to print the tree
+   */
+  void print(llvm::raw_ostream &O) const { print(O, this->_root, 0); }
+
+  /*
+   * Loop-node iterator object.  Delivers loop nodes in breadth-first order.
+   */
+  class loop_iterator {
+  public:
+    typedef loop_iterator self_type;
+    typedef const llvm::BasicBlock *value_type;
+    typedef value_type& reference;
+    typedef value_type* pointer;
+    typedef std::forward_iterator_tag iterator_category;
+
+    self_type operator++(void);
+    self_type operator++(int junk);
+    reference operator*(void) { return cur->bb; }
+    pointer operator->(void) { return &cur->bb; }
+    bool operator==(const self_type& rhs) { return cur == rhs.cur; }
+    bool operator!=(const self_type& rhs) { return cur != rhs.cur; }
+
+    friend class LoopNestingTree;
+    friend class child_iterator;
+  private:
+    Node *cur;
+    std::queue<Node *> remaining;
+
+    loop_iterator(Node *start) : cur(start) { addLoopHeaders(); }
+    void addLoopHeaders(void);
+  };
+
+  /*
+   * Child iterator object.  Traverses children of tree nodes.
+   */
+  class child_iterator {
+  public:
+    typedef child_iterator self_type;
+    typedef const llvm::BasicBlock *value_type;
+    typedef value_type& reference;
+    typedef value_type* pointer;
+    typedef std::forward_iterator_tag iterator_category;
+    enum location { BEGIN, END };
+
+    self_type operator++(void)
+      { self_type me = *this; it.operator++(); return me; }
+    self_type operator++(int junk) { it.operator++(junk); return *this; }
+    reference operator*(void) { return (*it)->bb; }
+    pointer operator->(void) { return &(*it)->bb; }
+    bool operator==(const self_type& rhs) { return it == rhs.it; }
+    bool operator!=(const self_type& rhs) { return it != rhs.it; }
+
+    friend class LoopNestingTree;
+  private:
+    std::list<Node *>::const_iterator it;
+
+    child_iterator(loop_iterator &parent, enum location loc);
+  };
+
+  /**
+   * Return an iterator for traversing all loop nodes (i.e., loop header basic
+   * blocks) in the tree.  Delivers nodes in a breadth-first ordering.
+   * @return an iterator to traverse the loop nodes in the tree
+   */
+  loop_iterator loop_begin() const { loop_iterator it(_root); return it; };
+
+  /**
+   * Return an iterator marking the end of the loop nodes in the tree.
+   * @return an iterator marking the end of the traversal
+   */
+  loop_iterator loop_end() const { loop_iterator it(nullptr); return it; };
+
+  /**
+   * Return an iterator for traversing the children of a loop node.
+   * @param an iterator associated with a loop node
+   * @return an iterator to traverse the children of a loop node
+   */
+  child_iterator children_begin(loop_iterator &parent) const
+    { child_iterator it(parent, child_iterator::BEGIN); return it; }
+
+  /**
+   * Return an iterator marking the end of the children of a loop node.
+   * @param an iterator associated with a loop node
+   * @return an iterator marking the end of the traversal
+   */
+  child_iterator children_end(loop_iterator &parent) const
+    { child_iterator it(parent, child_iterator::END); return it; }
+};
+
+#endif /* _LOOP_NESTING_TREE_H */
+
Index: include/llvm/Analysis/Passes.h
===================================================================
--- include/llvm/Analysis/Passes.h	(revision 277823)
+++ include/llvm/Analysis/Passes.h	(working copy)
@@ -173,6 +173,13 @@
   //
   FunctionPass *createMemDerefPrinter();
 
+  //===--------------------------------------------------------------------===//
+  //
+  // createLiveValuesPass - This pass calculates live-value sets for basic
+  // blocks in a function.
+  //
+  FunctionPass *createLiveValuesPass();
+
 }
 
 #endif
Index: include/llvm/CodeGen/AsmPrinter.h
===================================================================
--- include/llvm/CodeGen/AsmPrinter.h	(revision 277823)
+++ include/llvm/CodeGen/AsmPrinter.h	(working copy)
@@ -199,9 +199,10 @@
 
   /// Emit the specified function out to the OutStreamer.
   bool runOnMachineFunction(MachineFunction &MF) override {
+    bool modified = TagCallSites(MF);
     SetupMachineFunction(MF);
     EmitFunctionBody();
-    return false;
+    return modified;
   }
 
   //===------------------------------------------------------------------===//
@@ -329,6 +330,13 @@
   /// instructions in verbose mode.
   virtual void emitImplicitDef(const MachineInstr *MI) const;
 
+  /// Some machine instructions encapsulate a call with follow-on boilerplate
+  /// instructions, meaning labels emitted after the "instruction" do not
+  /// capture the call's true return address.  Return an offset for correcting
+  /// these labels to refer to the call's actual return address.
+  virtual int getCanonicalReturnAddr(const MachineInstr *Call) const
+  { return 0; }
+
   //===------------------------------------------------------------------===//
   // Symbol Lowering Routines.
   //===------------------------------------------------------------------===//
@@ -393,6 +401,14 @@
     EmitLabelPlusOffset(Label, 0, Size, IsSectionRelative);
   }
 
+  /// Find the stackmap intrinsic associated with a function call
+  MachineInstr *FindStackMap(MachineBasicBlock &MBB,
+                             MachineInstr *MI) const;
+
+  /// Move stackmap intrinsics directly after calls to correctly capture
+  /// return addresses
+  bool TagCallSites(MachineFunction &MF);
+
   //===------------------------------------------------------------------===//
   // Dwarf Emission Helper Routines
   //===------------------------------------------------------------------===//
Index: include/llvm/CodeGen/MachineFunction.h
===================================================================
--- include/llvm/CodeGen/MachineFunction.h	(revision 277823)
+++ include/llvm/CodeGen/MachineFunction.h	(working copy)
@@ -20,6 +20,8 @@
 
 #include "llvm/ADT/ilist.h"
 #include "llvm/CodeGen/MachineBasicBlock.h"
+#include "llvm/CodeGen/StackTransformTypes.h"
+#include "llvm/IR/Instructions.h"
 #include "llvm/IR/DebugLoc.h"
 #include "llvm/IR/Metadata.h"
 #include "llvm/Support/Allocator.h"
@@ -145,6 +147,12 @@
   /// True if the function includes any inline assembly.
   bool HasInlineAsm;
 
+  /// Duplicate live value locations for stackmap operands
+  InstToOperands SMDuplicateLocs;
+
+  /// Architecture-specific live value locations for each stackmap
+  InstToArchLiveValues SMArchSpecificLocs;
+
   MachineFunction(const MachineFunction &) = delete;
   void operator=(const MachineFunction&) = delete;
 public:
@@ -457,6 +465,9 @@
     return Mask;
   }
 
+  /// Is a register caller-saved?
+  bool isCallerSaved(unsigned Reg) const;
+
   /// allocateMemRefsArray - Allocate an array to hold MachineMemOperand
   /// pointers.  This array is owned by the MachineFunction.
   MachineInstr::mmo_iterator allocateMemRefsArray(unsigned long Num);
@@ -488,6 +499,39 @@
   /// getPICBaseSymbol - Return a function-local symbol to represent the PIC
   /// base.
   MCSymbol *getPICBaseSymbol() const;
+
+  //===--------------------------------------------------------------------===//
+  // Architecture-specific stack transformation metadata
+  //
+
+  /// Add an IR/architecture-specific location mapping for a stackmap operand
+  void addSMOpLocation(const CallInst *SM, const Value *Val,
+                       const MachineLiveLoc &MLL);
+  void addSMOpLocation(const CallInst *SM, unsigned Op,
+                       const MachineLiveLoc &MLL);
+
+  /// Add an architecture-specific live value & location for a stackmap
+  void addSMArchSpecificLocation(const CallInst *SM,
+                                 const MachineLiveLoc &MLL,
+                                 const MachineLiveVal &MLV);
+
+  /// Update stack slot references to new indexes after stack slot coloring
+  void updateSMStackSlotRefs(SmallDenseMap<int, int, 16> &Changes);
+
+  /// Are there any architecture-specific locations for operand Val in stackmap
+  /// SM?
+  bool hasSMOpLocations(const CallInst *SM, const Value *Val) const;
+
+  /// Are there any architecture-specific locations for stackmap SM?
+  bool hasSMArchSpecificLocations(const CallInst *SM) const;
+
+  /// Return the architecture-specific locations for a stackmap operand.
+  const MachineLiveLocs &getSMOpLocations(const CallInst *SM,
+                                          const Value *Val) const;
+
+  /// Return the architecture-specific locations for a stackmap that are not
+  /// associated with any operand.
+  const ArchLiveValues &getSMArchSpecificLocations(const CallInst *SM) const;
 };
 
 //===--------------------------------------------------------------------===//
Index: include/llvm/CodeGen/Passes.h
===================================================================
--- include/llvm/CodeGen/Passes.h	(revision 277823)
+++ include/llvm/CodeGen/Passes.h	(working copy)
@@ -123,6 +123,15 @@
   /// Default setting for -enable-shrink-wrap on this target.
   bool EnableShrinkWrap;
 
+  /// Add equivalence points into the application
+  bool AddMigrationPoints;
+
+  /// Add stackmaps at function call sites & equivalence points
+  bool AddStackMaps;
+
+  /// Add stackmaps describing stack state in libc thread start functions
+  bool AddLibcStackMaps;
+
 public:
   TargetPassConfig(TargetMachine *tm, PassManagerBase &pm);
   // Dummy constructor.
@@ -142,6 +151,9 @@
 
   CodeGenOpt::Level getOptLevel() const { return TM->getOptLevel(); }
 
+  CodeGenOpt::Level getArchIROptLevel() const
+  { return TM->getArchIROptLevel(); }
+
   /// Set the StartAfter, StartBefore and StopAfter passes to allow running only
   /// a portion of the normal code-gen pass sequence.
   ///
@@ -193,6 +205,26 @@
   /// Return true if shrink wrapping is enabled.
   bool getEnableShrinkWrap() const;
 
+  /// Return whether we should instrument the code with equivalence points.
+  bool addMigrationPoints() const { return AddMigrationPoints; }
+
+  /// Return whether we should emit stack transformation metadata by
+  /// instrumenting the code with IR-level StackMaps.
+  bool addStackMaps() const { return AddStackMaps; }
+
+  /// Return whether we should emit transformation metadata (via IR-level
+  /// StackMaps) for libc thread start functions.
+  bool addLibcStackMaps() const { return AddLibcStackMaps; }
+
+  /// \brief Enable/disable adding equivalence points.
+  void setAddMigrationPoints(bool Set) { AddMigrationPoints = Set; }
+
+  /// \brief Enable/disable adding StackMaps.
+  void setAddStackMaps(bool Set) { AddStackMaps = Set; }
+
+  /// \brief Enable/disable adding StackMaps to libc thread start function.
+  void setAddLibcStackMaps(bool Set) { AddLibcStackMaps = Set; }
+
   /// Return true if the default global register allocator is in use and
   /// has not be overriden on the command line with '-regalloc=...'
   bool usingDefaultRegAlloc() const;
@@ -448,6 +480,10 @@
   // instruction and update the MachineFunctionInfo with that information.
   extern char &ShrinkWrapID;
 
+  /// Stack transformation metadata pass.  Gather additional stack
+  /// transformation metadata from machine functions.
+  extern char &StackTransformMetadataID;
+
   /// VirtRegRewriter pass. Rewrite virtual registers to physical registers as
   /// assigned in VirtRegMap.
   extern char &VirtRegRewriterID;
Index: include/llvm/CodeGen/StackMaps.h
===================================================================
--- include/llvm/CodeGen/StackMaps.h	(revision 277823)
+++ include/llvm/CodeGen/StackMaps.h	(working copy)
@@ -13,6 +13,8 @@
 #include "llvm/ADT/MapVector.h"
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/CodeGen/MachineInstr.h"
+#include "llvm/CodeGen/StackTransformTypes.h"
+#include "llvm/IR/Instructions.h"
 #include "llvm/Support/Debug.h"
 #include <map>
 #include <vector>
@@ -22,6 +24,7 @@
 class AsmPrinter;
 class MCExpr;
 class MCStreamer;
+class UnwindInfo;
 
 /// \brief MI-level patchpoint operands.
 ///
@@ -142,9 +145,16 @@
     unsigned Size;
     unsigned Reg;
     int64_t Offset;
-    Location() : Type(Unprocessed), Size(0), Reg(0), Offset(0) {}
-    Location(LocationType Type, unsigned Size, unsigned Reg, int64_t Offset)
-        : Type(Type), Size(Size), Reg(Reg), Offset(Offset) {}
+    bool Ptr;
+    bool Alloca;
+    bool Duplicate;
+    unsigned AllocaSize;
+    Location() : Type(Unprocessed), Size(0), Reg(0), Offset(0),
+                 Ptr(false), Alloca(false), Duplicate(false), AllocaSize(0) {}
+    Location(LocationType Type, unsigned Size, unsigned Reg, int64_t Offset,
+             bool Ptr, bool Alloca, bool Duplicate, unsigned AllocaSize)
+        : Type(Type), Size(Size), Reg(Reg), Offset(Offset), Ptr(Ptr),
+          Alloca(Alloca), Duplicate(Duplicate), AllocaSize(AllocaSize) {}
   };
 
   struct LiveOutReg {
@@ -158,6 +168,20 @@
         : Reg(Reg), DwarfRegNum(DwarfRegNum), Size(Size) {}
   };
 
+  struct Operation {
+    ValueGenInst::InstType InstType;
+    Location::LocationType OperandType;
+    unsigned Size;
+    unsigned DwarfReg;
+    int64_t Constant;
+    bool isGenerated;
+    bool isSymbol;
+    const MCSymbol *Symbol;
+    Operation()
+      : Size(0), DwarfReg(0), Constant(0), isGenerated(false),
+        isSymbol(false), Symbol(nullptr) {}
+  };
+
   // OpTypes are used to encode information about the following logical
   // operand (which may consist of several MachineOperands) for the
   // OpParser.
@@ -185,7 +209,7 @@
   /// If there is any stack map data, create a stack map section and serialize
   /// the map info into it. This clears the stack map data structures
   /// afterwards.
-  void serializeToStackMapSection();
+  void serializeToStackMapSection(const UnwindInfo *UI = nullptr);
 
 private:
   static const char *WSMP;
@@ -193,17 +217,23 @@
   typedef SmallVector<LiveOutReg, 8> LiveOutVec;
   typedef MapVector<uint64_t, uint64_t> ConstantPool;
   typedef MapVector<const MCSymbol *, uint64_t> FnStackSizeMap;
+  typedef std::pair<Location, Operation> ArchValue;
+  typedef SmallVector<ArchValue, 8> ArchValues;
 
   struct CallsiteInfo {
+    const MCSymbol *Func;
     const MCExpr *CSOffsetExpr;
     uint64_t ID;
     LocationVec Locations;
     LiveOutVec LiveOuts;
-    CallsiteInfo() : CSOffsetExpr(nullptr), ID(0) {}
-    CallsiteInfo(const MCExpr *CSOffsetExpr, uint64_t ID,
-                 LocationVec &&Locations, LiveOutVec &&LiveOuts)
-        : CSOffsetExpr(CSOffsetExpr), ID(ID), Locations(std::move(Locations)),
-          LiveOuts(std::move(LiveOuts)) {}
+    ArchValues Vals;
+    CallsiteInfo() : Func(nullptr), CSOffsetExpr(nullptr), ID(0) {}
+    CallsiteInfo(const MCSymbol *Func, const MCExpr *CSOffsetExpr,
+                 uint64_t ID, LocationVec &&Locations,
+                 LiveOutVec &&LiveOuts, ArchValues &&Vals)
+        : Func(Func), CSOffsetExpr(CSOffsetExpr), ID(ID),
+          Locations(std::move(Locations)), LiveOuts(std::move(LiveOuts)),
+          Vals(std::move(Vals)) {}
   };
 
   typedef std::vector<CallsiteInfo> CallsiteInfoList;
@@ -213,10 +243,22 @@
   ConstantPool ConstPool;
   FnStackSizeMap FnStackSize;
 
+  /// Get stackmap information for register location
+  void getRegLocation(unsigned Phys, unsigned &Dwarf, unsigned &Offset) const;
+
+  /// Get pointer typing information for stackmap operand
+  void getPointerInfo(const Value *Op, const DataLayout &DL, bool &isPtr,
+                      bool &isAlloca, unsigned &AllocaSize) const;
+
+  /// Add duplicate target-specific locations for a stackmap operand
+  void addDuplicateLocs(const CallInst *StackMap, const Value *Oper,
+                        LocationVec &Locs, unsigned Size, bool Ptr,
+                        bool Alloca, unsigned AllocaSize) const;
+
   MachineInstr::const_mop_iterator
   parseOperand(MachineInstr::const_mop_iterator MOI,
                MachineInstr::const_mop_iterator MOE, LocationVec &Locs,
-               LiveOutVec &LiveOuts) const;
+               LiveOutVec &LiveOuts, User::const_op_iterator &Op) const;
 
   /// \brief Create a live-out register record for the given register @p Reg.
   LiveOutReg createLiveOutReg(unsigned Reg,
@@ -226,6 +268,15 @@
   /// registers that need to be recorded in the stackmap.
   LiveOutVec parseRegisterLiveOutMask(const uint32_t *Mask) const;
 
+  /// Convert a list of instructions used to generate an architecture-specific
+  /// live value into multiple individual records.
+  void genArchValsFromInsts(ArchValues &AV,
+                            Location &Loc,
+                            const MachineLiveVal &MLV);
+
+  /// Add architecture-specific locations for the stackmap.
+  void addArchLiveVals(const CallInst *SM, ArchValues &AV);
+
   /// This should be called by the MC lowering code _immediately_ before
   /// lowering the MI to an MCInst. It records where the operands for the
   /// instruction are stored, and outputs a label to record the offset of
@@ -240,7 +291,7 @@
   void emitStackmapHeader(MCStreamer &OS);
 
   /// \brief Emit the function frame record for each function.
-  void emitFunctionFrameRecords(MCStreamer &OS);
+  void emitFunctionFrameRecords(MCStreamer &OS, const UnwindInfo *UI);
 
   /// \brief Emit the constant pool.
   void emitConstantPoolEntries(MCStreamer &OS);
Index: include/llvm/CodeGen/StackTransformTypes.def
===================================================================
--- include/llvm/CodeGen/StackTransformTypes.def	(nonexistent)
+++ include/llvm/CodeGen/StackTransformTypes.def	(working copy)
@@ -0,0 +1,34 @@
+//===-- llvm/Target/StackTransformTypes.def - Generator Opcodes -*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Macros which define the set of available instructions for the ISA-agnostic
+// value generator.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_CODEGEN_STACKTRANSFORMTYPES_DEF
+#define LLVM_CODEGEN_STACKTRANSFORMTYPES_DEF
+
+// TODO generate using TableGen rather than X macros
+
+/// Each instruction is defined by a mnemonic and an operand (represented using
+/// the various ValueGenInst types).
+#define VALUE_GEN_INST \
+  X(Set)             /* Set the destination to another value */ \
+  X(Add)             /* Add a value to the destination */ \
+  X(Subtract)        /* Subtract a value from the destination */ \
+  X(Multiply)        /* Multiply the destination by another value */ \
+  X(Divide)          /* Divide the destination by another value */ \
+  X(LeftShift)       /* Left-shift the destination */ \
+  X(RightShiftLog)   /* Right-shift (logical) the destination */ \
+  X(RightShiftArith) /* Right-shift (arithmetic) the destination */ \
+  X(Mask)            /* Apply bit mask to the destination */ \
+
+#endif
+
Index: include/llvm/CodeGen/StackTransformTypes.h
===================================================================
--- include/llvm/CodeGen/StackTransformTypes.h	(nonexistent)
+++ include/llvm/CodeGen/StackTransformTypes.h	(working copy)
@@ -0,0 +1,544 @@
+//===------- StackTransformTypes.h - Stack Transform Types ------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_CODEGEN_STACKTRANFORMTYPES_H
+#define LLVM_CODEGEN_STACKTRANFORMTYPES_H
+
+#include <cstdint>
+#include <string>
+#include <vector>
+#include <map>
+#include <memory>
+#include "llvm/CodeGen/MachineOperand.h"
+#include "llvm/CodeGen/StackTransformTypes.def"
+#include "llvm/ADT/SmallVector.h"
+
+namespace llvm {
+
+class AsmPrinter;
+class Instruction;
+class MachineInstr;
+class MCSymbol;
+class Value;
+
+//===----------------------------------------------------------------------===//
+// Types for generating more complex architecture-specific live values
+//
+
+#define INV_INST_TYPE "Invalid instruction type"
+
+/// ValueGenInst - an instruction for the transformation runtime which generates
+/// a live value.  These instructions specify a simple operation (e.g., add) and
+/// an operand.  These instructions, coupled together with a destination
+/// location (i.e., a register or stack slot), allow the runtime to construct
+/// more complex live values like bit-shifts or pointers into arrays of structs.
+class ValueGenInst {
+public:
+  virtual ~ValueGenInst() {}
+
+  /// Instruction types
+  enum InstType {
+#define X(type) type,
+    VALUE_GEN_INST
+#undef X
+  };
+  virtual InstType type() const = 0;
+
+  /// Operand types
+  enum OpType { Register, Immediate };
+  virtual OpType opType() const = 0;
+
+  /// Equivalence checking.  Depends on both instruction & operand type, and
+  /// any operand-specific information.
+  virtual bool operator==(const ValueGenInst &RHS) const = 0;
+
+  /// Get a human-readable name for the instruction type
+  static const char *getInstName(enum InstType Type);
+  static std::string getInstNameStr(enum InstType Type);
+
+  /// Get a human-readable description of the instruction & operand
+  virtual std::string str() const = 0;
+private:
+  static const char *InstTypeStr[];
+};
+
+/// RegInstructionBase - base class for register operand instructions.  The
+/// register is stored as an architecture-specific physical register.
+class RegInstructionBase : public ValueGenInst {
+public:
+  virtual OpType opType() const { return Register; }
+  unsigned getReg() const { return Reg; }
+  void setReg(unsigned Reg) { this->Reg = Reg; }
+
+protected:
+  /// The register used in the instruction
+  // Note: will be converted to DWARF during metadata emission
+  unsigned Reg;
+
+  RegInstructionBase(unsigned Reg) : Reg(Reg) {}
+};
+
+/// RegInstruction<T> - register-based instructions.  Instructions are
+/// specified via template argument.
+template<ValueGenInst::InstType Type>
+class RegInstruction : public RegInstructionBase {
+  static_assert(Type == Set || Type == Add || Type == Subtract ||
+                Type == Multiply || Type == Divide,
+                INV_INST_TYPE " for register instruction");
+public:
+  RegInstruction(unsigned Reg) : RegInstructionBase(Reg) {}
+
+  virtual InstType type() const { return Type; }
+
+  virtual bool operator==(const ValueGenInst &RHS) const {
+    if(RHS.type() == Type && RHS.opType() == Register) {
+      const RegInstruction<Type> &RI = (const RegInstruction<Type> &)RHS;
+      if(RI.Reg == Reg) return true;
+    }
+    return false;
+  }
+
+  virtual std::string str() const
+  { return getInstNameStr(Type) + " register " + std::to_string(Reg); }
+};
+
+/// ImmInstructionBase - base class for immediate operand instructions.
+class ImmInstructionBase : public ValueGenInst {
+public:
+  virtual OpType opType() const { return Immediate; }
+  unsigned getImmSize() const { return Size; }
+  int64_t getImm() const { return Imm; }
+  void setImm(unsigned Size, int64_t Imm)
+  { this->Size = Size; this->Imm = Imm; }
+
+protected:
+  unsigned Size; // in bytes
+  int64_t Imm;
+
+  ImmInstructionBase(unsigned Size, int64_t Imm) : Size(Size), Imm(Imm) {}
+};
+
+/// ImmInstruction<T> - rmmediate-based instructions.  Instructions are
+/// specified via template argument.
+template<ValueGenInst::InstType Type>
+class ImmInstruction : public ImmInstructionBase {
+public:
+  ImmInstruction(unsigned Size, int64_t Imm) : ImmInstructionBase(Size, Imm) {}
+
+  virtual InstType type() const { return Type; }
+
+  virtual bool operator==(const ValueGenInst &RHS) const {
+    if(RHS.type() == Type && RHS.opType() == Immediate) {
+      const ImmInstruction<Type> &II = (const ImmInstruction<Type> &)RHS;
+      if(II.Imm == Imm && II.Size == Size) return true;
+    }
+    return false;
+  }
+
+  virtual std::string str() const
+  { return getInstNameStr(Type) + " immediate " + std::to_string(Imm); }
+};
+
+/// Wrap raw pointers to ValueGenInst in smart pointers.  Use shared_ptr so we
+/// can use copy constructors for containers of these instructions.
+typedef std::shared_ptr<ValueGenInst> ValueGenInstPtr;
+
+/// A list of instructions used to generate a value
+typedef std::vector<ValueGenInstPtr> ValueGenInstList;
+
+#undef INV_INST_TYPE
+
+//===----------------------------------------------------------------------===//
+// Machine-specific live values
+//
+// These are the live values used to populate an architecture-specific location,
+// e.g., a reference to a global symbol or an immediate value
+
+/// MachineLiveVal - A machine-specific live value
+class MachineLiveVal {
+public:
+  /// Constructors & destructors.
+  // Note: create child class objects rather than objects of this class.
+  virtual ~MachineLiveVal() {}
+  virtual MachineLiveVal *copy() const = 0;
+
+  /// Possible live value types
+  enum Type { SymbolRef, ConstPoolRef, StackObject, Immediate, Generated };
+
+  /// Determine the live value's type
+  virtual enum Type getType() const = 0;
+  virtual bool isReference() const { return false; }
+  virtual bool isSymbolRef() const { return false; }
+  virtual bool isConstPoolRef() const { return false; }
+  virtual bool isStackObject() const { return false; }
+  virtual bool isImm() const { return false; }
+  virtual bool isGenerated() const { return false; }
+
+  /// Equivalence checking
+  virtual bool operator==(const MachineLiveVal &RHS) const = 0;
+
+  /// Generate a human-readable string describing the value
+  virtual std::string toString() const = 0;
+
+  /// Get the machine instruction which defines the live value
+  const MachineInstr *getDefiningInst() const { return DefMI; }
+
+  /// Return whether this value is a pointer
+  // Note: if the value *could* be a pointer, this should be set so the runtime
+  // can do pointer-to-stack checks
+  bool isPtr() const { return Ptr; }
+
+protected:
+  /// Defining instruction for live value
+  const MachineInstr *DefMI;
+
+  /// Is this a pointer?
+  // Note: if the value *could* be a pointer, this should be set so the runtime
+  // can do pointer-to-stack checks
+  bool Ptr;
+
+  MachineLiveVal(const MachineInstr *DefMI, bool Ptr)
+    : DefMI(DefMI), Ptr(Ptr) {}
+  MachineLiveVal(const MachineLiveVal &C) : DefMI(C.DefMI), Ptr(C.Ptr) {}
+};
+
+/// MachineReference - a reference to some binary object outside of thread
+/// local storage
+class MachineReference : public MachineLiveVal {
+public:
+  virtual bool isReference() const { return true; }
+
+  /// Get a symbol reference for label generation
+  virtual MCSymbol *getReference(AsmPrinter &AP) const = 0;
+
+protected:
+  MachineReference(const MachineInstr *DefMI, bool Ptr)
+    : MachineLiveVal(DefMI, Ptr) {}
+  MachineReference(const MachineReference &C) : MachineLiveVal(C) {}
+};
+
+/// MachineSymbolRef - a reference to a global symbol
+class MachineSymbolRef : public MachineReference {
+public:
+  MachineSymbolRef(const MachineOperand &Symbol,
+                   const MachineInstr *DefMI,
+                   bool Ptr)
+    : MachineReference(DefMI, Ptr), Symbol(Symbol) {}
+  MachineSymbolRef(const MachineSymbolRef &C)
+    : MachineReference(C), Symbol(C.Symbol) {}
+  virtual MachineLiveVal *copy() const { return new MachineSymbolRef(*this); }
+
+  virtual enum Type getType() const { return SymbolRef; }
+  virtual bool isSymbolRef() const { return true; }
+
+  virtual bool operator==(const MachineLiveVal &RHS) const;
+  virtual std::string toString() const;
+  virtual MCSymbol *getReference(AsmPrinter &AP) const;
+
+private:
+  // MCSymbols may not exist yet, so instead store the operand to look up the
+  // MCSymbol at metadata emission time.
+  // Note: store hard-copy (not reference) because optimizations may convert
+  // symbol reference to a different type, e.g., register
+  const MachineOperand Symbol;
+};
+
+/// MachineConstPoolRef - a reference to a constant pool entry
+class MachineConstPoolRef : public MachineReference {
+public:
+  MachineConstPoolRef(int Index, const MachineInstr *DefMI, bool Ptr = false)
+    : MachineReference(DefMI, Ptr), Index(Index) {}
+  MachineConstPoolRef(const MachineConstPoolRef &C)
+    : MachineReference(C), Index(C.Index) {}
+  virtual MachineLiveVal *copy() const
+  { return new MachineConstPoolRef(*this); }
+
+  virtual enum Type getType() const { return ConstPoolRef; }
+  virtual bool isConstPoolRef() const { return true; }
+
+  virtual bool operator==(const MachineLiveVal &RHS) const;
+
+  virtual std::string toString() const
+  { return "reference to constant pool index " + std::to_string(Index); }
+
+  virtual MCSymbol *getReference(AsmPrinter &AP) const;
+
+private:
+  int Index;
+};
+
+/// MachineStackObject - an object on the stack
+class MachineStackObject : public MachineLiveVal {
+public:
+  MachineStackObject(int Index,
+                     bool Load,
+                     const MachineInstr *DefMI,
+                     bool Ptr = false)
+    : MachineLiveVal(DefMI, Ptr), Index(Index), Load(Load) {}
+  MachineStackObject(const MachineStackObject &C)
+    : MachineLiveVal(C), Index(C.Index), Load(C.Load) {}
+  virtual MachineLiveVal *copy() const
+  { return new MachineStackObject(*this); }
+
+  /// Objects common across stack frames for all supported architectures
+  enum Common { None, ReturnAddr = INT32_MAX };
+
+  virtual enum Type getType() const { return StackObject; }
+  virtual bool isStackObject() const { return true; }
+  virtual enum Common getCommonObjectType() const { return None; }
+  virtual bool isCommonObject() const { return false; }
+
+  virtual bool operator==(const MachineLiveVal &RHS) const;
+
+  virtual std::string toString() const;
+
+  /// Return the object's offset from a base register (returned in BR)
+  virtual int getOffsetFromReg(AsmPrinter &AP, unsigned &BR) const;
+
+  int getIndex() const { return Index; }
+  void setIndex(int Index) { this->Index = Index; }
+  bool isLoad() const { return Load; }
+  void setLoad(bool Load) { this->Load = Load; }
+
+private:
+  /// The stack slot index of a stack object
+  int Index;
+
+  /// Are we generating a reference to a stack object or loading a value from
+  /// the stack slot?
+  bool Load;
+};
+
+/// ReturnAddress - the return address stored on the stack
+class ReturnAddress : public MachineStackObject {
+public:
+  ReturnAddress(const MachineInstr *DefMI)
+    : MachineStackObject(ReturnAddr, true, DefMI, false) {}
+  ReturnAddress(const ReturnAddress &C) : MachineStackObject(C) {}
+  virtual MachineLiveVal *copy() const { return new ReturnAddress(*this); }
+
+  virtual enum Common getCommonObjectType() const { return ReturnAddr; }
+  virtual bool isCommonObject() const { return true; }
+
+  virtual std::string toString() const
+  { return "function return address"; }
+
+  virtual int getOffsetFromReg(AsmPrinter &AP, unsigned &BR) const;
+};
+
+/// MachineImmediate - an immediate value
+class MachineImmediate : public MachineLiveVal {
+public:
+  MachineImmediate(unsigned Size,
+                   uint64_t Value,
+                   const MachineInstr *DefMI,
+                   bool Ptr = false);
+  MachineImmediate(const MachineImmediate &C)
+    : MachineLiveVal(C), Size(C.Size), Value(C.Value) {}
+  virtual MachineLiveVal *copy() const { return new MachineImmediate(*this); }
+
+  virtual enum Type getType() const { return Immediate; }
+  virtual bool isImm() const { return true; }
+
+  virtual bool operator==(const MachineLiveVal &RHS) const;
+
+  virtual std::string toString() const
+  { return "immediate value: " + std::to_string(Value); }
+
+  unsigned getSize() const { return Size; }
+  uint64_t getValue() const { return Value; }
+
+private:
+  unsigned Size; // in bytes
+  uint64_t Value;
+};
+
+/// MachineGeneratedVal - a value generated through a set of small operations
+class MachineGeneratedVal : public MachineLiveVal {
+public:
+  MachineGeneratedVal(const ValueGenInstList &VG,
+                      const MachineInstr *DefMI,
+                      bool Ptr)
+    : MachineLiveVal(DefMI, Ptr), VG(VG) {}
+  virtual MachineLiveVal *copy() const
+  { return new MachineGeneratedVal(*this); }
+
+  enum Type getType() const { return Generated; }
+  bool isGenerated() const { return true; }
+
+  virtual bool operator==(const MachineLiveVal &RHS) const;
+
+  virtual std::string toString() const
+  { return "generated value, " + std::to_string(VG.size()) + " instruction(s)"; }
+
+  const ValueGenInstList &getInstructions() const { return VG; }
+
+private:
+  ValueGenInstList VG;
+};
+
+// TODO add API to generate "Operation"
+
+//===----------------------------------------------------------------------===//
+// Machine-specific locations
+//
+// These are locations to be populated with the live values, e.g., a register or
+// stack slot.
+//
+// Note: these represent a live value's *destination*, not the live value
+// itself.  For example, don't confuse MachineStackObject above (a live value
+// to be copied from a stack slot) versus MachineLiveStackSlot below (the
+// location where a live value will be stored).
+
+/// MachineLiveLoc - an architecture-specific location for a live value
+class MachineLiveLoc {
+public:
+  /// Constructors & destructors.
+  // Note: create child class objects rather than objects of this class.
+  virtual ~MachineLiveLoc() {}
+  virtual MachineLiveLoc *copy() const = 0;
+  virtual bool operator==(const MachineLiveLoc &R) const = 0;
+
+  /// Determine the live value location type
+  virtual bool isReg() const { return false; }
+  virtual bool isStackAddr() const { return false; }
+  virtual bool isStackSlot() const { return false; }
+
+  virtual std::string toString() const = 0;
+};
+
+/// MachineLiveReg - a live value stored in a register.  Stores the register
+/// number as an architecture-specific physical register.
+class MachineLiveReg : public MachineLiveLoc {
+public:
+  MachineLiveReg(unsigned Reg) : Reg(Reg) {}
+  MachineLiveReg(const MachineLiveReg &C) : Reg(C.Reg) {}
+  virtual MachineLiveLoc *copy() const { return new MachineLiveReg(*this); }
+
+  virtual bool isReg() const { return true; }
+
+  virtual bool operator==(const MachineLiveLoc &RHS) const;
+
+  unsigned getReg() const { return Reg; }
+  void setReg(unsigned Reg) { this->Reg = Reg; }
+
+  virtual std::string toString() const
+  { return "live value in register " + std::to_string(Reg); }
+
+private:
+  unsigned Reg;
+};
+
+/// MachineLiveStackAddr - a live value stored at a known stack address.  Can
+/// be used for stack objects at hard-coded offsets, e.g., the TOC pointer save
+/// location for PowerPC/ELFv2 ABI.
+class MachineLiveStackAddr : public MachineLiveLoc {
+public:
+  MachineLiveStackAddr() : Offset(INT32_MAX), Reg(UINT32_MAX), Size(0) {}
+  MachineLiveStackAddr(int Offset, unsigned Reg, unsigned Size)
+    : Offset(Offset), Reg(Reg), Size(Size) {}
+  MachineLiveStackAddr(const MachineLiveStackAddr &C)
+    : Offset(C.Offset), Reg(C.Reg), Size(C.Size) {}
+  virtual MachineLiveLoc *copy() const
+  { return new MachineLiveStackAddr(*this); }
+
+  virtual bool isStackAddr() const { return true; }
+
+  virtual bool operator==(const MachineLiveLoc &RHS) const;
+
+  int getOffset() const { return Offset; }
+  void setOffset(int Offset) { this->Offset = Offset; }
+  unsigned getReg() const { return Reg; }
+  void setReg(unsigned Reg) { this->Reg = Reg; }
+  void setSize(unsigned Size) { this->Size = Size; }
+
+  // Calculate the final position of the stack object.  Return the object's
+  // location as an offset from a base pointer register.
+  virtual int calcAndGetRegOffset(const AsmPrinter &AP, unsigned &BP)
+  { BP = Reg; return Offset; }
+
+  // The size of a stack object may need to be determined by code emission
+  // metadata in child classes, hence the AsmPrinter argument
+  virtual unsigned getSize(const AsmPrinter &AP) { return Size; }
+
+  virtual std::string toString() const
+  {
+    return "live value at register " + std::to_string(Reg) +
+           " + " + std::to_string(Offset);
+  }
+
+protected:
+  // The object is referenced by an offset from a (physical) register's value.
+  int Offset;
+  unsigned Reg, Size;
+};
+
+/// MachineLiveStackSlot - a live value stored in a stack slot.  A more
+/// abstract version of MachineLiveStackAddr, where the value is in a virtual
+/// stack slot whose address won't be determined until instruction emission.
+class MachineLiveStackSlot : public MachineLiveStackAddr {
+public:
+  MachineLiveStackSlot(int Index) : Index(Index) {}
+  MachineLiveStackSlot(const MachineLiveStackSlot &C)
+    : MachineLiveStackAddr(C), Index(C.Index) {}
+  virtual MachineLiveLoc *copy() const
+  { return new MachineLiveStackSlot(*this); }
+
+  virtual bool isStackSlot() const { return true; }
+
+  virtual bool operator==(const MachineLiveLoc &RHS) const;
+
+  unsigned getStackSlot() const { return Index; }
+  void setStackSlot(int Index) { this->Index = Index; }
+  virtual int calcAndGetRegOffset(const AsmPrinter &AP, unsigned &BP);
+  virtual unsigned getSize(const AsmPrinter &AP);
+
+  virtual std::string toString() const
+  { return "live value in stack slot " + std::to_string(Index); }
+
+private:
+  int Index;
+};
+
+/// Useful typedefs for data structures needed to store additional stack
+/// transformation metadata not captured in the stackmap instructions.
+
+/// Tidy up objects defined above into smart pointers
+typedef std::unique_ptr<MachineLiveVal> MachineLiveValPtr;
+typedef std::unique_ptr<MachineLiveLoc> MachineLiveLocPtr;
+
+/// A vector of architecture-specific live value locations
+// Note: we could use a set instead (because we want unique live values), but
+// because we're using MachineLiveLoc pointers the set would only uniquify
+// based on the pointer, not the pointed-to value.
+typedef SmallVector<MachineLiveLocPtr, 4> MachineLiveLocs;
+
+/// Map IR value to a list of architecture-specific live value locations.
+/// Usually used to store duplicate locations for an IR value.
+typedef std::map<const Value *, MachineLiveLocs> IRToMachineLocs;
+typedef std::pair<const Value *, MachineLiveLocs> IRMachineLocPair;
+
+/// Map an IR instruction to the metadata about its IR operands (and their
+/// associated architecture-specific live values locations).
+typedef std::map<const Instruction *, IRToMachineLocs> InstToOperands;
+typedef std::pair<const Instruction *, IRToMachineLocs> InstOperandPair;
+
+/// A pair to couple an architecture-specific location to the value used to
+/// populate it, and a vector for storing several of them.
+typedef std::pair<MachineLiveLocPtr, MachineLiveValPtr> ArchLiveValue;
+typedef SmallVector<ArchLiveValue, 8> ArchLiveValues;
+
+// Map an IR instruction to architecture-specific live values
+typedef std::map<const Instruction *, ArchLiveValues> InstToArchLiveValues;
+typedef std::pair<const Instruction *, ArchLiveValues> InstArchLiveValuePair;
+
+}
+
+#endif
+
Index: include/llvm/CodeGen/UnwindInfo.h
===================================================================
--- include/llvm/CodeGen/UnwindInfo.h	(nonexistent)
+++ include/llvm/CodeGen/UnwindInfo.h	(working copy)
@@ -0,0 +1,112 @@
+//===----------------- UnwindInfo.h - UnwindInfo ---------------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Generate unwinding information for stack transformation runtime.  Note that
+// this is implemented assuming the function uses a frame base pointer (FBP).
+// This requirement is guaranteed to be satisfied if the function has a
+// stackmap, which are the only functions for which we want to generate
+// unwinding information.
+//
+//===---------------------------------------------------------------------===//
+
+#ifndef LLVM_CODEGEN_UNWINDINFO_H
+#define LLVM_CODEGEN_UNWINDINFO_H
+
+#include "llvm/CodeGen/AsmPrinter.h"
+#include "llvm/CodeGen/MachineFrameInfo.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/MC/MCContext.h"
+#include "llvm/MC/MCStreamer.h"
+#include "llvm/Support/Debug.h"
+#include <map>
+
+namespace llvm {
+
+class UnwindInfo {
+public:
+  /// Per-function unwinding metadata classes & typedefs
+  class FuncUnwindInfo {
+  public:
+    uint32_t SecOffset; // Offset into unwinding record section
+    uint32_t NumUnwindRecord; // Number of unwinding records
+
+    FuncUnwindInfo() : SecOffset(UINT32_MAX), NumUnwindRecord(0) {}
+    FuncUnwindInfo(uint32_t SecOffset, uint32_t NumUnwindRecord)
+      : SecOffset(SecOffset), NumUnwindRecord(NumUnwindRecord) {}
+  };
+
+  typedef std::pair<const MCSymbol *, FuncUnwindInfo> FuncUnwindPair;
+  typedef std::map<const MCSymbol *, FuncUnwindInfo> FuncUnwindMap;
+
+  /// Unwinding record classes & typedefs
+  class RegOffset {
+  public:
+    uint32_t DwarfReg;
+    int32_t Offset;
+
+    RegOffset() : DwarfReg(0), Offset(0) {}
+    RegOffset(uint32_t DwarfReg, int32_t Offset) :
+      DwarfReg(DwarfReg), Offset(Offset) {}
+  };
+
+  typedef SmallVector<RegOffset, 32> CalleeSavedRegisters;
+  typedef std::pair<const MCSymbol *, CalleeSavedRegisters> FuncCalleePair;
+  typedef std::map<const MCSymbol *, CalleeSavedRegisters> FuncCalleeMap;
+
+  /// \brief Constructors
+  UnwindInfo() = delete;
+  UnwindInfo(AsmPrinter &AP)
+    : AP(AP), OutContext(AP.OutStreamer->getContext()), Emitted(false) {};
+
+  /// \bried Clear all saved unwinding information
+  void reset() {
+    Emitted = false;
+    FuncCalleeSaved.clear();
+    FuncUnwindMetadata.clear();
+  }
+
+  /// \brief Store unwinding information for a function
+  void recordUnwindInfo(const MachineFunction &MF);
+
+  /// \brief Add a register restore offset for a function.  MachineReg will get
+  /// converted to a DWARF register internally.
+  void addRegisterUnwindInfo(const MachineFunction &MF,
+                             uint32_t MachineReg,
+                             int32_t Offset);
+
+  /// Create an unwinding information section and serialize the map info into
+  /// it.
+  ///
+  /// Note: unlike StackMaps::serializeToStackMapSection, this function *does
+  /// not* clear out the data structures.  This is so that the stack map
+  /// machinery can access per-function unwinding information.
+  void serializeToUnwindInfoSection();
+
+  /// Get unwinding section metadata for a function
+  const FuncUnwindInfo &getUnwindInfo(const MCSymbol *Func) const;
+
+private:
+  AsmPrinter &AP;
+  MCContext &OutContext;
+  FuncCalleeMap FuncCalleeSaved;
+  FuncUnwindMap FuncUnwindMetadata;
+  bool Emitted;
+
+  /// \brief Emit the unwind info for each function.
+  void emitUnwindInfo(MCStreamer &OS);
+
+  /// \brief Emit the address range info for each function.
+  void emitAddrRangeInfo(MCStreamer &OS);
+
+  void print(raw_ostream &OS);
+  void debug() { print(dbgs()); }
+};
+}
+
+#endif
Index: include/llvm/InitializePasses.h
===================================================================
--- include/llvm/InitializePasses.h	(revision 277823)
+++ include/llvm/InitializePasses.h	(working copy)
@@ -131,6 +131,7 @@
 void initializeScalarizerPass(PassRegistry&);
 void initializeEarlyCSELegacyPassPass(PassRegistry &);
 void initializeEliminateAvailableExternallyPass(PassRegistry&);
+void initializeMigrationPointsPass(PassRegistry&);
 void initializeExpandISelPseudosPass(PassRegistry&);
 void initializeFunctionAttrsPass(PassRegistry&);
 void initializeGCMachineCodeAnalysisPass(PassRegistry&);
@@ -146,6 +147,7 @@
 void initializeInductiveRangeCheckEliminationPass(PassRegistry&);
 void initializeIndVarSimplifyPass(PassRegistry&);
 void initializeInlineCostAnalysisPass(PassRegistry&);
+void initializeInsertStackMapsPass(PassRegistry&);
 void initializeInstructionCombiningPassPass(PassRegistry&);
 void initializeInstCountPass(PassRegistry&);
 void initializeInstNamerPass(PassRegistry&);
@@ -153,6 +155,7 @@
 void initializeIntervalPartitionPass(PassRegistry&);
 void initializeJumpThreadingPass(PassRegistry&);
 void initializeLCSSAPass(PassRegistry&);
+void initializeLibcStackMapsPass(PassRegistry&);
 void initializeLICMPass(PassRegistry&);
 void initializeLazyValueInfoPass(PassRegistry&);
 void initializeLibCallAliasAnalysisPass(PassRegistry&);
@@ -162,6 +165,7 @@
 void initializeLiveRegMatrixPass(PassRegistry&);
 void initializeLiveStacksPass(PassRegistry&);
 void initializeLiveVariablesPass(PassRegistry&);
+void initializeLiveValuesPass(PassRegistry&);
 void initializeLoaderPassPass(PassRegistry&);
 void initializeLocalStackSlotPassPass(PassRegistry&);
 void initializeLoopDeletionPass(PassRegistry&);
@@ -208,6 +212,7 @@
 void initializeMetaRenamerPass(PassRegistry&);
 void initializeMergeFunctionsPass(PassRegistry&);
 void initializeModuleDebugInfoPrinterPass(PassRegistry&);
+void initializeNameStringLiteralsPass(PassRegistry&);
 void initializeNaryReassociatePass(PassRegistry&);
 void initializeNoAAPass(PassRegistry&);
 void initializeObjCARCAliasAnalysisPass(PassRegistry&);
@@ -252,7 +257,8 @@
 void initializeScalarEvolutionPass(PassRegistry&);
 void initializeShrinkWrapPass(PassRegistry &);
 void initializeSimpleInlinerPass(PassRegistry&);
-void initializeShadowStackGCLoweringPass(PassRegistry&);  
+void initializeShadowStackGCLoweringPass(PassRegistry&);
+void initializeStaticVarSectionsPass(PassRegistry&);
 void initializeRegisterCoalescerPass(PassRegistry&);
 void initializeSingleLoopExtractorPass(PassRegistry&);
 void initializeSinkingPass(PassRegistry&);
@@ -263,6 +269,7 @@
 void initializeStackProtectorPass(PassRegistry&);
 void initializeStackColoringPass(PassRegistry&);
 void initializeStackSlotColoringPass(PassRegistry&);
+void initializeStackTransformMetadataPass(PassRegistry&);
 void initializeStraightLineStrengthReducePass(PassRegistry &);
 void initializeStripDeadDebugInfoPass(PassRegistry&);
 void initializeStripDeadPrototypesPassPass(PassRegistry&);
Index: include/llvm/LinkAllPasses.h
===================================================================
--- include/llvm/LinkAllPasses.h	(revision 277823)
+++ include/llvm/LinkAllPasses.h	(working copy)
@@ -33,6 +33,7 @@
 #include "llvm/Transforms/Instrumentation.h"
 #include "llvm/Transforms/ObjCARC.h"
 #include "llvm/Transforms/Scalar.h"
+#include "llvm/Transforms/Utils.h"
 #include "llvm/Transforms/Utils/SymbolRewriter.h"
 #include "llvm/Transforms/Utils/UnifyFunctionExitNodes.h"
 #include "llvm/Transforms/Vectorize.h"
@@ -81,6 +82,7 @@
       (void) llvm::createDomPrinterPass();
       (void) llvm::createDomOnlyViewerPass();
       (void) llvm::createDomViewerPass();
+      (void) llvm::createMigrationPointsPass();
       (void) llvm::createGCOVProfilerPass();
       (void) llvm::createInstrProfilingPass();
       (void) llvm::createFunctionInliningPass();
@@ -92,11 +94,14 @@
       (void) llvm::createIPSCCPPass();
       (void) llvm::createInductiveRangeCheckEliminationPass();
       (void) llvm::createIndVarSimplifyPass();
+      (void) llvm::createInsertStackMapsPass();
       (void) llvm::createInstructionCombiningPass();
       (void) llvm::createInternalizePass();
       (void) llvm::createLCSSAPass();
+      (void) llvm::createLibcStackMapsPass();
       (void) llvm::createLICMPass();
       (void) llvm::createLazyValueInfoPass();
+      (void) llvm::createLiveValuesPass();
       (void) llvm::createLoopExtractorPass();
       (void) llvm::createLoopInterchangePass();
       (void) llvm::createLoopSimplifyPass();
@@ -110,6 +115,7 @@
       (void) llvm::createLowerInvokePass();
       (void) llvm::createLowerSwitchPass();
       (void) llvm::createNaryReassociatePass();
+      (void) llvm::createNameStringLiteralsPass();
       (void) llvm::createNoAAPass();
       (void) llvm::createObjCARCAliasAnalysisPass();
       (void) llvm::createObjCARCAPElimPass();
@@ -134,6 +140,7 @@
       (void) llvm::createSafeStackPass();
       (void) llvm::createScalarReplAggregatesPass();
       (void) llvm::createSingleLoopExtractorPass();
+      (void) llvm::createStaticVarSectionsPass();
       (void) llvm::createStripSymbolsPass();
       (void) llvm::createStripNonDebugSymbolsPass();
       (void) llvm::createStripDeadDebugInfoPass();
Index: include/llvm/MC/MCCodeGenInfo.h
===================================================================
--- include/llvm/MC/MCCodeGenInfo.h	(revision 277823)
+++ include/llvm/MC/MCCodeGenInfo.h	(working copy)
@@ -32,6 +32,11 @@
   ///
   CodeGenOpt::Level OptLevel;
 
+  /// ArchIROptLevel - Optimization level (architecture-specific IR
+  /// optimizations only).  Defaults to OptLevel.
+  ///
+  CodeGenOpt::Level ArchIROptLevel;
+
 public:
   void initMCCodeGenInfo(Reloc::Model RM = Reloc::Default,
                          CodeModel::Model CM = CodeModel::Default,
@@ -43,8 +48,12 @@
 
   CodeGenOpt::Level getOptLevel() const { return OptLevel; }
 
+  CodeGenOpt::Level getArchIROptLevel() const { return ArchIROptLevel; }
+
   // Allow overriding OptLevel on a per-function basis.
   void setOptLevel(CodeGenOpt::Level Level) { OptLevel = Level; }
+
+  void setArchIROptLevel(CodeGenOpt::Level Level) { ArchIROptLevel = Level; }
 };
 } // namespace llvm
 
Index: include/llvm/MC/MCObjectFileInfo.h
===================================================================
--- include/llvm/MC/MCObjectFileInfo.h	(revision 277823)
+++ include/llvm/MC/MCObjectFileInfo.h	(working copy)
@@ -135,6 +135,10 @@
   /// Null if this target doesn't support a BSS section. ELF and MachO only.
   MCSection *TLSBSSSection; // Defaults to ".tbss".
 
+  /// Unwinding address ranges & register location sections.
+  MCSection *UnwindAddrRangeSection;
+  MCSection *UnwindInfoSection;
+
   /// StackMap section.
   MCSection *StackMapSection;
 
@@ -267,6 +271,8 @@
   const MCSection *getTLSDataSection() const { return TLSDataSection; }
   MCSection *getTLSBSSSection() const { return TLSBSSSection; }
 
+  MCSection *getUnwindInfoSection() const { return UnwindInfoSection; }
+  MCSection *getUnwindAddrRangeSection() const { return UnwindAddrRangeSection; }
   MCSection *getStackMapSection() const { return StackMapSection; }
   MCSection *getFaultMapSection() const { return FaultMapSection; }
 
Index: include/llvm/Target/TargetFrameLowering.h
===================================================================
--- include/llvm/Target/TargetFrameLowering.h	(revision 277823)
+++ include/llvm/Target/TargetFrameLowering.h	(working copy)
@@ -227,6 +227,14 @@
     return 0;
   }
 
+  /// Same as above, except that the 'frame register' will always be the ISA's
+  /// frame pointer (which can be different from the variable 'frame register'
+  /// which may be the stack pointer, frame pointer, etc.)
+  virtual int getFrameIndexReferenceFromFP(const MachineFunction &MF, int FI,
+                                           unsigned &FrameReg) const {
+    return getFrameIndexReference(MF, FI, FrameReg);
+  }
+
   /// This method determines which of the registers reported by
   /// TargetRegisterInfo::getCalleeSavedRegs() should actually get saved.
   /// The default implementation checks populates the \p SavedRegs bitset with
Index: include/llvm/Target/TargetMachine.h
===================================================================
--- include/llvm/Target/TargetMachine.h	(revision 277823)
+++ include/llvm/Target/TargetMachine.h	(working copy)
@@ -171,6 +171,13 @@
   /// \brief Overrides the optimization level.
   void setOptLevel(CodeGenOpt::Level Level) const;
 
+  /// Returns the architecture-specific IR optimization level: None, Less,
+  /// Default or Aggressive.
+  CodeGenOpt::Level getArchIROptLevel() const;
+
+  /// \brief Overrides the architecture-specific IR optimization level.
+  void setArchIROptLevel(CodeGenOpt::Level Level) const;
+
   void setFastISel(bool Enable) { Options.EnableFastISel = Enable; }
 
   bool shouldPrintMachineCode() const { return Options.PrintMachineCode; }
Index: include/llvm/Target/TargetRegisterInfo.h
===================================================================
--- include/llvm/Target/TargetRegisterInfo.h	(revision 277823)
+++ include/llvm/Target/TargetRegisterInfo.h	(working copy)
@@ -869,6 +869,17 @@
   /// getFrameRegister - This method should return the register used as a base
   /// for values allocated in the current stack frame.
   virtual unsigned getFrameRegister(const MachineFunction &MF) const = 0;
+
+  /// getReturnAddrLoc - This method should return the location of the saved
+  /// return address on the stack, expressed as a base register (returned via
+  /// BaseReg) and an offset
+  virtual int getReturnAddrLoc(const MachineFunction &MF,
+                               unsigned &BaseReg) const
+  {
+    llvm_unreachable("Not implemented for target!");
+    BaseReg = 0;
+    return INT32_MAX;
+  }
 };
 
 
Index: include/llvm/Target/TargetSubtargetInfo.h
===================================================================
--- include/llvm/Target/TargetSubtargetInfo.h	(revision 277823)
+++ include/llvm/Target/TargetSubtargetInfo.h	(working copy)
@@ -32,6 +32,7 @@
 class TargetRegisterInfo;
 class TargetSchedModel;
 class TargetSelectionDAGInfo;
+class TargetValues;
 struct MachineSchedPolicy;
 template <typename T> class SmallVectorImpl;
 
@@ -95,6 +96,13 @@
     return nullptr;
   }
 
+  /// getValues - Returns the value generator object for the target or specific
+  /// subtarget
+  ///
+  virtual const TargetValues *getValues() const {
+    return nullptr;
+  };
+
   /// Resolve a SchedClass at runtime, where SchedClass identifies an
   /// MCSchedClassDesc with the isVariant property. This may return the ID of
   /// another variant SchedClass, but repeated invocation must quickly terminate
Index: include/llvm/Target/TargetValues.h
===================================================================
--- include/llvm/Target/TargetValues.h	(nonexistent)
+++ include/llvm/Target/TargetValues.h	(working copy)
@@ -0,0 +1,53 @@
+//===----- llvm/Target/TargetValues.h - Value Properties ----*- C++ -----*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file provides an API for detecting properties of architecture-specific
+// values & for generating a series of simple metadata instructions for
+// reconstituting a value.  This is used by the stack transformation runtime to
+// set up architecture-specific live values.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_TARGET_TARGETVAL_H
+#define LLVM_TARGET_TARGETVAL_H
+
+#include "llvm/CodeGen/StackTransformTypes.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/IR/Instructions.h"
+
+namespace llvm {
+
+//===----------------------------------------------------------------------===//
+// Superclass for ISA-specific values
+//
+
+class TargetValues {
+public:
+  TargetValues(const TargetValues &) = delete;
+  void operator=(const TargetValues &) = delete;
+  virtual ~TargetValues() {};
+
+  /// Return a machine-specific value generated by a machine instruction.
+  virtual MachineLiveValPtr getMachineValue(const MachineInstr *MI) const = 0;
+
+  /// Add any required architecture-specific live values, e.g., the TOC pointer
+  /// on PowerPC.
+  virtual void addRequiredArchLiveValues(MachineFunction *MF,
+                                         const MachineInstr *MIStackMap,
+                                         const CallInst *IRStackMap) const
+  { return; }
+
+protected:
+  TargetValues() {}
+};
+
+} // End llvm namespace
+
+#endif
+
Index: include/llvm/Transforms/Instrumentation.h
===================================================================
--- include/llvm/Transforms/Instrumentation.h	(revision 277823)
+++ include/llvm/Transforms/Instrumentation.h	(working copy)
@@ -136,6 +136,17 @@
 /// protect against stack-based overflow vulnerabilities.
 FunctionPass *createSafeStackPass();
 
+/// \brief This pass inserts equivalence points into functions.
+FunctionPass *createMigrationPointsPass();
+
+/// \brief This pass inserts stack map intrinsics at equivalence points in
+/// order to record live value locations
+ModulePass *createInsertStackMapsPass();
+
+/// \brief This pass inserts stack map intrinsics similarly to InsertStackMaps,
+/// but only in thread start functions inside of libc
+ModulePass *createLibcStackMapsPass();
+
 } // End llvm namespace
 
 #endif
Index: include/llvm/Transforms/Utils.h
===================================================================
--- include/llvm/Transforms/Utils.h	(nonexistent)
+++ include/llvm/Transforms/Utils.h	(working copy)
@@ -0,0 +1,16 @@
+namespace llvm {
+
+//===----------------------------------------------------------------------===//
+//
+// NameStringLiterals - Give symbol names to anonymous string literals so they
+// can be aligned at link-time
+//
+ModulePass *createNameStringLiteralsPass();
+
+//===----------------------------------------------------------------------===//
+//
+// StaticVarSections - Put static global variables into their own sections
+//
+ModulePass *createStaticVarSectionsPass();
+
+}
Index: lib/Analysis/Analysis.cpp
===================================================================
--- lib/Analysis/Analysis.cpp	(revision 277823)
+++ lib/Analysis/Analysis.cpp	(working copy)
@@ -53,6 +53,7 @@
   initializeLazyValueInfoPass(Registry);
   initializeLibCallAliasAnalysisPass(Registry);
   initializeLintPass(Registry);
+  initializeLiveValuesPass(Registry);
   initializeLoopInfoWrapperPassPass(Registry);
   initializeMemDepPrinterPass(Registry);
   initializeMemDerefPrinterPass(Registry);
Index: lib/Analysis/CMakeLists.txt
===================================================================
--- lib/Analysis/CMakeLists.txt	(revision 277823)
+++ lib/Analysis/CMakeLists.txt	(working copy)
@@ -34,9 +34,11 @@
   LibCallAliasAnalysis.cpp
   LibCallSemantics.cpp
   Lint.cpp
+  LiveValues.cpp
   Loads.cpp
   LoopAccessAnalysis.cpp
   LoopInfo.cpp
+  LoopNestingTree.cpp
   LoopPass.cpp
   MemDepPrinter.cpp
   MemDerefPrinter.cpp
Index: lib/Analysis/LiveValues.cpp
===================================================================
--- lib/Analysis/LiveValues.cpp	(nonexistent)
+++ lib/Analysis/LiveValues.cpp	(working copy)
@@ -0,0 +1,374 @@
+#include "llvm/Analysis/LiveValues.h"
+#include "llvm/IR/Metadata.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/CFG.h"
+#include "llvm/Analysis/CFG.h"
+#include "llvm/Analysis/LoopInfo.h"
+#include "llvm/ADT/PostOrderIterator.h"
+#include "llvm/ADT/SCCIterator.h"
+#include "llvm/Support/Debug.h"
+
+#define DEBUG_TYPE "live-values"
+
+using namespace llvm;
+
+char LiveValues::ID = 0;
+INITIALIZE_PASS_BEGIN(LiveValues, "live-values", 
+                    "Live-value set calculation", true, true)
+INITIALIZE_PASS_DEPENDENCY(LoopInfoWrapperPass)
+INITIALIZE_PASS_END(LiveValues, "live-values", 
+                    "Live-value set calculation", true, true)
+
+namespace llvm {
+  FunctionPass *createLiveValuesPass() { return new LiveValues(); }
+}
+
+///////////////////////////////////////////////////////////////////////////////
+// Public API
+///////////////////////////////////////////////////////////////////////////////
+
+LiveValues::LiveValues(void)
+  : FunctionPass(ID), inlineasm(false), bitcasts(false), comparisons(true),
+    constants(false), metadata(false) {}
+
+void LiveValues::getAnalysisUsage(AnalysisUsage &AU) const
+{
+  AU.addRequired<LoopInfoWrapperPass>();
+  AU.setPreservesAll();
+}
+
+bool LiveValues::runOnFunction(Function &F)
+{
+  if(FuncBBLiveIn.count(&F))
+  {
+    DEBUG(
+      errs() << "\nFound previous analysis for " << F.getName() << "\n\n";
+      print(errs(), &F);
+    );
+  }
+  else
+  {
+    DEBUG(errs() << "\n********** Beginning LiveValues **********\n"
+                 << "********** Function: " << F.getName() << " **********\n\n"
+                    "LiveValues: performing bottom-up dataflow analysis\n");
+
+    LoopNestingForest LNF;
+    FuncBBLiveIn.emplace(&F, LiveVals());
+    FuncBBLiveOut.emplace(&F, LiveVals());
+
+    /* 1. Compute partial liveness sets using a postorder traversal. */
+    dagDFS(F, FuncBBLiveIn[&F], FuncBBLiveOut[&F]);
+
+    DEBUG(errs() << "LiveValues: constructing loop-nesting forest\n");
+
+    /* 2. Construct loop-nesting forest. */
+    constructLoopNestingForest(F, LNF);
+
+    DEBUG(errs() << "LiveValues: propagating values within loop-nests\n");
+
+    /* 3. Propagate live variables within loop bodies. */
+    loopTreeDFS(LNF, FuncBBLiveIn[&F], FuncBBLiveOut[&F]);
+
+    DEBUG(
+      print(errs(), &F);
+      errs() << "LiveValues: finished analysis\n"
+    );
+  }
+
+  return false;
+}
+
+void
+LiveValues::print(raw_ostream &O, const Function *F) const
+{
+  LiveVals::const_iterator bbIt;
+  std::set<const Value *>::const_iterator valIt;
+  const Module *M = F->getParent();
+
+  O << "LiveValues: results of live-value analysis\n";
+
+  if(!FuncBBLiveIn.count(F) || !FuncBBLiveOut.count(F))
+  {
+    if(F->hasName())
+      O << "No liveness information for function " << F->getName() << "\n";
+    else
+      O << "No liveness information for requested function\n";
+  }
+  else
+  {
+    for(bbIt = FuncBBLiveIn.at(F).cbegin();
+        bbIt != FuncBBLiveIn.at(F).cend();
+        bbIt++)
+    {
+      const BasicBlock *bb = bbIt->first;
+      const std::set<const Value *> &liveInVals = bbIt->second;
+      const std::set<const Value *> &liveOutVals = FuncBBLiveOut.at(F).at(bb);
+
+      bb->printAsOperand(O, false, M);
+      O << "\n  Live-in:\n    ";
+      for(valIt = liveInVals.cbegin(); valIt != liveInVals.cend(); valIt++)
+      {
+        (*valIt)->printAsOperand(O, false, M);
+        O << " ";
+      }
+
+      O << "\n  Live-out:\n    ";
+      for(valIt = liveOutVals.cbegin(); valIt != liveOutVals.cend(); valIt++)
+      {
+        (*valIt)->printAsOperand(O, false, M);
+        O << " ";
+      }
+
+      O << "\n";
+    }
+  }
+}
+
+std::set<const Value *> *LiveValues::getLiveIn(const BasicBlock *BB) const
+{
+  const Function *F = BB->getParent();
+  return new std::set<const Value *>(FuncBBLiveIn.at(F).at(BB));
+}
+
+std::set<const Value *> *LiveValues::getLiveOut(const BasicBlock *BB) const
+{
+  const Function *F = BB->getParent();
+  return new std::set<const Value *>(FuncBBLiveOut.at(F).at(BB));
+}
+
+std::set<const Value *>
+*LiveValues::getLiveValues(const Instruction *inst) const
+{
+  const BasicBlock *BB = inst->getParent();
+  const Function *F = BB->getParent();
+  BasicBlock::const_reverse_iterator ri, rie;
+  std::set<const Value *> *live =
+    new std::set<const Value *>(FuncBBLiveOut.at(F).at(BB));
+
+  for(ri = BB->rbegin(), rie = BB->rend(); ri != rie; ri++)
+  {
+    if(&*ri == inst) break;
+
+    live->erase(&*ri);
+    for(User::const_op_iterator op = ri->op_begin();
+        op != ri->op_end();
+        op++)
+      if(includeVal(*op))
+        live->insert(*op);
+  }
+  live->erase(&*ri);
+
+  return live;
+}
+
+///////////////////////////////////////////////////////////////////////////////
+// Private API
+///////////////////////////////////////////////////////////////////////////////
+
+bool LiveValues::includeVal(const llvm::Value *val) const
+{
+  bool include = true;
+
+  // TODO other values that should be filtered out?
+  if(isa<BasicBlock>(val))
+    include = false;
+  else if(isa<InlineAsm>(val) && !inlineasm)
+    include = false;
+  else if(isa<BitCastInst>(val) && !bitcasts)
+    include = false;
+  else if(isa<CmpInst>(val) && !comparisons)
+    include = false;
+  else if(isa<Constant>(val) && !constants)
+    include = false;
+  else if(isa<MetadataAsValue>(val) && !metadata)
+    include = false;
+
+  return include;
+}
+
+unsigned LiveValues::phiUses(const BasicBlock *B,
+                             const BasicBlock *S,
+                             std::set<const Value *> &uses)
+{
+  const PHINode *phi;
+  unsigned added = 0;
+
+  for(BasicBlock::const_iterator it = S->begin(); it != S->end(); it++)
+  {
+    if((phi = dyn_cast<PHINode>(&*it))) {
+      for(unsigned i = 0; i < phi->getNumIncomingValues(); i++)
+        if(phi->getIncomingBlock(i) == B &&
+           includeVal(phi->getIncomingValue(i)))
+          if(uses.insert(phi->getIncomingValue(i)).second)
+            added++;
+    }
+    else break; // phi-nodes are always at the start of the basic block
+  }
+
+  return added;
+}
+
+unsigned LiveValues::phiDefs(const BasicBlock *B,
+                             std::set<const Value *> &uses)
+{
+  const PHINode *phi;
+  unsigned added = 0;
+
+  for(BasicBlock::const_iterator it = B->begin(); it != B->end(); it++)
+  {
+    if((phi = dyn_cast<PHINode>(&*it))) {
+      if(includeVal(phi))
+        if(uses.insert(&*it).second)
+          added++;
+    }
+    else break; // phi-nodes are always at the start of the basic block
+  }
+
+  return added;
+}
+
+void LiveValues::dagDFS(Function &F, LiveVals &liveIn, LiveVals &liveOut)
+{
+  std::set<const Value *> live, phiDefined;
+  std::set<Edge> loopEdges;
+  SmallVector<Edge, 16> loopEdgeVec;
+
+  /* Find loop edges & convert to set for existence checking. */
+  FindFunctionBackedges(F, loopEdgeVec);
+  for(SmallVectorImpl<Edge>::const_iterator eit = loopEdgeVec.begin();
+      eit != loopEdgeVec.end();
+      eit++)
+    loopEdges.insert(*eit);
+
+  /* Calculate partial liveness sets for CFG nodes. */
+  for(auto B = po_iterator<const BasicBlock *>::begin(&F.getEntryBlock());
+      B != po_iterator<const BasicBlock *>::end(&F.getEntryBlock());
+      B++)
+  {
+    /* Calculate live-out set (lines 4-7 of Algorithm 2). */
+    for(succ_const_iterator S = succ_begin(*B); S != succ_end(*B); S++)
+    {
+      // Note: skip self-loop-edges, as adding values from phi-uses of this
+      // block causes use-def violations, and LLVM will complain.  This
+      // shouldn't matter, as phi-defs will cover this case.
+      if(*S == *B) continue;
+
+      phiUses(*B, *S, live);
+      if(!loopEdges.count(Edge(*B, *S)))
+      {
+        phiDefs(*S, phiDefined);
+        for(std::set<const Value *>::const_iterator vi = liveIn[*S].begin();
+            vi != liveIn[*S].end();
+            vi++)
+          if(!phiDefined.count(*vi) && includeVal(*vi)) live.insert(*vi);
+        phiDefined.clear();
+      }
+    }
+    liveOut.insert(LiveValsPair(*B, std::set<const Value *>(live)));
+
+    /* Calculate live-in set (lines 8-11 of Algorithm 2). */
+    for(BasicBlock::const_reverse_iterator inst = (*B)->rbegin();
+        inst != (*B)->rend();
+        inst++)
+    {
+      if(isa<PHINode>(&*inst)) break;
+
+      live.erase(&*inst);
+      for(User::const_op_iterator op = inst->op_begin();
+          op != inst->op_end();
+          op++)
+        if(includeVal(*op)) live.insert(*op);
+    }
+    phiDefs(*B, live);
+    liveIn.insert(LiveValsPair(*B, std::set<const Value *>(live)));
+
+    live.clear();
+
+    DEBUG(
+      errs() << "  ";
+      (*B)->printAsOperand(errs(), false);
+      errs() << ":\n";
+      errs() << "    Live-in:\n      ";
+      std::set<const Value *>::const_iterator it;
+      for(it = liveIn[*B].begin(); it != liveIn[*B].end(); it++)
+      {
+        (*it)->printAsOperand(errs(), false);
+        errs() << " ";
+      }
+      errs() << "\n    Live-out:\n      ";
+      for(it = liveOut[*B].begin(); it != liveOut[*B].end(); it++)
+      {
+        (*it)->printAsOperand(errs(), false);
+        errs() << " ";
+      }
+      errs() << "\n";
+    );
+  }
+}
+
+void LiveValues::constructLoopNestingForest(Function &F,
+                                            LoopNestingForest &LNF)
+{
+  LoopInfo &LI = getAnalysis<LoopInfoWrapperPass>().getLoopInfo();
+
+  for(scc_iterator<Function *> scc = scc_begin(&F);
+      scc != scc_end(&F);
+      ++scc)
+  {
+    const std::vector<BasicBlock *> &SCC = *scc;
+    LNF.emplace_back(SCC, LI);
+
+    DEBUG(
+      errs() << "Loop nesting tree: "
+             << LNF.back().size() << " node(s), loop-nesting depth: "
+             << LNF.back().depth() << "\n";
+      LNF.back().print(errs());
+      errs() << "\n"
+    );
+  }
+}
+
+void LiveValues::propagateValues(const LoopNestingTree &loopNest,
+                                 LiveVals &liveIn,
+                                 LiveVals &liveOut)
+{
+  std::set<const Value *> liveLoop, phiDefined;
+
+  /* Iterate over all loop nodes. */
+  for(LoopNestingTree::loop_iterator loop = loopNest.loop_begin();
+      loop != loopNest.loop_end();
+      loop++)
+  {
+    /* Calculate LiveLoop (lines 3 & 4 of Algorithm 3). */
+    phiDefs(*loop, phiDefined);
+    for(std::set<const Value *>::const_iterator it = liveIn[*loop].begin();
+        it != liveIn[*loop].end();
+        it++)
+      if(!phiDefined.count(*it) && includeVal(*it))
+        liveLoop.insert(*it);
+
+    /* Propagate values to children (lines 5-8 of Algorithm 3). */
+    for(LoopNestingTree::child_iterator child = loopNest.children_begin(loop);
+        child != loopNest.children_end(loop);
+        child++) {
+      for(std::set<const Value *>::const_iterator it = liveLoop.begin();
+          it != liveLoop.end();
+          it++) {
+        liveIn[*child].insert(*it);
+        liveOut[*child].insert(*it);
+      }
+    }
+
+    liveLoop.clear();
+  }
+}
+
+void LiveValues::loopTreeDFS(LoopNestingForest &LNF,
+                             LiveVals &liveIn,
+                             LiveVals &liveOut)
+{
+  LoopNestingForest::const_iterator it;
+  for(it = LNF.begin(); it != LNF.end(); it++)
+    propagateValues(*it, liveIn, liveOut);
+}
+
Index: lib/Analysis/LoopNestingTree.cpp
===================================================================
--- lib/Analysis/LoopNestingTree.cpp	(nonexistent)
+++ lib/Analysis/LoopNestingTree.cpp	(working copy)
@@ -0,0 +1,148 @@
+#include "llvm/Analysis/LoopNestingTree.h"
+#include "llvm/Support/raw_ostream.h"
+
+using namespace llvm;
+
+///////////////////////////////////////////////////////////////////////////////
+// Public API
+///////////////////////////////////////////////////////////////////////////////
+
+LoopNestingTree::LoopNestingTree(const std::vector<BasicBlock *> &SCC,
+                                 const LoopInfo &LI)
+  : _size(1), _depth(1), _root(nullptr)
+{
+  unsigned depth = 1, nodeDepth;
+  const Loop *loop = nullptr;
+  Node *loopHeader = nullptr, *newHeader = nullptr;
+  std::list<Node *> work;
+
+  /* Bootstrap by grabbing the loop of the first basic block encountered. */
+  loop = LI[SCC.front()];
+  if(!loop) // Is the SCC actually a loop?
+  {
+    _root = new Node(SCC.front(), nullptr, true);
+    return;
+  }
+
+  /* Get header of outermost loop, the tree's root. */
+  while(loop->getLoopDepth() > 1)
+    loop = loop->getParentLoop();
+  _root = new Node(loop->getHeader(), nullptr, true);
+  work.push_back(_root);
+
+  /* Parse the loop-headers of the SCC into the tree. */
+  while(work.size())
+  {
+    loopHeader = work.front();
+    work.pop_front();
+    loop = LI[loopHeader->bb];
+    depth = LI.getLoopDepth(loopHeader->bb);
+    _depth = (depth > _depth ? depth : _depth);
+
+    /* Add children of the loop header. */
+    for(auto bbi = loop->block_begin()++; bbi != loop->block_end(); bbi++)
+    {
+      nodeDepth = LI.getLoopDepth(*bbi);
+      if(nodeDepth == depth) // Regular child node
+      {
+        loopHeader->addChild(new Node(*bbi, loopHeader, false));
+        _size++;
+      }
+      else if(nodeDepth == (depth + 1) && // Header of nested loop
+              LI.isLoopHeader(*bbi))
+      {
+        newHeader = new Node(*bbi, loopHeader, true);
+        loopHeader->addChild(newHeader);
+        work.push_back(newHeader);
+        _size++;
+      }
+    }
+  }
+}
+
+LoopNestingTree::loop_iterator LoopNestingTree::loop_iterator::operator++()
+{
+  loop_iterator me = *this;
+  if(remaining.size())
+  {
+    cur = remaining.front();
+    remaining.pop();
+    addLoopHeaders();
+  }
+  else cur = nullptr;
+  return me;
+}
+
+LoopNestingTree::loop_iterator
+LoopNestingTree::loop_iterator::operator++(int junk)
+{
+  if(remaining.size())
+  {
+    cur = remaining.front();
+    remaining.pop();
+    addLoopHeaders();
+  }
+  else cur = nullptr;
+  return *this;
+}
+
+LoopNestingTree::child_iterator::child_iterator(loop_iterator &parent,
+                                                enum location loc)
+{
+  if(loc == BEGIN) it = parent.cur->children.begin();
+  else it = parent.cur->children.end();
+}
+
+///////////////////////////////////////////////////////////////////////////////
+// Private API
+///////////////////////////////////////////////////////////////////////////////
+
+void LoopNestingTree::print(raw_ostream &O, Node *node, unsigned depth) const
+{
+  for(unsigned i = 0; i < depth; i++) O << " ";
+  node->bb->printAsOperand(O, false);
+  O << "\n";
+  if(node->children.size())
+  {
+    for(unsigned i = 0; i < depth; i++) O << " ";
+    O << "\\\n";
+
+    for(std::list<Node *>::const_iterator it = node->children.begin();
+        it != node->children.end();
+        it++)
+    {
+      if((*it)->isLoopHeader) print(O, (*it), depth + 1);
+      else
+      {
+        for(unsigned i = 0; i < depth + 1; i++) O << " ";
+        (*it)->bb->printAsOperand(O, false);
+        O << "\n";
+      }
+    }
+  }
+}
+
+void LoopNestingTree::deleteRecursive(Node *node)
+{
+  for(std::list<Node *>::iterator it = node->children.begin();
+      it != node->children.end();
+      it++)
+  {
+    if((*it)->isLoopHeader) deleteRecursive(*it);
+    else delete *it;
+  }
+  delete node;
+}
+
+void LoopNestingTree::loop_iterator::addLoopHeaders()
+{
+  if(cur != nullptr)
+  {
+    for(std::list<Node *>::const_iterator it = cur->children.begin();
+        it != cur->children.end();
+        it++)
+      if((*it)->isLoopHeader)
+        remaining.push(*it);
+  }
+}
+
Index: lib/CodeGen/AsmPrinter/AsmPrinter.cpp
===================================================================
--- lib/CodeGen/AsmPrinter/AsmPrinter.cpp	(revision 277823)
+++ lib/CodeGen/AsmPrinter/AsmPrinter.cpp	(working copy)
@@ -1160,6 +1160,41 @@
   return CurExceptionSym;
 }
 
+MachineInstr *AsmPrinter::FindStackMap(MachineBasicBlock &MBB,
+                                       MachineInstr *MI) const {
+  MachineBasicBlock::instr_iterator i, ie;
+  for(i = MI->getNextNode(), ie = MBB.instr_end();
+      i != ie;
+      i = i->getNextNode()) {
+    if(i->getOpcode() == TargetOpcode::STACKMAP)
+      return &*i;
+    else if(i->isCall())
+      break;
+  }
+
+  // Call site without a stackmap implies that either the call was generated by
+  // the backend or the LLVM bitcode was never instrumented by the StackInfo
+  // pass.  This is not necessarily an error!
+  return nullptr;
+}
+
+bool AsmPrinter::TagCallSites(MachineFunction &MF) {
+  bool tagged = false;
+  for(auto MBB = MF.begin(), MBBE = MF.end(); MBB != MBBE; MBB++) {
+    for(auto MI = MBB->instr_begin(), MIE = MBB->instr_end(); MI != MIE; MI++) {
+      if(MI->isCall() && !MI->isPseudo()) {
+        MachineInstr *SMI = FindStackMap(*MBB, &*MI);
+        if(SMI != nullptr) {
+          MBB->remove(SMI);
+          MI = MBB->insert(++MI, SMI);
+          tagged = true;
+        }
+      }
+    }
+  }
+  return tagged;
+}
+
 void AsmPrinter::SetupMachineFunction(MachineFunction &MF) {
   this->MF = &MF;
   // Get the function symbol.
Index: lib/CodeGen/CMakeLists.txt
===================================================================
--- lib/CodeGen/CMakeLists.txt	(revision 277823)
+++ lib/CodeGen/CMakeLists.txt	(working copy)
@@ -111,6 +111,8 @@
   StackSlotColoring.cpp
   StackMapLivenessAnalysis.cpp
   StackMaps.cpp
+  StackTransformMetadata.cpp
+  StackTransformTypes.cpp
   StatepointExampleGC.cpp
   TailDuplication.cpp
   TargetFrameLoweringImpl.cpp
@@ -122,6 +124,7 @@
   TargetSchedule.cpp
   TwoAddressInstructionPass.cpp
   UnreachableBlockElim.cpp
+  UnwindInfo.cpp
   VirtRegMap.cpp
   WinEHPrepare.cpp
 
Index: lib/CodeGen/CodeGen.cpp
===================================================================
--- lib/CodeGen/CodeGen.cpp	(revision 277823)
+++ lib/CodeGen/CodeGen.cpp	(working copy)
@@ -68,6 +68,7 @@
   initializeStackMapLivenessPass(Registry);
   initializeStackProtectorPass(Registry);
   initializeStackSlotColoringPass(Registry);
+  initializeStackTransformMetadataPass(Registry);
   initializeTailDuplicatePassPass(Registry);
   initializeTargetPassConfigPass(Registry);
   initializeTwoAddressInstructionPassPass(Registry);
Index: lib/CodeGen/LLVMTargetMachine.cpp
===================================================================
--- lib/CodeGen/LLVMTargetMachine.cpp	(revision 277823)
+++ lib/CodeGen/LLVMTargetMachine.cpp	(working copy)
@@ -42,6 +42,26 @@
 EnableFastISelOption("fast-isel", cl::Hidden,
   cl::desc("Enable the \"fast\" instruction selector"));
 
+// Popcorn-specific IR-level instrumentation
+enum PopcornInstrumentation {
+  none, // No instrumentation
+  migpoints, // Only add equivalence points, don't generate rewriting metadata
+  migration, // Add migration points & generate rewriting metadata for migration
+  libc // Generate rewriting metadata for libc thread start functions
+};
+
+static cl::opt<PopcornInstrumentation> PopcornInstrument("popcorn-instrument",
+  cl::desc("Add Popcorn-specific instrumentation to applications"),
+  cl::init(none),
+  cl::values(
+    clEnumVal(none, "No instrumentation (default)"),
+    clEnumVal(migpoints, "Only add migration points (no migration metadata)"),
+    clEnumVal(migration, "Add equivalence points & migration metadata"),
+    clEnumVal(libc, "Instrument libc thread start functions for migration"),
+    NULL
+  )
+);
+
 void LLVMTargetMachine::initAsmInfo() {
   MRI = TheTarget.createMCRegInfo(getTargetTriple().str());
   MII = TheTarget.createMCInstrInfo();
@@ -105,6 +125,29 @@
   // Set PassConfig options provided by TargetMachine.
   PassConfig->setDisableVerify(DisableVerify);
 
+  /// Popcorn compiler - multi-ISA binary configurations.  Requires that IR
+  /// passed to backends is identical, save for certain architecture-specific
+  /// quirks like atomic operations or intrinsics
+  if(PopcornInstrument != PopcornInstrumentation::none) {
+    TM->setArchIROptLevel(CodeGenOpt::None);
+
+    switch(PopcornInstrument) {
+    case PopcornInstrumentation::migpoints:
+      PassConfig->setAddMigrationPoints(true);
+      break;
+    case PopcornInstrumentation::migration:
+      PassConfig->setAddMigrationPoints(true);
+      PassConfig->setAddStackMaps(true);
+      break;
+    case PopcornInstrumentation::libc:
+      PassConfig->setAddLibcStackMaps(true);
+      break;
+    default:
+      llvm_unreachable("Invalid instrumentation type");
+      break;
+    }
+  }
+
   PM.add(PassConfig);
 
   PassConfig->addIRPasses();
Index: lib/CodeGen/MachineFunction.cpp
===================================================================
--- lib/CodeGen/MachineFunction.cpp	(revision 277823)
+++ lib/CodeGen/MachineFunction.cpp	(working copy)
@@ -253,6 +253,15 @@
                                MMO->getBaseAlignment());
 }
 
+/// Is a register caller-saved?
+bool MachineFunction::isCallerSaved(unsigned Reg) const {
+  assert(TargetRegisterInfo::isPhysicalRegister(Reg) && "Invalid register");
+  CallingConv::ID CC = Fn->getCallingConv();
+  const uint32_t *Mask =
+    RegInfo->getTargetRegisterInfo()->getCallPreservedMask(*this, CC);
+  return !((Mask[Reg / 32] >> Reg % 32) & 1);
+}
+
 MachineInstr::mmo_iterator
 MachineFunction::allocateMemRefsArray(unsigned long Num) {
   return Allocator.Allocate<MachineMemOperand *>(Num);
@@ -482,6 +491,164 @@
                                Twine(getFunctionNumber()) + "$pb");
 }
 
+/// Get the value for key K in Map M, or create a new one if it doesn't exist.
+template<typename Key, typename Val, typename Map>
+static Val &getOrCreateMapping(const Key *K, Map &M) {
+  typename Map::iterator It;
+  if((It = M.find(K)) == M.end())
+    It = M.insert(std::pair<const Key *, Val>(K, Val())).first;
+  return It->second;
+}
+
+/// Add an IR/architecture-specific location mapping for a stackmap operand
+void MachineFunction::addSMOpLocation(const CallInst *SM,
+                                      const Value *Val,
+                                      const MachineLiveLoc &MLL) {
+  auto containsLoc = [](const MachineLiveLocs &Vals,
+                        const MachineLiveLoc &Cur) -> bool {
+    for(auto &LV : Vals) if(Cur == *LV) return true;
+    return false;
+  };
+
+  assert(SM && "Invalid stackmap");
+  assert(Val && "Invalid stackmap operand");
+
+  IRToMachineLocs &IRMap =
+    getOrCreateMapping<Instruction,
+                       IRToMachineLocs,
+                       InstToOperands>(SM, SMDuplicateLocs);
+  MachineLiveLocs &Vals =
+    getOrCreateMapping<Value,
+                       MachineLiveLocs,
+                       IRToMachineLocs>(Val, IRMap);
+  if(!containsLoc(Vals, MLL))
+    Vals.push_back(MachineLiveLocPtr(MLL.copy()));
+}
+
+/// Add an IR/architecture-specific location mapping for a stackmap operand
+void MachineFunction::addSMOpLocation(const CallInst *SM,
+                                      unsigned Op,
+                                      const MachineLiveLoc &MLL) {
+  assert(SM && "Invalid stackmap");
+  assert(Op < SM->getNumArgOperands() && "Invalid operand number");
+  addSMOpLocation(SM, SM->getArgOperand(Op), MLL);
+}
+
+
+/// Add an architecture-specific live value & location for a stackmap
+void MachineFunction::addSMArchSpecificLocation(const CallInst *SM,
+                                                const MachineLiveLoc &MLL,
+                                                const MachineLiveVal &MLV) {
+  auto containsLoc = [](const ArchLiveValues &Vals,
+                        const MachineLiveLoc &Cur) -> bool {
+    for(auto &LV : Vals) if(Cur == *LV.first) return true;
+    return false;
+  };
+
+  assert(SM && "Invalid stackmap");
+  ArchLiveValues &Vals =
+    getOrCreateMapping<Instruction,
+                       ArchLiveValues,
+                       InstToArchLiveValues>(SM, SMArchSpecificLocs);
+  if(!containsLoc(Vals, MLL))
+    Vals.push_back(ArchLiveValue(MachineLiveLocPtr(MLL.copy()),
+                                 MachineLiveValPtr(MLV.copy())));
+}
+
+/// Update stack slot references to new indexes after stack slot coloring
+void
+MachineFunction::updateSMStackSlotRefs(SmallDenseMap<int, int, 16> &Changes) {
+
+  if(Changes.size()) {
+    DEBUG(dbgs() << "Updating stackmap stack slot references\n";);
+
+    int SS;
+    SmallDenseMap<int, int, 16>::iterator Change;
+
+    // Iterate over all operand duplicate locations
+    for(auto &InstIt : SMDuplicateLocs) {
+      for(auto &IRIt : InstIt.second) {
+        for(auto &MLL : IRIt.second) {
+          if(MLL->isStackSlot()) {
+            MachineLiveStackSlot &LSS = (MachineLiveStackSlot &)*MLL;
+            SS = LSS.getStackSlot();
+            Change = Changes.find(SS);
+            if(Change != Changes.end())
+              LSS.setStackSlot(Change->second);
+          }
+        }
+      }
+    }
+
+    // Iterate over all architecture-specific locations.
+    // Note: both the destination & source location can be stack slots
+    for(auto &InstIt : SMArchSpecificLocs) {
+      for(auto &MLL : InstIt.second) {
+        if(MLL.first->isStackSlot()) {
+          MachineLiveStackSlot &LSS = (MachineLiveStackSlot &)*MLL.first;
+          SS = LSS.getStackSlot();
+          Change = Changes.find(SS);
+          if(Change != Changes.end())
+            LSS.setStackSlot(Change->second);
+        }
+
+        if(MLL.second->isStackObject()) {
+          MachineStackObject &MSO = (MachineStackObject &)*MLL.second;
+          SS = MSO.getIndex();
+          Change = Changes.find(SS);
+          if(Change != Changes.end())
+            MSO.setIndex(Change->second);
+        }
+      }
+    }
+  }
+}
+
+/// Are there any architecture-specific locations for operand Val in stackmap
+/// SM?
+bool
+MachineFunction::hasSMOpLocations(const CallInst *SM, const Value *Val) const {
+  assert(SM && "Invalid stackmap");
+  assert(Val && "Invalid stackmap operand");
+  InstToOperands::const_iterator InstIt;
+  if((InstIt = SMDuplicateLocs.find(SM)) != SMDuplicateLocs.end())
+    return InstIt->second.find(Val) != InstIt->second.end();
+  return false;
+}
+
+/// Are there any architecture-specific locations for stackmap SM?
+bool
+MachineFunction::hasSMArchSpecificLocations(const llvm::CallInst *SM) const {
+  assert(SM && "Invalid stackmap");
+  return SMArchSpecificLocs.find(SM) != SMArchSpecificLocs.end();
+}
+
+/// Return the architecture-specific locations for a stackmap operand.
+const MachineLiveLocs &
+MachineFunction::getSMOpLocations(const CallInst *SM,
+                                  const Value *Val ) const {
+  assert(SM && "Invalid stackmap");
+  assert(Val && "Invalid stackmap operand");
+  InstToOperands::const_iterator InstIt = SMDuplicateLocs.find(SM);
+  assert(InstIt != SMDuplicateLocs.end() &&
+         "No duplicate locations for stackmap");
+  IRToMachineLocs::const_iterator IRIt = InstIt->second.find(Val);
+  assert(IRIt != InstIt->second.end() &&
+         "No duplicate locations for stackmap operand");
+  return IRIt->second;
+}
+
+/// Return the architecture-specific locations for a stackmap that are not
+/// associated with any operand.
+const ArchLiveValues &
+MachineFunction::getSMArchSpecificLocations(const CallInst *SM) const {
+  assert(SM && "Invalid stackmap");
+  InstToArchLiveValues::const_iterator InstIt = SMArchSpecificLocs.find(SM);
+  assert(InstIt != SMArchSpecificLocs.end() &&
+         "No architecture-specific locations for stackmap");
+  return InstIt->second;
+}
+
 //===----------------------------------------------------------------------===//
 //  MachineFrameInfo implementation
 //===----------------------------------------------------------------------===//
Index: lib/CodeGen/Passes.cpp
===================================================================
--- lib/CodeGen/Passes.cpp	(revision 277823)
+++ lib/CodeGen/Passes.cpp	(working copy)
@@ -217,7 +217,9 @@
     : ImmutablePass(ID), PM(&pm), StartBefore(nullptr), StartAfter(nullptr),
       StopAfter(nullptr), Started(true), Stopped(false),
       AddingMachinePasses(false), TM(tm), Impl(nullptr), Initialized(false),
-      DisableVerify(false), EnableTailMerge(true), EnableShrinkWrap(false) {
+      DisableVerify(false), EnableTailMerge(true), EnableShrinkWrap(false),
+      AddMigrationPoints(false), AddStackMaps(false),
+      AddLibcStackMaps(false) {
 
   Impl = new PassConfigImpl();
 
@@ -459,6 +461,21 @@
 void TargetPassConfig::addISelPrepare() {
   addPreISel();
 
+  // Add pass to instrument IR with equivalence points, which are implemented
+  // various ways depending on other command-line arguments
+  if(AddMigrationPoints) addPass(createMigrationPointsPass());
+
+  assert(!(AddStackMaps && AddLibcStackMaps) &&
+    "Cannot add both InsertStackMapsPass and LibcStackMapsPass");
+
+  // Add pass to instrument IR with stackmap instructions, which get lowered to
+  // metadata needed for Popcorn's stack transformation
+  if(AddStackMaps) addPass(createInsertStackMapsPass());
+
+  // Similar to creatInsertStackMaps pass, but only instruments libc thread
+  // start functions
+  if(AddLibcStackMaps) addPass(createLibcStackMapsPass());
+
   // Add both the safe stack and the stack protection passes: each of them will
   // only protect functions that have corresponding attributes.
   addPass(createSafeStackPass());
@@ -754,6 +771,10 @@
   // Allow targets to change the register assignments before rewriting.
   addPreRewrite();
 
+  // Gather additional stack transformation metadata before rewriting virtual
+  // registers
+  addPass(&StackTransformMetadataID);
+
   // Finally rewrite virtual registers.
   addPass(&VirtRegRewriterID);
 
Index: lib/CodeGen/RegAllocFast.cpp
===================================================================
--- lib/CodeGen/RegAllocFast.cpp	(revision 277823)
+++ lib/CodeGen/RegAllocFast.cpp	(working copy)
@@ -1078,6 +1078,13 @@
 /// runOnMachineFunction - Register allocate the whole function
 ///
 bool RAFast::runOnMachineFunction(MachineFunction &Fn) {
+  // TODO the fast register allocator behaves poorly for stackmaps with lots
+  // of operands, and since it doesn't use the VirtRegRewriter pass we can't
+  // capture correct stackmap operand locations
+  if(Fn.getFrameInfo()->hasStackMap())
+    llvm_unreachable("Fast register allocator not supported for stack"
+                     "transformation");
+
   DEBUG(dbgs() << "********** FAST REGISTER ALLOCATION **********\n"
                << "********** Function: " << Fn.getName() << '\n');
   MF = &Fn;
Index: lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp	(revision 277823)
+++ lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp	(working copy)
@@ -886,6 +886,9 @@
   case ISD::SRL:
   case ISD::ROTL:
   case ISD::ROTR: Res = PromoteIntOp_Shift(N); break;
+
+  case (uint16_t)~TargetOpcode::STACKMAP:
+    Res = PromoteIntOp_STACKMAP(N, OpNo); break;
   }
 
   // If the result is null, the sub-method took care of registering results etc.
@@ -1131,6 +1134,23 @@
                                 SExtPromotedInteger(N->getOperand(0))), 0);
 }
 
+SDValue DAGTypeLegalizer::PromoteIntOp_STACKMAP(SDNode *N, unsigned OpNo) {
+  std::vector<SDValue> Ops(N->getNumOperands());
+  SDLoc dl(N);
+
+  for(unsigned i = 0; i < N->getNumOperands(); i++) {
+    if(i == OpNo) {
+      if(N->getOperand(i).getValueType() == MVT::i1 ||
+         N->getOperand(i).getValueType() == MVT::i8 ||
+         N->getOperand(i).getValueType() == MVT::i16)
+        Ops[i] = DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i32, N->getOperand(i));
+    }
+    else Ops[i] = N->getOperand(i);
+  }
+
+  return SDValue(DAG.UpdateNodeOperands(N, Ops), 0);
+}
+
 SDValue DAGTypeLegalizer::PromoteIntOp_STORE(StoreSDNode *N, unsigned OpNo){
   assert(ISD::isUNINDEXEDStore(N) && "Indexed store during type legalization!");
   SDValue Ch = N->getChain(), Ptr = N->getBasePtr();
Index: lib/CodeGen/SelectionDAG/LegalizeTypes.h
===================================================================
--- lib/CodeGen/SelectionDAG/LegalizeTypes.h	(revision 277823)
+++ lib/CodeGen/SelectionDAG/LegalizeTypes.h	(working copy)
@@ -288,6 +288,7 @@
   SDValue PromoteIntOp_Shift(SDNode *N);
   SDValue PromoteIntOp_SIGN_EXTEND(SDNode *N);
   SDValue PromoteIntOp_SINT_TO_FP(SDNode *N);
+  SDValue PromoteIntOp_STACKMAP(SDNode *N, unsigned OpNo);
   SDValue PromoteIntOp_STORE(StoreSDNode *N, unsigned OpNo);
   SDValue PromoteIntOp_TRUNCATE(SDNode *N);
   SDValue PromoteIntOp_UINT_TO_FP(SDNode *N);
Index: lib/CodeGen/StackColoring.cpp
===================================================================
--- lib/CodeGen/StackColoring.cpp	(revision 277823)
+++ lib/CodeGen/StackColoring.cpp	(working copy)
@@ -713,7 +713,7 @@
 
   // This is a simple greedy algorithm for merging allocas. First, sort the
   // slots, placing the largest slots first. Next, perform an n^2 scan and look
-  // for disjoint slots. When you find disjoint slots, merge the samller one
+  // for disjoint slots. When you find disjoint slots, merge the smaller one
   // into the bigger one and update the live interval. Remove the small alloca
   // and continue.
 
Index: lib/CodeGen/StackMaps.cpp
===================================================================
--- lib/CodeGen/StackMaps.cpp	(revision 277823)
+++ lib/CodeGen/StackMaps.cpp	(working copy)
@@ -12,13 +12,18 @@
 #include "llvm/CodeGen/MachineFrameInfo.h"
 #include "llvm/CodeGen/MachineFunction.h"
 #include "llvm/CodeGen/MachineInstr.h"
+#include "llvm/CodeGen/UnwindInfo.h"
 #include "llvm/IR/DataLayout.h"
+#include "llvm/IR/DiagnosticInfo.h"
+#include "llvm/IR/IntrinsicInst.h"
 #include "llvm/MC/MCContext.h"
 #include "llvm/MC/MCExpr.h"
 #include "llvm/MC/MCObjectFileInfo.h"
 #include "llvm/MC/MCSectionMachO.h"
 #include "llvm/MC/MCStreamer.h"
+#include "llvm/MC/MCSymbol.h"
 #include "llvm/Support/CommandLine.h"
+#include "llvm/Target/TargetFrameLowering.h"
 #include "llvm/Target/TargetMachine.h"
 #include "llvm/Target/TargetOpcodes.h"
 #include "llvm/Target/TargetRegisterInfo.h"
@@ -29,6 +34,15 @@
 
 #define DEBUG_TYPE "stackmaps"
 
+#define TYPE_AND_FLAGS(type, ptr, alloca, dup) \
+  ((uint8_t)type) << 4 | ((uint8_t)ptr) << 2 | \
+  ((uint8_t)alloca) << 1 | ((uint8_t)dup)
+
+#define ARCH_TYPE_AND_FLAGS(type, ptr) ((uint8_t)type) << 4 | ((uint8_t)ptr)
+
+#define ARCH_OP_TYPE(inst, gen, op) \
+  ((uint8_t)inst) << 4 | ((uint8_t)gen) << 3 | (uint8_t)op
+
 static cl::opt<int> StackMapVersion(
     "stackmap-version", cl::init(1),
     cl::desc("Specify the stackmap encoding version (default = 1)"));
@@ -84,32 +98,109 @@
   return (unsigned)RegNum;
 }
 
+/// Get pointer typing information for a stackmap operand
+void StackMaps::getPointerInfo(const Value *Op, const DataLayout &DL,
+                               bool &isPtr, bool &isAlloca,
+                               unsigned &AllocaSize) const {
+  isPtr = false;
+  isAlloca = false;
+  AllocaSize = 0;
+
+  assert(Op != nullptr && "Invalid stackmap operand");
+  Type *Ty = Op->getType();
+  if(Ty->isPointerTy())
+  {
+    isPtr = true;
+    PointerType *PTy = cast<PointerType>(Ty);
+    if(PTy->getElementType()->isSized() && isa<AllocaInst>(Op)) {
+      isAlloca = true;
+      AllocaSize = DL.getTypeAllocSize(PTy->getElementType());
+    }
+  }
+}
+
+/// Get stackmap information for register location
+void StackMaps::getRegLocation(unsigned Phys,
+                               unsigned &Dwarf,
+                               unsigned &Offset) const {
+  const TargetRegisterInfo *TRI = AP.MF->getSubtarget().getRegisterInfo();
+  assert(!TRI->isVirtualRegister(Phys) &&
+         "Virtual registers should have been rewritten by now");
+  Offset = 0;
+  Dwarf = getDwarfRegNum(Phys, TRI);
+  unsigned LLVMRegNum = TRI->getLLVMRegNum(Dwarf, false);
+  unsigned SubRegIdx = TRI->getSubRegIndex(LLVMRegNum, Phys);
+  if(SubRegIdx)
+    Offset = TRI->getSubRegIdxOffset(SubRegIdx);
+}
+
+/// Add duplicate target-specific locations for a stackmap operand
+void StackMaps::addDuplicateLocs(const CallInst *StackMap, const Value *Oper,
+                                 LocationVec &Locs, unsigned Size, bool Ptr,
+                                 bool Alloca, unsigned AllocaSize) const {
+  unsigned DwarfRegNum, Offset;
+  int FrameOff;
+
+  if(AP.MF->hasSMOpLocations(StackMap, Oper)) {
+    const MachineLiveLocs &Dups = AP.MF->getSMOpLocations(StackMap, Oper);
+    const TargetRegisterInfo *TRI = AP.MF->getSubtarget().getRegisterInfo();
+
+    for(const MachineLiveLocPtr &LL : Dups) {
+      if(LL->isReg()) {
+        const MachineLiveReg &MR = (const MachineLiveReg &)*LL;
+        getRegLocation(MR.getReg(), DwarfRegNum, Offset);
+
+        Locs.emplace_back(Location::Register, Size, DwarfRegNum, Offset,
+                          Ptr, Alloca, true, AllocaSize);
+      }
+      else if(LL->isStackAddr()) {
+        MachineLiveStackAddr &MLSA = (MachineLiveStackAddr &)*LL;
+        FrameOff = MLSA.calcAndGetRegOffset(AP, DwarfRegNum);
+
+        Locs.emplace_back(Location::Indirect, Size,
+          getDwarfRegNum(DwarfRegNum, TRI),
+          FrameOff, Ptr, Alloca, true, AllocaSize);
+      }
+      else llvm_unreachable("Unknown machine live location type");
+    }
+  }
+}
+
 MachineInstr::const_mop_iterator
 StackMaps::parseOperand(MachineInstr::const_mop_iterator MOI,
                         MachineInstr::const_mop_iterator MOE, LocationVec &Locs,
-                        LiveOutVec &LiveOuts) const {
+                        LiveOutVec &LiveOuts, User::const_op_iterator &Op) const {
+  bool isPtr, isAlloca;
+  unsigned AllocaSize;
+  auto &DL = AP.MF->getDataLayout();
   const TargetRegisterInfo *TRI = AP.MF->getSubtarget().getRegisterInfo();
+  const CallInst *IRSM = cast<CallInst>(Op->getUser());
+  const Value *IROp = Op->get();
+  getPointerInfo(IROp, DL, isPtr, isAlloca, AllocaSize);
+
   if (MOI->isImm()) {
     switch (MOI->getImm()) {
     default:
       llvm_unreachable("Unrecognized operand type.");
     case StackMaps::DirectMemRefOp: {
-      unsigned Size = AP.TM.getDataLayout()->getPointerSizeInBits();
+      unsigned Size = DL.getPointerSizeInBits();
       assert((Size % 8) == 0 && "Need pointer size in bytes.");
       Size /= 8;
       unsigned Reg = (++MOI)->getReg();
       int64_t Imm = (++MOI)->getImm();
-      Locs.emplace_back(StackMaps::Location::Direct, Size,
-                        getDwarfRegNum(Reg, TRI), Imm);
+      Locs.emplace_back(Location::Direct, Size, getDwarfRegNum(Reg, TRI), Imm,
+                        isPtr, isAlloca, false, AllocaSize);
       break;
     }
     case StackMaps::IndirectMemRefOp: {
       int64_t Size = (++MOI)->getImm();
       assert(Size > 0 && "Need a valid size for indirect memory locations.");
+      Size = DL.getTypeAllocSize(IROp->getType());
       unsigned Reg = (++MOI)->getReg();
       int64_t Imm = (++MOI)->getImm();
-      Locs.emplace_back(StackMaps::Location::Indirect, Size,
-                        getDwarfRegNum(Reg, TRI), Imm);
+      Locs.emplace_back(Location::Indirect, (unsigned)Size,
+                        getDwarfRegNum(Reg, TRI), Imm, isPtr, isAlloca, false,
+                        AllocaSize);
       break;
     }
     case StackMaps::ConstantOp: {
@@ -116,17 +207,24 @@
       ++MOI;
       assert(MOI->isImm() && "Expected constant operand.");
       int64_t Imm = MOI->getImm();
-      Locs.emplace_back(Location::Constant, sizeof(int64_t), 0, Imm);
+      Locs.emplace_back(Location::Constant, sizeof(int64_t), 0, Imm,
+                        isPtr, isAlloca, false, AllocaSize);
       break;
     }
     }
+    // Note: we shouldn't have alternate locations -- constants aren't stored
+    // anywhere, and stack slots should be either allocas (which shouldn't have
+    // alternate locations) or register spill locations (handled below in the
+    // register path)
+    assert(!AP.MF->hasSMOpLocations(IRSM, IROp) &&
+           "Unhandled duplicate locations");
+    ++Op;
     return ++MOI;
   }
 
   // The physical register number will ultimately be encoded as a DWARF regno.
   // The stack map also records the size of a spill slot that can hold the
-  // register content. (The runtime can track the actual size of the data type
-  // if it needs to.)
+  // register content, accurate to the actual size of the data type.
   if (MOI->isReg()) {
     // Skip implicit registers (this includes our scratch registers)
     if (MOI->isImplicit())
@@ -134,17 +232,16 @@
 
     assert(TargetRegisterInfo::isPhysicalRegister(MOI->getReg()) &&
            "Virtreg operands should have been rewritten before now.");
-    const TargetRegisterClass *RC = TRI->getMinimalPhysRegClass(MOI->getReg());
     assert(!MOI->getSubReg() && "Physical subreg still around.");
 
-    unsigned Offset = 0;
-    unsigned DwarfRegNum = getDwarfRegNum(MOI->getReg(), TRI);
-    unsigned LLVMRegNum = TRI->getLLVMRegNum(DwarfRegNum, false);
-    unsigned SubRegIdx = TRI->getSubRegIndex(LLVMRegNum, MOI->getReg());
-    if (SubRegIdx)
-      Offset = TRI->getSubRegIdxOffset(SubRegIdx);
+    size_t ValSize = DL.getTypeAllocSize(IROp->getType());
+    unsigned Offset, DwarfRegNum;
+    getRegLocation(MOI->getReg(), DwarfRegNum, Offset);
 
-    Locs.emplace_back(Location::Register, RC->getSize(), DwarfRegNum, Offset);
+    Locs.emplace_back(Location::Register, ValSize, DwarfRegNum, Offset,
+                      isPtr, isAlloca, false, AllocaSize);
+    addDuplicateLocs(IRSM, IROp, Locs, ValSize, isPtr, isAlloca, AllocaSize);
+    ++Op;
     return ++MOI;
   }
 
@@ -161,6 +258,7 @@
   for (const auto &CSI : CSInfos) {
     const LocationVec &CSLocs = CSI.Locations;
     const LiveOutVec &LiveOuts = CSI.LiveOuts;
+    const ArchValues &Values = CSI.Vals;
 
     OS << WSMP << "callsite " << CSI.ID << "\n";
     OS << WSMP << "  has " << CSLocs.size() << " locations\n";
@@ -194,7 +292,7 @@
           OS << TRI->getName(Loc.Reg);
         else
           OS << Loc.Reg;
-        OS << "+" << Loc.Offset;
+        OS << " + " << Loc.Offset;
         break;
       case Location::Constant:
         OS << "Constant " << Loc.Offset;
@@ -203,8 +301,15 @@
         OS << "Constant Index " << Loc.Offset;
         break;
       }
-      OS << "\t[encoding: .byte " << Loc.Type << ", .byte " << Loc.Size
-         << ", .short " << Loc.Reg << ", .int " << Loc.Offset << "]\n";
+      OS << ", pointer? " << Loc.Ptr << ", alloca? " << Loc.Alloca
+         << ", duplicate? " << Loc.Duplicate;
+
+      unsigned TypeAndFlags =
+        TYPE_AND_FLAGS(Loc.Type, Loc.Ptr, Loc.Alloca, Loc.Duplicate);
+
+      OS << "\t[encoding: .byte " << TypeAndFlags << ", .byte " << Loc.Size
+         << ", .short " << Loc.Reg << ", .int " << Loc.Offset
+         << ", .uint " << Loc.AllocaSize << "]\n";
       Idx++;
     }
 
@@ -221,6 +326,78 @@
          << LO.Size << "]\n";
       Idx++;
     }
+
+    OS << WSMP << "\thas " << Values.size() << " arch-specific live values\n";
+
+    Idx = 0;
+    for (const auto &V : Values) {
+      const Location &Loc = V.first;
+      const Operation &Op = V.second;
+
+      OS << WSMP << "\t\tArch-Val " << Idx << ": ";
+      switch(Loc.Type) {
+      case Location::Register:
+        OS << "Register ";
+        if (TRI)
+          OS << TRI->getName(Loc.Reg);
+        else
+          OS << Loc.Reg;
+        break;
+      case Location::Indirect:
+        OS << "Indirect ";
+        if (TRI)
+          OS << TRI->getName(Loc.Reg);
+        else
+          OS << Loc.Reg;
+        if (Loc.Offset)
+          OS << " + " << Loc.Offset;
+        break;
+      default:
+        OS << "<Unknown live value type>";
+        break;
+      }
+
+      OS << ", " << ValueGenInst::getInstName(Op.InstType) << " ";
+      switch(Op.OperandType) {
+      case Location::Register:
+        OS << "register ";
+        if (TRI)
+          OS << TRI->getName(Op.DwarfReg);
+        else
+          OS << Op.DwarfReg;
+        break;
+      case Location::Direct:
+        OS << "value stored at register ";
+        if (TRI)
+          OS << TRI->getName(Op.DwarfReg);
+        else
+          OS << Op.DwarfReg;
+        if (Op.Constant)
+          OS << " + " << Op.Constant;
+        break;
+      case Location::Constant:
+        if(Op.isSymbol)
+          OS << "address of " << Op.Symbol->getName();
+        else {
+          OS << "immediate ";
+          OS.write_hex(Op.Constant);
+        }
+        break;
+      default:
+        OS << "<Unknown operand type>";
+        break;
+      }
+
+      unsigned TypeAndFlags = ARCH_TYPE_AND_FLAGS(Loc.Type, Loc.Ptr);
+      unsigned OpType = ARCH_OP_TYPE(Op.InstType,
+                                     Op.isGenerated,
+                                     Op.OperandType);
+      OS << "\t[encoding: .byte " << TypeAndFlags << ", .byte " << Loc.Size
+         << ", .short " << Loc.Reg << ", .int " << Loc.Offset
+         << ", .byte " << OpType << ", .byte " << Op.Size << ", .short "
+         << Op.DwarfReg << ", .int64 " << (Op.isSymbol ? 0 : Op.Constant)
+         << "]\n";
+    }
   }
 }
 
@@ -277,6 +454,135 @@
   return LiveOuts;
 }
 
+/// Convert a list of instructions used to generate an architecture-specific
+/// live value into multiple individual records.
+void StackMaps::genArchValsFromInsts(ArchValues &AV,
+                                     Location &Loc,
+                                     const MachineLiveVal &MLV) {
+  assert(MLV.isGenerated() && "Invalid live value type");
+
+  const MachineGeneratedVal &MGV = (const MachineGeneratedVal &)MLV;
+  const ValueGenInstList &I = MGV.getInstructions();
+  const TargetRegisterInfo *TRI = AP.MF->getSubtarget().getRegisterInfo();
+  const TargetRegisterClass *RC;
+  Operation Op;
+
+  Op.isGenerated = true;
+  Op.isSymbol = false;
+  Op.Symbol = nullptr;
+
+  for(auto &Inst : I) {
+    const RegInstructionBase *RI;
+    const ImmInstructionBase *II;
+
+    Op.InstType = Inst->type();
+    switch(Inst->opType()) {
+    case ValueGenInst::OpType::Register:
+      RI = (const RegInstructionBase *)Inst.get();
+      assert(TRI->isPhysicalRegister(RI->getReg()) &&
+             "Virtual should have been converted to physical register");
+      RC = TRI->getMinimalPhysRegClass(RI->getReg());
+      Op.OperandType = Location::Register;
+      Op.Size = RC->getSize();
+      Op.DwarfReg = getDwarfRegNum(RI->getReg(), TRI);
+      Op.Constant = 0;
+      break;
+    case ValueGenInst::OpType::Immediate:
+      II = (const ImmInstructionBase *)Inst.get();
+      Op.OperandType = Location::Constant;
+      Op.Size = II->getImmSize();
+      Op.DwarfReg = 0;
+      Op.Constant = II->getImm();
+      break;
+    default: llvm_unreachable("Invalid operand type"); break;
+    }
+    AV.emplace_back(ArchValue(Loc, Op));
+  }
+}
+
+/// Add architecture-specific locations for the stackmap
+void StackMaps::addArchLiveVals(const CallInst *SM, ArchValues &AV) {
+  unsigned Offset, DwarfReg;
+  unsigned PtrSize = AP.MF->getDataLayout().getPointerSizeInBits() / 8;
+  const MachineFrameInfo *MFI = AP.MF->getFrameInfo();
+  const TargetRegisterInfo *TRI = AP.MF->getSubtarget().getRegisterInfo();
+
+  if(AP.MF->hasSMArchSpecificLocations(SM)) {
+    const ArchLiveValues &Vals = AP.MF->getSMArchSpecificLocations(SM);
+
+    for(auto &Val : Vals) {
+      Location Loc;
+      Operation Op;
+
+      Loc.Ptr = Val.second->isPtr();
+      Loc.Alloca = false;
+      Loc.Duplicate = false;
+      Loc.AllocaSize = 0;
+
+      // Parse the location
+      if(Val.first->isReg()) {
+        const MachineLiveReg &MR = (const MachineLiveReg &)*Val.first;
+        const TargetRegisterClass *RC =
+          TRI->getMinimalPhysRegClass(MR.getReg());
+        getRegLocation(MR.getReg(), DwarfReg, Offset);
+
+        Loc.Type = Location::Register;
+        Loc.Size = RC->getSize();
+        Loc.Reg = DwarfReg;
+        Loc.Offset = Offset;
+      }
+      else if(Val.first->isStackAddr()) {
+        MachineLiveStackAddr &MLSA = (MachineLiveStackAddr &)*Val.first;
+
+        Loc.Type = Location::Indirect;
+        Loc.Size = MLSA.getSize(AP);
+        Loc.Offset = MLSA.calcAndGetRegOffset(AP, DwarfReg);
+        Loc.Reg = getDwarfRegNum(DwarfReg, TRI);
+      }
+      else llvm_unreachable("Invalid architecture-specific live value");
+
+      // Parse the operation
+      Op.InstType = ValueGenInst::Set;
+      Op.isGenerated = false;
+      if(Val.second->isReference()) {
+        const MachineReference &MR = (const MachineReference &)*Val.second;
+        Op.OperandType = Location::Constant;
+        Op.Size = PtrSize;
+        Op.isSymbol = true;
+        Op.Symbol = MR.getReference(AP);
+        AV.emplace_back(ArchValue(Loc, Op));
+      }
+      else if(Val.second->isStackObject()) {
+        const MachineStackObject &MSO = (const MachineStackObject &)*Val.second;
+        if(MSO.isLoad()) { // Loading a value from a stack slot
+          Op.OperandType = Location::Direct;
+          if(MSO.isCommonObject()) Op.Size = PtrSize;
+          else Op.Size = MFI->getObjectSize(MSO.getIndex());
+        }
+        else { // Generating a reference to a stack slot
+          Op.OperandType = Location::Indirect;
+          Op.Size = PtrSize;
+        }
+        Op.Constant = MSO.getOffsetFromReg(AP, DwarfReg);
+        Op.DwarfReg = getDwarfRegNum(DwarfReg, TRI);
+        Op.isSymbol = false;
+        AV.emplace_back(ArchValue(Loc, Op));
+      }
+      else if(Val.second->isImm()) {
+        const MachineImmediate &MI = (const MachineImmediate &)*Val.second;
+        Op.OperandType = Location::Constant;
+        Op.Size = MI.getSize();
+        Op.Constant = MI.getValue();
+        Op.isSymbol = false;
+        AV.emplace_back(ArchValue(Loc, Op));
+      }
+      else if(Val.second->isGenerated())
+        genArchValsFromInsts(AV, Loc, *Val.second);
+      else llvm_unreachable("Invalid architecture-specific live value");
+    }
+  }
+}
+
 void StackMaps::recordStackMapOpers(const MachineInstr &MI, uint64_t ID,
                                     MachineInstr::const_mop_iterator MOI,
                                     MachineInstr::const_mop_iterator MOE,
@@ -285,21 +591,46 @@
   MCContext &OutContext = AP.OutStreamer->getContext();
   MCSymbol *MILabel = OutContext.createTempSymbol();
   AP.OutStreamer->EmitLabel(MILabel);
+  User::const_op_iterator Op = nullptr;
 
   LocationVec Locations;
   LiveOutVec LiveOuts;
+  ArchValues Constants;
 
   if (recordResult) {
     assert(PatchPointOpers(&MI).hasDef() && "Stackmap has no return value.");
     parseOperand(MI.operands_begin(), std::next(MI.operands_begin()), Locations,
-                 LiveOuts);
+                 LiveOuts, Op);
   }
 
+  // Find the IR stackmap instruction which corresponds to MI so we can emit
+  // type information along with the value's location
+  const BasicBlock *BB = MI.getParent()->getBasicBlock();
+  const IntrinsicInst *IRSM = nullptr;
+  const std::string SMName("llvm.experimental.stackmap");
+  for(auto BBI = BB->begin(), BBE = BB->end(); BBI != BBE; BBI++)
+  {
+    const IntrinsicInst *II;
+    if((II = dyn_cast<IntrinsicInst>(&*BBI)) &&
+       II->getCalledFunction()->getName() == SMName &&
+       cast<ConstantInt>(II->getArgOperand(0))->getZExtValue() == ID)
+    {
+      IRSM = cast<IntrinsicInst>(&*BBI);
+      break;
+    }
+  }
+  assert(IRSM && "Could not find associated stackmap instruction");
+
   // Parse operands.
+  Op = std::next(IRSM->op_begin(), 2);
   while (MOI != MOE) {
-    MOI = parseOperand(MOI, MOE, Locations, LiveOuts);
+    MOI = parseOperand(MOI, MOE, Locations, LiveOuts, Op);
   }
+  assert(Op == (IRSM->op_end() - 1) && "did not lower all stackmap operands");
 
+  // Add architecture-specific live values
+  addArchLiveVals(IRSM, Constants);
+
   // Move large constants into the constant pool.
   for (auto &Loc : Locations) {
     // Constants are encoded as sign-extended integers.
@@ -323,12 +654,21 @@
 
   // Create an expression to calculate the offset of the callsite from function
   // entry.
-  const MCExpr *CSOffsetExpr = MCBinaryExpr::createSub(
+  // TODO for Popcorn, we actually want the return address of the call
+  // instruction to which this stackmap is attached.  However some backend
+  // writers, in their infinite wisdom, decided to abstract multiple assembly
+  // instructions into a single machine IR instruction (*ahem* PowerPC *ahem*).
+  // Generate an expression to correct for this "feature".
+  int RAOffset = AP.getCanonicalReturnAddr(MI.getPrevNode());
+  const MCExpr *RAFixup = MCBinaryExpr::createSub(
       MCSymbolRefExpr::create(MILabel, OutContext),
+      MCConstantExpr::create(RAOffset, OutContext), OutContext);
+  const MCExpr *CSOffsetExpr = MCBinaryExpr::createSub(RAFixup,
       MCSymbolRefExpr::create(AP.CurrentFnSymForSize, OutContext), OutContext);
 
-  CSInfos.emplace_back(CSOffsetExpr, ID, std::move(Locations),
-                       std::move(LiveOuts));
+  CSInfos.emplace_back(AP.CurrentFnSym, CSOffsetExpr, ID,
+                       std::move(Locations), std::move(LiveOuts),
+                       std::move(Constants));
 
   // Record the stack size of the current function.
   const MachineFrameInfo *MFI = AP.MF->getFrameInfo();
@@ -411,8 +751,11 @@
 /// StkSizeRecord[NumFunctions] {
 ///   uint64 : Function Address
 ///   uint64 : Stack Size
+///   uint32 : Number of Unwinding Entries
+///   uint32 : Offset into Unwinding Section
 /// }
-void StackMaps::emitFunctionFrameRecords(MCStreamer &OS) {
+void StackMaps::emitFunctionFrameRecords(MCStreamer &OS,
+                                         const UnwindInfo *UI) {
   // Function Frame records.
   DEBUG(dbgs() << WSMP << "functions:\n");
   for (auto const &FR : FnStackSize) {
@@ -420,6 +763,15 @@
                  << " frame size: " << FR.second);
     OS.EmitSymbolValue(FR.first, 8);
     OS.EmitIntValue(FR.second, 8);
+
+    if(UI) {
+      const UnwindInfo::FuncUnwindInfo &FUI = UI->getUnwindInfo(FR.first);
+      DEBUG(dbgs() << " unwind info start: " << FUI.SecOffset
+                   << " (" << FUI.NumUnwindRecord << " entries)\n");
+      OS.EmitIntValue(FUI.NumUnwindRecord, 4);
+      OS.EmitIntValue(FUI.SecOffset, 4);
+    }
+    else OS.EmitIntValue(0, 8);
   }
 }
 
@@ -439,14 +791,20 @@
 ///
 /// StkMapRecord[NumRecords] {
 ///   uint64 : PatchPoint ID
+///   uint32 : Index of Function Record
 ///   uint32 : Instruction Offset
 ///   uint16 : Reserved (record flags)
 ///   uint16 : NumLocations
 ///   Location[NumLocations] {
-///     uint8  : Register | Direct | Indirect | Constant | ConstantIndex
-///     uint8  : Size in Bytes
-///     uint16 : Dwarf RegNum
-///     int32  : Offset
+///     uint8 (4 bits) : Register | Direct | Indirect | Constant | ConstantIndex
+///     uint8 (1 bit)  : Padding
+///     uint8 (1 bit)  : Is it a pointer?
+///     uint8 (1 bit)  : Is it an alloca?
+///     uint8 (1 bit)  : Is it a duplicate record for the same live value?
+///     uint8          : Size in Bytes
+///     uint16         : Dwarf RegNum
+///     int32          : Offset
+///     uint32         : Size of pointed-to alloca data
 ///   }
 ///   uint16 : Padding
 ///   uint16 : NumLiveOuts
@@ -455,6 +813,25 @@
 ///     uint8  : Reserved
 ///     uint8  : Size in Bytes
 ///   }
+///   uint16 : Padding
+///   uint16 : NumArchValues
+///   ArchValues[NumArchValues] {
+///     Location {
+///       uint8 (4 bits) : Register | Indirect
+///       uint8 (3 bits) : Padding
+///       uint8 (1 bit)  : Is it a pointer?
+///       uint8          : Size in Bytes
+///       uint16         : Dwarf RegNum
+///       int32          : Offset
+///     }
+///     Value {
+///       uint8_t (4 bits) : Instruction
+///       uint8_t (4 bits) : Register | Direct | Constant
+///       uint8_t          : Size
+///       uint16_t         : Dwarf RegNum
+///       int64_t          : Offset or Constant
+///     }
+///   }
 ///   uint32 : Padding (only if required to align to 8 byte)
 /// }
 ///
@@ -470,23 +847,29 @@
   for (const auto &CSI : CSInfos) {
     const LocationVec &CSLocs = CSI.Locations;
     const LiveOutVec &LiveOuts = CSI.LiveOuts;
+    const ArchValues &Values = CSI.Vals;
 
     // Verify stack map entry. It's better to communicate a problem to the
     // runtime than crash in case of in-process compilation. Currently, we do
     // simple overflow checks, but we may eventually communicate other
     // compilation errors this way.
-    if (CSLocs.size() > UINT16_MAX || LiveOuts.size() > UINT16_MAX) {
+    if (CSLocs.size() > UINT16_MAX || LiveOuts.size() > UINT16_MAX ||
+        Values.size() > UINT16_MAX) {
       OS.EmitIntValue(UINT64_MAX, 8); // Invalid ID.
+      OS.EmitIntValue(UINT32_MAX, 4); // Invalid index.
       OS.EmitValue(CSI.CSOffsetExpr, 4);
       OS.EmitIntValue(0, 2); // Reserved.
       OS.EmitIntValue(0, 2); // 0 locations.
       OS.EmitIntValue(0, 2); // padding.
       OS.EmitIntValue(0, 2); // 0 live-out registers.
+      OS.EmitIntValue(0, 2); // padding.
+      OS.EmitIntValue(0, 2); // 0 arch-specific values.
       OS.EmitIntValue(0, 4); // padding.
       continue;
     }
 
     OS.EmitIntValue(CSI.ID, 8);
+    OS.EmitIntValue(FnStackSize.find(CSI.Func) - FnStackSize.begin(), 4);
     OS.EmitValue(CSI.CSOffsetExpr, 4);
 
     // Reserved for flags.
@@ -494,10 +877,13 @@
     OS.EmitIntValue(CSLocs.size(), 2);
 
     for (const auto &Loc : CSLocs) {
-      OS.EmitIntValue(Loc.Type, 1);
+      uint8_t TypeAndFlags =
+        TYPE_AND_FLAGS(Loc.Type, Loc.Ptr, Loc.Alloca, Loc.Duplicate);
+      OS.EmitIntValue(TypeAndFlags, 1);
       OS.EmitIntValue(Loc.Size, 1);
       OS.EmitIntValue(Loc.Reg, 2);
       OS.EmitIntValue(Loc.Offset, 4);
+      OS.EmitIntValue(Loc.AllocaSize, 4);
     }
 
     // Num live-out registers and padding to align to 4 byte.
@@ -509,6 +895,31 @@
       OS.EmitIntValue(0, 1);
       OS.EmitIntValue(LO.Size, 1);
     }
+
+    // Num arch-specific constants and padding to align to 4 bytes.
+    OS.EmitIntValue(0, 2);
+    OS.EmitIntValue(Values.size(), 2);
+
+    for (const auto &C : Values) {
+      const Location &Loc = C.first;
+      const Operation &Op = C.second;
+
+      uint8_t TypeAndFlags = ARCH_TYPE_AND_FLAGS(Loc.Type, Loc.Ptr);
+      OS.EmitIntValue(TypeAndFlags, 1);
+      OS.EmitIntValue(Loc.Size, 1);
+      OS.EmitIntValue(Loc.Reg, 2);
+      OS.EmitIntValue(Loc.Offset, 4);
+
+      uint8_t OpType = ARCH_OP_TYPE(Op.InstType,
+                                    Op.isGenerated,
+                                    Op.OperandType);
+      OS.EmitIntValue(OpType, 1);
+      OS.EmitIntValue(Op.Size, 1);
+      OS.EmitIntValue(Op.DwarfReg, 2);
+      if(Op.isSymbol) OS.EmitSymbolValue(Op.Symbol, 8);
+      else OS.EmitIntValue(Op.Constant, 8);
+    }
+
     // Emit alignment to 8 byte.
     OS.EmitValueToAlignment(8);
   }
@@ -515,7 +926,7 @@
 }
 
 /// Serialize the stackmap data.
-void StackMaps::serializeToStackMapSection() {
+void StackMaps::serializeToStackMapSection(const UnwindInfo *UI) {
   (void)WSMP;
   // Bail out if there's no stack map data.
   assert((!CSInfos.empty() || (CSInfos.empty() && ConstPool.empty())) &&
@@ -539,7 +950,7 @@
   // Serialize data.
   DEBUG(dbgs() << "********** Stack Map Output **********\n");
   emitStackmapHeader(OS);
-  emitFunctionFrameRecords(OS);
+  emitFunctionFrameRecords(OS, UI);
   emitConstantPoolEntries(OS);
   emitCallsiteEntries(OS);
   OS.AddBlankLine();
Index: lib/CodeGen/StackSlotColoring.cpp
===================================================================
--- lib/CodeGen/StackSlotColoring.cpp	(revision 277823)
+++ lib/CodeGen/StackSlotColoring.cpp	(working copy)
@@ -278,6 +278,7 @@
   SmallVector<int, 16> SlotMapping(NumObjs, -1);
   SmallVector<float, 16> SlotWeights(NumObjs, 0.0);
   SmallVector<SmallVector<int, 4>, 16> RevMap(NumObjs);
+  SmallDenseMap<int, int, 16> SlotChanges;
   BitVector UsedColors(NumObjs);
 
   DEBUG(dbgs() << "Color spill slot intervals:\n");
@@ -292,7 +293,9 @@
     SlotWeights[NewSS] += li->weight;
     UsedColors.set(NewSS);
     Changed |= (SS != NewSS);
+    if(SS != NewSS) SlotChanges[SS] = NewSS;
   }
+  MF.updateSMStackSlotRefs(SlotChanges);
 
   DEBUG(dbgs() << "\nSpill slots after coloring:\n");
   for (unsigned i = 0, e = SSIntervals.size(); i != e; ++i) {
Index: lib/CodeGen/StackTransformMetadata.cpp
===================================================================
--- lib/CodeGen/StackTransformMetadata.cpp	(nonexistent)
+++ lib/CodeGen/StackTransformMetadata.cpp	(working copy)
@@ -0,0 +1,974 @@
+//=== llvm/CodeGen/StackTransformMetadata.cpp - Stack Transformation Metadata ===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file accumulates additional data from machine functions needed to do
+// correct and complete stack transformation.
+//
+// Note: the dataflow analysis in this implementation assumes the ISA does not
+// allow memory-to-memory copies.
+//
+//===----------------------------------------------------------------------===//
+
+#include <queue>
+#include "llvm/CodeGen/LiveIntervalAnalysis.h"
+#include "llvm/CodeGen/LiveStackAnalysis.h"
+#include "llvm/CodeGen/MachineFrameInfo.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/MachineRegisterInfo.h"
+#include "llvm/CodeGen/Passes.h"
+#include "llvm/CodeGen/StackMaps.h"
+#include "llvm/CodeGen/StackTransformTypes.h"
+#include "llvm/CodeGen/VirtRegMap.h"
+#include "llvm/IR/DiagnosticInfo.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/LLVMContext.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/Target/TargetInstrInfo.h"
+#include "llvm/Target/TargetValues.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "stacktransform"
+
+//===----------------------------------------------------------------------===//
+//                          StackTransformMetadata
+//===----------------------------------------------------------------------===//
+//
+// Run analyses over machine functions (before virtual register rewriting) to
+// glean additional information about live values.  This analysis finds
+// duplicate locations for live values (including backing stack slots and other
+// registers) and architecture-specific live values that must be materialized.
+//
+//===----------------------------------------------------------------------===//
+namespace {
+class StackTransformMetadata : public MachineFunctionPass {
+
+  /* Types */
+
+  /// A bundle tying together a stackmap IR instruction, the generated stackmap
+  /// machine instruction and the call machine instruction that caused the
+  /// stackmap to be emitted in the IR, respectively
+  typedef std::tuple<const CallInst *,
+                     const MachineInstr *,
+                     const MachineInstr *> SMInstBundle;
+
+  /// Getters for individual elements of instruction bundles
+  static inline const
+  CallInst *getIRSM(const SMInstBundle &B) { return std::get<0>(B); }
+  static inline const
+  MachineInstr *getMISM(const SMInstBundle &B) { return std::get<1>(B); }
+  static inline const
+  MachineInstr *getMICall(const SMInstBundle &B) { return std::get<2>(B); }
+
+  /// A vector of IR values.  Used when mapping from registers/stack slots to
+  /// IR values.
+  typedef SmallVector<const Value *, 4> ValueVec;
+  typedef std::shared_ptr<ValueVec> ValueVecPtr;
+
+  /// Mapping between virtual registers and IR operands
+  typedef std::pair<unsigned, ValueVecPtr> RegValsPair;
+  typedef std::map<unsigned, ValueVecPtr> RegValsMap;
+
+  /// Mapping between stackmaps and virtual registers referenced by the stackmap
+  typedef std::pair<const MachineInstr *, RegValsMap> SMVregsPair;
+  typedef std::map<const MachineInstr *, RegValsMap> SMVregsMap;
+
+  /// Mapping between stack slots and IR operands
+  typedef std::pair<int, ValueVecPtr> StackValsPair;
+  typedef std::map<int, ValueVecPtr> StackValsMap;
+
+  /// Mapping between stackmaps and stack slots referenced by the stackmap
+  typedef std::pair<const MachineInstr *, StackValsMap> SMStackSlotPair;
+  typedef std::map<const MachineInstr *, StackValsMap> SMStackSlotMap;
+
+  /// A value's spill location
+  class CopyLoc {
+  public:
+    enum Type { NONE, VREG, STACK_LOAD, STACK_STORE };
+    unsigned Vreg;
+    const MachineInstr *Instr;
+    CopyLoc() : Vreg(VirtRegMap::NO_PHYS_REG), Instr(nullptr) {}
+    CopyLoc(unsigned Vreg, const MachineInstr *Instr) :
+      Vreg(Vreg), Instr(Instr) {}
+    virtual CopyLoc *copy() const = 0;
+    virtual ~CopyLoc() {}
+    virtual Type getType() const = 0;
+  };
+  typedef std::shared_ptr<CopyLoc> CopyLocPtr;
+
+  /// A spill to a stack slot
+  class StackCopyLoc : public CopyLoc {
+  public:
+    int StackSlot;
+    StackCopyLoc() : StackSlot(VirtRegMap::NO_STACK_SLOT) {}
+    StackCopyLoc(unsigned Vreg, int StackSlot, const MachineInstr *Instr) :
+      CopyLoc(Vreg, Instr), StackSlot(StackSlot) {}
+    virtual CopyLoc *copy() const = 0;
+    virtual Type getType() const = 0;
+  };
+
+  /// A load from a stack slot
+  class StackLoadLoc : public StackCopyLoc {
+  public:
+    StackLoadLoc() {}
+    StackLoadLoc(unsigned Vreg, int StackSlot, const MachineInstr *Instr) :
+      StackCopyLoc(Vreg, StackSlot, Instr) {}
+    virtual CopyLoc *copy() const
+    { return new StackLoadLoc(Vreg, StackSlot, Instr); }
+    virtual Type getType() const { return CopyLoc::STACK_LOAD; }
+  };
+
+  /// A store to a stack slot
+  class StackStoreLoc : public StackCopyLoc {
+  public:
+    StackStoreLoc() {}
+    StackStoreLoc(unsigned Vreg, int StackSlot, const MachineInstr *Instr) :
+      StackCopyLoc(Vreg, StackSlot, Instr) {}
+    virtual CopyLoc *copy() const
+    { return new StackStoreLoc(Vreg, StackSlot, Instr); }
+    virtual Type getType() const { return CopyLoc::STACK_STORE; }
+  };
+
+  /// A spill to another register
+  class RegCopyLoc : public CopyLoc {
+  public:
+    unsigned SrcVreg;
+    RegCopyLoc() : SrcVreg(VirtRegMap::NO_PHYS_REG) {}
+    RegCopyLoc(unsigned DefVreg, unsigned SrcVreg, const MachineInstr *Instr) :
+      CopyLoc(DefVreg, Instr), SrcVreg(SrcVreg) {}
+    virtual CopyLoc *copy() const
+    { return new RegCopyLoc(Vreg, SrcVreg, Instr); }
+    virtual Type getType() const { return CopyLoc::VREG; }
+  };
+
+  /// Mapping between stack slots and copy locations (e.g., load from or store
+  /// to the stack slot)
+  typedef SmallVector<CopyLocPtr, 8> CopyLocVec;
+  typedef std::shared_ptr<CopyLocVec> CopyLocVecPtr;
+  typedef std::pair<int, CopyLocVecPtr> StackSlotCopyPair;
+  typedef std::map<int, CopyLocVecPtr> StackSlotCopies;
+
+  /* Data */
+
+  /// LLVM-provided analysis & metadata
+  MachineFunction *MF;
+  const MachineFrameInfo *MFI;
+  const MachineRegisterInfo *MRI;
+  const TargetInstrInfo *TII;
+  const TargetRegisterInfo *TRI;
+  const TargetValues *TVG;
+  const LiveIntervals *LI;
+  const LiveStacks *LS;
+  const SlotIndexes *Indexes;
+  const VirtRegMap *VRM;
+
+  /// Stackmap/call instructions, mapping of virtual registers & stack slots to
+  /// IR values, list of instructions that copy to/from the stack
+  SmallVector<SMInstBundle, 32> SM;
+  SMVregsMap SMVregs;
+  SMStackSlotMap SMStackSlots;
+  StackSlotCopies SSUses;
+
+  /* Functions */
+
+  // Reset the analysis for a new function
+  void reset() {
+    SM.clear();
+    SMVregs.clear();
+    SMStackSlots.clear();
+    SSUses.clear();
+  }
+
+  /// Print information about a virtual register and it's associated IR value
+  void dumpReg(unsigned Reg, const Value *IRVal) const;
+
+  /// Print information about a stack slot and it's associated IR value
+  void dumpStackSlot(int SS, const Value *IRVal) const;
+
+  /// Analyze a machine instruction to see if a value is getting copied from
+  /// another location such as a stack slot or register.
+  CopyLocPtr getCopyLocation(const MachineInstr *MI) const;
+
+  /// Gather stackmap machine instructions, the IR instructions which generated
+  /// the stackmaps, and their associated call machine instructions.  Also,
+  /// find copies to/from stack slots (since there's no other mechanism to
+  /// find/traverse them).
+  void findStackmapsAndStackSlotCopies();
+
+  /// Find all virtual register/stack slot operands in a stackmap and collect
+  /// virtual register/stack slot <-> IR value mappings
+  void mapOpsToIR(const CallInst *IRSM, const MachineInstr *MISM);
+
+  /// Is a virtual register live across the machine instruction?
+  /// Note: returns false if the MI is the last instruction for which the
+  /// virtual register is alive
+  bool isVregLiveAcrossInstr(unsigned Vreg, const MachineInstr *MI) const;
+
+  /// Is a stack slot live across the machine instruction?
+  /// Note: returns false if the MI is the last instruction for which the stack
+  /// slot is alive
+  bool isSSLiveAcrossInstr(int SS, const MachineInstr *MI) const;
+
+  /// Add duplicate location information for a virtual register.  Return true
+  /// if metadata was added, or false if the virtual register is not live
+  /// across the call instruction/stackmap.
+  bool addVregMetadata(unsigned Vreg,
+                       ValueVecPtr IRVals,
+                       const SMInstBundle &SM);
+
+  /// Add duplicate location information for a stack slot.  Return true if
+  /// metadata was added, or false if the stack slot is not live across the
+  /// call instruction/stackmap.
+  bool addSSMetadata(int SS, ValueVecPtr IRVals, const SMInstBundle &SM);
+
+  /// Search stack slot copies for additional virtual registers which are live
+  /// across the stackmap.  Will check to see if the copy instructions have
+  /// already been visited, and if appropriate, will add virtual registers to
+  /// work queue.
+  void inline
+  searchStackSlotCopies(int SS,
+                        ValueVecPtr IRVals,
+                        const SMInstBundle &SM,
+                        SmallPtrSet<const MachineInstr *, 32> &Visited,
+                        std::queue<unsigned> &work);
+
+  /// Find all alternate locations for virtual registers in a stackmap, and add
+  /// them to the metadata to be generated.
+  void findAlternateVregLocs(const SMInstBundle &SM);
+
+  /// Find stackmap operands that have been spilled to alternate locations
+  void findAlternateOpLocs();
+
+  /// Ensure virtual registers used to generate architecture-specific values
+  /// are handled by the stackmap & convert to physical registers
+  void sanitizeVregs(MachineLiveValPtr &LV, const MachineInstr *SM) const;
+
+  /// Find architecture-specific live values added by the backend
+  void findArchSpecificLiveVals();
+
+  /// Warn about unhandled registers & stack slots
+  void warnUnhandled() const;
+
+public:
+  static char ID;
+  static const std::string SMName;
+
+  StackTransformMetadata() : MachineFunctionPass(ID) {}
+
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const override;
+
+  virtual bool runOnMachineFunction(MachineFunction&) override;
+
+};
+
+} // end anonymous namespace
+
+char &llvm::StackTransformMetadataID = StackTransformMetadata::ID;
+const std::string StackTransformMetadata::SMName("llvm.experimental.stackmap");
+
+INITIALIZE_PASS_BEGIN(StackTransformMetadata, "stacktransformmetadata",
+  "Gather stack transformation metadata", false, true)
+INITIALIZE_PASS_DEPENDENCY(SlotIndexes)
+INITIALIZE_PASS_DEPENDENCY(LiveIntervals)
+INITIALIZE_PASS_DEPENDENCY(LiveStacks)
+INITIALIZE_PASS_DEPENDENCY(VirtRegMap)
+INITIALIZE_PASS_END(StackTransformMetadata, "stacktransformmetadata",
+  "Gather stack transformation metadata", false, true)
+
+char StackTransformMetadata::ID = 0;
+
+void StackTransformMetadata::getAnalysisUsage(AnalysisUsage &AU) const {
+  AU.setPreservesAll();
+  AU.addRequired<LiveIntervals>();
+  AU.addRequired<LiveStacks>();
+  AU.addRequired<SlotIndexes>();
+  AU.addRequired<VirtRegMap>();
+  MachineFunctionPass::getAnalysisUsage(AU);
+}
+
+bool StackTransformMetadata::runOnMachineFunction(MachineFunction &fn) {
+  if(fn.getFrameInfo()->hasStackMap()) {
+    MF = &fn;
+    MFI = MF->getFrameInfo();
+    MRI = &MF->getRegInfo();
+    TII = MF->getSubtarget().getInstrInfo();
+    TRI = MF->getSubtarget().getRegisterInfo();
+    TVG = MF->getSubtarget().getValues();
+    Indexes = &getAnalysis<SlotIndexes>();
+    LI = &getAnalysis<LiveIntervals>();
+    LS = &getAnalysis<LiveStacks>();
+    VRM = &getAnalysis<VirtRegMap>();
+    reset();
+
+    DEBUG(
+      dbgs() << "\n********** STACK TRANSFORMATION METADATA **********\n"
+             << "********** Function: " << MF->getName() << "\n";
+      VRM->dump();
+    );
+
+    findStackmapsAndStackSlotCopies();
+    findAlternateOpLocs();
+    findArchSpecificLiveVals();
+    warnUnhandled();
+  }
+
+  return false;
+}
+
+/// Print information about a virtual register and it's associated IR value
+void StackTransformMetadata::dumpReg(unsigned Reg, const Value *IRVal) const {
+  IRVal->printAsOperand(dbgs());
+  if(TargetRegisterInfo::isPhysicalRegister(Reg))
+    dbgs() << ": in register " << PrintReg(Reg, TRI);
+  else {
+    assert(VRM->hasPhys(Reg) && "Invalid virtual register");
+    unsigned Phys = VRM->getPhys(Reg);
+    dbgs() << ": in register " << PrintReg(Phys, TRI)
+           << " (vreg " << TargetRegisterInfo::virtReg2Index(Reg) << ")";
+  }
+  dbgs() << "\n";
+}
+
+/// Print information about a stack slot and it's associated IR value
+void StackTransformMetadata::dumpStackSlot(int SS, const Value *IRVal) const {
+  assert(!MFI->isDeadObjectIndex(SS) && "Invalid stack slot");
+  IRVal->printAsOperand(dbgs());
+  dbgs() << ": in stack slot " << SS << " (size: " << MFI->getObjectSize(SS)
+         << ")\n";
+}
+
+/// Analyze a machine instruction to see if a value is getting copied from
+/// another location such as a stack slot or register.
+StackTransformMetadata::CopyLocPtr
+StackTransformMetadata::getCopyLocation(const MachineInstr *MI) const {
+  unsigned SrcVreg = 0;
+  unsigned DefVreg = 0;
+  int SS;
+
+  assert(MI && "Invalid machine instruction");
+
+  // Is it a copy from another register?
+  if(MI->isCopyLike()) {
+    for(unsigned i = 0, e = MI->getNumOperands(); i != e; i++) {
+      const MachineOperand &MO = MI->getOperand(i);
+      if(MO.isReg()) {
+        if(MO.isDef()) DefVreg = MO.getReg();
+        else SrcVreg = MO.getReg();
+      }
+    }
+
+    // TODO does it have to be a virtual register or can it be a physical one?
+    // Liveness analysis seems to apply only to virtual registers.
+    if(TargetRegisterInfo::isVirtualRegister(SrcVreg) &&
+       TargetRegisterInfo::isVirtualRegister(DefVreg))
+      return CopyLocPtr(new RegCopyLoc(DefVreg, SrcVreg, MI));
+  }
+
+  // Is it a load from the stack?
+  if((DefVreg = TII->isLoadFromStackSlot(MI, SS)) &&
+     TargetRegisterInfo::isVirtualRegister(DefVreg))
+    return CopyLocPtr(new StackLoadLoc(DefVreg, SS, MI));
+
+  // Is it a store to the stack?
+  if((SrcVreg = TII->isStoreToStackSlot(MI, SS)) &&
+     TargetRegisterInfo::isVirtualRegister(SrcVreg))
+    return CopyLocPtr(new StackStoreLoc(SrcVreg, SS, MI));
+
+  // A non-copylike instruction
+  return CopyLocPtr(nullptr);
+}
+
+/// Gather stackmap machine instructions, the IR instructions which generated
+/// the stackmaps, and their associated call machine instructions.  Also,
+/// find copies to/from stack slots (since there's no other mechanism to
+/// find/traverse them).
+void StackTransformMetadata::findStackmapsAndStackSlotCopies() {
+  for(auto MBB = MF->begin(), MBBE = MF->end(); MBB != MBBE; MBB++) {
+    for(auto MI = MBB->instr_begin(), ME = MBB->instr_end(); MI != ME; MI++) {
+      if(MI->getOpcode() == TargetOpcode::STACKMAP) {
+        // Find the stackmap IR instruction
+        assert(MI->getOperand(0).isImm() && "Invalid stackmap ID");
+        int64_t ID = MI->getOperand(0).getImm();
+        const BasicBlock *BB = MI->getParent()->getBasicBlock();
+        const CallInst *IRSM = nullptr;
+        for(auto I = BB->begin(), IE = BB->end(); I != IE; I++)
+        {
+          const IntrinsicInst *II;
+          if((II = dyn_cast<IntrinsicInst>(&*I)) &&
+             II->getCalledFunction()->getName() == SMName &&
+             cast<ConstantInt>(II->getArgOperand(0))->getSExtValue() == ID) {
+            IRSM = cast<CallInst>(II);
+            break;
+          }
+        }
+        assert(IRSM && "Could not find stackmap IR instruction");
+
+        // Find the call instruction
+        const MachineInstr *MCI = MI->getPrevNode();
+        while(MCI != nullptr) {
+          if(MCI->isCall()) {
+            if(MCI->getOpcode() == TargetOpcode::STACKMAP)
+              MCI = nullptr;
+            break;
+          }
+          MCI = MCI->getPrevNode();
+        }
+
+        if(!MCI) {
+          DEBUG(dbgs() << "NOTE: stackmap " << ID << " ";
+                IRSM->print(dbgs());
+                dbgs() << ": could not find associated call instruction "
+                          "(lowered to a native instruction?)\n");
+          continue;
+        }
+
+        SM.push_back(SMInstBundle(IRSM, &*MI, MCI));
+      }
+      else {
+        // See if instruction copies to/from stack slot
+        StackSlotCopies::iterator it;
+        CopyLocPtr loc;
+
+        if(!(loc = getCopyLocation(&*MI))) continue;
+        enum CopyLoc::Type type = loc->getType();
+        if(type == CopyLoc::STACK_LOAD || type == CopyLoc::STACK_STORE) {
+          StackCopyLoc *SCL = (StackCopyLoc *)loc.get();
+          if((it = SSUses.find(SCL->StackSlot)) == SSUses.end()) {
+            StackSlotCopyPair ins(SCL->StackSlot,
+                                  CopyLocVecPtr(new CopyLocVec));
+            it = SSUses.insert(std::move(ins)).first;
+          }
+          it->second->push_back(loc);
+        }
+      }
+    }
+  }
+
+  DEBUG(
+    dbgs() << "\n*** Stack slot copies ***\n\n";
+    for(auto SC = SSUses.begin(), SCe = SSUses.end(); SC != SCe; SC++) {
+      dbgs() << "Stack slot " << SC->first << ":\n";
+      for(size_t i = 0, e = SC->second->size(); i < e; i++) {
+        (*SC->second)[i]->Instr->dump();
+      }
+    }
+  );
+}
+
+/// Find all virtual register/stack slot operands in a stackmap and collect
+/// virtual register/stack slot <-> IR value mappings
+void StackTransformMetadata::mapOpsToIR(const CallInst *IRSM,
+                                        const MachineInstr *MISM) {
+  RegValsMap::iterator VregIt;
+  StackValsMap::iterator SSIt;
+  MachineInstr::const_mop_iterator MOit;
+  CallInst::const_op_iterator IRit;
+
+  // Initialize new storage location/IR map objects (i.e., for virtual
+  // registers & stack slots) for the stackmap
+  SMVregs.insert(SMVregsPair(MISM, RegValsMap()));
+  SMStackSlots.insert(SMStackSlotPair(MISM, StackValsMap()));
+
+  // Loop over all operands
+  for(MOit = std::next(MISM->operands_begin(), 2),
+      IRit = std::next(IRSM->op_begin(), 2);
+      MOit != MISM->operands_end() && IRit != (IRSM->op_end() - 1);
+      MOit++, IRit++) {
+    if(MOit->isImm()) { // Map IR values to stack slots
+      int FrameIdx = INT32_MAX;
+      const Value *IRVal = IRit->get();
+
+      switch(MOit->getImm()) {
+      case StackMaps::DirectMemRefOp:
+        MOit++;
+        assert(MOit->isFI() && "Invalid operand type");
+        FrameIdx = MOit->getIndex();
+        MOit++;
+        break;
+      case StackMaps::IndirectMemRefOp:
+        MOit++; MOit++;
+        assert(MOit->isFI() && "Invalid operand type");
+        FrameIdx = MOit->getIndex();
+        MOit++;
+        break;
+      case StackMaps::ConstantOp: MOit++; continue;
+      default: llvm_unreachable("Unrecognized stackmap operand type"); break;
+      }
+
+      assert(IRVal && "Invalid stackmap IR operand");
+      assert(MFI->getObjectIndexBegin() <= FrameIdx &&
+             FrameIdx <= MFI->getObjectIndexEnd() && "Invalid frame index");
+      assert(!MFI->isDeadObjectIndex(FrameIdx) && "Dead frame index");
+      DEBUG(dumpStackSlot(FrameIdx, IRVal););
+
+      // Update the list of IR values mapped to the stack slot (multiple IR
+      // values may be mapped to a single stack slot).
+      SSIt = SMStackSlots[MISM].find(FrameIdx);
+      if(SSIt == SMStackSlots[MISM].end()) {
+        StackValsPair OpMap(FrameIdx, ValueVecPtr(new ValueVec));
+        SSIt = SMStackSlots[MISM].insert(std::move(OpMap)).first;
+      }
+      SSIt->second->push_back(IRVal);
+    }
+    else if(MOit->isReg()) { // Map IR values to virtual registers
+      const Value *IRVal = IRit->get();
+      unsigned Reg = MOit->getReg();
+
+      assert(IRVal && "Invalid stackmap IR operand");
+      assert(TargetRegisterInfo::isVirtualRegister(Reg) &&
+             "Should not have been converted to physical registers yet");
+      DEBUG(dumpReg(Reg, IRVal););
+
+      // Update the list of IR values mapped to the virtual register (multiple
+      // IR values may be mapped to a single virtual register).
+      VregIt = SMVregs[MISM].find(Reg);
+      if(VregIt == SMVregs[MISM].end()) {
+        RegValsPair OpMap(Reg, ValueVecPtr(new ValueVec));
+        VregIt = SMVregs[MISM].insert(std::move(OpMap)).first;
+      }
+      VregIt->second->push_back(IRVal);
+    } else {
+      llvm_unreachable("Unrecognized stackmap operand type.");
+    }
+  }
+  assert(IRit == (IRSM->op_end() - 1) && "Did not search all stackmap operands");
+}
+
+/// Is a virtual register live across the machine instruction?
+/// Note: returns false if the MI is the last instruction for which the virtual
+/// register is alive
+bool
+StackTransformMetadata::isVregLiveAcrossInstr(unsigned Vreg,
+                                              const MachineInstr *MI) const {
+  assert(TRI->isVirtualRegister(Vreg) && "Invalid virtual register");
+
+  if(LI->hasInterval(Vreg)) {
+    const LiveInterval &TheLI = LI->getInterval(Vreg);
+    SlotIndex InstrIdx = Indexes->getInstructionIndex(MI);
+    LiveInterval::const_iterator Seg = TheLI.find(InstrIdx);
+    if(Seg != TheLI.end() && Seg->contains(InstrIdx) &&
+       InstrIdx.getInstrDistance(Seg->end) != 0)
+      return true;
+  }
+  return false;
+}
+
+/// Is a stack slot live across the machine instruction?
+/// Note: returns false if the MI is the last instruction for which the stack
+/// slot is alive
+bool
+StackTransformMetadata::isSSLiveAcrossInstr(int SS,
+                                            const MachineInstr *MI) const {
+  if(LS->hasInterval(SS)) {
+    const LiveInterval &TheLI = LS->getInterval(SS);
+    SlotIndex InstrIdx = Indexes->getInstructionIndex(MI);
+    LiveInterval::const_iterator Seg = TheLI.find(InstrIdx);
+    if(Seg != TheLI.end() && Seg->contains(InstrIdx) &&
+       InstrIdx.getInstrDistance(Seg->end) != 0)
+      return true;
+  }
+  return false;
+}
+
+/// Add duplicate location information for a virtual register.
+bool StackTransformMetadata::addVregMetadata(unsigned Vreg,
+                                             ValueVecPtr IRVals,
+                                             const SMInstBundle &SM) {
+  const CallInst *IRSM = getIRSM(SM);
+  const MachineInstr *MISM = getMISM(SM);
+  const MachineInstr *MICall = getMICall(SM);
+  RegValsMap &Vregs = SMVregs[MISM];
+
+  assert(TargetRegisterInfo::isVirtualRegister(Vreg) && VRM->hasPhys(Vreg) &&
+         "Cannot add virtual register metadata -- invalid virtual register");
+
+  if(Vregs.find(Vreg) == Vregs.end() &&
+     (isVregLiveAcrossInstr(Vreg, MISM) || isVregLiveAcrossInstr(Vreg, MICall)))
+  {
+    unsigned Phys = VRM->getPhys(Vreg);
+    for(size_t sz = 0; sz < IRVals->size(); sz++) {
+      DEBUG(dumpReg(Vreg, (*IRVals)[sz]););
+      MF->addSMOpLocation(IRSM, (*IRVals)[sz], MachineLiveReg(Phys));
+    }
+    Vregs[Vreg] = IRVals;
+    return true;
+  }
+  else return false;
+}
+
+/// Add duplicate location information for a stack slot.
+bool StackTransformMetadata::addSSMetadata(int SS,
+                                           ValueVecPtr IRVals,
+                                           const SMInstBundle &SM) {
+  const CallInst *IRSM = getIRSM(SM);
+  const MachineInstr *MISM = getMISM(SM);
+  const MachineInstr *MICall = getMICall(SM);
+  StackValsMap &SSlots = SMStackSlots[MISM];
+
+  assert(!MFI->isDeadObjectIndex(SS) &&
+         "Cannot add stack slot metadata -- invalid stack slot");
+
+  if(SSlots.find(SS) == SSlots.end() &&
+     (isSSLiveAcrossInstr(SS, MISM) || isSSLiveAcrossInstr(SS, MICall)))
+  {
+    for(size_t sz = 0; sz < IRVals->size(); sz++) {
+      DEBUG(dumpStackSlot(SS, (*IRVals)[sz]););
+      MF->addSMOpLocation(IRSM, (*IRVals)[sz], MachineLiveStackSlot(SS));
+    }
+    SSlots[SS] = IRVals;
+    return true;
+  }
+  else return false;
+}
+
+/// Search stack slot copies for additional virtual registers which are live
+/// across the stackmap.  Will check to see if the copy instructions have
+/// already been visited, and if appropriate, will add virtual registers to
+/// work queue.
+void inline
+StackTransformMetadata::searchStackSlotCopies(int SS,
+                                       ValueVecPtr IRVals,
+                                       const SMInstBundle &SM,
+                                       SmallPtrSet<const MachineInstr *, 32> &Visited,
+                                       std::queue<unsigned> &work) {
+  StackSlotCopies::const_iterator Copies;
+  CopyLocVecPtr CL;
+  CopyLocVec::const_iterator Copy, CE;
+
+  if((Copies = SSUses.find(SS)) != SSUses.end()) {
+    CL = Copies->second;
+    for(Copy = CL->begin(), CE = CL->end(); Copy != CE; Copy++) {
+      unsigned Vreg = (*Copy)->Vreg;
+      const MachineInstr *Instr = (*Copy)->Instr;
+
+      if(!Visited.count(Instr)) {
+        addVregMetadata(Vreg, IRVals, SM);
+        Visited.insert(Instr);
+        work.push(Vreg);
+      }
+    }
+  }
+}
+
+/// Find all alternate locations for virtual registers in a stackmap, and add
+/// them to the metadata to be generated.
+void
+StackTransformMetadata::findAlternateVregLocs(const SMInstBundle &SM) {
+  RegValsMap &Vregs = SMVregs[getMISM(SM)];
+  std::queue<unsigned> work;
+  SmallPtrSet<const MachineInstr *, 32> Visited;
+  StackCopyLoc *SCL;
+  RegCopyLoc *RCL;
+
+  DEBUG(dbgs() << "\nDuplicate operand locations:\n\n";);
+
+  // Iterate over all vregs in the stackmap
+  for(RegValsMap::iterator it = Vregs.begin(), end = Vregs.end();
+      it != end; it++) {
+    unsigned origVreg = it->first;
+    ValueVecPtr IRVals = it->second;
+    Visited.clear();
+
+    // Follow data flow to search for all duplicate locations, including stack
+    // slots and other registers.  It's a duplicate if the following are true:
+    //
+    //   1. It's a copy-like instruction, e.g., a register move or a load
+    //      from/store to stack slot
+    //   2. The alternate location (virtual register/stack slot) is live across
+    //      either the machine call instruction or the stackmap
+    //
+    // Note: we *must* search exhaustively (i.e., across copies from registers
+    // that are *not* live across the call) because the following can happen:
+    //
+    //   STORE vreg0, <fi#0>
+    //   ...
+    //   COPY vreg0, vreg1
+    //   ...
+    //   STACKMAP 0, 0, vreg1
+    //
+    // Here, vreg0 is *not* live across the stackmap, but <fi#0> *is*
+    //
+    work.push(origVreg);
+    while(!work.empty()) {
+      unsigned cur, vreg;
+      int ss;
+
+      // Walk over definitions
+      cur = work.front();
+      work.pop();
+      for(auto instr = MRI->def_instr_begin(cur), ei = MRI->def_instr_end();
+          instr != ei;
+          instr++) {
+        if(Visited.count(&*instr)) continue;
+        CopyLocPtr loc = getCopyLocation(&*instr);
+        if(!loc) continue;
+
+        switch(loc->getType()) {
+        case CopyLoc::VREG:
+          RCL = (RegCopyLoc *)loc.get();
+          vreg = RCL->SrcVreg;
+          addVregMetadata(vreg, IRVals, SM);
+          Visited.insert(&*instr);
+          work.push(vreg);
+          break;
+        case CopyLoc::STACK_LOAD:
+          SCL = (StackCopyLoc *)loc.get();
+          ss = SCL->StackSlot;
+          if(addSSMetadata(ss, IRVals, SM)) {
+            Visited.insert(&*instr);
+            searchStackSlotCopies(ss, IRVals, SM, Visited, work);
+          }
+          break;
+        default: llvm_unreachable("Unknown/invalid location type"); break;
+        }
+      }
+
+      // Walk over uses
+      for(auto instr = MRI->use_instr_begin(cur), ei = MRI->use_instr_end();
+          instr != ei; instr++) {
+        if(Visited.count(&*instr)) continue;
+        CopyLocPtr loc = getCopyLocation(&*instr);
+        if(!loc) continue;
+
+        switch(loc->getType()) {
+        case CopyLoc::VREG:
+          RCL = (RegCopyLoc *)loc.get();
+          vreg = RCL->Vreg;
+          addVregMetadata(vreg, IRVals, SM);
+          Visited.insert(&*instr);
+          work.push(vreg);
+          break;
+        case CopyLoc::STACK_STORE:
+          SCL = (StackCopyLoc *)loc.get();
+          ss = SCL->StackSlot;
+          if(addSSMetadata(ss, IRVals, SM)) {
+            Visited.insert(&*instr);
+            searchStackSlotCopies(ss, IRVals, SM, Visited, work);
+          }
+          break;
+        default: llvm_unreachable("Unknown/invalid location type"); break;
+        }
+      }
+    }
+  }
+}
+
+/// Find alternate storage locations for stackmap operands
+void StackTransformMetadata::findAlternateOpLocs() {
+  RegValsMap::iterator vregIt, vregEnd;
+
+  for(auto S = SM.begin(), SE = SM.end(); S != SE; S++) {
+    const CallInst *IRSM = getIRSM(*S);
+    const MachineInstr *MISM = getMISM(*S);
+
+    DEBUG(
+      dbgs() << "\nStackmap " << MISM->getOperand(0).getImm() << ":\n";
+      MISM->dump();
+      dbgs() << "\n";
+    );
+
+    // Get all virtual register/stack slot operands & their associated IR
+    // values
+    mapOpsToIR(IRSM, MISM);
+
+    // Find alternate locations for vregs in stack map.  Note we don't need to
+    // find alternate stack slot locations, as allocas *should* already be in
+    // the stackmap, so the remaining stack slots are spilled registers (which
+    // are covered here).
+    findAlternateVregLocs(*S);
+  }
+}
+
+/// Ensure virtual registers used to generate architecture-specific values are
+/// handled by the stackmap & convert to physical registers
+void StackTransformMetadata::sanitizeVregs(MachineLiveValPtr &LV,
+                                           const MachineInstr *SM) const {
+  if(!LV) return;
+  if(LV->isGenerated()) {
+    MachineGeneratedVal *MGV = (MachineGeneratedVal *)LV.get();
+    const ValueGenInstList &Inst = MGV->getInstructions();
+    for(size_t i = 0, num = Inst.size(); i < num; i++) {
+      if(Inst[i]->opType() == ValueGenInst::OpType::Register) {
+        RegInstructionBase *RI = (RegInstructionBase *)Inst[i].get();
+        if(!TRI->isVirtualRegister(RI->getReg())) {
+          if(RI->getReg() == TRI->getFrameRegister(*MF)) continue;
+          // TODO walk through stackmap and see if physical register in
+          // instruction is contained in stackmap
+          LV.reset(nullptr);
+          return;
+        }
+        else if(!SMVregs.at(SM).count(RI->getReg())) {
+          DEBUG(dbgs() << "WARNING: vreg "
+                       << TargetRegisterInfo::virtReg2Index(RI->getReg())
+                       << " used to generate value not handled in stackmap\n");
+          LV.reset(nullptr);
+          return;
+        }
+        else {
+          assert(VRM->hasPhys(RI->getReg()) && "Invalid virtual register");
+          RI->setReg(VRM->getPhys(RI->getReg()));
+        }
+      }
+    }
+  }
+}
+
+/// Find architecture-specific live values added by the backend
+void StackTransformMetadata::findArchSpecificLiveVals() {
+  DEBUG(dbgs() << "\n*** Finding architecture-specific live values ***\n\n";);
+
+  for(auto S = SM.begin(), SE = SM.end(); S != SE; S++)
+  {
+    const MachineInstr *MISM = getMISM(*S);
+    const MachineInstr *MICall = getMICall(*S);
+    const CallInst *IRSM = getIRSM(*S);
+    RegValsMap &CurVregs = SMVregs[MISM];
+    StackValsMap &CurSS = SMStackSlots[MISM];
+
+    DEBUG(
+      MISM->dump();
+      dbgs() << "  -> Call instruction SlotIndex ";
+      Indexes->getInstructionIndex(MICall).print(dbgs());
+      dbgs() << ", searching vregs 0 -> " << MRI->getNumVirtRegs()
+             << " and stack slots " << MFI->getObjectIndexBegin() << " -> "
+             << MFI->getObjectIndexEnd() << "\n";
+    );
+
+    // Include any mandatory architecture-specific live values
+    TVG->addRequiredArchLiveValues(MF, MISM, IRSM);
+
+    // Search for virtual registers not handled by the stackmap.  Registers
+    // spilled to the stack should have been converted to frame index
+    // references by now.
+    for(unsigned i = 0, numVregs = MRI->getNumVirtRegs(); i < numVregs; i++) {
+      unsigned Vreg = TargetRegisterInfo::index2VirtReg(i);
+      MachineLiveValPtr MLV;
+      MachineLiveReg MLR(0);
+
+      if(VRM->hasPhys(Vreg) && isVregLiveAcrossInstr(Vreg, MICall) &&
+         CurVregs.find(Vreg) == CurVregs.end()) {
+        DEBUG(dbgs() << "    + vreg" << i
+                     << " is live in register but not in stackmap\n";);
+
+        if(!MRI->hasOneDef(Vreg)) {
+          DEBUG(
+            dbgs() << "WARNING: multiple definitions for virtual "
+                      "register, missed in live-value analysis?\n";
+            for(auto d = MRI->def_instr_begin(Vreg), e = MRI->def_instr_end();
+                d != e; d++)
+              d->dump();
+          );
+          continue;
+        }
+
+        MLV = TVG->getMachineValue(&*MRI->def_instr_begin(Vreg));
+        sanitizeVregs(MLV, MISM);
+        if(MLV) {
+          DEBUG(dbgs() << "      Defining instruction: ";
+                MLV->getDefiningInst()->print(dbgs());
+                dbgs() << "      Value: " << MLV->toString() << "\n");
+
+          MLR.setReg(VRM->getPhys(Vreg));
+          MF->addSMArchSpecificLocation(IRSM, MLR, *MLV);
+          CurVregs.insert(RegValsPair(Vreg, ValueVecPtr(nullptr)));
+        }
+        else {
+          DEBUG(dbgs() << "      Unhandled defining instruction: ";
+                MRI->def_instr_begin(Vreg)->print(dbgs());
+                dbgs() << "\n");
+        }
+      }
+    }
+
+    // Search for stack slots not handled by the stackmap
+    // TODO handle function arguments on the stack (negative stack slots)
+    for(int SS = MFI->getObjectIndexBegin(), e = MFI->getObjectIndexEnd();
+        SS < e; SS++) {
+      if(!MFI->isDeadObjectIndex(SS) &&
+         isSSLiveAcrossInstr(SS, MICall) && CurSS.find(SS) == CurSS.end()) {
+        DEBUG(dbgs() << "    + stack slot " << SS
+                     << " is live but not in stackmap\n";);
+        // TODO add arch-specific stack slot information to machine function
+      }
+    }
+
+    DEBUG(dbgs() << "\n";);
+  }
+}
+
+/// Find IR call instruction which generated the stackmap
+static inline const CallInst *findCalledFunc(const llvm::CallInst *IRSM) {
+  const Instruction *Func = IRSM->getPrevNode();
+  while(Func && !isa<CallInst>(Func)) Func = Func->getPrevNode();
+  return dyn_cast<CallInst>(Func);
+}
+
+/// Display a warning about unhandled values
+static inline void displayWarning(std::string &Msg,
+                                  const CallInst *CI,
+                                  const Function *F) {
+  assert(CI && "Invalid arguments");
+
+  // Note: it may be possible for us to not have a called function, for example
+  // if we call a function using a function pointer
+  const Function *CurF = CI->getParent()->getParent();
+  const std::string &Triple = CurF->getParent()->getTargetTriple();
+  Msg = "(" + Triple + ") " + Msg;
+  if(F && F->hasName()) {
+    Msg += " across call to ";
+    Msg += F->getName();
+  }
+  DiagnosticInfoOptimizationFailure DI(*CurF, CI->getDebugLoc(), Msg);
+  CurF->getContext().diagnose(DI);
+}
+
+/// Warn about unhandled registers & stack slots
+void StackTransformMetadata::warnUnhandled() const {
+  std::string Msg;
+  const CallInst *IRCall;
+  const Function *CalledFunc;
+
+  for(auto S = SM.begin(), SE = SM.end(); S != SE; S++)
+  {
+    const MachineInstr *MISM = getMISM(*S);
+    const MachineInstr *MICall = getMICall(*S);
+    const RegValsMap &CurVregs = SMVregs.at(MISM);
+    const StackValsMap &CurSS = SMStackSlots.at(MISM);
+    IRCall = findCalledFunc(getIRSM(*S));
+    CalledFunc = IRCall->getCalledFunction();
+    assert(IRCall && "No call instruction for stackmap");
+
+    // Search for virtual registers not handled by the stackmap
+    for(unsigned i = 0; i < MRI->getNumVirtRegs(); i++) {
+      unsigned Vreg = TargetRegisterInfo::index2VirtReg(i);
+
+      // Virtual register allocated to physical register
+      if(VRM->hasPhys(Vreg) && isVregLiveAcrossInstr(Vreg, MICall) &&
+         CurVregs.find(Vreg) == CurVregs.end()) {
+        Msg = "Stack transformation: unhandled register ";
+        Msg += TRI->getName(VRM->getPhys(Vreg));
+        displayWarning(Msg, IRCall, CalledFunc);
+      }
+    }
+
+    // Search for all stack slots not handled by the stackmap
+    for(int SS = MFI->getObjectIndexBegin(), e = MFI->getObjectIndexEnd();
+        SS < e; SS++) {
+      if(!MFI->isDeadObjectIndex(SS) &&
+         isSSLiveAcrossInstr(SS, MICall) && CurSS.find(SS) == CurSS.end()) {
+        Msg = "Stack transformation: unhandled stack slot ";
+        Msg += std::to_string(SS);
+        displayWarning(Msg, IRCall, CalledFunc);
+      }
+    }
+  }
+}
+
Index: lib/CodeGen/StackTransformTypes.cpp
===================================================================
--- lib/CodeGen/StackTransformTypes.cpp	(nonexistent)
+++ lib/CodeGen/StackTransformTypes.cpp	(working copy)
@@ -0,0 +1,256 @@
+//===-- llvm/Target/TargetValueGenerator.cpp - Value Generator --*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/CodeGen/AsmPrinter.h"
+#include "llvm/CodeGen/MachineFrameInfo.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/StackTransformTypes.h"
+#include "llvm/IR/Mangler.h"
+#include "llvm/MC/MCContext.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Target/TargetFrameLowering.h"
+#include "llvm/Target/TargetMachine.h"
+#include "llvm/Target/TargetRegisterInfo.h"
+#include "llvm/Target/TargetSubtargetInfo.h"
+
+#define DEBUG_TYPE "stacktransform"
+
+using namespace llvm;
+
+//===----------------------------------------------------------------------===//
+// Types for generating more complex architecture-specific live values
+//
+
+const char *ValueGenInst::InstTypeStr[] = {
+#define X(type) #type ,
+  VALUE_GEN_INST
+#undef X
+};
+
+const char *ValueGenInst::getInstName(enum InstType Type) {
+  switch(Type) {
+#define X(type) case type: 
+    VALUE_GEN_INST
+#undef X
+    return InstTypeStr[Type];
+    break;
+  default:
+    return "unknown";
+  };
+}
+
+std::string ValueGenInst::getInstNameStr(enum InstType Type) {
+  return std::string(getInstName(Type));
+}
+
+//===----------------------------------------------------------------------===//
+// MachineSymbolRef implementation
+//
+
+bool MachineSymbolRef::operator==(const MachineLiveVal &RHS) const {
+  if(RHS.isSymbolRef()) {
+    const MachineSymbolRef &MSR = (const MachineSymbolRef &)RHS;
+    if(&MSR.Symbol == &Symbol) return true;
+  }
+  return false;
+}
+
+std::string MachineSymbolRef::toString() const {
+  std::string buf = "reference to symbol '";
+  switch(Symbol.getType()) {
+  case MachineOperand::MO_GlobalAddress:
+    buf += Symbol.getGlobal()->getName();
+    buf += "' (global)";
+    break;
+  case MachineOperand::MO_ExternalSymbol:
+    buf += Symbol.getSymbolName();
+    buf += "' (external)";
+    break;
+  case MachineOperand::MO_MCSymbol:
+    buf += Symbol.getMCSymbol()->getName();
+    buf += "' (MC symbol)"; break;
+  default:
+    DEBUG(dbgs() << "Unhandled reference type: ";
+          Symbol.print(dbgs());
+          dbgs() << "\n";);
+    buf += "n/a' (unhandled type)";
+    break;
+  }
+  return buf;
+}
+
+static MCSymbol *GetExternalSymbol(AsmPrinter &AP, StringRef Sym) {
+  SmallString<60> Name;
+  Mangler::getNameWithPrefix(Name, Sym, *AP.TM.getDataLayout());
+  return AP.OutContext.lookupSymbol(Name);
+}
+
+MCSymbol *MachineSymbolRef::getReference(AsmPrinter &AP) const {
+
+  switch(Symbol.getType()) {
+  case MachineOperand::MO_ExternalSymbol:
+    return GetExternalSymbol(AP, Symbol.getSymbolName());
+  case MachineOperand::MO_GlobalAddress:
+    return AP.TM.getSymbol(Symbol.getGlobal(), *AP.Mang);
+  case MachineOperand::MO_MCSymbol:
+    return Symbol.getMCSymbol();
+  default:
+    DEBUG(dbgs() << "Unhandled reference type: ";
+          Symbol.print(dbgs());
+          dbgs() << "\n";);
+    return nullptr;
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// MachineConstPoolRef implementation
+//
+
+bool MachineConstPoolRef::operator==(const MachineLiveVal &RHS) const {
+  if(RHS.isConstPoolRef()) {
+    const MachineConstPoolRef &MCPR = (const MachineConstPoolRef &)RHS;
+    if(MCPR.Index == Index) return true;
+  }
+  return false;
+}
+
+MCSymbol *MachineConstPoolRef::getReference(AsmPrinter &AP) const {
+  MCSymbol *Sym = AP.GetCPISymbol(Index);
+  assert(Sym && "Could not get constant pool reference");
+  return Sym;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineStackObject implementation
+//
+
+bool MachineStackObject::operator==(const MachineLiveVal &RHS) const {
+  if(RHS.isStackObject()) {
+    const MachineStackObject &MSO = (const MachineStackObject &)RHS;
+    if(MSO.Index == Index) return true;
+  }
+  return false;
+}
+
+std::string MachineStackObject::toString() const {
+  std::string buf;
+  if(Load) buf = "load from ";
+  else buf = "reference to ";
+  return buf + "stack slot " + std::to_string(Index);
+}
+
+int
+MachineStackObject::getOffsetFromReg(AsmPrinter &AP, unsigned &BR) const {
+  const TargetFrameLowering *TFL = AP.MF->getSubtarget().getFrameLowering();
+  return TFL->getFrameIndexReference(*AP.MF, Index, BR);
+}
+
+//===----------------------------------------------------------------------===//
+// ReturnAddress implementation
+//
+
+int ReturnAddress::getOffsetFromReg(AsmPrinter &AP, unsigned &BR) const {
+  int Off = AP.MF->getSubtarget().getRegisterInfo()->getReturnAddrLoc(*AP.MF,
+                                                                      BR);
+  if(BR == 0) llvm_unreachable("No saved return address!");
+  return Off;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineImmediate implementation
+//
+
+MachineImmediate::MachineImmediate(unsigned Size,
+                                   uint64_t Value,
+                                   const MachineInstr *DefMI,
+                                   bool Ptr)
+  : MachineLiveVal(DefMI, Ptr), Size(Size), Value(Value)
+{
+  if(Size > 8)
+    llvm_unreachable("Unsupported immediate value size of > 8 bytes");
+}
+
+bool MachineImmediate::operator==(const MachineLiveVal &RHS) const {
+  if(RHS.isImm()) {
+    const MachineImmediate &MI = (const MachineImmediate &)RHS;
+    if(MI.Size == Size && MI.Value == Value) return true;
+  }
+  return false;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineGeneratedVal implementation
+//
+
+bool MachineGeneratedVal::operator==(const MachineLiveVal &RHS) const {
+  if(!RHS.isGenerated()) return false;
+  const MachineGeneratedVal &MGV = (const MachineGeneratedVal &)RHS;
+
+  if(VG.size() != MGV.VG.size()) return false;
+  for(size_t i = 0, num = VG.size(); i < num; i++)
+    if(VG[i] != MGV.VG[i]) return false;
+  return true;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineLiveReg implementation
+//
+
+bool MachineLiveReg::operator==(const MachineLiveLoc &RHS) const {
+  if(RHS.isReg()) {
+    const MachineLiveReg &MLR = (const MachineLiveReg &)RHS;
+    if(MLR.Reg == Reg) return true;
+  }
+  return false;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineLiveStackAddr implementation
+//
+
+bool MachineLiveStackAddr::operator==(const MachineLiveLoc &RHS) const {
+  if(RHS.isStackAddr() && !RHS.isStackSlot()) {
+    const MachineLiveStackAddr &MLSA = (const MachineLiveStackAddr &)RHS;
+    if(Offset != INT32_MAX && MLSA.Offset != INT32_MAX &&
+       Offset == MLSA.Offset && Reg == MLSA.Reg && Size == MLSA.Size)
+      return true;
+  }
+  return false;
+}
+
+//===----------------------------------------------------------------------===//
+// MachineLiveStackSlot implementation
+//
+
+bool MachineLiveStackSlot::operator==(const MachineLiveLoc &RHS) const {
+  if(RHS.isStackSlot()) {
+    const MachineLiveStackSlot &MLSS = (const MachineLiveStackSlot &)RHS;
+    if(MLSS.Index == Index) return true;
+  }
+  return false;
+}
+
+int MachineLiveStackSlot::calcAndGetRegOffset(const AsmPrinter &AP, unsigned &BP) {
+  if(Offset == INT32_MAX) {
+    const TargetFrameLowering *TFL = AP.MF->getSubtarget().getFrameLowering();
+    Offset = TFL->getFrameIndexReference(*AP.MF, Index, Reg);
+  }
+  BP = Reg;
+  return Offset;
+}
+
+unsigned MachineLiveStackSlot::getSize(const AsmPrinter &AP) {
+  if(Size == 0) {
+    const MachineFrameInfo *MFI = AP.MF->getFrameInfo();
+    Size = MFI->getObjectSize(Index);
+  }
+  return Size;
+}
+
Index: lib/CodeGen/UnwindInfo.cpp
===================================================================
--- lib/CodeGen/UnwindInfo.cpp	(nonexistent)
+++ lib/CodeGen/UnwindInfo.cpp	(working copy)
@@ -0,0 +1,181 @@
+//===--------------------------- UnwindInfo.cpp ---------------------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/CodeGen/UnwindInfo.h"
+#include "llvm/MC/MCSectionELF.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/MC/MCObjectFileInfo.h"
+#include "llvm/Target/TargetFrameLowering.h"
+#include "llvm/Target/TargetRegisterInfo.h"
+#include "llvm/Target/TargetSubtargetInfo.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "unwindinfo"
+
+static const char *UIDbg = "Unwind Info: ";
+
+void UnwindInfo::recordUnwindInfo(const MachineFunction &MF) {
+  // We *only* need this information for functions which have a stackmap, as
+  // only those function activations can be unwound during stack
+  // transformation.  This may also be a correctness criterion since we record
+  // offsets from the FBP, and not all functions may have one (stackmaps are
+  // implemented using FBPs, and thus prevent the FP-elimination optimization).
+  if(!MF.getFrameInfo()->hasStackMap()) return;
+
+  const MachineFrameInfo *MFI = MF.getFrameInfo();
+  assert(MFI->isCalleeSavedInfoValid() && "No callee-saved information!");
+
+  // Get this function's saved registers
+  unsigned FrameReg;
+  const TargetFrameLowering *TFL = MF.getSubtarget().getFrameLowering();
+  const std::vector<CalleeSavedInfo> &CSI = MFI->getCalleeSavedInfo();
+
+  // Get DWARF register number and FBP offset using callee saved information
+  const TargetRegisterInfo *TRI = MF.getSubtarget().getRegisterInfo();
+  CalleeSavedRegisters SavedRegs(CSI.size());
+  for(unsigned i = 0; i < CSI.size(); i++) {
+    SavedRegs[i].DwarfReg = TRI->getDwarfRegNum(CSI[i].getReg(), false);
+    SavedRegs[i].Offset =
+      TFL->getFrameIndexReferenceFromFP(MF, CSI[i].getFrameIdx(), FrameReg);
+
+    assert(FrameReg == TRI->getFrameRegister(MF) &&
+           "Invalid register used as offset for unwinding information");
+    DEBUG(dbgs() << "Register " << SavedRegs[i].DwarfReg << " at register "
+                 << FrameReg << " + " << SavedRegs[i].Offset << "\n");
+  }
+
+  // Save the information for when we emit the section
+  const MCSymbol *FuncSym = OutContext.lookupSymbol(MF.getName());
+  assert(FuncSym && "Could not find function symbol");
+  FuncCalleeSaved.insert(FuncCalleePair(FuncSym, std::move(SavedRegs)));
+}
+
+void UnwindInfo::addRegisterUnwindInfo(const MachineFunction &MF,
+                                       uint32_t MachineReg,
+                                       int32_t Offset) {
+  if(!MF.getFrameInfo()->hasStackMap()) return;
+
+  const MCSymbol *FuncSym = OutContext.lookupSymbol(MF.getName());
+  assert(FuncSym && "Could not find function symbol");
+  assert(FuncCalleeSaved.find(FuncSym) != FuncCalleeSaved.end() &&
+         "Cannot add register restore information -- function not found");
+  const TargetRegisterInfo *TRI = MF.getSubtarget().getRegisterInfo();
+  FuncCalleeSaved[FuncSym].push_back(
+    RegOffset(TRI->getDwarfRegNum(MachineReg, false), Offset));
+}
+
+void UnwindInfo::emitUnwindInfo(MCStreamer &OS) {
+  unsigned curIdx = 0;
+  unsigned startIdx;
+  FuncCalleeMap::const_iterator f, e;
+  for(f = FuncCalleeSaved.begin(), e = FuncCalleeSaved.end(); f != e; f++) {
+    const MCSymbol *FuncSym = f->first;
+    const CalleeSavedRegisters &CSR = f->second;
+
+    assert(FuncSym && "Invalid machine function");
+    if(CSR.size() < 2)
+      DEBUG(dbgs() << "WARNING: should have at least 2 registers to restore "
+                               "(return address & saved FBP");
+
+    DEBUG(dbgs() << UIDbg << "Function " << FuncSym->getName()
+                 << " (offset " << curIdx << ", "
+                 << CSR.size() << " entries):\n");
+
+    startIdx = curIdx;
+    CalleeSavedRegisters::const_iterator cs, cse;
+    for(cs = CSR.begin(), cse = CSR.end(); cs != cse; cs++) {
+      assert(cs->DwarfReg < UINT16_MAX &&
+             "Register number too large for resolution");
+      assert(INT16_MIN < cs->Offset && cs->Offset < INT16_MAX &&
+             "Register save offset too large for resolution");
+
+      DEBUG(dbgs() << UIDbg << "  Register " << cs->DwarfReg
+                   << " saved at " << cs->Offset << "\n";);
+
+      OS.EmitIntValue(cs->DwarfReg, 2);
+      OS.EmitIntValue(cs->Offset, 2);
+      curIdx++;
+    }
+    FuncUnwindInfo FUI(startIdx, curIdx - startIdx);
+    FuncUnwindMetadata.insert(FuncUnwindPair(FuncSym, std::move(FUI)));
+  }
+}
+
+void UnwindInfo::emitAddrRangeInfo(MCStreamer &OS) {
+  FuncUnwindMap::const_iterator f, e;
+  for(f = FuncUnwindMetadata.begin(), e = FuncUnwindMetadata.end();
+      f != e;
+      f++) {
+    const MCSymbol *Func = f->first;
+    const FuncUnwindInfo &FUI = f->second;
+    OS.EmitSymbolValue(Func, 8);
+    OS.EmitIntValue(FUI.NumUnwindRecord, 4);
+    OS.EmitIntValue(FUI.SecOffset, 4);
+  }
+}
+
+/// Serialize the unwinding information.
+void UnwindInfo::serializeToUnwindInfoSection() {
+  // Bail out if there's no unwind info.
+  if(FuncCalleeSaved.empty()) return;
+
+  // Emit unwinding record information.
+  // FIXME: we only support ELF object files for now
+
+  // Switch to the unwind info section
+  MCStreamer &OS = *AP.OutStreamer;
+  MCSection *UnwindInfoSection =
+      OutContext.getObjectFileInfo()->getUnwindInfoSection();
+  OS.SwitchSection(UnwindInfoSection);
+
+  // Emit a dummy symbol to force section inclusion.
+  OS.EmitLabel(OutContext.getOrCreateSymbol(Twine("__StackTransform_UnwindInfo")));
+
+  // Serialize data.
+  DEBUG(dbgs() << "********** Unwind Info Output **********\n");
+  emitUnwindInfo(OS);
+  OS.AddBlankLine();
+
+  // Switch to the unwind address range section & emit section
+  MCSection *UnwindAddrRangeSection =
+      OutContext.getObjectFileInfo()->getUnwindAddrRangeSection();
+  OS.SwitchSection(UnwindAddrRangeSection);
+  OS.EmitLabel(OutContext.getOrCreateSymbol(Twine("__StackTransform_UnwindAddrRange")));
+  emitAddrRangeInfo(OS);
+  OS.AddBlankLine();
+
+  Emitted = true;
+}
+
+const UnwindInfo::FuncUnwindInfo &
+UnwindInfo::getUnwindInfo(const MCSymbol *Func) const {
+  assert(Emitted && "Have not yet calculated per-function unwinding metadata");
+
+  FuncUnwindMap::const_iterator it = FuncUnwindMetadata.find(Func);
+  assert(it != FuncUnwindMetadata.end() && "Invalid function");
+  return it->second;
+}
+
+void UnwindInfo::print(raw_ostream &OS) {
+  OS << UIDbg << "Function unwinding information\n";
+  FuncCalleeMap::const_iterator b, e;
+  for(b = FuncCalleeSaved.begin(), e = FuncCalleeSaved.end();
+      b != e;
+      b++) {
+    OS << UIDbg << "Function - " << b->first->getName() << "\n";
+    const CalleeSavedRegisters &CSR = b->second;
+    CalleeSavedRegisters::const_iterator br, be;
+    for(br = CSR.begin(), be = CSR.end(); br != be; br++) {
+      OS << UIDbg << "Register " << br->DwarfReg
+                  << " at offset " << br->Offset << "\n";
+    }
+  }
+}
+
Index: lib/MC/MCCodeGenInfo.cpp
===================================================================
--- lib/MC/MCCodeGenInfo.cpp	(revision 277823)
+++ lib/MC/MCCodeGenInfo.cpp	(working copy)
@@ -20,4 +20,5 @@
   RelocationModel = RM;
   CMModel = CM;
   OptLevel = OL;
+  ArchIROptLevel = OL;
 }
Index: lib/MC/MCObjectFileInfo.cpp
===================================================================
--- lib/MC/MCObjectFileInfo.cpp	(revision 277823)
+++ lib/MC/MCObjectFileInfo.cpp	(working copy)
@@ -519,6 +519,15 @@
   DwarfAddrSection =
       Ctx->getELFSection(".debug_addr", ELF::SHT_PROGBITS, 0, "addr_sec");
 
+  UnwindAddrRangeSection =
+      Ctx->getELFSection(".stack_transform.unwind_arange", ELF::SHT_PROGBITS,
+                         0, sizeof(uint64_t) + sizeof(uint64_t), "");
+  UnwindInfoSection =
+      Ctx->getELFSection(".stack_transform.unwind", ELF::SHT_PROGBITS, 0,
+                         sizeof(uint16_t) + sizeof(int16_t), "");
+  UnwindAddrRangeSection->setAlignment(sizeof(uint64_t));
+  UnwindInfoSection->setAlignment(sizeof(uint16_t) + sizeof(int16_t));
+
   StackMapSection =
       Ctx->getELFSection(".llvm_stackmaps", ELF::SHT_PROGBITS, ELF::SHF_ALLOC);
 
Index: lib/Target/AArch64/AArch64AsmPrinter.cpp
===================================================================
--- lib/Target/AArch64/AArch64AsmPrinter.cpp	(revision 277823)
+++ lib/Target/AArch64/AArch64AsmPrinter.cpp	(working copy)
@@ -28,6 +28,7 @@
 #include "llvm/CodeGen/MachineModuleInfoImpls.h"
 #include "llvm/CodeGen/StackMaps.h"
 #include "llvm/CodeGen/TargetLoweringObjectFileImpl.h"
+#include "llvm/CodeGen/UnwindInfo.h"
 #include "llvm/IR/DataLayout.h"
 #include "llvm/IR/DebugInfo.h"
 #include "llvm/MC/MCAsmInfo.h"
@@ -49,11 +50,12 @@
 class AArch64AsmPrinter : public AsmPrinter {
   AArch64MCInstLower MCInstLowering;
   StackMaps SM;
+  UnwindInfo UI;
 
 public:
   AArch64AsmPrinter(TargetMachine &TM, std::unique_ptr<MCStreamer> Streamer)
       : AsmPrinter(TM, std::move(Streamer)), MCInstLowering(OutContext, *this),
-        SM(*this), AArch64FI(nullptr) {}
+        SM(*this), UI(*this), AArch64FI(nullptr) {}
 
   const char *getPassName() const override {
     return "AArch64 Assembly Printer";
@@ -83,7 +85,9 @@
 
   bool runOnMachineFunction(MachineFunction &F) override {
     AArch64FI = F.getInfo<AArch64FunctionInfo>();
-    return AsmPrinter::runOnMachineFunction(F);
+    bool retval = AsmPrinter::runOnMachineFunction(F);
+    UI.recordUnwindInfo(F);
+    return retval;
   }
 
 private:
@@ -129,8 +133,10 @@
     // linker can safely perform dead code stripping.  Since LLVM never
     // generates code that does this, it is always safe to set.
     OutStreamer->EmitAssemblerFlag(MCAF_SubsectionsViaSymbols);
-    SM.serializeToStackMapSection();
   }
+  UI.serializeToUnwindInfoSection();
+  SM.serializeToStackMapSection(&UI);
+  UI.reset(); // Must reset after SM serialization to clear metadata
 }
 
 MachineLocation
Index: lib/Target/AArch64/AArch64FrameLowering.cpp
===================================================================
--- lib/Target/AArch64/AArch64FrameLowering.cpp	(revision 277823)
+++ lib/Target/AArch64/AArch64FrameLowering.cpp	(working copy)
@@ -655,6 +655,15 @@
   return resolveFrameIndexReference(MF, FI, FrameReg);
 }
 
+/// getFrameIndexReferenceFromFP - Provide a base+offset reference to an FI
+/// slot for debug info, but force base to be the frame pointer (x29).
+int
+AArch64FrameLowering::getFrameIndexReferenceFromFP(const MachineFunction &MF,
+                                                   int FI,
+                                                   unsigned &FrameReg) const {
+  return resolveFrameIndexReference(MF, FI, FrameReg, true);
+}
+
 int AArch64FrameLowering::resolveFrameIndexReference(const MachineFunction &MF,
                                                      int FI, unsigned &FrameReg,
                                                      bool PreferFP) const {
Index: lib/Target/AArch64/AArch64FrameLowering.h
===================================================================
--- lib/Target/AArch64/AArch64FrameLowering.h	(revision 277823)
+++ lib/Target/AArch64/AArch64FrameLowering.h	(working copy)
@@ -40,6 +40,8 @@
   int getFrameIndexOffset(const MachineFunction &MF, int FI) const override;
   int getFrameIndexReference(const MachineFunction &MF, int FI,
                              unsigned &FrameReg) const override;
+  int getFrameIndexReferenceFromFP(const MachineFunction &MF, int FI,
+                                   unsigned &FrameReg) const override;
   int resolveFrameIndexReference(const MachineFunction &MF, int FI,
                                  unsigned &FrameReg,
                                  bool PreferFP = false) const;
Index: lib/Target/AArch64/AArch64ISelLowering.cpp
===================================================================
--- lib/Target/AArch64/AArch64ISelLowering.cpp	(revision 277823)
+++ lib/Target/AArch64/AArch64ISelLowering.cpp	(working copy)
@@ -2032,6 +2032,8 @@
     return LowerFSINCOS(Op, DAG);
   case ISD::MUL:
     return LowerMUL(Op, DAG);
+  case (uint16_t)~TargetOpcode::STACKMAP:
+    return SDValue(); // Use generic stackmap type legalizer
   }
 }
 
Index: lib/Target/AArch64/AArch64RegisterInfo.cpp
===================================================================
--- lib/Target/AArch64/AArch64RegisterInfo.cpp	(revision 277823)
+++ lib/Target/AArch64/AArch64RegisterInfo.cpp	(working copy)
@@ -215,6 +215,23 @@
   return TFI->hasFP(MF) ? AArch64::FP : AArch64::SP;
 }
 
+int AArch64RegisterInfo::getReturnAddrLoc(const MachineFunction &MF,
+                                          unsigned &BaseReg) const {
+  const TargetFrameLowering *TFL = MF.getSubtarget().getFrameLowering();
+  const MachineFrameInfo *MFI = MF.getFrameInfo();
+  assert(MFI->isCalleeSavedInfoValid() && "No callee-saved information");
+  const std::vector<CalleeSavedInfo> &CSI = MFI->getCalleeSavedInfo();
+
+  // The return address' location is the the link register's spill slot
+  for(unsigned i = 0, e = CSI.size(); i < e; i++)
+    if(CSI[i].getReg() == AArch64::LR)
+      return TFL->getFrameIndexReference(MF, CSI[i].getFrameIdx(), BaseReg);
+
+  // We didn't find it, is it actually saved?
+  BaseReg = 0;
+  return INT32_MAX;
+}
+
 bool AArch64RegisterInfo::requiresRegisterScavenging(
     const MachineFunction &MF) const {
   return true;
Index: lib/Target/AArch64/AArch64RegisterInfo.h
===================================================================
--- lib/Target/AArch64/AArch64RegisterInfo.h	(revision 277823)
+++ lib/Target/AArch64/AArch64RegisterInfo.h	(working copy)
@@ -91,6 +91,9 @@
   // Debug information queries.
   unsigned getFrameRegister(const MachineFunction &MF) const override;
 
+  int getReturnAddrLoc(const MachineFunction &MF,
+                       unsigned &BaseReg) const override;
+
   unsigned getRegPressureLimit(const TargetRegisterClass *RC,
                                MachineFunction &MF) const override;
   // Base pointer (stack realignment) support.
Index: lib/Target/AArch64/AArch64Subtarget.h
===================================================================
--- lib/Target/AArch64/AArch64Subtarget.h	(revision 277823)
+++ lib/Target/AArch64/AArch64Subtarget.h	(working copy)
@@ -19,6 +19,7 @@
 #include "AArch64InstrInfo.h"
 #include "AArch64RegisterInfo.h"
 #include "AArch64SelectionDAGInfo.h"
+#include "AArch64Values.h"
 #include "llvm/IR/DataLayout.h"
 #include "llvm/Target/TargetSubtargetInfo.h"
 #include <string>
@@ -63,6 +64,7 @@
   AArch64InstrInfo InstrInfo;
   AArch64SelectionDAGInfo TSInfo;
   AArch64TargetLowering TLInfo;
+  AArch64Values VGen;
 private:
   /// initializeSubtargetDependencies - Initializes using CPUString and the
   /// passed in feature string so that we can use initializer lists for
@@ -89,6 +91,9 @@
   const AArch64RegisterInfo *getRegisterInfo() const override {
     return &getInstrInfo()->getRegisterInfo();
   }
+  const AArch64Values *getValues() const override {
+    return &VGen;
+  }
   const Triple &getTargetTriple() const { return TargetTriple; }
   bool enableMachineScheduler() const override { return true; }
   bool enablePostRAScheduler() const override {
Index: lib/Target/AArch64/AArch64TargetMachine.cpp
===================================================================
--- lib/Target/AArch64/AArch64TargetMachine.cpp	(revision 277823)
+++ lib/Target/AArch64/AArch64TargetMachine.cpp	(working copy)
@@ -220,16 +220,20 @@
   // Cmpxchg instructions are often used with a subsequent comparison to
   // determine whether it succeeded. We can exploit existing control-flow in
   // ldrex/strex loops to simplify this, but it needs tidying up.
-  if (TM->getOptLevel() != CodeGenOpt::None && EnableAtomicTidy)
+  if (TM->getOptLevel() != CodeGenOpt::None &&
+      TM->getArchIROptLevel() != CodeGenOpt::None &&
+      EnableAtomicTidy)
     addPass(createCFGSimplificationPass());
 
   TargetPassConfig::addIRPasses();
 
   // Match interleaved memory accesses to ldN/stN intrinsics.
-  if (TM->getOptLevel() != CodeGenOpt::None)
+  if (TM->getOptLevel() != CodeGenOpt::None &&
+      TM->getArchIROptLevel() != CodeGenOpt::None)
     addPass(createInterleavedAccessPass(TM));
 
-  if (TM->getOptLevel() == CodeGenOpt::Aggressive && EnableGEPOpt) {
+  if (TM->getOptLevel() == CodeGenOpt::Aggressive &&
+      TM->getArchIROptLevel() != CodeGenOpt::None && EnableGEPOpt) {
     // Call SeparateConstOffsetFromGEP pass to extract constants within indices
     // and lower a GEP with multiple indices to either arithmetic operations or
     // multiple GEPs with single index.
@@ -247,12 +251,14 @@
 bool AArch64PassConfig::addPreISel() {
   // Run promote constant before global merge, so that the promoted constants
   // get a chance to be merged
-  if (TM->getOptLevel() != CodeGenOpt::None && EnablePromoteConstant)
+  if (TM->getOptLevel() != CodeGenOpt::None &&
+      TM->getArchIROptLevel() != CodeGenOpt::None && EnablePromoteConstant)
     addPass(createAArch64PromoteConstantPass());
   // FIXME: On AArch64, this depends on the type.
   // Basically, the addressable offsets are up to 4095 * Ty.getSizeInBytes().
   // and the offset has to be a multiple of the related size in bytes.
   if ((TM->getOptLevel() != CodeGenOpt::None &&
+       TM->getArchIROptLevel() != CodeGenOpt::None &&
        EnableGlobalMerge == cl::BOU_UNSET) ||
       EnableGlobalMerge == cl::BOU_TRUE) {
     bool OnlyOptimizeForSize = (TM->getOptLevel() < CodeGenOpt::Aggressive) &&
@@ -260,7 +266,8 @@
     addPass(createGlobalMergePass(TM, 4095, OnlyOptimizeForSize));
   }
 
-  if (TM->getOptLevel() != CodeGenOpt::None)
+  if (TM->getOptLevel() != CodeGenOpt::None &&
+      TM->getArchIROptLevel() != CodeGenOpt::None)
     addPass(createAArch64AddressTypePromotionPass());
 
   return false;
Index: lib/Target/AArch64/AArch64Values.cpp
===================================================================
--- lib/Target/AArch64/AArch64Values.cpp	(nonexistent)
+++ lib/Target/AArch64/AArch64Values.cpp	(working copy)
@@ -0,0 +1,132 @@
+//===- AArch64TargetValues.cpp - AArch64 specific value generator -===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "AArch64Values.h"
+#include "AArch64.h"
+#include "MCTargetDesc/AArch64AddressingModes.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Target/TargetInstrInfo.h"
+#include "llvm/Target/TargetSubtargetInfo.h"
+
+#define DEBUG_TYPE "stacktransform"
+
+using namespace llvm;
+
+typedef ValueGenInst::InstType InstType;
+template <InstType T> using RegInstruction = RegInstruction<T>;
+template <InstType T> using ImmInstruction = ImmInstruction<T>;
+
+// Bitwise-conversions between floats & ints
+union IntFloat64 { double d; uint64_t i; };
+union IntFloat32 { float f; uint64_t i; };
+
+MachineLiveVal *
+AArch64Values::genADDInstructions(const MachineInstr *MI) const {
+  int Index;
+
+  switch(MI->getOpcode()) {
+  case AArch64::ADDXri:
+    if(MI->getOperand(1).isFI()) {
+      Index = MI->getOperand(1).getIndex();
+      assert(MI->getOperand(2).isImm() && MI->getOperand(2).getImm() == 0);
+      assert(MI->getOperand(3).isImm() && MI->getOperand(3).getImm() == 0);
+      return new MachineStackObject(Index, false, MI, true);
+    }
+    break;
+  default:
+    DEBUG(dbgs() << "Unhandled ADD machine instruction");
+    break;
+  }
+  return nullptr;
+}
+
+MachineLiveVal *
+AArch64Values::genBitfieldInstructions(const MachineInstr *MI) const {
+  int64_t R, S;
+  unsigned Size, Bits;
+  uint64_t Mask;
+  ValueGenInstList IL;
+
+  switch(MI->getOpcode()) {
+  case AArch64::UBFMXri:
+    Size = 8;
+    Bits = 64;
+    Mask = UINT64_MAX;
+
+    assert(MI->getOperand(1).isReg() &&
+           MI->getOperand(2).isImm() &&
+           MI->getOperand(3).isImm());
+
+    // TODO ensure this is correct
+    IL.emplace_back(
+      new RegInstruction<InstType::Set>(MI->getOperand(1).getReg()));
+    R = MI->getOperand(2).getImm();
+    S = MI->getOperand(3).getImm();
+    if(S >= R) {
+      IL.emplace_back(new ImmInstruction<InstType::RightShiftLog>(Size, R));
+      IL.emplace_back(
+        new ImmInstruction<InstType::Mask>(Size, ~(Mask << (S - R + 1))));
+    }
+    else {
+      IL.emplace_back(
+        new ImmInstruction<InstType::Mask>(Size, ~(Mask << (S + 1))));
+      IL.emplace_back(new ImmInstruction<InstType::LeftShift>(Size, Bits - R));
+    }
+    return new MachineGeneratedVal(IL, MI, false);
+    break;
+  default:
+    DEBUG(dbgs() << "Unhandled bitfield instruction");
+    break;
+  }
+  return nullptr;
+}
+
+MachineLiveValPtr AArch64Values::getMachineValue(const MachineInstr *MI) const {
+  unsigned Size;
+  IntFloat64 Conv64;
+  MachineLiveVal* Val = nullptr;
+  const MachineOperand *MO;
+  const TargetInstrInfo *TII;
+
+  switch(MI->getOpcode()) {
+  case AArch64::ADDXri:
+    Val = genADDInstructions(MI);
+    break;
+  case AArch64::ADRP:
+  case AArch64::MOVaddr:
+    MO = &MI->getOperand(1);
+    if(MO->isCPI())
+      Val = new MachineConstPoolRef(MO->getIndex(), MI, true);
+    else if(MO->isGlobal() || MO->isSymbol() || MO->isMCSymbol())
+      Val = new MachineSymbolRef(*MO, MI, true);
+    break;
+  case AArch64::COPY:
+    MO = &MI->getOperand(1);
+    if(MO->isReg() && MO->getReg() == AArch64::LR) Val = new ReturnAddress(MI);
+    break;
+  case AArch64::FMOVDi:
+    Size = 8;
+    Conv64.d = (double)AArch64_AM::getFPImmFloat(MI->getOperand(1).getImm());
+    Val = new MachineImmediate(Size, Conv64.i, MI, false);
+    break;
+  case AArch64::UBFMXri:
+    Val = genBitfieldInstructions(MI);
+    break;
+  default:
+    TII =  MI->getParent()->getParent()->getSubtarget().getInstrInfo();
+    DEBUG(dbgs() << "Unhandled opcode: "
+                 << TII->getName(MI->getOpcode()) << "\n");
+    break;
+  }
+
+  return MachineLiveValPtr(Val);
+}
+
Index: lib/Target/AArch64/AArch64Values.h
===================================================================
--- lib/Target/AArch64/AArch64Values.h	(nonexistent)
+++ lib/Target/AArch64/AArch64Values.h	(working copy)
@@ -0,0 +1,25 @@
+//===----- AArch64TargetValues.cpp - AArch64 specific value generator -----===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Target/TargetValues.h"
+
+namespace llvm {
+
+class AArch64Values final : public TargetValues {
+public:
+  AArch64Values() {}
+  virtual MachineLiveValPtr getMachineValue(const MachineInstr *MI) const;
+
+private:
+  MachineLiveVal *genADDInstructions(const MachineInstr *MI) const;
+  MachineLiveVal *genBitfieldInstructions(const MachineInstr *MI) const;
+};
+
+}
+
Index: lib/Target/AArch64/CMakeLists.txt
===================================================================
--- lib/Target/AArch64/CMakeLists.txt	(revision 277823)
+++ lib/Target/AArch64/CMakeLists.txt	(working copy)
@@ -43,6 +43,7 @@
   AArch64TargetMachine.cpp
   AArch64TargetObjectFile.cpp
   AArch64TargetTransformInfo.cpp
+  AArch64Values.cpp
 )
 
 add_dependencies(LLVMAArch64CodeGen intrinsics_gen)
Index: lib/Target/PowerPC/CMakeLists.txt
===================================================================
--- lib/Target/PowerPC/CMakeLists.txt	(revision 277823)
+++ lib/Target/PowerPC/CMakeLists.txt	(working copy)
@@ -34,6 +34,7 @@
   PPCTargetTransformInfo.cpp
   PPCTOCRegDeps.cpp
   PPCTLSDynamicCall.cpp
+  PPCValues.cpp
   PPCVSXCopy.cpp
   PPCVSXFMAMutate.cpp
   PPCVSXSwapRemoval.cpp
Index: lib/Target/PowerPC/PPCAsmPrinter.cpp
===================================================================
--- lib/Target/PowerPC/PPCAsmPrinter.cpp	(revision 277823)
+++ lib/Target/PowerPC/PPCAsmPrinter.cpp	(working copy)
@@ -36,6 +36,7 @@
 #include "llvm/CodeGen/MachineRegisterInfo.h"
 #include "llvm/CodeGen/StackMaps.h"
 #include "llvm/CodeGen/TargetLoweringObjectFileImpl.h"
+#include "llvm/CodeGen/UnwindInfo.h"
 #include "llvm/IR/Constants.h"
 #include "llvm/IR/DebugInfo.h"
 #include "llvm/IR/DerivedTypes.h"
@@ -70,10 +71,11 @@
     MapVector<MCSymbol*, MCSymbol*> TOC;
     const PPCSubtarget *Subtarget;
     StackMaps SM;
+    UnwindInfo UI;
   public:
     explicit PPCAsmPrinter(TargetMachine &TM,
                            std::unique_ptr<MCStreamer> Streamer)
-        : AsmPrinter(TM, std::move(Streamer)), SM(*this) {}
+        : AsmPrinter(TM, std::move(Streamer)), SM(*this), UI(*this) {}
 
     const char *getPassName() const override {
       return "PowerPC Assembly Printer";
@@ -99,9 +101,33 @@
     void LowerPATCHPOINT(MCStreamer &OutStreamer, StackMaps &SM,
                          const MachineInstr &MI);
     void EmitTlsCall(const MachineInstr *MI, MCSymbolRefExpr::VariantKind VK);
+
+    virtual int getCanonicalReturnAddr(const MachineInstr *Call) const override;
+
     bool runOnMachineFunction(MachineFunction &MF) override {
       Subtarget = &MF.getSubtarget<PPCSubtarget>();
-      return AsmPrinter::runOnMachineFunction(MF);
+      bool retval = AsmPrinter::runOnMachineFunction(MF);
+
+      // Add this function's register unwind info.  The PowerPC backend doesn't
+      // maintain the saved FBP (old r31) and link register as callee-saved
+      // registers, so manually add where they're saved.
+      if(MF.getFrameInfo()->hasStackMap()) {
+        UI.recordUnwindInfo(MF);
+
+        // Add the LR & FP save slots
+        const TargetFrameLowering *TFL = Subtarget->getFrameLowering();
+        const PPCFunctionInfo *FI = MF.getInfo<PPCFunctionInfo>();
+        int Index, Offset;
+        unsigned BaseReg;
+
+        Offset = MF.getFrameInfo()->getStackSize() + 16;
+        UI.addRegisterUnwindInfo(MF, PPC::LR8, Offset);
+
+        Index = FI->getFramePointerSaveIndex();
+        Offset = TFL->getFrameIndexReferenceFromFP(MF, Index, BaseReg);
+        UI.addRegisterUnwindInfo(MF, PPC::X31, Offset);
+      }
+      return retval;
     }
   };
 
@@ -327,7 +353,9 @@
 }
 
 void PPCAsmPrinter::EmitEndOfAsmFile(Module &M) {
-  SM.serializeToStackMapSection();
+  UI.serializeToUnwindInfoSection();
+  SM.serializeToStackMapSection(&UI);
+  UI.reset(); // Must reset after SM serialization to clear metadata
 }
 
 void PPCAsmPrinter::LowerSTACKMAP(MCStreamer &OutStreamer, StackMaps &SM,
@@ -490,6 +518,19 @@
                  .addExpr(SymVar));
 }
 
+/// getCanonicalReturnAddr -- for machine instructions which actually codegen
+/// a call + other instructions, return an offset which would correct a label
+/// to point to the call's actual return address.
+int PPCAsmPrinter::getCanonicalReturnAddr(const MachineInstr *Call) const {
+  switch(Call->getOpcode()) {
+  case PPC::BL8_NOP:
+  case PPC::BLA8_NOP:
+  case PPC::BL8_NOP_TLS:
+  case PPC::BCTRL8_LDinto_toc: return 4;
+  default: return 0;
+  }
+}
+
 /// EmitInstruction -- Print out a single PowerPC MI in Darwin syntax to
 /// the current output stream.
 ///
Index: lib/Target/PowerPC/PPCSubtarget.h
===================================================================
--- lib/Target/PowerPC/PPCSubtarget.h	(revision 277823)
+++ lib/Target/PowerPC/PPCSubtarget.h	(working copy)
@@ -17,6 +17,7 @@
 #include "PPCFrameLowering.h"
 #include "PPCISelLowering.h"
 #include "PPCInstrInfo.h"
+#include "PPCValues.h"
 #include "llvm/ADT/Triple.h"
 #include "llvm/IR/DataLayout.h"
 #include "llvm/MC/MCInstrItineraries.h"
@@ -129,6 +130,7 @@
   PPCFrameLowering FrameLowering;
   PPCInstrInfo InstrInfo;
   PPCTargetLowering TLInfo;
+  PPCValues VGen;
   TargetSelectionDAGInfo TSInfo;
 
 public:
@@ -171,6 +173,7 @@
     return &getInstrInfo()->getRegisterInfo();
   }
   const PPCTargetMachine &getTargetMachine() const { return TM; }
+  const PPCValues *getValues() const override { return &VGen; }
 
   /// initializeSubtargetDependencies - Initializes using a CPU and feature string
   /// so that we can use initializer lists for subtarget initialization.
Index: lib/Target/PowerPC/PPCValues.cpp
===================================================================
--- lib/Target/PowerPC/PPCValues.cpp	(nonexistent)
+++ lib/Target/PowerPC/PPCValues.cpp	(working copy)
@@ -0,0 +1,46 @@
+//===--------- PPCTargetValues.cpp - PPC specific value generator ---------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "PPCFrameLowering.h"
+#include "PPCValues.h"
+#include "PPC.h"
+#include "llvm/CodeGen/MachineRegisterInfo.h"
+#include "llvm/CodeGen/MachineOperand.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+#define DEBUG_TYPE "stacktransform"
+
+using namespace llvm;
+
+MachineLiveValPtr PPCValues::getMachineValue(const MachineInstr *MI) const {
+  // TODO
+  return nullptr;
+}
+
+void PPCValues::addRequiredArchLiveValues(MachineFunction *MF,
+                                          const MachineInstr *MIStackMap,
+                                          const CallInst *IRStackMap) const {
+  if(!MF->getRegInfo().use_empty(PPC::X2)) {
+    MachineOperand TOCRef = MachineOperand::CreateES(".TOC.");
+    MachineSymbolRef TOCSym(TOCRef, MIStackMap, true);
+
+    DEBUG(dbgs() << "   + Setting R2 to be TOC pointer\n");
+    MachineLiveReg TOCPtr(PPC::X2);
+    MF->addSMArchSpecificLocation(IRStackMap, TOCPtr, TOCSym);
+
+    // Per the ELFv2 ABI, the TOC Pointer Doubleword save area is at SP + 24
+    DEBUG(dbgs() << "   + Setting TOC pointer save slot to be TOC pointer\n");
+    const PPCFrameLowering *PFL =
+      (const PPCFrameLowering *)MF->getSubtarget().getFrameLowering();
+    MachineLiveStackAddr TOCSS(PFL->getTOCSaveOffset(), PPC::X1, 8);
+    MF->addSMArchSpecificLocation(IRStackMap, TOCSS, TOCSym);
+  }
+}
+
Index: lib/Target/PowerPC/PPCValues.h
===================================================================
--- lib/Target/PowerPC/PPCValues.h	(nonexistent)
+++ lib/Target/PowerPC/PPCValues.h	(working copy)
@@ -0,0 +1,24 @@
+//===--------- PPCTargetValues.cpp - PPC specific value generator ---------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Target/TargetValues.h"
+
+namespace llvm {
+
+class PPCValues final : public TargetValues {
+public:
+  PPCValues() {}
+  virtual MachineLiveValPtr getMachineValue(const MachineInstr *MI) const;
+  virtual void addRequiredArchLiveValues(MachineFunction *MF,
+                                         const MachineInstr *MIStackMap,
+                                         const CallInst *IRStackMap) const;
+};
+
+}
+
Index: lib/Target/TargetMachine.cpp
===================================================================
--- lib/Target/TargetMachine.cpp	(revision 277823)
+++ lib/Target/TargetMachine.cpp	(working copy)
@@ -149,6 +149,17 @@
     CodeGenInfo->setOptLevel(Level);
 }
 
+CodeGenOpt::Level TargetMachine::getArchIROptLevel() const {
+  if (!CodeGenInfo)
+    return CodeGenOpt::Default;
+  return CodeGenInfo->getArchIROptLevel();
+}
+
+void TargetMachine::setArchIROptLevel(CodeGenOpt::Level Level) const {
+  if (CodeGenInfo)
+    CodeGenInfo->setArchIROptLevel(Level);
+}
+
 TargetIRAnalysis TargetMachine::getTargetIRAnalysis() {
   return TargetIRAnalysis([this](Function &F) {
     return TargetTransformInfo(F.getParent()->getDataLayout());
Index: lib/Target/X86/CMakeLists.txt
===================================================================
--- lib/Target/X86/CMakeLists.txt	(revision 277823)
+++ lib/Target/X86/CMakeLists.txt	(working copy)
@@ -34,6 +34,7 @@
   X86VZeroUpper.cpp
   X86FixupLEAs.cpp
   X86WinEHState.cpp
+  X86Values.cpp
   )
 
 if( CMAKE_CL_64 )
Index: lib/Target/X86/X86AsmPrinter.cpp
===================================================================
--- lib/Target/X86/X86AsmPrinter.cpp	(revision 277823)
+++ lib/Target/X86/X86AsmPrinter.cpp	(working copy)
@@ -24,6 +24,7 @@
 #include "llvm/CodeGen/TargetLoweringObjectFileImpl.h"
 #include "llvm/IR/DebugInfo.h"
 #include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/DiagnosticInfo.h"
 #include "llvm/IR/Mangler.h"
 #include "llvm/IR/Module.h"
 #include "llvm/IR/Type.h"
@@ -49,6 +50,8 @@
 bool X86AsmPrinter::runOnMachineFunction(MachineFunction &MF) {
   Subtarget = &MF.getSubtarget<X86Subtarget>();
 
+  bool modified = TagCallSites(MF);
+
   SMShadowTracker.startFunction(MF);
 
   SetupMachineFunction(MF);
@@ -66,8 +69,24 @@
   // Emit the rest of the function body.
   EmitFunctionBody();
 
-  // We didn't modify anything.
-  return false;
+  // Add this function's register unwind info.  The x86 backend doesn't
+  // maintain the saved FBP (old RBP) and return address (RIP) as callee-saved
+  // registers, so manually add where they're saved.
+  if(MF.getFrameInfo()->hasStackMap()) {
+    const TargetFrameLowering *TFL = Subtarget->getFrameLowering();
+    const X86MachineFunctionInfo *FI = MF.getInfo<X86MachineFunctionInfo>();
+    int Offset;
+    unsigned BaseReg;
+
+    UI.recordUnwindInfo(MF);
+    Offset = TFL->getFrameIndexReferenceFromFP(MF, FI->getRAIndex(), BaseReg);
+    UI.addRegisterUnwindInfo(MF, X86::RIP, Offset);
+    Offset = TFL->getFrameIndexReferenceFromFP(MF, FI->getFAIndex(), BaseReg);
+    UI.addRegisterUnwindInfo(MF, X86::RBP, Offset);
+  }
+
+  // We may have modified where stack map intrinsics are located.
+  return modified;
 }
 
 /// printSymbolOperand - Print a raw symbol reference operand.  This handles
@@ -689,8 +708,10 @@
   }
 
   if (TT.isOSBinFormatELF()) {
-    SM.serializeToStackMapSection();
+    UI.serializeToUnwindInfoSection();
+    SM.serializeToStackMapSection(&UI);
     FM.serializeToFaultMapSection();
+    UI.reset(); // Must reset after SM serialization to clear metadata
   }
 }
 
Index: lib/Target/X86/X86AsmPrinter.h
===================================================================
--- lib/Target/X86/X86AsmPrinter.h	(revision 277823)
+++ lib/Target/X86/X86AsmPrinter.h	(working copy)
@@ -14,6 +14,7 @@
 #include "llvm/CodeGen/AsmPrinter.h"
 #include "llvm/CodeGen/FaultMaps.h"
 #include "llvm/CodeGen/StackMaps.h"
+#include "llvm/CodeGen/UnwindInfo.h"
 #include "llvm/Target/TargetMachine.h"
 
 // Implemented in X86MCInstLower.cpp
@@ -28,6 +29,7 @@
 class LLVM_LIBRARY_VISIBILITY X86AsmPrinter : public AsmPrinter {
   const X86Subtarget *Subtarget;
   StackMaps SM;
+  UnwindInfo UI;
   FaultMaps FM;
 
   // This utility class tracks the length of a stackmap instruction's 'shadow'.
@@ -90,8 +92,8 @@
  public:
    explicit X86AsmPrinter(TargetMachine &TM,
                           std::unique_ptr<MCStreamer> Streamer)
-       : AsmPrinter(TM, std::move(Streamer)), SM(*this), FM(*this),
-         SMShadowTracker(TM) {}
+       : AsmPrinter(TM, std::move(Streamer)), SM(*this), UI(*this),
+         FM(*this), SMShadowTracker(TM) {}
 
   const char *getPassName() const override {
     return "X86 Assembly / Object Emitter";
Index: lib/Target/X86/X86ISelLowering.cpp
===================================================================
--- lib/Target/X86/X86ISelLowering.cpp	(revision 277823)
+++ lib/Target/X86/X86ISelLowering.cpp	(working copy)
@@ -18621,6 +18621,8 @@
   case ISD::GC_TRANSITION_START:
                                 return LowerGC_TRANSITION_START(Op, DAG);
   case ISD::GC_TRANSITION_END:  return LowerGC_TRANSITION_END(Op, DAG);
+  case (uint16_t)~TargetOpcode::STACKMAP:
+    return SDValue(); // Use generic stackmap type legalizer
   }
 }
 
Index: lib/Target/X86/X86RegisterInfo.cpp
===================================================================
--- lib/Target/X86/X86RegisterInfo.cpp	(revision 277823)
+++ lib/Target/X86/X86RegisterInfo.cpp	(working copy)
@@ -596,6 +596,13 @@
   return FrameReg;
 }
 
+int X86RegisterInfo::getReturnAddrLoc(const MachineFunction &MF,
+                                      unsigned &BaseReg) const {
+  const X86MachineFunctionInfo *X86FI = MF.getInfo<X86MachineFunctionInfo>();
+  const TargetFrameLowering *TFL = MF.getSubtarget().getFrameLowering();
+  return TFL->getFrameIndexReference(MF, X86FI->getRAIndex(), BaseReg);
+}
+
 namespace llvm {
 unsigned getX86SubSuperRegisterOrZero(unsigned Reg, MVT::SimpleValueType VT,
                                       bool High) {
Index: lib/Target/X86/X86RegisterInfo.h
===================================================================
--- lib/Target/X86/X86RegisterInfo.h	(revision 277823)
+++ lib/Target/X86/X86RegisterInfo.h	(working copy)
@@ -126,6 +126,9 @@
   unsigned getBaseRegister() const { return BasePtr; }
   // FIXME: Move to FrameInfok
   unsigned getSlotSize() const { return SlotSize; }
+
+  int getReturnAddrLoc(const MachineFunction &MF,
+                       unsigned &BaseReg) const override;
 };
 
 /// Returns the sub or super register of a specific X86 register.
Index: lib/Target/X86/X86Subtarget.h
===================================================================
--- lib/Target/X86/X86Subtarget.h	(revision 277823)
+++ lib/Target/X86/X86Subtarget.h	(working copy)
@@ -18,6 +18,7 @@
 #include "X86ISelLowering.h"
 #include "X86InstrInfo.h"
 #include "X86SelectionDAGInfo.h"
+#include "X86Values.h"
 #include "llvm/ADT/Triple.h"
 #include "llvm/IR/CallingConv.h"
 #include "llvm/Target/TargetSubtargetInfo.h"
@@ -248,7 +249,7 @@
   X86InstrInfo InstrInfo;
   X86TargetLowering TLInfo;
   X86FrameLowering FrameLowering;
-
+  X86Values VGen;
 public:
   /// This constructor initializes the data members to match that
   /// of the specified triple.
@@ -269,6 +270,7 @@
   const X86RegisterInfo *getRegisterInfo() const override {
     return &getInstrInfo()->getRegisterInfo();
   }
+  const X86Values *getValues() const override { return &VGen; }
 
   /// Returns the minimum alignment known to hold of the
   /// stack frame on entry to the function and which must be maintained by every
Index: lib/Target/X86/X86Values.cpp
===================================================================
--- lib/Target/X86/X86Values.cpp	(nonexistent)
+++ lib/Target/X86/X86Values.cpp	(working copy)
@@ -0,0 +1,90 @@
+//===--------- X86TargetValues.cpp - X86 specific value generator ---------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "X86Values.h"
+#include "X86InstrInfo.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Target/TargetInstrInfo.h"
+#include "llvm/Target/TargetSubtargetInfo.h"
+
+#define DEBUG_TYPE "stacktransform"
+
+using namespace llvm;
+
+typedef ValueGenInst::InstType InstType;
+template <InstType T> using RegInstruction = RegInstruction<T>;
+template <InstType T> using ImmInstruction = ImmInstruction<T>;
+
+MachineLiveVal *X86Values::genLEAInstructions(const MachineInstr *MI) const {
+  unsigned Reg, Size;
+  int64_t Imm;
+  ValueGenInstList IL;
+
+  // TODO do we need to handle the segment register operand?
+  switch(MI->getOpcode()) {
+  case X86::LEA64r:
+    Size = 8;
+
+    // Initialize to index register * scale if indexing, or zero otherwise
+    Reg = MI->getOperand(1 + X86::AddrIndexReg).getReg();
+    if(Reg) {
+      Imm = MI->getOperand(1 + X86::AddrScaleAmt).getImm();
+      IL.emplace_back(new RegInstruction<InstType::Set>(Reg));
+      IL.emplace_back(new ImmInstruction<InstType::Multiply>(Size, Imm));
+    }
+    else IL.emplace_back(new ImmInstruction<InstType::Set>(8, 0));
+
+    // Add the base register & displacement
+    if(!MI->getOperand(1 + X86::AddrBaseReg).isFI()) {
+      assert(MI->getOperand(1 + X86::AddrBaseReg).isReg() &&
+             MI->getOperand(1 + X86::AddrDisp).isImm());
+
+      Reg = MI->getOperand(1 + X86::AddrBaseReg).getReg();
+      Imm = MI->getOperand(1 + X86::AddrDisp).getImm();
+      IL.emplace_back(new RegInstruction<InstType::Add>(Reg));
+      IL.emplace_back(new ImmInstruction<InstType::Add>(Size, Imm));
+      return new MachineGeneratedVal(IL, MI, true);
+    }
+    // TODO what if we're referencing a frame? The frame index becomes the base
+    // register + displacement after register rewriting & stack slot allocation
+    //Index = MI->getOperand(1 + X86::AddrBaseReg).getIndex();
+
+    break;
+  default:
+    DEBUG(dbgs() << "Unhandled LEA machine instruction");
+    break;
+  }
+  return nullptr;
+}
+
+MachineLiveValPtr X86Values::getMachineValue(const MachineInstr *MI) const {
+  MachineLiveVal* Val = nullptr;
+  const MachineOperand *MO;
+  const TargetInstrInfo *TII;
+
+  switch(MI->getOpcode()) {
+  case X86::LEA64r:
+    Val = genLEAInstructions(MI);
+    break;
+  case X86::MOV64ri:
+    MO = &MI->getOperand(1);
+    if(MO->isGlobal() || MO->isSymbol() || MO->isMCSymbol())
+      Val = new MachineSymbolRef(*MO, MI, true);
+    break;
+  default:
+    TII =  MI->getParent()->getParent()->getSubtarget().getInstrInfo();
+    DEBUG(dbgs() << "Unhandled opcode: "
+                 << TII->getName(MI->getOpcode()) << "\n");
+    break;
+  }
+
+  return MachineLiveValPtr(Val);
+}
+
Index: lib/Target/X86/X86Values.h
===================================================================
--- lib/Target/X86/X86Values.h	(nonexistent)
+++ lib/Target/X86/X86Values.h	(working copy)
@@ -0,0 +1,24 @@
+//===--------- X86TargetValues.cpp - X86 specific value generator ---------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Target/TargetValues.h"
+
+namespace llvm {
+
+class X86Values final : public TargetValues {
+public:
+  X86Values() {}
+  virtual MachineLiveValPtr getMachineValue(const MachineInstr *MI) const;
+
+private:
+  MachineLiveVal *genLEAInstructions(const MachineInstr *LEA) const;
+};
+
+}
+
Index: lib/Transforms/Instrumentation/CMakeLists.txt
===================================================================
--- lib/Transforms/Instrumentation/CMakeLists.txt	(revision 277823)
+++ lib/Transforms/Instrumentation/CMakeLists.txt	(working copy)
@@ -2,10 +2,13 @@
   AddressSanitizer.cpp
   BoundsChecking.cpp
   DataFlowSanitizer.cpp
+  MigrationPoints.cpp
   GCOVProfiling.cpp
   MemorySanitizer.cpp
+  InsertStackMaps.cpp
   Instrumentation.cpp
   InstrProfiling.cpp
+  LibcStackMaps.cpp
   SafeStack.cpp
   SanitizerCoverage.cpp
   ThreadSanitizer.cpp
Index: lib/Transforms/Instrumentation/InsertStackMaps.cpp
===================================================================
--- lib/Transforms/Instrumentation/InsertStackMaps.cpp	(nonexistent)
+++ lib/Transforms/Instrumentation/InsertStackMaps.cpp	(working copy)
@@ -0,0 +1,341 @@
+#include <map>
+#include <set>
+#include <vector>
+#include "llvm/Pass.h"
+#include "llvm/Analysis/LiveValues.h"
+#include "llvm/IR/Dominators.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/InstIterator.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/Type.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+#define DEBUG_TYPE "insert-stackmaps"
+
+using namespace llvm;
+
+namespace {
+
+/**
+ * This class instruments equivalence points in the IR with LLVM's stackmap
+ * intrinsic.  This tells the backend to record the locations of IR values
+ * after register allocation in a separate ELF section.
+ */
+class InsertStackMaps : public ModulePass
+{
+public:
+  static char ID;
+  size_t callSiteID;
+  size_t numInstrumented;
+
+  InsertStackMaps() : ModulePass(ID), callSiteID(0), numInstrumented(0) {
+    initializeInsertStackMapsPass(*PassRegistry::getPassRegistry());
+  }
+  ~InsertStackMaps() {}
+
+  /* ModulePass virtual methods */
+  virtual const char *getPassName() const { return "Insert stackmaps"; }
+
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const
+  {
+    AU.addRequired<LiveValues>();
+    AU.addRequired<DominatorTreeWrapperPass>();
+    AU.setPreservesCFG();
+  }
+
+  /**
+   * Use liveness analysis to insert stackmap intrinsics into the IR to record
+   * live values at equivalence points.
+   *
+   * Note: currently we only insert stackmaps at function call sites.
+   */
+  virtual bool runOnModule(Module &M)
+  {
+    bool modified = false;
+    std::set<const Value *> *live;
+    std::set<const Value *, ValueComp> sortedLive;
+    std::set<const Instruction *> hiddenInst;
+    std::set<const Argument *> hiddenArgs;
+
+    DEBUG(errs() << "\n********** Begin InsertStackMaps **********\n"
+                 << "********** Module: " << M.getName() << " **********\n\n");
+
+    this->createSMType(M);
+    if(this->addSMDeclaration(M)) modified = true;
+
+    modified |= this->removeOldStackmaps(M);
+
+    /* Iterate over all functions/basic blocks/instructions. */
+    for(Module::iterator f = M.begin(), fe = M.end(); f != fe; f++)
+    {
+      if(f->isDeclaration()) continue;
+
+      DEBUG(errs() << "InsertStackMaps: entering function "
+                   << f->getName() << "\n");
+
+      LiveValues &liveVals = getAnalysis<LiveValues>(*f);
+      DominatorTree &DT = getAnalysis<DominatorTreeWrapperPass>(*f).getDomTree();
+      std::set<const Value *>::const_iterator v, ve;
+      getHiddenVals(*f, hiddenInst, hiddenArgs);
+
+      /* Find call sites in the function. */
+      for(Function::iterator b = f->begin(), be = f->end(); b != be; b++)
+      {
+        DEBUG(
+          errs() << "InsertStackMaps: entering basic block ";
+          b->printAsOperand(errs(), false);
+          errs() << "\n"
+        );
+
+        for(BasicBlock::iterator i = b->begin(), ie = b->end(); i != ie; i++)
+        {
+          CallInst *CI;
+          if((CI = dyn_cast<CallInst>(&*i)) &&
+             !CI->isInlineAsm() &&
+             !isa<IntrinsicInst>(CI))
+          {
+            live = liveVals.getLiveValues(&*i);
+            for(const Value *val : *live) sortedLive.insert(val);
+            for(const Instruction *val : hiddenInst) {
+              /*
+               * The two criteria for inclusion of a hidden value are:
+               *   1. The value's definition dominates the call
+               *   2. A use which hides the definition is in the stackmap
+               */
+              if(DT.dominates(val, CI) && hasLiveUser(val, *live))
+                sortedLive.insert(val);
+            }
+            for(const Argument *val : hiddenArgs) {
+              /*
+               * Similar criteria apply as above, except we know arguments
+               * dominate the entire function.
+               */
+              if(hasLiveUser(val, *live))
+                sortedLive.insert(val);
+            }
+            delete live;
+
+            /* If the call's value is used, add it to the stackmap */
+            if(CI->use_begin() != CI->use_end())
+              sortedLive.insert(CI);
+
+            DEBUG(
+              const Function *calledFunc;
+
+              errs() << "  ";
+              if(!CI->getType()->isVoidTy()) {
+                CI->printAsOperand(errs(), false);
+                errs() << " ";
+              }
+              else errs() << "(void) ";
+
+              calledFunc = CI->getCalledFunction();
+              if(calledFunc && calledFunc->hasName())
+              {
+                StringRef name = CI->getCalledFunction()->getName();
+                errs() << name << " ";
+              }
+              errs() << "ID: " << this->callSiteID;
+
+              errs() << ", " << sortedLive.size() << " live value(s)\n   ";
+              for(const Value *val : sortedLive) {
+                errs() << " ";
+                val->printAsOperand(errs(), false);
+              }
+              errs() << "\n";
+            );
+
+            IRBuilder<> builder(CI->getNextNode());
+            std::vector<Value *> args(2);
+            args[0] = ConstantInt::getSigned(Type::getInt64Ty(M.getContext()), this->callSiteID++);
+            args[1] = ConstantInt::getSigned(Type::getInt32Ty(M.getContext()), 0);
+            for(v = sortedLive.begin(), ve = sortedLive.end(); v != ve; v++)
+              args.push_back((Value*)*v);
+            builder.CreateCall(this->SMFunc, ArrayRef<Value*>(args));
+            sortedLive.clear();
+            this->numInstrumented++;
+          }
+        }
+      }
+
+      hiddenInst.clear();
+      hiddenArgs.clear();
+      this->callSiteID = 0;
+    }
+
+    DEBUG(
+      errs() << "InsertStackMaps: finished module " << M.getName() << ", added "
+             << this->numInstrumented << " stackmaps\n\n";
+    );
+
+    if(numInstrumented > 0) modified = true;
+
+    return modified;
+  }
+
+private:
+  /* Name of stack map intrinsic */
+  static const StringRef SMName;
+
+  /* Stack map instruction creation */
+  Function *SMFunc;
+  FunctionType *SMTy; // Used for creating function declaration
+
+  /* Sort values based on name */
+  struct ValueComp {
+    bool operator() (const Value *a, const Value *b) const
+    {
+      if(a->hasName() && b->hasName())
+        return a->getName().compare(b->getName()) < 0;
+      else if(a->hasName()) return true;
+      else if(b->hasName()) return false;
+      else return a < b;
+    }
+  };
+
+  /**
+   * Create the function type for the stack map intrinsic.
+   */
+  void createSMType(const Module &M)
+  {
+    std::vector<Type*> params(2);
+    params[0] = Type::getInt64Ty(M.getContext());
+    params[1] = Type::getInt32Ty(M.getContext());
+    this->SMTy = FunctionType::get(Type::getVoidTy(M.getContext()),
+                                                   ArrayRef<Type*>(params),
+                                                   true);
+  }
+
+  /**
+   * Add the stackmap intrinisic's function declaration if not already present.
+   * Return true if the declaration was added, or false if it's already there.
+   */
+  bool addSMDeclaration(Module &M)
+  {
+    if(!(this->SMFunc = M.getFunction(this->SMName)))
+    {
+      DEBUG(errs() << "Adding stackmap function declaration to " << M.getName() << "\n");
+      this->SMFunc = cast<Function>(M.getOrInsertFunction(this->SMName, this->SMTy));
+      this->SMFunc->setCallingConv(CallingConv::C);
+      return true;
+    }
+    else return false;
+  }
+
+  /**
+   * Iterate over all instructions, removing previously found stackmaps.
+   */
+  bool removeOldStackmaps(Module &M)
+  {
+    bool modified = false;
+    CallInst* CI;
+    const Function *F;
+
+    DEBUG(dbgs() << "Searching for/removing old stackmaps\n";);
+
+    for(Module::iterator f = M.begin(), fe = M.end(); f != fe; f++) {
+      for(Function::iterator bb = f->begin(), bbe = f->end(); bb != bbe; bb++) {
+        for(BasicBlock::iterator i = bb->begin(), ie = bb->end(); i != ie; i++) {
+          if((CI = dyn_cast<CallInst>(&*i))) {
+            F = CI->getCalledFunction();
+            if(F && F->hasName() && F->getName() == SMName) {
+              i = i->eraseFromParent()->getPrevNode();
+              modified = true;
+            }
+          }
+        }
+      }
+    }
+
+    DEBUG(if(modified)
+            dbgs() << "WARNING: found previous run of Popcorn passes!\n";);
+
+    return modified;
+  }
+
+  /**
+   * Gather a list of values which may be "hidden" from live value analysis.
+   * This function collects the values used in these instructions, which are
+   * later added to the appropriate stackmaps.
+   *
+   * 1. Instructions which access fields of structs or entries of arrays, like
+   *    getelementptr, can interfere with the live value analysis to hide the
+   *    backing values used in the instruction.  For example, the following IR
+   *    obscures %arr from the live value analysis:
+   *
+   *  %arr = alloca [4 x double], align 8
+   *  %arrayidx = getelementptr inbounds [4 x double], [4 x double]* %arr, i64 0, i64 0
+   *
+   *  -> Access to %arr might only happen through %arrayidx, and %arr may not
+   *     be used any more
+   *
+   * 2. Compare instructions, such as icmp & fcmp, can be lowered to complex &
+   *    architecture-specific  machine code by the backend.  To help capture
+   *    all live values, we capture both the value used in the comparison and
+   *    the resulting condition value.
+   *
+   */
+  void getHiddenVals(Function &F,
+                     std::set<const Instruction *> &inst,
+                     std::set<const Argument *> &args)
+  {
+    /* Does the instruction potentially hide values from liveness analysis? */
+    auto hidesValues = [](const Instruction *I) {
+      if(isa<ExtractElementInst>(I) || isa<InsertElementInst>(I) ||
+         isa<ExtractValueInst>(I) || isa<InsertValueInst>(I) ||
+         isa<GetElementPtrInst>(I) || isa<ICmpInst>(I) || isa<FCmpInst>(I))
+        return true ;
+      else return false;
+    };
+
+    /* Search for instructions that obscure live values & record operands */
+    for(inst_iterator i = inst_begin(F), e = inst_end(F); i != e; ++i) {
+      if(hidesValues(&*i)) {
+        for(unsigned op = 0; op < i->getNumOperands(); op++) {
+          if(isa<Instruction>(i->getOperand(op)))
+            inst.insert(cast<Instruction>(i->getOperand(op)));
+          else if(isa<Argument>(i->getOperand(op)))
+            args.insert(cast<Argument>(i->getOperand(op)));
+        }
+      }
+    }
+  }
+
+  /**
+   * Return whether or not a value's user is in a liveness set.
+   *
+   * @param Val a value whose users are checked against the liveness set
+   * @param Live a set of live values
+   * @return true if a user is in the liveness set, false otherwise
+   */
+  bool hasLiveUser(const Value *Val,
+                   const std::set<const Value *> &Live) const {
+    Value::const_use_iterator use, e;
+    for(use = Val->use_begin(), e = Val->use_end(); use != e; use++)
+      if(Live.count(use->getUser())) return true;
+    return false;
+  }
+};
+
+} /* end anonymous namespace */
+
+
+char InsertStackMaps::ID = 0;
+const StringRef InsertStackMaps::SMName = "llvm.experimental.stackmap";
+
+INITIALIZE_PASS_BEGIN(InsertStackMaps, "insert-stackmaps",
+                      "Instrument equivalence points with stack maps",
+                      false, false)
+INITIALIZE_PASS_DEPENDENCY(LiveValues)
+INITIALIZE_PASS_DEPENDENCY(DominatorTreeWrapperPass)
+INITIALIZE_PASS_END(InsertStackMaps, "insert-stackmaps",
+                    "Instrument equivalence points with stack maps",
+                    false, false)
+
+namespace llvm {
+  ModulePass *createInsertStackMapsPass() { return new InsertStackMaps(); }
+}
+
Index: lib/Transforms/Instrumentation/Instrumentation.cpp
===================================================================
--- lib/Transforms/Instrumentation/Instrumentation.cpp	(revision 277823)
+++ lib/Transforms/Instrumentation/Instrumentation.cpp	(working copy)
@@ -24,8 +24,11 @@
   initializeAddressSanitizerPass(Registry);
   initializeAddressSanitizerModulePass(Registry);
   initializeBoundsCheckingPass(Registry);
+  initializeMigrationPointsPass(Registry);
   initializeGCOVProfilerPass(Registry);
+  initializeInsertStackMapsPass(Registry);
   initializeInstrProfilingPass(Registry);
+  initializeLibcStackMapsPass(Registry);
   initializeMemorySanitizerPass(Registry);
   initializeThreadSanitizerPass(Registry);
   initializeSanitizerCoverageModulePass(Registry);
Index: lib/Transforms/Instrumentation/LibcStackMaps.cpp
===================================================================
--- lib/Transforms/Instrumentation/LibcStackMaps.cpp	(nonexistent)
+++ lib/Transforms/Instrumentation/LibcStackMaps.cpp	(working copy)
@@ -0,0 +1,229 @@
+#include <map>
+#include <vector>
+#include "llvm/Pass.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/Type.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/Path.h"
+#include "llvm/Support/raw_ostream.h"
+
+#define DEBUG_TYPE "libc-stackmaps"
+
+using namespace llvm;
+
+namespace {
+
+/**
+ * Instrument thread starting points with stackmaps.  These are the only
+ * functions inside of libc for which we want to generate metadata, since we
+ * disallow migration inside the public libc API.
+ */
+// TODO: only implemented for musl-libc!
+class LibcStackMaps : public ModulePass
+{
+public:
+  static char ID;
+  size_t numInstrumented;
+
+  LibcStackMaps() : ModulePass(ID), numInstrumented(0) {
+    initializeLibcStackMapsPass(*PassRegistry::getPassRegistry());
+  }
+  ~LibcStackMaps() {}
+
+  /* ModulePass virtual methods */
+  virtual const char *getPassName() const
+  { return "Insert stackmaps in libc thread start functions"; }
+
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const
+  { AU.setPreservesCFG(); }
+
+  virtual bool runOnModule(Module &M)
+  {
+    int64_t smid;
+    bool modified = false;
+    Function *F;
+    std::map<std::string, std::vector<std::string> >::const_iterator file;
+
+    /* Is this a module (i.e., source file) we're interested in? */
+    if((file = funcs.find(sys::path::stem(M.getName()))) != funcs.end())
+    {
+      DEBUG(dbgs() << "\n********** Begin LibcStackMaps **********\n"
+                   << "********** Module: " << file->first << " **********\n\n");
+
+      this->createSMType(M);
+      modified |= this->addSMDeclaration(M);
+
+      /* Iterate over thread starting functions in the module */
+      for(size_t f = 0, fe = file->second.size(); f < fe; f++)
+      {
+        DEBUG(dbgs() << "LibcStackMaps: entering thread starting function "
+                     << file->second[f] << "\n");
+
+        F = M.getFunction(file->second[f]);
+        assert(F && !F->isDeclaration() && "No thread function definition");
+        modified |= this->removeOldStackmaps(F);
+        assert(smids.find(file->second[f]) != smids.end() && "No ID for function");
+        smid = smids.find(file->second[f])->second;
+
+        /*
+         * Look for & instrument a generic call instruction followed by a call
+         * to an exit function, e.g.,
+         *
+         *   %call = call i32 %main(...)
+         *   call void @exit(i32 %call)
+         */
+        for(Function::iterator bb = F->begin(), be = F->end(); bb != be; bb++)
+        {
+          bool track = false;
+          for(BasicBlock::reverse_iterator i = bb->rbegin(), ie = bb->rend();
+              i != ie; i++)
+          {
+            if(isExitCall(*i)) track = true;
+            else if(track && isa<CallInst>(*i))
+            {
+              IRBuilder<> builder(i->getNextNode());
+              std::vector<Value *> args(2);
+              args[0] = ConstantInt::getSigned(Type::getInt64Ty(M.getContext()), smid);
+              args[1] = ConstantInt::getSigned(Type::getInt32Ty(M.getContext()), 0);
+              builder.CreateCall(this->SMFunc, ArrayRef<Value*>(args));
+              this->numInstrumented++;
+              break;
+            }
+          }
+        }
+      }
+
+      DEBUG(dbgs() << "LibcStackMaps: finished module " << M.getName()
+                   << ", added " << this->numInstrumented << " stackmaps\n\n";);
+    }
+
+    if(numInstrumented > 0) modified = true;
+    return modified;
+  }
+
+private:
+  /* Name of stack map intrinsic */
+  static const StringRef SMName;
+
+  /* Stack map instruction creation */
+  Function *SMFunc;
+  FunctionType *SMTy; // Used for creating function declaration
+
+  /* Files, functions & IDs */
+  static const std::map<std::string, std::vector<std::string> > funcs;
+  static const std::map<std::string, int64_t> smids;
+  static const std::vector<std::string> exitFuncs;
+
+  /**
+   * Create the function type for the stack map intrinsic.
+   */
+  void createSMType(const Module &M)
+  {
+    std::vector<Type*> params(2);
+    params[0] = Type::getInt64Ty(M.getContext());
+    params[1] = Type::getInt32Ty(M.getContext());
+    this->SMTy = FunctionType::get(Type::getVoidTy(M.getContext()),
+                                                   ArrayRef<Type*>(params),
+                                                   true);
+  }
+
+  /**
+   * Add the stackmap intrinisic's function declaration if not already present.
+   * Return true if the declaration was added, or false if it's already there.
+   */
+  bool addSMDeclaration(Module &M)
+  {
+    if(!(this->SMFunc = M.getFunction(this->SMName)))
+    {
+      DEBUG(dbgs() << "Adding stackmap function declaration to " << M.getName() << "\n");
+      this->SMFunc = cast<Function>(M.getOrInsertFunction(this->SMName, this->SMTy));
+      this->SMFunc->setCallingConv(CallingConv::C);
+      return true;
+    }
+    else return false;
+  }
+
+  /**
+   * Iterate over all instructions, removing previously found stackmaps.
+   */
+  bool removeOldStackmaps(Function *F)
+  {
+    bool modified = false;
+    CallInst* CI;
+    const Function *CurF;
+
+    DEBUG(dbgs() << "Searching for/removing old stackmaps\n";);
+
+    for(Function::iterator bb = F->begin(), bbe = F->end(); bb != bbe; bb++) {
+      for(BasicBlock::iterator i = bb->begin(), ie = bb->end(); i != ie; i++) {
+        if((CI = dyn_cast<CallInst>(&*i))) {
+          CurF = CI->getCalledFunction();
+          if(CurF && CurF->hasName() && CurF->getName() == SMName) {
+            i = i->eraseFromParent()->getPrevNode();
+            modified = true;
+          }
+        }
+      }
+    }
+
+    DEBUG(if(modified) dbgs() << "WARNING: found previous stackmaps!\n";);
+    return modified;
+  }
+
+  /**
+   * Return whether or not the instruction is a call to an exit function.
+   */
+  bool isExitCall(Instruction &I)
+  {
+    CallInst *CI;
+    Function *F;
+
+    if((CI = dyn_cast<CallInst>(&I)))
+    {
+      F = CI->getCalledFunction();
+      if(F && F->hasName())
+        for(size_t i = 0, e = exitFuncs.size(); i < e; i++)
+          if(F->getName() == exitFuncs[i]) return true;
+    }
+
+    return false;
+  }
+};
+
+} /* end anonymous namespace */
+
+char LibcStackMaps::ID = 0;
+const StringRef LibcStackMaps::SMName = "llvm.experimental.stackmap";
+
+/**
+ * Map a source code filename (minus the extension) to the names of functions
+ * inside which are to be instrumented.
+ */
+const std::map<std::string, std::vector<std::string> > LibcStackMaps::funcs = {
+  {"__libc_start_main", {"__libc_start_main"}},
+  {"pthread_create", {"start", "start_c11"}}
+};
+
+/* Map a function name to the stackmap ID representing that function. */
+const std::map<std::string, int64_t> LibcStackMaps::smids = {
+  {"__libc_start_main", UINT64_MAX},
+  {"start", UINT64_MAX - 1},
+  {"start_c11", UINT64_MAX - 2}
+};
+
+/**
+ * Thread exit function names, used to search for starting function call site
+ * to be instrumented with stackmap.
+ */
+const std::vector<std::string> LibcStackMaps::exitFuncs = {
+  "exit", "pthread_exit", "__pthread_exit"
+};
+
+INITIALIZE_PASS(LibcStackMaps, "libc-stackmaps",
+  "Instrument libc thread start functions with stack maps", false, false)
+
+namespace llvm {
+  ModulePass *createLibcStackMapsPass() { return new LibcStackMaps(); }
+}
+
Index: lib/Transforms/Instrumentation/MigrationPoints.cpp
===================================================================
--- lib/Transforms/Instrumentation/MigrationPoints.cpp	(nonexistent)
+++ lib/Transforms/Instrumentation/MigrationPoints.cpp	(working copy)
@@ -0,0 +1,952 @@
+//===- MigrationPoints.cpp ------------------------------------------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Instrument the code with migration points, which are locations where threads
+// make calls to invoke the migration process in addition to any other
+// instrumentation (e.g., hardware transactional memory, HTM, stops & starts).
+// Migration points only occur at equivalence points, or locations in the
+// program code where there is a direct mapping between architecture-specific
+// execution state like the registers and stack across different ISAs.  In our
+// implementation, every function call site is an equivalence point; hence,
+// calls inserted to invoke the migration by definition create equivalence
+// points at the migration point.  Thus, all migration points are equivalence
+// points, but not all equivalence points are migration points.
+//
+// By default, the pass only inserts migration points at the beginning and end
+// of a function.  More advanced analyses can be used to instrument function
+// bodies (in particular, loops) with more migration points.
+//
+// More details about equivalence points can be found in the paper "A Unified
+// Model of Pointwise Migration of Procedural Computations" by von Bank et. al
+// (http://dl.acm.org/citation.cfm?id=197402).
+//
+//===----------------------------------------------------------------------===//
+
+#include <map>
+#include <memory>
+#include <queue>
+#include <set>
+#include "llvm/Pass.h"
+#include "llvm/ADT/PostOrderIterator.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/StringSet.h"
+#include "llvm/ADT/Triple.h"
+#include "llvm/ADT/SCCIterator.h"
+#include "llvm/Analysis/CFG.h"
+#include "llvm/Analysis/LoopInfo.h"
+#include "llvm/Analysis/LoopIterator.h"
+#include "llvm/IR/DiagnosticInfo.h"
+#include "llvm/IR/Intrinsics.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/Module.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "migration-points"
+
+/// Insert more migration points into the body of a function.  Analyze memory
+/// usage & attempt to instrument the code to reduce the time until the thread
+/// reaches an migration point.  If HTM instrumentation is enabled, analysis is
+/// tailored to avoid hardware transactional memory (HTM) capacity aborts.
+const static cl::opt<bool>
+MoreMigPoints("more-mig-points", cl::Hidden, cl::init(false),
+  cl::desc("Add additional migration points into the body of functions"));
+
+/// Cover the application in transactional execution by inserting HTM
+/// stop/start instructions at migration points.
+const static cl::opt<bool>
+HTMExec("htm-execution", cl::NotHidden, cl::init(false),
+  cl::desc("Instrument migration points with HTM execution "
+           "(only supported on PowerPC 64-bit & x86-64)"));
+
+/// Disable wrapping libc functions which are likely to cause HTM aborts with
+/// HTM stop/start intrinsics.  Wrapping happens by default with HTM execution.
+const static cl::opt<bool>
+NoWrapLibc("htm-no-wrap-libc", cl::Hidden, cl::init(false),
+  cl::desc("Disable wrapping libc functions with HTM stop/start"));
+
+/// Disable rollback-only transactions for PowerPC.
+const static cl::opt<bool>
+NoROTPPC("htm-ppc-no-rot", cl::Hidden, cl::init(false),
+  cl::desc("Disable rollback-only transactions in HTM instrumentation "
+           "(PowerPC only)"));
+
+/// HTM memory read buffer size for tuning analysis when inserting additional
+/// migration points.
+const static cl::opt<unsigned>
+HTMReadBufSizeArg("htm-buf-read", cl::Hidden, cl::init(8),
+  cl::desc("HTM analysis tuning - HTM read buffer size, in kilobytes"),
+  cl::value_desc("size"));
+
+/// HTM memory write buffer size for tuning analysis when inserting additional
+/// migration points.
+const static cl::opt<unsigned>
+HTMWriteBufSizeArg("htm-buf-write", cl::Hidden, cl::init(8),
+  cl::desc("HTM analysis tuning - HTM write buffer size, in kilobytes"),
+  cl::value_desc("size"));
+
+#define KB 1024
+#define HTMReadBufSize (HTMReadBufSizeArg * KB)
+#define HTMWriteBufSize (HTMWriteBufSizeArg * KB)
+
+STATISTIC(NumMigPoints, "Number of migration points added");
+STATISTIC(NumHTMBegins, "Number of HTM begin intrinsics added");
+STATISTIC(NumHTMEnds, "Number of HTM end intrinsics added");
+STATISTIC(LoopsTransformed, "Number of loops transformed");
+
+namespace {
+
+/// Weight metrics.  Child classes implement for different analyses.
+class Weight {
+public:
+  virtual ~Weight() {};
+  virtual Weight *copy() const = 0;
+
+  /// Expose types of child implementations.
+  virtual bool isHTMWeight() const { return false; }
+
+  /// Analyze an instruction & update accounting.
+  virtual void analyze(const Instruction *I) = 0;
+
+  /// Return whether or not we should add an migration point.
+  virtual bool shouldAddMigPoint() const = 0;
+
+  /// Reset internal weights after finding or placing an migration point.
+  virtual void reset() = 0;
+
+  /// Merge weights of predecessors to get the maximum starting weight of a
+  /// code section being analyzed.
+  virtual void mergeMax(const Weight *RHS) = 0;
+  virtual void mergeMax(const std::unique_ptr<Weight> &RHS) = 0;
+
+  /// Scale the weight by a factor, e.g., a number of loop iterations.
+  virtual void scale(size_t factor) = 0;
+
+  /// Number of times this weight "fits" into a given resource before we need
+  /// to place an migration point.  This is used for calculating how many
+  /// iterations of a loop can be executed between migration points.
+  virtual size_t numIters() const = 0;
+
+  /// Return whether or not the weight is within some percent (0-100) of the
+  /// threshold.
+  virtual bool withinPercent(unsigned percent) const = 0;
+
+  /// Return a human-readable string describing weight information.
+  virtual std::string toString() const = 0;
+};
+
+/// Weight metrics for HTM analysis, which basically depend on the number
+/// of bytes loaded & stored.
+class HTMWeight : public Weight {
+private:
+  // The number of bytes loaded & stored, respectively
+  size_t LoadBytes, StoreBytes;
+
+public:
+  HTMWeight(size_t LoadBytes = 0, size_t StoreBytes = 0)
+    : LoadBytes(LoadBytes), StoreBytes(StoreBytes) {}
+  HTMWeight(const HTMWeight &C)
+    : LoadBytes(C.LoadBytes), StoreBytes(C.StoreBytes) {}
+  virtual Weight *copy() const { return new HTMWeight(*this); }
+
+  virtual bool isHTMWeight() const { return true; }
+
+  /// Update the number of bytes loaded & stored from memory operations.
+  virtual void analyze(const Instruction *I) {
+    // TODO more advanced analysis, e.g., register pressure heuristics?
+    // TODO do extractelement, insertelement, shufflevector, extractvalue, or
+    // insertvalue read/write memory?
+    // TODO Need to handle the following instructions/instrinsics (also see
+    // Instruction::mayLoad() / Instruction::mayStore()):
+    //   cmpxchg
+    //   atomicrmw
+    //   llvm.memcpy
+    //   llvm.memmove
+    //   llvm.memset
+    //   llvm.masked.load
+    //   llvm.masked.store
+    //   llvm.masked.gather
+    //   llvm.masked.store
+    if(isa<LoadInst>(I)) {
+      const LoadInst *LI = cast<LoadInst>(I);
+      const DataLayout &DL = I->getModule()->getDataLayout();
+      Type *Ty = LI->getPointerOperand()->getType()->getPointerElementType();
+      LoadBytes += DL.getTypeStoreSize(Ty);
+    }
+    else if(isa<StoreInst>(I)) {
+      const StoreInst *SI = cast<StoreInst>(I);
+      const DataLayout &DL = I->getModule()->getDataLayout();
+      Type *Ty = SI->getPointerOperand()->getType()->getPointerElementType();
+      StoreBytes += DL.getTypeStoreSize(Ty);
+    }
+  }
+
+  /// Return true if we think we're going to overflow the load or store
+  /// buffer, false otherwise.
+  virtual bool shouldAddMigPoint() const {
+    // TODO some tolerance threshold, i.e., load buf size +- 10%?
+    if(LoadBytes > HTMReadBufSize || StoreBytes > HTMWriteBufSize) return true;
+    else return false;
+  }
+
+  virtual void reset() { LoadBytes = StoreBytes = 0; }
+
+  /// The max value for HTM weights of predecessors is the max of potential
+  /// load and store bytes over all predecessors.
+  virtual void mergeMax(const Weight *RHS) {
+    assert(RHS->isHTMWeight() && "Cannot mix weight types");
+    const HTMWeight *W = (const HTMWeight *)RHS;
+    if(W->LoadBytes > LoadBytes) LoadBytes = W->LoadBytes;
+    if(W->StoreBytes > StoreBytes) StoreBytes = W->StoreBytes;
+  }
+
+  virtual void mergeMax(const std::unique_ptr<Weight> &RHS)
+  { mergeMax(RHS.get()); }
+
+  virtual void scale(size_t factor) {
+    LoadBytes *= factor;
+    StoreBytes *= factor;
+  }
+
+  /// The number of times this weight's load & stores could be executed without
+  /// overflowing the HTM buffers.
+  virtual size_t numIters() const {
+    size_t NumLoadIters = UINT64_MAX, NumStoreIters = UINT64_MAX;
+    if(LoadBytes) NumLoadIters = HTMReadBufSize / LoadBytes;
+    if(StoreBytes) NumStoreIters = HTMWriteBufSize / StoreBytes;
+    return NumLoadIters < NumStoreIters ? NumLoadIters : NumStoreIters;
+  }
+
+  virtual bool withinPercent(unsigned percent) const {
+    float fppercent = (float)percent / 100.0f;
+    if((float)LoadBytes > ((float)HTMReadBufSize * fppercent))
+      return true;
+    else if((float)StoreBytes > ((float)HTMWriteBufSize * fppercent))
+      return true;
+    else return false;
+  }
+
+  virtual std::string toString() const {
+    return std::to_string(LoadBytes) + " byte(s) loaded, " +
+           std::to_string(StoreBytes) + " byte(s) stored";
+  }
+};
+
+// TODO non-HTM weight implementation
+
+typedef std::unique_ptr<Weight> WeightPtr;
+
+/// MigrationPoints - insert migration points into functions, optionally adding
+/// HTM execution.
+class MigrationPoints : public FunctionPass
+{
+public:
+  static char ID;
+
+  MigrationPoints() : FunctionPass(ID) {}
+  ~MigrationPoints() {}
+
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const
+  { AU.addRequired<LoopInfoWrapperPass>(); }
+
+  virtual const char *getPassName() const
+  { return "Insert migration points"; }
+
+  virtual bool doInitialization(Module &M) {
+    bool modified = false;
+
+    // Ensure HTM is supported on this architecture if attempting to instrument
+    // with transactional execution, otherwise disable it and warn the user
+    if(HTMExec) {
+      Triple TheTriple(M.getTargetTriple());
+      Arch = TheTriple.getArch();
+
+      if(HTMBegin.find(Arch) != HTMBegin.end()) {
+        // Add HTM intrinsic declarations which are called inside functions
+        HTMBeginDecl = addIntrinsicDecl(M, HTMBegin);
+        HTMEndDecl = addIntrinsicDecl(M, HTMEnd);
+        HTMTestDecl = addIntrinsicDecl(M, HTMTest);
+        modified = true;
+      }
+      else {
+        std::string Msg("HTM instrumentation not supported for '");
+        Msg += TheTriple.getArchName();
+        Msg += "'";
+        DiagnosticInfoInlineAsm DI(Msg, DiagnosticSeverity::DS_Warning);
+        M.getContext().diagnose(DI);
+      }
+    }
+
+    return modified;
+  }
+
+  /// Insert migration points into functions
+  virtual bool runOnFunction(Function &F)
+  {
+    initializeAnalysis(F);
+    DEBUG(dbgs() << "\n********** ADD MIGRATION POINTS **********\n"
+                 << "********** Function: " << F.getName() << "\n\n");
+
+    // Mark function entry point.  Regardless if we're placing more migration
+    // points in the function, we assume that function calls are migration
+    // points in caller, so we might as well add one in the callee body.
+    DEBUG(dbgs() << "-> Marking function entry as a migration point <-\n");
+    markAsMigPoint(F.getEntryBlock().getFirstInsertionPt(), true, true);
+
+    // Some libc functions (e.g., I/O) will cause aborts from system calls.
+    // Instrument libc calls to stop & resume transactions afterwards.
+    if(DoHTMInstrumentation && !NoWrapLibc) wrapLibcWithHTM(F);
+
+    if(MoreMigPoints) analyzeFunctionBody(F);
+    else // Instrument function exit point(s)
+      for(Function::iterator BB = F.begin(), E = F.end(); BB != E; BB++)
+        if(isa<ReturnInst>(BB->getTerminator()))
+          markAsMigPoint(BB->getTerminator(), true, true);
+
+    // Finally, apply code transformations to marked functions
+    addMigrationPoints(F);
+
+    NumMigPoints += NumMigPointAdded;
+    NumHTMBegins += NumHTMBeginAdded;
+    NumHTMEnds += NumHTMEndAdded;
+    return NumMigPointAdded > 0 || NumHTMBeginAdded > 0 || NumHTMEndAdded > 0;
+  }
+
+  /// Reset all analysis.
+  virtual void initializeAnalysis(const Function &F) {
+    NumMigPointAdded = 0;
+    NumHTMBeginAdded = 0;
+    NumHTMEndAdded = 0;
+    BBWeight.clear();
+    LoopWeight.clear();
+    LoopMigPoints.clear();
+    MigPointInsts.clear();
+    HTMBeginInsts.clear();
+    HTMEndInsts.clear();
+
+    // We've checked at a global scope whether the architecture supports HTM,
+    // but we need to check whether the target-specific feature for HTM is
+    // enabled for the current function
+    if(!F.hasFnAttribute("target-features")) {
+      DoHTMInstrumentation = false;
+      return;
+    }
+
+    Attribute TargetAttr = F.getFnAttribute("target-features");
+    assert(TargetAttr.isStringAttribute() && "Invalid target features");
+    StringRef AttrVal = TargetAttr.getValueAsString();
+    size_t pos = StringRef::npos;
+
+    switch(Arch) {
+    case Triple::ppc64le: pos = AttrVal.find("+htm"); break;
+    case Triple::x86_64: pos = AttrVal.find("+rtm"); break;
+    default: break;
+    }
+    DoHTMInstrumentation = HTMExec && (pos != StringRef::npos);
+
+    DEBUG(
+      if(DoHTMInstrumentation) dbgs() << "Enabling HTM instrumentation\n";
+      else if(HTMExec) dbgs() << "Disabled HTM instrumentation, "
+                                 "no target-features support\n";
+    );
+  }
+
+private:
+  //===--------------------------------------------------------------------===//
+  // Types & fields
+  //===--------------------------------------------------------------------===//
+
+  /// Number of various types of instrumentation added to the function
+  size_t NumMigPointAdded;
+  size_t NumHTMBeginAdded;
+  size_t NumHTMEndAdded;
+
+  /// Should we instrument code with HTM execution?  Set if HTM is enabled on
+  /// the command line and if the target is supported
+  bool DoHTMInstrumentation;
+
+  /// The current architecture - used to access architecture-specific HTM calls
+  Triple::ArchType Arch;
+
+  /// Function declarations for HTM intrinsics
+  Value *HTMBeginDecl;
+  Value *HTMEndDecl;
+  Value *HTMTestDecl;
+
+  /// Per-architecture LLVM intrinsic IDs for HTM begin, HTM end, and testing
+  /// if executing transactionally
+  typedef std::map<Triple::ArchType, Intrinsic::ID> IntrinsicMap;
+  const static IntrinsicMap HTMBegin;
+  const static IntrinsicMap HTMEnd;
+  const static IntrinsicMap HTMTest;
+
+  /// libc functions which are likely to cause an HTM abort through a syscall
+  // TODO LLVM has to have a better way to detect these
+  const static StringSet<> LibcIO;
+
+  /// Weight information for basic blocks
+  struct BasicBlockWeightInfo {
+  public:
+    /// Weight of the basic block at the end of its execution.  Note that if
+    /// the block is instrumented with an migration point, the weight
+    /// information *only* captures the instructions following the migration
+    /// point (migration points "reset" the weight).
+    WeightPtr BlockWeight;
+
+    BasicBlockWeightInfo(const Weight *BlockWeight)
+      : BlockWeight(BlockWeight->copy()) {}
+    BasicBlockWeightInfo(const WeightPtr &BlockWeight)
+      : BlockWeight(BlockWeight->copy()) {}
+
+    std::string toString() const { return BlockWeight->toString(); }
+  };
+
+  /// Weight information for loops
+  struct LoopWeightInfo {
+  public:
+    /// Weight a single iteration of a loop, based on the "heaviest" path
+    /// through the loop.
+    WeightPtr IterWeight;
+
+    /// The number of iterations between consecutive migration points, e.g.,
+    /// a value of 5 means there's an migration point every 5 iterations.
+    size_t ItersPerMigPoint;
+
+    /// True if we placed or found an migration point inside the loop's body
+    bool MigPointInBody;
+
+    LoopWeightInfo(const Weight *IterWeight,
+                   size_t ItersPerMigPoint,
+                   bool MigPointInBody)
+      : IterWeight(IterWeight->copy()),
+        ItersPerMigPoint(MigPointInBody ? 1 : ItersPerMigPoint),
+        MigPointInBody(MigPointInBody) {}
+    LoopWeightInfo(const WeightPtr &IterWeight,
+                   size_t ItersPerMigPoint,
+                   bool MigPointInBody)
+      : IterWeight(IterWeight->copy()),
+        ItersPerMigPoint(MigPointInBody ? 1 : ItersPerMigPoint),
+        MigPointInBody(MigPointInBody) {}
+
+    std::string toString() const {
+      return IterWeight->toString() + ", " + std::to_string(ItersPerMigPoint) +
+             " iteration(s) per migration point";
+    }
+  };
+
+  /// Get a weight object with zero-initialized weight based on the type of
+  /// analysis being used to instrument the application
+  Weight *getZeroWeight() const {
+    if(DoHTMInstrumentation) return new HTMWeight();
+    else llvm_unreachable("Unknown weight type");
+  }
+
+  /// Weight information gathered by analyses for basic blocks & loops
+  typedef std::map<const BasicBlock *, BasicBlockWeightInfo> BlockWeightMap;
+  typedef std::map<const Loop *, LoopWeightInfo> LoopWeightMap;
+  BlockWeightMap BBWeight;
+  LoopWeightMap LoopWeight;
+
+  /// Code locations marked for instrumentation.
+  SmallPtrSet<Loop *, 16> LoopMigPoints;
+  SmallPtrSet<Instruction *, 32> MigPointInsts;
+  SmallPtrSet<Instruction *, 32> HTMBeginInsts;
+  SmallPtrSet<Instruction *, 32> HTMEndInsts;
+
+  //===--------------------------------------------------------------------===//
+  // Analysis implementation
+  //===--------------------------------------------------------------------===//
+
+  /// Return whether the call instruction is a libc I/O call
+  static inline bool isLibcIO(const Instruction *I) {
+    if(!I || !isa<CallInst>(I)) return false;
+    const Function *CalledFunc = cast<CallInst>(I)->getCalledFunction();
+    if(CalledFunc && CalledFunc->hasName())
+      return LibcIO.find(CalledFunc->getName()) != LibcIO.end();
+    return false;
+  }
+
+  /// Return whether the instruction requires HTM begin instrumentation.
+  bool shouldAddHTMBegin(Instruction *I) { return HTMBeginInsts.count(I); }
+
+  /// Return whether the instruction requires HTM end instrumentation.
+  bool shouldAddHTMEnd(Instruction *I) { return HTMEndInsts.count(I); }
+
+  /// Return whether the instruction is a migration point.
+  bool isMigrationPoint(Instruction *I) { return MigPointInsts.count(I); }
+
+  /// Mark an instruction to be instrumented with an HTM begin, directly before
+  /// the instruction
+  void markAsHTMBegin(Instruction *I) {
+    DEBUG(dbgs() << "  + Marking"; I->print(dbgs());
+          dbgs() << " as HTM begin\n");
+    HTMBeginInsts.insert(I);
+  }
+
+  /// Mark an instruction to be instrumented with an HTM end, directly before
+  /// the instruction
+  void markAsHTMEnd(Instruction *I) {
+    DEBUG(dbgs() << "  + Marking"; I->print(dbgs());
+          dbgs() << " as HTM end\n");
+    HTMEndInsts.insert(I);
+  }
+
+  /// Mark an instruction as a migration point, directly before the
+  /// instruction.  Optionally mark instruction as needing HTM start/stop
+  /// intrinsics.
+  void markAsMigPoint(Instruction *I, bool AddHTMBegin, bool AddHTMEnd) {
+    DEBUG(dbgs() << "  + Marking"; I->print(dbgs());
+          dbgs() << " as a migration point\n");
+    MigPointInsts.insert(I);
+    if(AddHTMBegin) markAsHTMBegin(I);
+    if(AddHTMEnd) markAsHTMEnd(I);
+  }
+
+  /// Search for & bookend libc functions which are likely to cause an HTM
+  /// abort with HTM stop/start intrinsics.
+  void wrapLibcWithHTM(Function &F) {
+    DEBUG(dbgs() << "\n-> Wrapping I/O functions with HTM stop/start <-\n");
+    for(Function::iterator BB = F.begin(), BE = F.end(); BB != BE; BB++) {
+      for(BasicBlock::iterator I = BB->begin(), E = BB->end(); I != E; I++) {
+        if(isLibcIO(I)) {
+          markAsHTMEnd(I);
+
+          // Search subsequent instructions for other libc calls to prevent
+          // pathological transaction stop/starts.
+          const static size_t searchSpan = 10;
+          BasicBlock::iterator NextI(I->getNextNode());
+          for(size_t rem = searchSpan; rem > 0 && NextI != E; rem--, NextI++) {
+            if(isLibcIO(NextI)) {
+              I = NextI;
+              rem = searchSpan;
+            }
+          }
+          markAsMigPoint(I->getNextNode(), true, false);
+        }
+      }
+    }
+  }
+
+  /// Get the starting weight for a basic block based on the merged max ending
+  /// weights of its predecessors.
+  Weight *getInitialWeight(const BasicBlock *BB,
+                           const LoopInfo &LI) const {
+    Weight *PredWeight = getZeroWeight();
+    const Loop *L = LI[BB];
+
+    for(auto Pred : predecessors(BB)) {
+      const Loop *PredLoop = LI[Pred];
+      if(PredLoop && PredLoop != L) {
+        // TODO rather than trying to determine if there's a migration point
+        // between the loop's header and the exit block (and hence whether we
+        // should only analyze the weight from the migration point to the
+        // exit), just assume we're doing one extra full iteration
+        assert(LoopWeight.find(PredLoop) != LoopWeight.end() &&
+               "Invalid reverse post-order traversal");
+        const LoopWeightInfo &LWI = LoopWeight.find(PredLoop)->second;
+        WeightPtr Tmp(LWI.IterWeight->copy());
+        Tmp->scale(LWI.ItersPerMigPoint + 1);
+        PredWeight->mergeMax(Tmp);
+      }
+      else {
+        assert(BBWeight.find(Pred) != BBWeight.end() &&
+               "Invalid reverse post-order traversal");
+        PredWeight->mergeMax(BBWeight.at(Pred).BlockWeight);
+      }
+    }
+
+    return PredWeight;
+  }
+
+  /// Analyze a single basic block with an initial starting weight.  Return
+  /// true if we placed (or there is an existing) migration point inside
+  /// the block.
+  bool traverseBlock(BasicBlock *BB, const Weight *Initial) {
+    bool hasMigPoint = false;
+    BBWeight.emplace(BB, BasicBlockWeightInfo(Initial));
+    WeightPtr &CurWeight = BBWeight.at(BB).BlockWeight;
+
+    DEBUG(
+      dbgs() << "\nAnalyzing basic block";
+      if(BB->hasName()) dbgs() << " '" << BB->getName() << "'";
+      dbgs() << "\n";
+    );
+
+    for(BasicBlock::iterator I = BB->begin(), E = BB->end(); I != E; I++) {
+      // Check if there is or there should be a migration point before the
+      // instruction, and if so, reset the weight.  This looks a little funky
+      // because we don't want to tamper with existing instrumentation, only
+      // add a new equivalence point w/ HTM if it's not already there.
+      if(isMigrationPoint(I)) goto reset_weight;
+      else if(CurWeight->shouldAddMigPoint()) goto mark_mig_point;
+      else goto end;
+
+mark_mig_point:
+      markAsMigPoint(I, true, true);
+reset_weight:
+      CurWeight->reset();
+      hasMigPoint = true;
+end:
+      CurWeight->analyze(I);
+    }
+
+    DEBUG(dbgs() << "  Weight: " << CurWeight->toString() << "\n");
+
+    return hasMigPoint;
+  }
+
+  bool traverseBlock(BasicBlock *BB, const WeightPtr &Initial)
+  { return traverseBlock(BB, Initial.get()); }
+
+  /// Sort loops based on nesting depth, i.e., deeper-nested loops come first
+  struct LoopNestCmp {
+    bool operator() (const Loop * const &A, const Loop * const &B)
+    { return A->getLoopDepth() > B->getLoopDepth(); }
+  };
+
+  /// Sort loops in a loop nest by their nesting depth to traverse inside-out.
+  static void sortLoopsByDepth(const std::vector<BasicBlock *> &SCC,
+                               LoopInfo &LI,
+                               std::set<Loop *, LoopNestCmp> &Nest) {
+    Loop *L;
+    std::queue<Loop *> ToVisit;
+
+    // Grab the outermost loop in the nest to bootstrap indexing
+    L = LI.getLoopFor(SCC.front());
+    assert(L && "SCC was marked as having loop but none found in LoopInfo");
+    while(L->getLoopDepth() > 1) L = L->getParentLoop();
+    Nest.insert(L);
+    ToVisit.push(L);
+
+    // Find & index loops from innermost loop outwards
+    while(!ToVisit.empty()) {
+      L = ToVisit.front();
+      ToVisit.pop();
+      for(auto CurLoop : L->getSubLoops()) {
+        Nest.insert(CurLoop);
+        ToVisit.push(CurLoop);
+      }
+    }
+  }
+
+  /// Analyze loop nests & mark locations for migration points.
+  void traverseLoopNest(const std::vector<BasicBlock *> &SCC,
+                        LoopInfo &LI) {
+    std::set<Loop *, LoopNestCmp> Nest;
+    sortLoopsByDepth(SCC, LI, Nest);
+
+    // Walk loops & mark instructions at which we want migration points
+    // TODO what about loops for which we have known numbers of iterations?
+    // TODO what about loops which can be contained in a single transaction?
+    for(auto CurLoop : Nest) {
+      DEBUG(
+        const BasicBlock *H = CurLoop->getHeader();
+        dbgs() << "\nAnalyzing loop ";
+        if(H->hasName()) dbgs() << "with header '" << H->getName() << "'";
+        dbgs() << " (depth = " << std::to_string(CurLoop->getLoopDepth())
+               << ")\n";
+      );
+
+      bool bodyHasMigPoint;
+      BasicBlock *CurBB;
+      LoopBlocksDFS DFS(CurLoop); DFS.perform(&LI);
+      LoopBlocksDFS::RPOIterator Block = DFS.beginRPO(), E = DFS.endRPO();
+      assert(Block != E && "Loop with no basic blocks");
+
+      // Mark start of loop as migration point, set loop starting weight to
+      // zero & analyze header
+      // TODO what if its an irreducible loop, i.e., > 1 header?
+      CurBB = *Block;
+      WeightPtr TmpWeight(getZeroWeight());
+      LoopMigPoints.insert(CurLoop);
+      bodyHasMigPoint = traverseBlock(CurBB, TmpWeight);
+
+      // Traverse the loop's blocks
+      for(++Block; Block != E; ++Block) {
+        CurBB = *Block;
+        if(LI[CurBB] != CurLoop) continue; // Skip blocks in nested loops
+        WeightPtr PredWeight(getInitialWeight(CurBB, LI));
+        bodyHasMigPoint |= traverseBlock(CurBB, PredWeight);
+      }
+
+      // Calculate maximum iteration weight & add loop weight information
+      SmallVector<BasicBlock *, 4> ExitBlocks;
+      CurLoop->getExitingBlocks(ExitBlocks);
+      for(auto Exit : ExitBlocks) {
+        assert(LI[Exit] == CurLoop && "exiting from sub-loop?");
+        assert(BBWeight.find(Exit) != BBWeight.end() &&
+               "No weight information for exit basic block");
+        TmpWeight->mergeMax(BBWeight.at(Exit).BlockWeight);
+      }
+
+      LoopWeight.emplace(CurLoop,
+        LoopWeightInfo(TmpWeight, TmpWeight->numIters(), bodyHasMigPoint));
+
+      DEBUG(dbgs() << "\nLoop analysis: "
+                   << LoopWeight.at(CurLoop).toString() << "\n");
+    }
+  }
+
+  /// Analyze the function's body to add migration points.
+  void analyzeFunctionBody(Function &F) {
+    LoopInfo &LI = getAnalysis<LoopInfoWrapperPass>().getLoopInfo();
+
+    // Start with loop nests, where the bulk of the instrumentation needs to
+    // occur.  This will also affect where migration points are placed in
+    // other parts of the function.
+    DEBUG(dbgs() << "\n-> Analyzing loop nests <-\n");
+    for(scc_iterator<Function *> SCC = scc_begin(&F), E = scc_end(&F);
+        SCC != E; ++SCC)
+      if(SCC.hasLoop()) traverseLoopNest(*SCC, LI);
+
+    // Analyze the rest of the function body
+    DEBUG(dbgs() << "\n-> Analyzing the rest of the function body <-\n");
+    ReversePostOrderTraversal<Function *> RPOT(&F);
+    for(auto BB = RPOT.begin(), BE = RPOT.end(); BB != BE; ++BB) {
+      if(LI[*BB]) continue; // Skip loops
+      WeightPtr PredWeight(getInitialWeight(*BB, LI));
+      traverseBlock(*BB, PredWeight);
+    }
+
+    // Finally, determine if we should add an migration point at exit block(s)
+    // TODO tune threshold
+    for(Function::iterator BB = F.begin(), E = F.end(); BB != E; BB++) {
+      if(isa<ReturnInst>(BB->getTerminator())) {
+        assert(BBWeight.find(BB) != BBWeight.end() && "Missing block weight");
+        const BasicBlockWeightInfo &BBWI = BBWeight.at(BB).BlockWeight;
+        if(BBWI.BlockWeight->withinPercent(20))
+          markAsMigPoint(BB->getTerminator(), true, true);
+      }
+    }
+  }
+
+  //===--------------------------------------------------------------------===//
+  // Instrumentation implementation
+  //===--------------------------------------------------------------------===//
+
+  /// Add a declaration for an architecture-specific intrinsic (contained in
+  /// the map).
+  Constant *addIntrinsicDecl(Module &M, const IntrinsicMap &Map) {
+    IntrinsicMap::const_iterator It = Map.find(Arch);
+    assert(It != Map.end() && "Unsupported architecture");
+    FunctionType *FuncTy = Intrinsic::getType(M.getContext(), It->second);
+    return M.getOrInsertFunction(Intrinsic::getName(It->second), FuncTy);
+  }
+
+  /// Transform a loop header so that migration points (and any concomitant
+  /// costs) are only experienced every nth iteration, based on weight metrics
+  void transformLoopHeader(Loop *L) {
+    assert(LoopWeight.find(L) != LoopWeight.end() && "No loop analysis");
+    const size_t LNum = LoopsTransformed++;
+    size_t ItersPerMigPoint = LoopWeight.find(L)->second.ItersPerMigPoint;
+    BasicBlock *Header = L->getHeader(), *NewSuccBB, *MigPointBB;
+    PHINode *IV = L->getCanonicalInductionVariable();
+    // TODO add our own induction variable?
+
+    if(ItersPerMigPoint > 1 && IV) {
+      DEBUG(
+        dbgs() << "Instrumenting loop ";
+        if(Header->hasName()) dbgs() << "header '" << Header->getName() << "' ";
+        dbgs() << "to hit migration point every "
+               << std::to_string(ItersPerMigPoint) << " iterations\n"
+      );
+
+      Type *IVType = IV->getType();
+      Function *CurF = Header->getParent();
+      LLVMContext &C = Header->getContext();
+
+      // Create new successor for all instructions after migration point
+      NewSuccBB =
+        Header->splitBasicBlock(Header->getFirstInsertionPt(),
+                                "l.postmigpoint" + std::to_string(LNum));
+
+      // Create new block for migration point
+      MigPointBB = BasicBlock::Create(C, "l.migpoint" + std::to_string(LNum),
+                                     CurF, NewSuccBB);
+      IRBuilder<> MigPointWorker(MigPointBB);
+      Instruction *Br = cast<Instruction>(MigPointWorker.CreateBr(NewSuccBB));
+      markAsMigPoint(Br, true, true);
+
+      // Add check and branch to migration point only every nth iteration
+      IRBuilder<> Worker(Header->getTerminator());
+      Constant *N = ConstantInt::get(IVType, ItersPerMigPoint, false),
+               *Zero = ConstantInt::get(IVType, 0, false);
+      Value *Rem = Worker.CreateURem(IV, N);
+      Value *Cmp = Worker.CreateICmpEQ(Rem, Zero);
+      Worker.CreateCondBr(Cmp, MigPointBB, NewSuccBB);
+      Header->getTerminator()->eraseFromParent();
+    }
+    else {
+      DEBUG(
+        dbgs() << "Instrumenting loop ";
+        if(Header->hasName()) dbgs() << "header '" << Header->getName() << "' ";
+        dbgs() << "to hit migration point every iteration";
+        if(!IV) dbgs() << " (no loop induction variable)";
+        dbgs() << "\n";
+      );
+
+      markAsMigPoint(Header->getFirstInsertionPt(), true, true);
+    }
+  }
+
+  /// Add an migration point directly before an instruction.
+  void addMigrationPoint(Instruction *I) {
+    // TODO insert flag check & migration call if flag is set
+  }
+
+  // Note: because we're only supporting 2 architectures for now, we're not
+  // going to abstract this out into the appropriate Target/* folders
+
+  /// Add a transactional execution begin intrinsic for PowerPC, optionally
+  /// with rollback-only transactions.
+  void addPowerPCHTMBegin(Instruction *I) {
+    LLVMContext &C = I->getContext();
+    IRBuilder<> Worker(I);
+    ConstantInt *ROT = ConstantInt::get(IntegerType::getInt32Ty(C),
+                                        !NoROTPPC, false);
+    Worker.CreateCall(HTMBeginDecl, ArrayRef<Value *>(ROT));
+  }
+
+  /// Add a transactional execution begin intrinsic for x86.
+  void addX86HTMBegin(Instruction *I) {
+    IRBuilder<> Worker(I);
+    Worker.CreateCall(HTMBeginDecl);
+  }
+
+  /// Add transactional execution end intrinsic for PowerPC.
+  void addPowerPCHTMEnd(Instruction *I) {
+    LLVMContext &C = I->getContext();
+    IRBuilder<> EndWorker(I);
+    ConstantInt *Zero = ConstantInt::get(IntegerType::getInt32Ty(C),
+                                         0, false);
+    EndWorker.CreateCall(HTMEndDecl, ArrayRef<Value *>(Zero));
+  }
+
+  /// Add transactional execution check & end intrinsics for x86.
+  void addX86HTMCheckAndEnd(Instruction *I) {
+    // Note: x86's HTM facility will cause a segfault if an xend instruction is
+    // called outside of a transaction, hence we need to check if we're in a
+    // transaction before actually trying to end it.
+    LLVMContext &C = I->getContext();
+    BasicBlock *CurBB = I->getParent(), *NewSuccBB, *HTMEndBB;
+    Function *CurF = CurBB->getParent();
+
+    // Create a new successor which contains all instructions after the HTM
+    // check & end
+    NewSuccBB = CurBB->splitBasicBlock(I,
+      ".htmendsucc" + std::to_string(NumMigPointAdded));
+
+    // Create an HTM end block, which ends the transaction and jumps to the
+    // new successor
+    HTMEndBB = BasicBlock::Create(C,
+      ".htmend" + std::to_string(NumMigPointAdded), CurF, NewSuccBB);
+    IRBuilder<> EndWorker(HTMEndBB);
+    EndWorker.CreateCall(HTMEndDecl);
+    EndWorker.CreateBr(NewSuccBB);
+
+    // Finally, add the HTM test & replace the unconditional branch created by
+    // splitBasicBlock() with a conditional branch to either end the
+    // transaction or continue on to the new successor
+    IRBuilder<> PredWorker(CurBB->getTerminator());
+    CallInst *HTMTestVal = PredWorker.CreateCall(HTMTestDecl);
+    ConstantInt *Zero = ConstantInt::get(IntegerType::getInt32Ty(C), 0, true);
+    Value *Cmp =
+      PredWorker.CreateICmpNE(HTMTestVal, Zero,
+                              "htmcmp" + std::to_string(NumMigPointAdded));
+    PredWorker.CreateCondBr(Cmp, HTMEndBB, NewSuccBB);
+    CurBB->getTerminator()->eraseFromParent();
+  }
+
+  /// Insert migration points & HTM instrumentation for instructions.
+  void addMigrationPoints(Function &F) {
+    DEBUG(dbgs() << "\n-> Instrumenting with migration points & HTM <-\n");
+
+    for(auto Loop : LoopMigPoints) transformLoopHeader(Loop);
+
+    for(auto I = MigPointInsts.begin(), E = MigPointInsts.end(); I != E; ++I) {
+      addMigrationPoint(*I);
+      NumMigPointAdded++;
+    }
+
+    if(DoHTMInstrumentation) {
+      // Note: add the HTM ends before begins
+      for(auto I = HTMEndInsts.begin(), E = HTMEndInsts.end(); I != E; ++I) {
+        switch(Arch) {
+        case Triple::ppc64le: addPowerPCHTMEnd(*I); break;
+        case Triple::x86_64: addX86HTMCheckAndEnd(*I); break;
+        default: llvm_unreachable("HTM -- unsupported architecture");
+        }
+        NumHTMEndAdded++;
+      }
+
+      for(auto I = HTMBeginInsts.begin(), E = HTMBeginInsts.end();
+          I != E; ++I) {
+        switch(Arch) {
+        case Triple::ppc64le: addPowerPCHTMBegin(*I); break;
+        case Triple::x86_64: addX86HTMBegin(*I); break;
+        default: llvm_unreachable("HTM -- unsupported architecture");
+        }
+        NumHTMBeginAdded++;
+      }
+    }
+  }
+};
+
+} /* end anonymous namespace */
+
+char MigrationPoints::ID = 0;
+
+const std::map<Triple::ArchType, Intrinsic::ID> MigrationPoints::HTMBegin = {
+  {Triple::x86_64, Intrinsic::x86_xbegin},
+  {Triple::ppc64le, Intrinsic::ppc_tbegin}
+};
+
+const std::map<Triple::ArchType, Intrinsic::ID> MigrationPoints::HTMEnd = {
+  {Triple::x86_64, Intrinsic::x86_xend},
+  {Triple::ppc64le, Intrinsic::ppc_tend}
+};
+
+const std::map<Triple::ArchType, Intrinsic::ID> MigrationPoints::HTMTest = {
+  {Triple::x86_64, Intrinsic::x86_xtest},
+  {Triple::ppc64le, Intrinsic::ppc_ttest}
+};
+
+INITIALIZE_PASS_BEGIN(MigrationPoints, "migration-points",
+                      "Insert migration points into functions",
+                      true, false)
+INITIALIZE_PASS_DEPENDENCY(LoopInfoWrapperPass)
+INITIALIZE_PASS_END(MigrationPoints, "migration-points",
+                    "Insert migration points into functions",
+                    true, false)
+
+const StringSet<> MigrationPoints::LibcIO = {
+  "fopen", "freopen", "fclose", "fflush", "fwide",
+  "setbuf", "setvbuf", "fread", "fwrite",
+  "fgetc", "getc", "fgets", "fputc", "putc", "fputs",
+  "getchar", "gets", "putchar", "puts", "ungetc",
+  "fgetwc", "getwc", "fgetws", "fputwc", "putwc", "fputws",
+  "getwchar", "putwchar", "ungetwc",
+  "scanf", "fscanf", "vscanf", "vfscanf",
+  "printf", "fprintf", "vprintf", "vfprintf",
+  "wscanf", "fwscanf", "vwscanf", "vfwscanf",
+  "wprintf", "fwprintf", "vwprintf", "vfwprintf",
+  "ftell", "fgetpos", "fseek", "fsetpos", "rewind",
+  "clearerr", "feof", "ferror", "perror",
+  "remove", "rename", "tmpfile", "tmpnam"
+};
+
+namespace llvm {
+  FunctionPass *createMigrationPointsPass()
+  { return new MigrationPoints(); }
+}
+
Index: lib/Transforms/Utils/CMakeLists.txt
===================================================================
--- lib/Transforms/Utils/CMakeLists.txt	(revision 277823)
+++ lib/Transforms/Utils/CMakeLists.txt	(working copy)
@@ -28,6 +28,7 @@
   Mem2Reg.cpp
   MetaRenamer.cpp
   ModuleUtils.cpp
+  NameStringLiterals.cpp
   PromoteMemoryToRegister.cpp
   SSAUpdater.cpp
   SimplifyCFG.cpp
@@ -34,6 +35,7 @@
   SimplifyIndVar.cpp
   SimplifyInstructions.cpp
   SimplifyLibCalls.cpp
+  StaticVarSections.cpp
   SymbolRewriter.cpp
   UnifyFunctionExitNodes.cpp
   Utils.cpp
Index: lib/Transforms/Utils/NameStringLiterals.cpp
===================================================================
--- lib/Transforms/Utils/NameStringLiterals.cpp	(nonexistent)
+++ lib/Transforms/Utils/NameStringLiterals.cpp	(working copy)
@@ -0,0 +1,119 @@
+#include <algorithm>
+#include <cctype>
+#include "llvm/Pass.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/GlobalVariable.h"
+#include "llvm/IR/GlobalValue.h"
+#include "llvm/IR/Module.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+#define DEBUG_TYPE "name-string-literals"
+#define CHARS_FOR_NAME 10
+
+using namespace llvm;
+
+namespace
+{
+
+/**
+ * Generate unique name for private anonymous string literals.  Uses the
+ * filename, LLVM's temporary name and (up to) the first 10 characters of the
+ * string.  Converts non-alphanumeric characters to underscores.
+ */
+std::string UniquifySymbol(const Module &M, GlobalVariable &Sym)
+{
+  std::string newName;
+  std::string::size_type loc;
+  auto filter = [](char c){ return !isalnum(c); };
+
+  newName = M.getName();
+  loc = newName.find_last_of('.');
+  newName = newName.substr(0, loc) + "_" + Sym.getName().str() + "_";
+  std::replace_if(newName.begin(), newName.end(), filter, '_');
+
+  // Check if it's a string, and if so use string content to uniquify
+  if(Sym.hasInitializer()) {
+    Constant *Initializer = Sym.getInitializer();
+    if(isa<ConstantDataSequential>(Initializer)) {
+      ConstantDataSequential *CDS = cast<ConstantDataSequential>(Initializer);
+      if(CDS->isString()) {
+        std::string data = CDS->getAsString().substr(0, CHARS_FOR_NAME);
+        std::replace_if(data.begin(), data.end(), filter, '_');
+        newName += data;
+      }
+    }
+  }
+
+  return newName;
+}
+
+/**
+ * This pass searches for anonymous read-only data for which there is no symbol
+ * and generates a symbol for the data.  This is required by the Popcorn
+ * compiler in order to align the data at link-time.
+ */
+class NameStringLiterals : public ModulePass
+{
+public:
+	static char ID;
+
+  NameStringLiterals() : ModulePass(ID) {}
+  ~NameStringLiterals() {}
+
+	/* ModulePass virtual methods */
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const { AU.setPreservesCFG(); }
+	virtual bool runOnModule(Module &M)
+  {
+    bool modified = false;
+    std::string newName;
+    Module::global_iterator gl, gle; // for global variables
+
+    DEBUG(errs() << "\n********** Begin NameStringLiterals **********\n"
+                 << "********** Module: " << M.getName() << " **********\n\n");
+
+    // Iterate over all globals and generate symbol for anonymous string
+    // literals in each module
+    for(gl = M.global_begin(), gle = M.global_end(); gl != gle; gl++) {
+      // DONT NEED TO CHANGE NAME PER-SE just change type
+      // PrivateLinkage does NOT show up in any symbol table in the object file!
+      if(gl->getLinkage() == GlobalValue::PrivateLinkage) {
+        //change Linkage
+        //FROM private unnamed_addr constant [num x i8]
+        //TO global [num x i8]
+        gl->setLinkage(GlobalValue::ExternalLinkage);
+
+        // Make the global's name unique so we don't clash when linking with
+        // other files
+        newName = UniquifySymbol(M, *gl);
+        gl->setName(newName);
+
+        // Also REMOVE unnamed_addr value
+        if(gl->hasUnnamedAddr()) {
+          gl->setUnnamedAddr(false);
+        }
+
+        modified = true;
+
+        DEBUG(errs() << "New anonymous string name: " << newName << "\n";);
+      } else {
+        DEBUG(errs() << "> " <<  *gl << ", linkage: "
+                     << gl->getLinkage() << "\n");
+      }
+    }
+  
+    return modified;
+  }
+  virtual const char *getPassName() const { return "Name string literals"; }
+};
+
+} /* end anonymous namespace */
+
+char NameStringLiterals::ID = 0;
+INITIALIZE_PASS(NameStringLiterals, "name-string-literals",
+  "Generate symbols for anonymous string literals", false, false)
+
+namespace llvm {
+  ModulePass *createNameStringLiteralsPass() { return new NameStringLiterals(); }
+}
+
Index: lib/Transforms/Utils/StaticVarSections.cpp
===================================================================
--- lib/Transforms/Utils/StaticVarSections.cpp	(nonexistent)
+++ lib/Transforms/Utils/StaticVarSections.cpp	(working copy)
@@ -0,0 +1,106 @@
+#include <algorithm>
+#include "llvm/Pass.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/GlobalVariable.h"
+#include "llvm/IR/GlobalValue.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+#define DEBUG_TYPE "static-var-sections"
+
+using namespace llvm;
+
+namespace
+{
+
+std::string UniquifySymbol(const Module &M,
+                           std::string &section,
+                           GlobalVariable &Sym)
+{
+  std::string newName;
+  auto filter = [](char c){ return !isalnum(c); };
+
+  newName = M.getName().str() + "_" + Sym.getName().str();
+  std::replace_if(newName.begin(), newName.end(), filter, '_');
+
+  return section + newName;
+}
+
+/**
+ * This pass searches for static, i.e., module-private, global variables and
+ * modifies their linkage to be in their own sections similarly to other
+ * global variables with the -fdata-sections switch.  By default, LLVM doesn't
+ * apply -fdata-sections to static global variables.
+ */
+class StaticVarSections : public ModulePass
+{
+public:
+	static char ID;
+
+	StaticVarSections() : ModulePass(ID) {}
+	~StaticVarSections() {}
+
+	/* ModulePass virtual methods */
+  virtual void getAnalysisUsage(AnalysisUsage &AU) const { AU.setPreservesCFG(); }
+	virtual bool runOnModule(Module &M)
+  {
+    bool modified = false;
+    Module::iterator it, ite;
+    Module::global_iterator gl, gle; // for global variables
+  
+    DEBUG(errs() << "\n********** Beginning StaticVarSections **********\n"
+                 << "********** Module: " << M.getName() << " **********\n\n");
+  
+    // Iterate over all static globals and place them in their own section
+    for(gl = M.global_begin(), gle = M.global_end(); gl != gle; gl++) {
+      std::string secName = ".";
+      if(gl->isThreadLocal()) secName += "t";
+  
+      if(gl->hasCommonLinkage() &&
+         gl->getName().find(".cache.") != std::string::npos) {
+        gl->setLinkage(GlobalValue::InternalLinkage);
+      }
+  
+      // InternalLinkage is specifically for STATIC variables
+      if(gl->hasInternalLinkage()) {
+        if(gl->isConstant()) {
+          //Belongs in RODATA
+          assert(!gl->isThreadLocal() && "TLS data should not be in .rodata");
+          secName += "rodata.";
+        }
+        else if(gl->getInitializer()->isZeroValue()) {
+          //Belongs in BSS
+          secName += "bss.";
+        }
+        else {
+          //Belongs in DATA
+          secName += "data.";
+        }
+
+        secName = UniquifySymbol(M, secName, *gl);
+        gl->setSection(secName);
+        modified = true;
+
+        DEBUG(errs() << *gl << " - new section: " << secName << "\n");
+      } else {
+        DEBUG(errs() << "> " <<  *gl << ", linkage: "
+                     << gl->getLinkage() << "\n");
+        continue;
+      }
+    }
+    
+    return modified;
+  }
+  virtual const char *getPassName() const { return "Static variables in separate sections"; }
+};
+
+} /* end anonymous namespace */
+
+char StaticVarSections::ID = 0;
+INITIALIZE_PASS(StaticVarSections, "static-var-sections",
+  "Put static variables into separate sections", false, false)
+
+namespace llvm {
+  ModulePass *createStaticVarSectionsPass() { return new StaticVarSections(); }
+}
+
Index: lib/Transforms/Utils/Utils.cpp
===================================================================
--- lib/Transforms/Utils/Utils.cpp	(revision 277823)
+++ lib/Transforms/Utils/Utils.cpp	(working copy)
@@ -28,7 +28,9 @@
   initializeLoopSimplifyPass(Registry);
   initializeLowerInvokePass(Registry);
   initializeLowerSwitchPass(Registry);
+  initializeNameStringLiteralsPass(Registry);
   initializePromotePassPass(Registry);
+  initializeStaticVarSectionsPass(Registry);
   initializeUnifyFunctionExitNodesPass(Registry);
   initializeInstSimplifierPass(Registry);
   initializeMetaRenamerPass(Registry);
